{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) MONAI Consortium  \n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");  \n",
    "you may not use this file except in compliance with the License.  \n",
    "You may obtain a copy of the License at  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;http://www.apache.org/licenses/LICENSE-2.0  \n",
    "Unless required by applicable law or agreed to in writing, software  \n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,  \n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  \n",
    "See the License for the specific language governing permissions and  \n",
    "limitations under the License.\n",
    "\n",
    "# Brain tumor 3D segmentation with MONAI\n",
    "\n",
    "This tutorial shows how to construct a training workflow of multi-labels segmentation task.\n",
    "\n",
    "And it contains below features:\n",
    "1. Transforms for dictionary format data.\n",
    "1. Define a new transform according to MONAI transform API.\n",
    "1. Load Nifti image with metadata, load a list of images and stack them.\n",
    "1. Randomly adjust intensity for data augmentation.\n",
    "1. Cache IO and transforms to accelerate training and validation.\n",
    "1. 3D SegResNet model, Dice loss function, Mean Dice metric for 3D segmentation task.\n",
    "1. Deterministic training for reproducibility.\n",
    "\n",
    "The dataset comes from http://medicaldecathlon.com/.  \n",
    "Target: Gliomas segmentation necrotic/active tumour and oedema  \n",
    "Modality: Multimodal multisite MRI data (FLAIR, T1w, T1gd,T2w)  \n",
    "Size: 750 4D volumes (484 Training + 266 Testing)  \n",
    "Source: BRATS 2016 and 2017 datasets.  \n",
    "Challenge: Complex and heterogeneously-located targets\n",
    "\n",
    "Below figure shows image patches with the tumor sub-regions that are annotated in the different modalities (top left) and the final labels for the whole dataset (right).\n",
    "(Figure taken from the [BraTS IEEE TMI paper](https://ieeexplore.ieee.org/document/6975210/))\n",
    "\n",
    "![image](../figures/brats_tasks.png)\n",
    "\n",
    "The image patches show from left to right:\n",
    "1. the whole tumor (yellow) visible in T2-FLAIR (Fig.A).\n",
    "1. the tumor core (red) visible in T2 (Fig.B).\n",
    "1. the enhancing tumor structures (light blue) visible in T1Gd, surrounding the cystic/necrotic components of the core (green) (Fig. C).\n",
    "1. The segmentations are combined to generate the final labels of the tumor sub-regions (Fig.D): edema (yellow), non-enhancing solid core (red), necrotic/cystic core (green), enhancing core (blue).\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Project-MONAI/tutorials/blob/main/3d_segmentation/brats_segmentation_3d.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python -c \"import monai\" || pip install -q \"monai-weekly[nibabel, tqdm]\"\n",
    "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
    "!python -c \"import onnxruntime\" || pip install -q onnxruntime\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.4.dev2427\n",
      "Numpy version: 1.22.3\n",
      "Pytorch version: 1.11.0+cu113\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: cbf90d0ddb27dc96a91385e4dd2f4eb239dea976\n",
      "MONAI __file__: /<username>/miniconda3/lib/python3.8/site-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: 5.2.1\n",
      "scikit-image version: 0.21.0\n",
      "scipy version: 1.10.1\n",
      "Pillow version: 9.1.1\n",
      "Tensorboard version: 2.9.1\n",
      "gdown version: 5.2.0\n",
      "TorchVision version: 0.12.0+cu113\n",
      "tqdm version: 4.61.2\n",
      "lmdb version: 1.6.2\n",
      "psutil version: 5.9.1\n",
      "pandas version: 1.4.2\n",
      "einops version: 0.8.1\n",
      "transformers version: 4.46.3\n",
      "mlflow version: 2.17.2\n",
      "pynrrd version: 1.1.3\n",
      "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from monai.apps import DecathlonDataset\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader, decollate_batch\n",
    "from monai.handlers.utils import from_engine\n",
    "from monai.losses import DiceLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.networks.nets import SegResNet\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    Activationsd,\n",
    "    AsDiscrete,\n",
    "    AsDiscreted,\n",
    "    Compose,\n",
    "    Invertd,\n",
    "    LoadImaged,\n",
    "    MapTransform,\n",
    "    NormalizeIntensityd,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandScaleIntensityd,\n",
    "    RandShiftIntensityd,\n",
    "    RandSpatialCropd,\n",
    "    Spacingd,\n",
    "    EnsureTyped,\n",
    "    EnsureChannelFirstd,\n",
    ")\n",
    "from monai.utils import set_determinism\n",
    "import onnxruntime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup data directory\n",
    "\n",
    "You can specify a directory with the `MONAI_DATA_DIRECTORY` environment variable.  \n",
    "This allows you to save results and reuse downloads.  \n",
    "If not specified a temporary directory will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data\n"
     ]
    }
   ],
   "source": [
    "# directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "# if directory is not None:\n",
    "#     os.makedirs(directory, exist_ok=True)\n",
    "# root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "root_dir = './data'\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set deterministic training for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_determinism(seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a new transform to convert brain tumor labels\n",
    "\n",
    "Here we convert the multi-classes labels into multi-labels segmentation task in One-Hot format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvertToMultiChannelBasedOnBratsClassesd(MapTransform):\n",
    "    \"\"\"\n",
    "    Convert labels to multi channels based on brats classes:\n",
    "    label 1 is the peritumoral edema\n",
    "    label 2 is the GD-enhancing tumor\n",
    "    label 3 is the necrotic and non-enhancing tumor core\n",
    "    The possible classes are TC (Tumor core), WT (Whole tumor)\n",
    "    and ET (Enhancing tumor).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            result = []\n",
    "            # merge label 2 and label 3 to construct TC\n",
    "            result.append(torch.logical_or(d[key] == 2, d[key] == 3))\n",
    "            # merge labels 1, 2 and 3 to construct WT\n",
    "            result.append(torch.logical_or(torch.logical_or(d[key] == 2, d[key] == 3), d[key] == 1))\n",
    "            # label 2 is ET\n",
    "            result.append(d[key] == 2)\n",
    "            d[key] = torch.stack(result, axis=0).float()\n",
    "        return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup transforms for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = Compose(\n",
    "    [\n",
    "        # load 4 Nifti images and stack them together\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=\"image\"),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "        ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            pixdim=(1.0, 1.0, 1.0),\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        RandSpatialCropd(keys=[\"image\", \"label\"], roi_size=[224, 224, 144], random_size=False),\n",
    "        RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n",
    "        RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=1),\n",
    "        RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=2),\n",
    "        NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "        RandScaleIntensityd(keys=\"image\", factors=0.1, prob=1.0),\n",
    "        RandShiftIntensityd(keys=\"image\", offsets=0.1, prob=1.0),\n",
    "    ]\n",
    ")\n",
    "val_transform = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=\"image\"),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "        ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            pixdim=(1.0, 1.0, 1.0),\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data with DecathlonDataset\n",
    "\n",
    "Here we use `DecathlonDataset` to automatically download and extract the dataset.\n",
    "It inherits MONAI `CacheDataset`, if you want to use less memory, you can set `cache_num=N` to cache N items for training and use the default args to cache all the items for validation, it depends on your memory size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 19:58:09,815 - INFO - Verified 'Task01_BrainTumour.tar', md5: 240a19d752f0d9e9101544901065d872.\n",
      "2025-05-04 19:58:09,818 - INFO - File exists: data/Task01_BrainTumour.tar, skipped downloading.\n",
      "2025-05-04 19:58:09,819 - INFO - Non-empty folder exists in data/Task01_BrainTumour, skipped extracting.\n"
     ]
    }
   ],
   "source": [
    "# here we don't cache any data in case out of memory issue\n",
    "train_ds = DecathlonDataset(\n",
    "    root_dir=root_dir,\n",
    "    task=\"Task01_BrainTumour\",\n",
    "    transform=train_transform,\n",
    "    section=\"training\",\n",
    "    download=True,\n",
    "    cache_rate=0.0,\n",
    "    num_workers=4,\n",
    ")\n",
    "train_loader = DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=4)\n",
    "val_ds = DecathlonDataset(\n",
    "    root_dir=root_dir,\n",
    "    task=\"Task01_BrainTumour\",\n",
    "    transform=val_transform,\n",
    "    section=\"validation\",\n",
    "    download=False,\n",
    "    cache_rate=0.0,\n",
    "    num_workers=4,\n",
    ")\n",
    "val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data shape and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape: torch.Size([4, 240, 240, 155])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1728x432 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label shape: torch.Size([3, 240, 240, 155])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1296x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pick one image from DecathlonDataset to visualize and check the 4 channels\n",
    "val_data_example = val_ds[2]\n",
    "print(f\"image shape: {val_data_example['image'].shape}\")\n",
    "plt.figure(\"image\", (24, 6))\n",
    "for i in range(4):\n",
    "    plt.subplot(1, 4, i + 1)\n",
    "    plt.title(f\"image channel {i}\")\n",
    "    plt.imshow(val_data_example[\"image\"][i, :, :, 60].detach().cpu(), cmap=\"gray\")\n",
    "plt.show()\n",
    "# also visualize the 3 channels label corresponding to this image\n",
    "print(f\"label shape: {val_data_example['label'].shape}\")\n",
    "plt.figure(\"label\", (18, 6))\n",
    "for i in range(3):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    plt.title(f\"label channel {i}\")\n",
    "    plt.imshow(val_data_example[\"label\"][i, :, :, 60].detach().cpu())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model, Loss, Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 100\n",
    "val_interval = 1\n",
    "VAL_AMP = True\n",
    "\n",
    "# standard PyTorch program style: create SegResNet, DiceLoss and Adam optimizer\n",
    "device = torch.device(\"cuda:0\")\n",
    "model = SegResNet(\n",
    "    blocks_down=[1, 2, 2, 4],\n",
    "    blocks_up=[1, 1, 1],\n",
    "    init_filters=16,\n",
    "    in_channels=4,\n",
    "    out_channels=3,\n",
    "    dropout_prob=0.2,\n",
    ").to(device)\n",
    "loss_function = DiceLoss(smooth_nr=0, smooth_dr=1e-5, squared_pred=True, to_onehot_y=False, sigmoid=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-4, weight_decay=1e-5)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs)\n",
    "\n",
    "dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
    "dice_metric_batch = DiceMetric(include_background=True, reduction=\"mean_batch\")\n",
    "\n",
    "post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n",
    "\n",
    "\n",
    "# define inference method\n",
    "def inference(input):\n",
    "    def _compute(input):\n",
    "        return sliding_window_inference(\n",
    "            inputs=input,\n",
    "            roi_size=(240, 240, 160),\n",
    "            sw_batch_size=1,\n",
    "            predictor=model,\n",
    "            overlap=0.5,\n",
    "        )\n",
    "\n",
    "    if VAL_AMP:\n",
    "        with torch.autocast(\"cuda\"):\n",
    "            return _compute(input)\n",
    "    else:\n",
    "        return _compute(input)\n",
    "\n",
    "\n",
    "# # use amp to accelerate training\n",
    "# scaler = torch.GradScaler(\"cuda\")\n",
    "# enable cuDNN benchmark\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute a typical PyTorch training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/388, train_loss: 0.9782, step time: 10.7611\n",
      "2/388, train_loss: 0.9804, step time: 1.5682\n",
      "3/388, train_loss: 0.9880, step time: 1.5566\n",
      "4/388, train_loss: 0.9565, step time: 1.5551\n",
      "5/388, train_loss: 0.9837, step time: 1.5568\n",
      "6/388, train_loss: 0.9386, step time: 1.5570\n",
      "7/388, train_loss: 0.9719, step time: 1.5601\n",
      "8/388, train_loss: 0.9391, step time: 1.5559\n",
      "9/388, train_loss: 0.9850, step time: 1.5514\n",
      "10/388, train_loss: 0.9426, step time: 1.5571\n",
      "11/388, train_loss: 0.8942, step time: 1.5560\n",
      "12/388, train_loss: 0.8891, step time: 1.5523\n",
      "13/388, train_loss: 0.9641, step time: 1.5550\n",
      "14/388, train_loss: 0.8992, step time: 1.5527\n",
      "15/388, train_loss: 0.9293, step time: 1.5505\n",
      "16/388, train_loss: 0.9565, step time: 1.5551\n",
      "17/388, train_loss: 0.9710, step time: 1.5551\n",
      "18/388, train_loss: 0.9358, step time: 1.5436\n",
      "19/388, train_loss: 0.9587, step time: 1.5508\n",
      "20/388, train_loss: 0.8866, step time: 1.5537\n",
      "21/388, train_loss: 0.9417, step time: 1.5656\n",
      "22/388, train_loss: 0.9378, step time: 1.5556\n",
      "23/388, train_loss: 0.9301, step time: 1.5488\n",
      "24/388, train_loss: 0.9364, step time: 1.5579\n",
      "25/388, train_loss: 0.9509, step time: 1.5538\n",
      "26/388, train_loss: 0.9509, step time: 1.5637\n",
      "27/388, train_loss: 0.9825, step time: 1.5526\n",
      "28/388, train_loss: 0.9501, step time: 1.5557\n",
      "29/388, train_loss: 0.9560, step time: 1.5520\n",
      "30/388, train_loss: 0.9362, step time: 1.5496\n",
      "31/388, train_loss: 0.9679, step time: 1.5619\n",
      "32/388, train_loss: 0.9924, step time: 1.5555\n",
      "33/388, train_loss: 0.9338, step time: 1.5561\n",
      "34/388, train_loss: 0.9585, step time: 1.5550\n",
      "35/388, train_loss: 0.9481, step time: 1.5545\n",
      "36/388, train_loss: 0.9330, step time: 1.5538\n",
      "37/388, train_loss: 0.9021, step time: 1.5512\n",
      "38/388, train_loss: 0.9795, step time: 1.5527\n",
      "39/388, train_loss: 0.8885, step time: 1.5576\n",
      "40/388, train_loss: 0.9005, step time: 1.5563\n",
      "41/388, train_loss: 0.9876, step time: 1.5494\n",
      "42/388, train_loss: 0.9527, step time: 1.5618\n",
      "43/388, train_loss: 0.9061, step time: 1.5514\n",
      "44/388, train_loss: 0.9228, step time: 1.5614\n",
      "45/388, train_loss: 0.9626, step time: 1.5558\n",
      "46/388, train_loss: 0.9389, step time: 1.5529\n",
      "47/388, train_loss: 0.9279, step time: 1.5540\n",
      "48/388, train_loss: 0.9632, step time: 1.5580\n",
      "49/388, train_loss: 0.8557, step time: 1.5576\n",
      "50/388, train_loss: 0.9882, step time: 1.5571\n",
      "51/388, train_loss: 0.9889, step time: 1.5632\n",
      "52/388, train_loss: 0.9617, step time: 1.5540\n",
      "53/388, train_loss: 0.8085, step time: 1.5539\n",
      "54/388, train_loss: 0.9435, step time: 1.5560\n",
      "55/388, train_loss: 0.9104, step time: 1.5573\n",
      "56/388, train_loss: 0.9641, step time: 1.5572\n",
      "57/388, train_loss: 0.9492, step time: 1.5583\n",
      "58/388, train_loss: 0.9447, step time: 1.5495\n",
      "59/388, train_loss: 0.9568, step time: 1.5593\n",
      "60/388, train_loss: 0.9640, step time: 1.5531\n",
      "61/388, train_loss: 0.9410, step time: 1.5528\n",
      "62/388, train_loss: 0.9046, step time: 1.5543\n",
      "63/388, train_loss: 0.8752, step time: 1.5543\n",
      "64/388, train_loss: 0.9404, step time: 1.5534\n",
      "65/388, train_loss: 0.9603, step time: 1.5595\n",
      "66/388, train_loss: 0.9597, step time: 1.5543\n",
      "67/388, train_loss: 0.8826, step time: 1.5539\n",
      "68/388, train_loss: 0.7838, step time: 1.5565\n",
      "69/388, train_loss: 0.8874, step time: 1.5526\n",
      "70/388, train_loss: 0.9051, step time: 1.5521\n",
      "71/388, train_loss: 0.9497, step time: 1.5549\n",
      "72/388, train_loss: 0.9349, step time: 1.5532\n",
      "73/388, train_loss: 0.9163, step time: 1.5528\n",
      "74/388, train_loss: 0.9293, step time: 1.5532\n",
      "75/388, train_loss: 0.9898, step time: 1.5507\n",
      "76/388, train_loss: 0.9108, step time: 1.5580\n",
      "77/388, train_loss: 0.9357, step time: 1.5565\n",
      "78/388, train_loss: 0.8876, step time: 1.5579\n",
      "79/388, train_loss: 0.8788, step time: 1.5665\n",
      "80/388, train_loss: 0.9924, step time: 1.5599\n",
      "81/388, train_loss: 0.9834, step time: 1.5548\n",
      "82/388, train_loss: 0.9061, step time: 1.5590\n",
      "83/388, train_loss: 0.9681, step time: 1.5620\n",
      "84/388, train_loss: 0.9530, step time: 1.5633\n",
      "85/388, train_loss: 0.9166, step time: 1.5557\n",
      "86/388, train_loss: 0.9522, step time: 1.5541\n",
      "87/388, train_loss: 0.9640, step time: 1.5605\n",
      "88/388, train_loss: 0.8528, step time: 1.5596\n",
      "89/388, train_loss: 0.9411, step time: 1.5582\n",
      "90/388, train_loss: 0.9957, step time: 1.5543\n",
      "91/388, train_loss: 0.9819, step time: 1.5587\n",
      "92/388, train_loss: 0.9948, step time: 1.5682\n",
      "93/388, train_loss: 0.9647, step time: 1.5512\n",
      "94/388, train_loss: 0.9139, step time: 1.5537\n",
      "95/388, train_loss: 0.9100, step time: 1.5593\n",
      "96/388, train_loss: 0.9049, step time: 1.5662\n",
      "97/388, train_loss: 0.8293, step time: 1.5529\n",
      "98/388, train_loss: 0.7686, step time: 1.5581\n",
      "99/388, train_loss: 0.9246, step time: 1.5621\n",
      "100/388, train_loss: 0.9134, step time: 1.5610\n",
      "101/388, train_loss: 0.9666, step time: 1.5552\n",
      "102/388, train_loss: 0.8986, step time: 1.5566\n",
      "103/388, train_loss: 0.9032, step time: 1.5620\n",
      "104/388, train_loss: 0.9794, step time: 1.5573\n",
      "105/388, train_loss: 0.9145, step time: 1.5576\n",
      "106/388, train_loss: 0.9717, step time: 1.5569\n",
      "107/388, train_loss: 0.9869, step time: 1.5576\n",
      "108/388, train_loss: 0.9428, step time: 1.5613\n",
      "109/388, train_loss: 0.8278, step time: 1.5519\n",
      "110/388, train_loss: 0.9241, step time: 1.5584\n",
      "111/388, train_loss: 0.9758, step time: 1.5611\n",
      "112/388, train_loss: 0.9179, step time: 1.5611\n",
      "113/388, train_loss: 0.9707, step time: 1.5578\n",
      "114/388, train_loss: 0.9019, step time: 1.5554\n",
      "115/388, train_loss: 0.9341, step time: 1.5599\n",
      "116/388, train_loss: 0.9475, step time: 1.5580\n",
      "117/388, train_loss: 0.9022, step time: 1.5549\n",
      "118/388, train_loss: 0.9437, step time: 1.5586\n",
      "119/388, train_loss: 0.9123, step time: 1.5622\n",
      "120/388, train_loss: 0.8678, step time: 1.5599\n",
      "121/388, train_loss: 0.9381, step time: 1.5558\n",
      "122/388, train_loss: 0.9620, step time: 1.5746\n",
      "123/388, train_loss: 0.9595, step time: 1.5614\n",
      "124/388, train_loss: 0.8722, step time: 1.5605\n",
      "125/388, train_loss: 0.8677, step time: 1.5575\n",
      "126/388, train_loss: 0.9741, step time: 1.5578\n",
      "127/388, train_loss: 0.9735, step time: 1.5575\n",
      "128/388, train_loss: 0.9821, step time: 1.5626\n",
      "129/388, train_loss: 0.9419, step time: 1.5577\n",
      "130/388, train_loss: 0.8941, step time: 1.5572\n",
      "131/388, train_loss: 0.9596, step time: 1.5593\n",
      "132/388, train_loss: 0.9361, step time: 1.5599\n",
      "133/388, train_loss: 0.9276, step time: 1.5578\n",
      "134/388, train_loss: 0.9590, step time: 1.5568\n",
      "135/388, train_loss: 0.9639, step time: 1.5620\n",
      "136/388, train_loss: 0.7992, step time: 1.5630\n",
      "137/388, train_loss: 0.9408, step time: 1.5533\n",
      "138/388, train_loss: 0.9585, step time: 1.5604\n",
      "139/388, train_loss: 0.8905, step time: 1.5594\n",
      "140/388, train_loss: 0.9095, step time: 1.5622\n",
      "141/388, train_loss: 0.9414, step time: 1.5630\n",
      "142/388, train_loss: 0.8802, step time: 1.5516\n",
      "143/388, train_loss: 0.8707, step time: 1.5574\n",
      "144/388, train_loss: 0.9021, step time: 1.5634\n",
      "145/388, train_loss: 0.8876, step time: 1.5577\n",
      "146/388, train_loss: 0.9697, step time: 1.5598\n",
      "147/388, train_loss: 0.9941, step time: 1.5543\n",
      "148/388, train_loss: 0.9640, step time: 1.5607\n",
      "149/388, train_loss: 0.9354, step time: 1.5578\n",
      "150/388, train_loss: 0.9720, step time: 1.5592\n",
      "151/388, train_loss: 0.9291, step time: 1.5569\n",
      "152/388, train_loss: 0.9679, step time: 1.5608\n",
      "153/388, train_loss: 0.8484, step time: 1.5608\n",
      "154/388, train_loss: 0.8826, step time: 1.5550\n",
      "155/388, train_loss: 0.8642, step time: 1.5619\n",
      "156/388, train_loss: 0.7956, step time: 1.5588\n",
      "157/388, train_loss: 0.9079, step time: 1.5536\n",
      "158/388, train_loss: 0.9095, step time: 1.5618\n",
      "159/388, train_loss: 0.9904, step time: 1.5616\n",
      "160/388, train_loss: 0.9754, step time: 1.5620\n",
      "161/388, train_loss: 0.8823, step time: 1.5543\n",
      "162/388, train_loss: 0.9674, step time: 1.5652\n",
      "163/388, train_loss: 0.9223, step time: 1.5617\n",
      "164/388, train_loss: 0.8976, step time: 1.5635\n",
      "165/388, train_loss: 0.9245, step time: 1.5600\n",
      "166/388, train_loss: 0.9659, step time: 1.5581\n",
      "167/388, train_loss: 0.9733, step time: 1.5586\n",
      "168/388, train_loss: 0.8028, step time: 1.5657\n",
      "169/388, train_loss: 0.8603, step time: 1.5517\n",
      "170/388, train_loss: 0.8473, step time: 1.5641\n",
      "171/388, train_loss: 0.9685, step time: 1.5596\n",
      "172/388, train_loss: 0.9760, step time: 1.5631\n",
      "173/388, train_loss: 0.8119, step time: 1.5502\n",
      "174/388, train_loss: 0.9786, step time: 1.5622\n",
      "175/388, train_loss: 0.9750, step time: 1.5599\n",
      "176/388, train_loss: 0.8068, step time: 1.5626\n",
      "177/388, train_loss: 0.9909, step time: 1.5555\n",
      "178/388, train_loss: 0.9471, step time: 1.5622\n",
      "179/388, train_loss: 0.9768, step time: 1.5625\n",
      "180/388, train_loss: 0.9665, step time: 1.5616\n",
      "181/388, train_loss: 0.9840, step time: 1.5519\n",
      "182/388, train_loss: 0.8722, step time: 1.5736\n",
      "183/388, train_loss: 0.9419, step time: 1.5615\n",
      "184/388, train_loss: 0.9089, step time: 1.5669\n",
      "185/388, train_loss: 0.9873, step time: 1.5563\n",
      "186/388, train_loss: 0.9640, step time: 1.5569\n",
      "187/388, train_loss: 0.8022, step time: 1.5604\n",
      "188/388, train_loss: 0.9716, step time: 1.5604\n",
      "189/388, train_loss: 0.9803, step time: 1.5615\n",
      "190/388, train_loss: 0.9234, step time: 1.5609\n",
      "191/388, train_loss: 0.9331, step time: 1.5630\n",
      "192/388, train_loss: 0.9841, step time: 1.5606\n",
      "193/388, train_loss: 0.9542, step time: 1.5613\n",
      "194/388, train_loss: 0.9521, step time: 1.5620\n",
      "195/388, train_loss: 0.8939, step time: 1.5626\n",
      "196/388, train_loss: 0.9379, step time: 1.5595\n",
      "197/388, train_loss: 0.8759, step time: 1.5603\n",
      "198/388, train_loss: 0.9089, step time: 1.5592\n",
      "199/388, train_loss: 0.9623, step time: 1.5665\n",
      "200/388, train_loss: 0.8668, step time: 1.5618\n",
      "201/388, train_loss: 0.9176, step time: 1.5589\n",
      "202/388, train_loss: 0.8968, step time: 1.5639\n",
      "203/388, train_loss: 0.8350, step time: 1.5640\n",
      "204/388, train_loss: 0.8684, step time: 1.5595\n",
      "205/388, train_loss: 0.8884, step time: 1.5633\n",
      "206/388, train_loss: 0.8463, step time: 1.5615\n",
      "207/388, train_loss: 0.9753, step time: 1.5604\n",
      "208/388, train_loss: 0.9531, step time: 1.5553\n",
      "209/388, train_loss: 0.9460, step time: 1.5621\n",
      "210/388, train_loss: 0.9459, step time: 1.5579\n",
      "211/388, train_loss: 0.9278, step time: 1.5589\n",
      "212/388, train_loss: 0.9425, step time: 1.5564\n",
      "213/388, train_loss: 0.7870, step time: 1.5589\n",
      "214/388, train_loss: 0.9563, step time: 1.5572\n",
      "215/388, train_loss: 0.9072, step time: 1.5616\n",
      "216/388, train_loss: 0.8881, step time: 1.5589\n",
      "217/388, train_loss: 0.9450, step time: 1.5583\n",
      "218/388, train_loss: 0.9031, step time: 1.5609\n",
      "219/388, train_loss: 0.7582, step time: 1.5590\n",
      "220/388, train_loss: 0.9202, step time: 1.5620\n",
      "221/388, train_loss: 0.8396, step time: 1.5611\n",
      "222/388, train_loss: 0.9771, step time: 1.5632\n",
      "223/388, train_loss: 0.8040, step time: 1.5542\n",
      "224/388, train_loss: 0.9786, step time: 1.5594\n",
      "225/388, train_loss: 0.8753, step time: 1.5611\n",
      "226/388, train_loss: 0.9690, step time: 1.5621\n",
      "227/388, train_loss: 0.8604, step time: 1.5573\n",
      "228/388, train_loss: 0.8415, step time: 1.5601\n",
      "229/388, train_loss: 0.9018, step time: 1.5606\n",
      "230/388, train_loss: 0.9920, step time: 1.5637\n",
      "231/388, train_loss: 0.9083, step time: 1.5583\n",
      "232/388, train_loss: 0.8287, step time: 1.5591\n",
      "233/388, train_loss: 0.8296, step time: 1.5604\n",
      "234/388, train_loss: 0.9132, step time: 1.5589\n",
      "235/388, train_loss: 0.9836, step time: 1.5626\n",
      "236/388, train_loss: 0.9282, step time: 1.5565\n",
      "237/388, train_loss: 0.9882, step time: 1.5585\n",
      "238/388, train_loss: 0.9794, step time: 1.5582\n",
      "239/388, train_loss: 0.8830, step time: 1.5512\n",
      "240/388, train_loss: 0.8895, step time: 1.5355\n",
      "241/388, train_loss: 0.9540, step time: 1.5628\n",
      "242/388, train_loss: 0.9489, step time: 1.5664\n",
      "243/388, train_loss: 0.7921, step time: 1.5598\n",
      "244/388, train_loss: 0.9224, step time: 1.5585\n",
      "245/388, train_loss: 0.8389, step time: 1.5627\n",
      "246/388, train_loss: 0.9424, step time: 1.5639\n",
      "247/388, train_loss: 0.8885, step time: 1.5647\n",
      "248/388, train_loss: 0.9054, step time: 1.5584\n",
      "249/388, train_loss: 0.9268, step time: 1.5606\n",
      "250/388, train_loss: 0.9200, step time: 1.5617\n",
      "251/388, train_loss: 0.7693, step time: 1.5606\n",
      "252/388, train_loss: 0.8571, step time: 1.5572\n",
      "253/388, train_loss: 0.9828, step time: 1.5611\n",
      "254/388, train_loss: 0.8588, step time: 1.5571\n",
      "255/388, train_loss: 0.9418, step time: 1.5622\n",
      "256/388, train_loss: 0.9860, step time: 1.5545\n",
      "257/388, train_loss: 0.9157, step time: 1.5590\n",
      "258/388, train_loss: 0.9504, step time: 1.5609\n",
      "259/388, train_loss: 0.7904, step time: 1.5618\n",
      "260/388, train_loss: 0.8281, step time: 1.5649\n",
      "261/388, train_loss: 0.9715, step time: 1.5637\n",
      "262/388, train_loss: 0.9758, step time: 1.5579\n",
      "263/388, train_loss: 0.9137, step time: 1.5594\n",
      "264/388, train_loss: 0.9152, step time: 1.5631\n",
      "265/388, train_loss: 0.8770, step time: 1.5611\n",
      "266/388, train_loss: 0.9213, step time: 1.5627\n",
      "267/388, train_loss: 0.9344, step time: 1.5617\n",
      "268/388, train_loss: 0.8458, step time: 1.5577\n",
      "269/388, train_loss: 0.9900, step time: 1.5564\n",
      "270/388, train_loss: 0.9106, step time: 1.5677\n",
      "271/388, train_loss: 0.9643, step time: 1.5622\n",
      "272/388, train_loss: 0.8682, step time: 1.5623\n",
      "273/388, train_loss: 0.8692, step time: 1.5594\n",
      "274/388, train_loss: 0.8607, step time: 1.5645\n",
      "275/388, train_loss: 0.9500, step time: 1.5646\n",
      "276/388, train_loss: 0.9622, step time: 1.5588\n",
      "277/388, train_loss: 0.8279, step time: 1.5610\n",
      "278/388, train_loss: 0.9482, step time: 1.5595\n",
      "279/388, train_loss: 0.8781, step time: 1.5663\n",
      "280/388, train_loss: 0.9559, step time: 1.5600\n",
      "281/388, train_loss: 0.8807, step time: 1.5618\n",
      "282/388, train_loss: 0.8763, step time: 1.5618\n",
      "283/388, train_loss: 0.9322, step time: 1.5709\n",
      "284/388, train_loss: 0.9859, step time: 1.5609\n",
      "285/388, train_loss: 0.9085, step time: 1.5602\n",
      "286/388, train_loss: 0.9424, step time: 1.5620\n",
      "287/388, train_loss: 0.9625, step time: 1.5613\n",
      "288/388, train_loss: 0.8390, step time: 1.5670\n",
      "289/388, train_loss: 0.8824, step time: 1.5622\n",
      "290/388, train_loss: 0.9193, step time: 1.5579\n",
      "291/388, train_loss: 0.9713, step time: 1.5576\n",
      "292/388, train_loss: 0.9090, step time: 1.5557\n",
      "293/388, train_loss: 0.8222, step time: 1.5600\n",
      "294/388, train_loss: 0.9345, step time: 1.5573\n",
      "295/388, train_loss: 0.9015, step time: 1.5608\n",
      "296/388, train_loss: 0.9094, step time: 1.5598\n",
      "297/388, train_loss: 0.9228, step time: 1.5610\n",
      "298/388, train_loss: 0.8817, step time: 1.5578\n",
      "299/388, train_loss: 0.8954, step time: 1.5609\n",
      "300/388, train_loss: 0.8724, step time: 1.5612\n",
      "301/388, train_loss: 0.9543, step time: 1.5619\n",
      "302/388, train_loss: 0.9279, step time: 1.5574\n",
      "303/388, train_loss: 0.9864, step time: 1.5630\n",
      "304/388, train_loss: 0.9180, step time: 1.5603\n",
      "305/388, train_loss: 0.8745, step time: 1.5615\n",
      "306/388, train_loss: 0.9664, step time: 1.5562\n",
      "307/388, train_loss: 0.7787, step time: 1.5570\n",
      "308/388, train_loss: 0.9362, step time: 1.5618\n",
      "309/388, train_loss: 0.9303, step time: 1.5619\n",
      "310/388, train_loss: 0.9218, step time: 1.5597\n",
      "311/388, train_loss: 0.9652, step time: 1.5576\n",
      "312/388, train_loss: 0.9199, step time: 1.5600\n",
      "313/388, train_loss: 0.9202, step time: 1.5562\n",
      "314/388, train_loss: 0.8690, step time: 1.5571\n",
      "315/388, train_loss: 0.9948, step time: 1.5553\n",
      "316/388, train_loss: 0.9204, step time: 1.5605\n",
      "317/388, train_loss: 0.8037, step time: 1.5600\n",
      "318/388, train_loss: 0.8128, step time: 1.5628\n",
      "319/388, train_loss: 0.9153, step time: 1.5617\n",
      "320/388, train_loss: 0.9349, step time: 1.5622\n",
      "321/388, train_loss: 0.8865, step time: 1.5594\n",
      "322/388, train_loss: 0.8338, step time: 1.5589\n",
      "323/388, train_loss: 0.9432, step time: 1.5601\n",
      "324/388, train_loss: 0.9639, step time: 1.5541\n",
      "325/388, train_loss: 0.9008, step time: 1.5597\n",
      "326/388, train_loss: 0.8996, step time: 1.5590\n",
      "327/388, train_loss: 0.9023, step time: 1.5608\n",
      "328/388, train_loss: 0.9493, step time: 1.5600\n",
      "329/388, train_loss: 0.8519, step time: 1.5616\n",
      "330/388, train_loss: 0.8894, step time: 1.5567\n",
      "331/388, train_loss: 0.9733, step time: 1.5588\n",
      "332/388, train_loss: 0.9848, step time: 1.5548\n",
      "333/388, train_loss: 0.8975, step time: 1.5595\n",
      "334/388, train_loss: 0.8462, step time: 1.5615\n",
      "335/388, train_loss: 0.9281, step time: 1.5648\n",
      "336/388, train_loss: 0.9166, step time: 1.5595\n",
      "337/388, train_loss: 0.9857, step time: 1.5634\n",
      "338/388, train_loss: 0.9417, step time: 1.5589\n",
      "339/388, train_loss: 0.9696, step time: 1.5592\n",
      "340/388, train_loss: 0.8665, step time: 1.5593\n",
      "341/388, train_loss: 0.9793, step time: 1.5617\n",
      "342/388, train_loss: 0.8269, step time: 1.5612\n",
      "343/388, train_loss: 0.8564, step time: 1.5593\n",
      "344/388, train_loss: 0.9522, step time: 1.5716\n",
      "345/388, train_loss: 0.7947, step time: 1.5610\n",
      "346/388, train_loss: 0.9207, step time: 1.5664\n",
      "347/388, train_loss: 0.8289, step time: 1.5617\n",
      "348/388, train_loss: 0.9294, step time: 1.5618\n",
      "349/388, train_loss: 0.9107, step time: 1.5598\n",
      "350/388, train_loss: 0.8676, step time: 1.5632\n",
      "351/388, train_loss: 0.9414, step time: 1.5615\n",
      "352/388, train_loss: 0.9324, step time: 1.5653\n",
      "353/388, train_loss: 0.9308, step time: 1.5596\n",
      "354/388, train_loss: 0.8956, step time: 1.5591\n",
      "355/388, train_loss: 0.8166, step time: 1.5570\n",
      "356/388, train_loss: 0.9693, step time: 1.5573\n",
      "357/388, train_loss: 0.8516, step time: 1.5621\n",
      "358/388, train_loss: 0.8969, step time: 1.5617\n",
      "359/388, train_loss: 0.8468, step time: 1.5586\n",
      "360/388, train_loss: 0.8431, step time: 1.5610\n",
      "361/388, train_loss: 0.8739, step time: 1.5593\n",
      "362/388, train_loss: 0.9210, step time: 1.5445\n",
      "363/388, train_loss: 0.9155, step time: 1.5606\n",
      "364/388, train_loss: 0.8823, step time: 1.5601\n",
      "365/388, train_loss: 0.8444, step time: 1.5591\n",
      "366/388, train_loss: 0.8960, step time: 1.5596\n",
      "367/388, train_loss: 0.9406, step time: 1.5620\n",
      "368/388, train_loss: 0.8873, step time: 1.5603\n",
      "369/388, train_loss: 0.9852, step time: 1.5609\n",
      "370/388, train_loss: 0.8318, step time: 1.5599\n",
      "371/388, train_loss: 0.9728, step time: 1.5520\n",
      "372/388, train_loss: 0.9525, step time: 1.5595\n",
      "373/388, train_loss: 0.8914, step time: 1.5538\n",
      "374/388, train_loss: 0.9460, step time: 1.5654\n",
      "375/388, train_loss: 0.9024, step time: 1.5609\n",
      "376/388, train_loss: 0.9281, step time: 1.5602\n",
      "377/388, train_loss: 0.8832, step time: 1.5607\n",
      "378/388, train_loss: 0.9886, step time: 1.5544\n",
      "379/388, train_loss: 0.8608, step time: 1.5615\n",
      "380/388, train_loss: 0.8933, step time: 1.5618\n",
      "381/388, train_loss: 0.9331, step time: 1.5591\n",
      "382/388, train_loss: 0.8513, step time: 1.5549\n",
      "383/388, train_loss: 0.9427, step time: 1.5621\n",
      "384/388, train_loss: 0.9270, step time: 1.5598\n",
      "385/388, train_loss: 0.8392, step time: 1.5594\n",
      "386/388, train_loss: 0.9919, step time: 1.5587\n",
      "387/388, train_loss: 0.7940, step time: 1.5590\n",
      "388/388, train_loss: 0.9650, step time: 1.5608\n",
      "epoch 1 average loss: 0.9190\n",
      "saved new best metric model\n",
      "current epoch: 1 current mean dice: 0.4030 tc: 0.4131 wt: 0.5935 et: 0.2023\n",
      "best mean dice: 0.4030 at epoch: 1\n",
      "time consuming of epoch 1 is: 723.0364\n",
      "----------\n",
      "epoch 2/100\n",
      "1/388, train_loss: 0.8580, step time: 1.5753\n",
      "2/388, train_loss: 0.8241, step time: 1.5515\n",
      "3/388, train_loss: 0.8424, step time: 1.5508\n",
      "4/388, train_loss: 0.8647, step time: 1.5543\n",
      "5/388, train_loss: 0.8885, step time: 1.5553\n",
      "6/388, train_loss: 0.8833, step time: 1.5548\n",
      "7/388, train_loss: 0.8514, step time: 1.5487\n",
      "8/388, train_loss: 0.7675, step time: 1.5529\n",
      "9/388, train_loss: 0.9606, step time: 1.5524\n",
      "10/388, train_loss: 0.7854, step time: 1.5512\n",
      "11/388, train_loss: 0.9432, step time: 1.5591\n",
      "12/388, train_loss: 0.8407, step time: 1.5504\n",
      "13/388, train_loss: 0.8701, step time: 1.5511\n",
      "14/388, train_loss: 0.8854, step time: 1.5500\n",
      "15/388, train_loss: 0.9001, step time: 1.5486\n",
      "16/388, train_loss: 0.9375, step time: 1.5514\n",
      "17/388, train_loss: 0.7477, step time: 1.5667\n",
      "18/388, train_loss: 0.9490, step time: 1.5499\n",
      "19/388, train_loss: 0.9696, step time: 1.5468\n",
      "20/388, train_loss: 0.9497, step time: 1.5522\n",
      "21/388, train_loss: 0.8960, step time: 1.5496\n",
      "22/388, train_loss: 0.8127, step time: 1.5636\n",
      "23/388, train_loss: 0.9073, step time: 1.5494\n",
      "24/388, train_loss: 0.9141, step time: 1.5532\n",
      "25/388, train_loss: 0.8952, step time: 1.5514\n",
      "26/388, train_loss: 0.9525, step time: 1.5545\n",
      "27/388, train_loss: 0.8162, step time: 1.5570\n",
      "28/388, train_loss: 0.9850, step time: 1.5501\n",
      "29/388, train_loss: 0.8929, step time: 1.5551\n",
      "30/388, train_loss: 0.9207, step time: 1.5521\n",
      "31/388, train_loss: 0.8486, step time: 1.5514\n",
      "32/388, train_loss: 0.7156, step time: 1.5527\n",
      "33/388, train_loss: 0.9779, step time: 1.5522\n",
      "34/388, train_loss: 0.7535, step time: 1.5513\n",
      "35/388, train_loss: 0.8687, step time: 1.5512\n",
      "36/388, train_loss: 0.9165, step time: 1.5567\n",
      "37/388, train_loss: 0.8387, step time: 1.5542\n",
      "38/388, train_loss: 0.7497, step time: 1.5533\n",
      "39/388, train_loss: 0.8790, step time: 1.5526\n",
      "40/388, train_loss: 0.8420, step time: 1.5563\n",
      "41/388, train_loss: 0.8877, step time: 1.5542\n",
      "42/388, train_loss: 0.9080, step time: 1.5530\n",
      "43/388, train_loss: 0.8816, step time: 1.5561\n",
      "44/388, train_loss: 0.9226, step time: 1.5521\n",
      "45/388, train_loss: 0.9346, step time: 1.5537\n",
      "46/388, train_loss: 0.8263, step time: 1.5576\n",
      "47/388, train_loss: 0.9663, step time: 1.5564\n",
      "48/388, train_loss: 0.9570, step time: 1.5543\n",
      "49/388, train_loss: 0.8396, step time: 1.5556\n",
      "50/388, train_loss: 0.8384, step time: 1.5522\n",
      "51/388, train_loss: 0.9136, step time: 1.5682\n",
      "52/388, train_loss: 0.9864, step time: 1.5503\n",
      "53/388, train_loss: 0.9491, step time: 1.5537\n",
      "54/388, train_loss: 0.6813, step time: 1.5566\n",
      "55/388, train_loss: 0.9697, step time: 1.5499\n",
      "56/388, train_loss: 0.8713, step time: 1.5587\n",
      "57/388, train_loss: 0.8774, step time: 1.5559\n",
      "58/388, train_loss: 0.9589, step time: 1.5494\n",
      "59/388, train_loss: 0.8479, step time: 1.5573\n",
      "60/388, train_loss: 0.7891, step time: 1.5518\n",
      "61/388, train_loss: 0.8611, step time: 1.5595\n",
      "62/388, train_loss: 0.8779, step time: 1.5569\n",
      "63/388, train_loss: 0.8843, step time: 1.5522\n",
      "64/388, train_loss: 0.9128, step time: 1.5602\n",
      "65/388, train_loss: 0.7653, step time: 1.5541\n",
      "66/388, train_loss: 0.9300, step time: 1.5487\n",
      "67/388, train_loss: 0.9329, step time: 1.5548\n",
      "68/388, train_loss: 0.8745, step time: 1.5553\n",
      "69/388, train_loss: 0.9799, step time: 1.5494\n",
      "70/388, train_loss: 0.8495, step time: 1.5584\n",
      "71/388, train_loss: 0.8871, step time: 1.5560\n",
      "72/388, train_loss: 0.8512, step time: 1.5504\n",
      "73/388, train_loss: 0.8280, step time: 1.5586\n",
      "74/388, train_loss: 0.7464, step time: 1.5514\n",
      "75/388, train_loss: 0.8917, step time: 1.5604\n",
      "76/388, train_loss: 0.8912, step time: 1.5525\n",
      "77/388, train_loss: 0.9784, step time: 1.5439\n",
      "78/388, train_loss: 0.8873, step time: 1.5630\n",
      "79/388, train_loss: 0.9215, step time: 1.5593\n",
      "80/388, train_loss: 0.8988, step time: 1.5616\n",
      "81/388, train_loss: 0.9313, step time: 1.5513\n",
      "82/388, train_loss: 0.9110, step time: 1.5509\n",
      "83/388, train_loss: 0.7902, step time: 1.5529\n",
      "84/388, train_loss: 0.8855, step time: 1.5550\n",
      "85/388, train_loss: 0.9508, step time: 1.5497\n",
      "86/388, train_loss: 0.8560, step time: 1.5497\n",
      "87/388, train_loss: 0.8736, step time: 1.5530\n",
      "88/388, train_loss: 0.9626, step time: 1.5562\n",
      "89/388, train_loss: 0.9883, step time: 1.5612\n",
      "90/388, train_loss: 0.9447, step time: 1.5584\n",
      "91/388, train_loss: 0.8303, step time: 1.5533\n",
      "92/388, train_loss: 0.9911, step time: 1.5556\n",
      "93/388, train_loss: 0.9712, step time: 1.5526\n",
      "94/388, train_loss: 0.9414, step time: 1.5489\n",
      "95/388, train_loss: 0.9806, step time: 1.5526\n",
      "96/388, train_loss: 0.9102, step time: 1.5513\n",
      "97/388, train_loss: 0.9286, step time: 1.5513\n",
      "98/388, train_loss: 0.8222, step time: 1.5540\n",
      "99/388, train_loss: 0.8866, step time: 1.5539\n",
      "100/388, train_loss: 0.8130, step time: 1.5523\n",
      "101/388, train_loss: 0.7877, step time: 1.5513\n",
      "102/388, train_loss: 0.8414, step time: 1.5584\n",
      "103/388, train_loss: 0.8546, step time: 1.5482\n",
      "104/388, train_loss: 0.9714, step time: 1.5529\n",
      "105/388, train_loss: 0.9244, step time: 1.5568\n",
      "106/388, train_loss: 0.9907, step time: 1.5510\n",
      "107/388, train_loss: 0.8022, step time: 1.5555\n",
      "108/388, train_loss: 0.9423, step time: 1.5411\n",
      "109/388, train_loss: 0.8912, step time: 1.5534\n",
      "110/388, train_loss: 0.9193, step time: 1.5540\n",
      "111/388, train_loss: 0.9591, step time: 1.5534\n",
      "112/388, train_loss: 0.7713, step time: 1.5571\n",
      "113/388, train_loss: 0.8807, step time: 1.5532\n",
      "114/388, train_loss: 0.9100, step time: 1.5523\n",
      "115/388, train_loss: 0.9110, step time: 1.5493\n",
      "116/388, train_loss: 0.9008, step time: 1.5561\n",
      "117/388, train_loss: 0.8185, step time: 1.5504\n",
      "118/388, train_loss: 0.8286, step time: 1.5539\n",
      "119/388, train_loss: 0.8871, step time: 1.5548\n",
      "120/388, train_loss: 0.7421, step time: 1.5568\n",
      "121/388, train_loss: 0.9294, step time: 1.5526\n",
      "122/388, train_loss: 0.8974, step time: 1.5522\n",
      "123/388, train_loss: 0.9040, step time: 1.5522\n",
      "124/388, train_loss: 0.8645, step time: 1.5531\n",
      "125/388, train_loss: 0.8797, step time: 1.5576\n",
      "126/388, train_loss: 0.8370, step time: 1.5518\n",
      "127/388, train_loss: 0.9620, step time: 1.5511\n",
      "128/388, train_loss: 0.8092, step time: 1.5610\n",
      "129/388, train_loss: 0.9294, step time: 1.5526\n",
      "130/388, train_loss: 0.8873, step time: 1.5525\n",
      "131/388, train_loss: 0.9013, step time: 1.5559\n",
      "132/388, train_loss: 0.9061, step time: 1.5506\n",
      "133/388, train_loss: 0.9294, step time: 1.5541\n",
      "134/388, train_loss: 0.8561, step time: 1.5539\n",
      "135/388, train_loss: 0.9002, step time: 1.5497\n",
      "136/388, train_loss: 0.9743, step time: 1.5571\n",
      "137/388, train_loss: 0.9436, step time: 1.5522\n",
      "138/388, train_loss: 0.9369, step time: 1.5612\n",
      "139/388, train_loss: 0.9167, step time: 1.5532\n",
      "140/388, train_loss: 0.9187, step time: 1.5542\n",
      "141/388, train_loss: 0.7626, step time: 1.5539\n",
      "142/388, train_loss: 0.9326, step time: 1.5596\n",
      "143/388, train_loss: 0.9830, step time: 1.5510\n",
      "144/388, train_loss: 0.8701, step time: 1.5543\n",
      "145/388, train_loss: 0.9146, step time: 1.5498\n",
      "146/388, train_loss: 0.7691, step time: 1.5498\n",
      "147/388, train_loss: 0.8445, step time: 1.5466\n",
      "148/388, train_loss: 0.8970, step time: 1.5786\n",
      "149/388, train_loss: 0.9634, step time: 1.5581\n",
      "150/388, train_loss: 0.9591, step time: 1.5550\n",
      "151/388, train_loss: 0.8926, step time: 1.5666\n",
      "152/388, train_loss: 0.8402, step time: 1.5557\n",
      "153/388, train_loss: 0.9880, step time: 1.5544\n",
      "154/388, train_loss: 0.9217, step time: 1.5547\n",
      "155/388, train_loss: 0.9869, step time: 1.5506\n",
      "156/388, train_loss: 0.9040, step time: 1.5631\n",
      "157/388, train_loss: 0.9823, step time: 1.5471\n",
      "158/388, train_loss: 0.9939, step time: 1.5555\n",
      "159/388, train_loss: 0.9223, step time: 1.5568\n",
      "160/388, train_loss: 0.9077, step time: 1.5530\n",
      "161/388, train_loss: 0.8083, step time: 1.5504\n",
      "162/388, train_loss: 0.8953, step time: 1.5507\n",
      "163/388, train_loss: 0.9316, step time: 1.5493\n",
      "164/388, train_loss: 0.9314, step time: 1.5573\n",
      "165/388, train_loss: 0.9516, step time: 1.5553\n",
      "166/388, train_loss: 0.8820, step time: 1.5511\n",
      "167/388, train_loss: 0.9803, step time: 1.5554\n",
      "168/388, train_loss: 0.8614, step time: 1.5533\n",
      "169/388, train_loss: 0.7819, step time: 1.5518\n",
      "170/388, train_loss: 0.9436, step time: 1.5771\n",
      "171/388, train_loss: 0.9050, step time: 1.5508\n",
      "172/388, train_loss: 0.7789, step time: 1.5544\n",
      "173/388, train_loss: 0.8582, step time: 1.5547\n",
      "174/388, train_loss: 0.9332, step time: 1.5516\n",
      "175/388, train_loss: 0.9688, step time: 1.5575\n",
      "176/388, train_loss: 0.9727, step time: 1.5541\n",
      "177/388, train_loss: 0.7879, step time: 1.5503\n",
      "178/388, train_loss: 0.9418, step time: 1.5579\n",
      "179/388, train_loss: 0.7592, step time: 1.5541\n",
      "180/388, train_loss: 0.9152, step time: 1.5528\n",
      "181/388, train_loss: 0.9195, step time: 1.5542\n",
      "182/388, train_loss: 0.9470, step time: 1.5540\n",
      "183/388, train_loss: 0.8807, step time: 1.5513\n",
      "184/388, train_loss: 0.9247, step time: 1.5545\n",
      "185/388, train_loss: 0.9597, step time: 1.5533\n",
      "186/388, train_loss: 0.9080, step time: 1.5501\n",
      "187/388, train_loss: 0.9430, step time: 1.5553\n",
      "188/388, train_loss: 0.9503, step time: 1.5516\n",
      "189/388, train_loss: 0.9300, step time: 1.5561\n",
      "190/388, train_loss: 0.9798, step time: 1.5542\n",
      "191/388, train_loss: 0.8338, step time: 1.5543\n",
      "192/388, train_loss: 0.9451, step time: 1.5580\n",
      "193/388, train_loss: 0.8646, step time: 1.5583\n",
      "194/388, train_loss: 0.8697, step time: 1.5524\n",
      "195/388, train_loss: 0.7144, step time: 1.5538\n",
      "196/388, train_loss: 0.8199, step time: 1.5548\n",
      "197/388, train_loss: 0.6944, step time: 1.5518\n",
      "198/388, train_loss: 0.8870, step time: 1.5569\n",
      "199/388, train_loss: 0.8308, step time: 1.5512\n",
      "200/388, train_loss: 0.8060, step time: 1.5528\n",
      "201/388, train_loss: 0.7556, step time: 1.5518\n",
      "202/388, train_loss: 0.9155, step time: 1.5523\n",
      "203/388, train_loss: 0.9702, step time: 1.5553\n",
      "204/388, train_loss: 0.9477, step time: 1.5559\n",
      "205/388, train_loss: 0.9646, step time: 1.5501\n",
      "206/388, train_loss: 0.8476, step time: 1.5525\n",
      "207/388, train_loss: 0.9040, step time: 1.5568\n",
      "208/388, train_loss: 0.7966, step time: 1.5523\n",
      "209/388, train_loss: 0.8780, step time: 1.5632\n",
      "210/388, train_loss: 0.9055, step time: 1.5495\n",
      "211/388, train_loss: 0.9043, step time: 1.5556\n",
      "212/388, train_loss: 0.8310, step time: 1.5513\n",
      "213/388, train_loss: 0.9481, step time: 1.5583\n",
      "214/388, train_loss: 0.9434, step time: 1.5498\n",
      "215/388, train_loss: 0.9333, step time: 1.5555\n",
      "216/388, train_loss: 0.9654, step time: 1.5492\n",
      "217/388, train_loss: 0.8906, step time: 1.5522\n",
      "218/388, train_loss: 0.9102, step time: 1.5520\n",
      "219/388, train_loss: 0.8607, step time: 1.5570\n",
      "220/388, train_loss: 0.9666, step time: 1.5619\n",
      "221/388, train_loss: 0.8733, step time: 1.5593\n",
      "222/388, train_loss: 0.9822, step time: 1.5497\n",
      "223/388, train_loss: 0.8868, step time: 1.5606\n",
      "224/388, train_loss: 0.8319, step time: 1.5570\n",
      "225/388, train_loss: 0.9577, step time: 1.5546\n",
      "226/388, train_loss: 0.8440, step time: 1.5523\n",
      "227/388, train_loss: 0.9799, step time: 1.5609\n",
      "228/388, train_loss: 0.8870, step time: 1.5543\n",
      "229/388, train_loss: 0.7940, step time: 1.5559\n",
      "230/388, train_loss: 0.7910, step time: 1.5516\n",
      "231/388, train_loss: 0.8388, step time: 1.5509\n",
      "232/388, train_loss: 0.8261, step time: 1.5537\n",
      "233/388, train_loss: 0.8873, step time: 1.5563\n",
      "234/388, train_loss: 0.8780, step time: 1.5559\n",
      "235/388, train_loss: 0.7600, step time: 1.5515\n",
      "236/388, train_loss: 0.8090, step time: 1.5559\n",
      "237/388, train_loss: 0.9832, step time: 1.5516\n",
      "238/388, train_loss: 0.7783, step time: 1.5485\n",
      "239/388, train_loss: 0.8389, step time: 1.5517\n",
      "240/388, train_loss: 0.8895, step time: 1.5522\n",
      "241/388, train_loss: 0.9252, step time: 1.5512\n",
      "242/388, train_loss: 0.8977, step time: 1.5557\n",
      "243/388, train_loss: 0.9076, step time: 1.5600\n",
      "244/388, train_loss: 0.8476, step time: 1.5519\n",
      "245/388, train_loss: 0.9848, step time: 1.5543\n",
      "246/388, train_loss: 0.8141, step time: 1.5574\n",
      "247/388, train_loss: 0.9017, step time: 1.5528\n",
      "248/388, train_loss: 0.9589, step time: 1.5529\n",
      "249/388, train_loss: 0.8507, step time: 1.5469\n",
      "250/388, train_loss: 0.8843, step time: 1.5575\n",
      "251/388, train_loss: 0.7896, step time: 1.5536\n",
      "252/388, train_loss: 0.9566, step time: 1.5537\n",
      "253/388, train_loss: 0.9238, step time: 1.5569\n",
      "254/388, train_loss: 0.9032, step time: 1.5584\n",
      "255/388, train_loss: 0.7731, step time: 1.5521\n",
      "256/388, train_loss: 0.7641, step time: 1.5581\n",
      "257/388, train_loss: 0.9237, step time: 1.5514\n",
      "258/388, train_loss: 0.9296, step time: 1.5564\n",
      "259/388, train_loss: 0.9280, step time: 1.5504\n",
      "260/388, train_loss: 0.8843, step time: 1.5530\n",
      "261/388, train_loss: 0.7869, step time: 1.5543\n",
      "262/388, train_loss: 0.8806, step time: 1.5567\n",
      "263/388, train_loss: 0.9331, step time: 1.5535\n",
      "264/388, train_loss: 0.8291, step time: 1.5562\n",
      "265/388, train_loss: 0.9173, step time: 1.5572\n",
      "266/388, train_loss: 0.9104, step time: 1.5612\n",
      "267/388, train_loss: 0.8669, step time: 1.5530\n",
      "268/388, train_loss: 0.8306, step time: 1.5529\n",
      "269/388, train_loss: 0.8873, step time: 1.5540\n",
      "270/388, train_loss: 0.8092, step time: 1.5556\n",
      "271/388, train_loss: 0.8017, step time: 1.5561\n",
      "272/388, train_loss: 0.7494, step time: 1.5558\n",
      "273/388, train_loss: 0.8791, step time: 1.5592\n",
      "274/388, train_loss: 0.7666, step time: 1.5544\n",
      "275/388, train_loss: 0.7677, step time: 1.5552\n",
      "276/388, train_loss: 0.9279, step time: 1.5523\n",
      "277/388, train_loss: 0.8934, step time: 1.5539\n",
      "278/388, train_loss: 0.8405, step time: 1.5529\n",
      "279/388, train_loss: 0.8542, step time: 1.5522\n",
      "280/388, train_loss: 0.7770, step time: 1.5517\n",
      "281/388, train_loss: 0.9781, step time: 1.5535\n",
      "282/388, train_loss: 0.9856, step time: 1.5524\n",
      "283/388, train_loss: 0.8396, step time: 1.5521\n",
      "284/388, train_loss: 0.8534, step time: 1.5756\n",
      "285/388, train_loss: 0.6648, step time: 1.5567\n",
      "286/388, train_loss: 0.8438, step time: 1.5551\n",
      "287/388, train_loss: 0.8381, step time: 1.5496\n",
      "288/388, train_loss: 0.8902, step time: 1.5479\n",
      "289/388, train_loss: 0.8256, step time: 1.5545\n",
      "290/388, train_loss: 0.8032, step time: 1.5519\n",
      "291/388, train_loss: 0.7159, step time: 1.5595\n",
      "292/388, train_loss: 0.8407, step time: 1.5540\n",
      "293/388, train_loss: 0.8239, step time: 1.5520\n",
      "294/388, train_loss: 0.9532, step time: 1.5571\n",
      "295/388, train_loss: 0.8830, step time: 1.5525\n",
      "296/388, train_loss: 0.7980, step time: 1.5579\n",
      "297/388, train_loss: 0.9570, step time: 1.5528\n",
      "298/388, train_loss: 0.8241, step time: 1.5537\n",
      "299/388, train_loss: 0.9252, step time: 1.5574\n",
      "300/388, train_loss: 0.9201, step time: 1.5525\n",
      "301/388, train_loss: 0.8278, step time: 1.5553\n",
      "302/388, train_loss: 0.8420, step time: 1.5532\n",
      "303/388, train_loss: 0.6814, step time: 1.5502\n",
      "304/388, train_loss: 0.9419, step time: 1.5571\n",
      "305/388, train_loss: 0.8768, step time: 1.5525\n",
      "306/388, train_loss: 0.7344, step time: 1.5566\n",
      "307/388, train_loss: 0.8786, step time: 1.5461\n",
      "308/388, train_loss: 0.9223, step time: 1.5539\n",
      "309/388, train_loss: 0.7595, step time: 1.5540\n",
      "310/388, train_loss: 0.7160, step time: 1.5512\n",
      "311/388, train_loss: 0.8561, step time: 1.5512\n",
      "312/388, train_loss: 0.7441, step time: 1.5578\n",
      "313/388, train_loss: 0.8740, step time: 1.5513\n",
      "314/388, train_loss: 0.8396, step time: 1.5560\n",
      "315/388, train_loss: 0.9525, step time: 1.5559\n",
      "316/388, train_loss: 0.8157, step time: 1.5590\n",
      "317/388, train_loss: 0.9544, step time: 1.5594\n",
      "318/388, train_loss: 0.9122, step time: 1.5505\n",
      "319/388, train_loss: 0.9407, step time: 1.5545\n",
      "320/388, train_loss: 0.7232, step time: 1.5509\n",
      "321/388, train_loss: 0.8774, step time: 1.5514\n",
      "322/388, train_loss: 0.9294, step time: 1.5516\n",
      "323/388, train_loss: 0.8191, step time: 1.5536\n",
      "324/388, train_loss: 0.8524, step time: 1.5582\n",
      "325/388, train_loss: 0.7458, step time: 1.5614\n",
      "326/388, train_loss: 0.9208, step time: 1.5487\n",
      "327/388, train_loss: 0.9517, step time: 1.5554\n",
      "328/388, train_loss: 0.9628, step time: 1.5595\n",
      "329/388, train_loss: 0.8295, step time: 1.5548\n",
      "330/388, train_loss: 0.9782, step time: 1.5521\n",
      "331/388, train_loss: 0.9155, step time: 1.5539\n",
      "332/388, train_loss: 0.6386, step time: 1.5485\n",
      "333/388, train_loss: 0.8852, step time: 1.5518\n",
      "334/388, train_loss: 0.7448, step time: 1.5543\n",
      "335/388, train_loss: 0.8909, step time: 1.5574\n",
      "336/388, train_loss: 0.9767, step time: 1.5538\n",
      "337/388, train_loss: 0.8486, step time: 1.5557\n",
      "338/388, train_loss: 0.8576, step time: 1.5545\n",
      "339/388, train_loss: 0.9912, step time: 1.5504\n",
      "340/388, train_loss: 0.9703, step time: 1.5538\n",
      "341/388, train_loss: 0.8624, step time: 1.5544\n",
      "342/388, train_loss: 0.9353, step time: 1.5614\n",
      "343/388, train_loss: 0.9593, step time: 1.5555\n",
      "344/388, train_loss: 0.8834, step time: 1.5514\n",
      "345/388, train_loss: 0.7838, step time: 1.5544\n",
      "346/388, train_loss: 0.7070, step time: 1.5578\n",
      "347/388, train_loss: 0.8530, step time: 1.5478\n",
      "348/388, train_loss: 0.9681, step time: 1.5568\n",
      "349/388, train_loss: 0.9184, step time: 1.5532\n",
      "350/388, train_loss: 0.7177, step time: 1.5517\n",
      "351/388, train_loss: 0.6043, step time: 1.5550\n",
      "352/388, train_loss: 0.9509, step time: 1.5552\n",
      "353/388, train_loss: 0.9733, step time: 1.5531\n",
      "354/388, train_loss: 0.7745, step time: 1.5536\n",
      "355/388, train_loss: 0.8614, step time: 1.5554\n",
      "356/388, train_loss: 0.9571, step time: 1.5504\n",
      "357/388, train_loss: 0.5963, step time: 1.5640\n",
      "358/388, train_loss: 0.7783, step time: 1.5549\n",
      "359/388, train_loss: 0.7966, step time: 1.5539\n",
      "360/388, train_loss: 0.9365, step time: 1.5539\n",
      "361/388, train_loss: 0.9346, step time: 1.5490\n",
      "362/388, train_loss: 0.9429, step time: 1.5544\n",
      "363/388, train_loss: 0.7784, step time: 1.5536\n",
      "364/388, train_loss: 0.8837, step time: 1.5533\n",
      "365/388, train_loss: 0.9032, step time: 1.5532\n",
      "366/388, train_loss: 0.7142, step time: 1.5555\n",
      "367/388, train_loss: 0.7917, step time: 1.5552\n",
      "368/388, train_loss: 0.9005, step time: 1.5547\n",
      "369/388, train_loss: 0.8570, step time: 1.5530\n",
      "370/388, train_loss: 0.8151, step time: 1.5545\n",
      "371/388, train_loss: 0.6188, step time: 1.5539\n",
      "372/388, train_loss: 0.8804, step time: 1.5502\n",
      "373/388, train_loss: 0.9484, step time: 1.5575\n",
      "374/388, train_loss: 0.8087, step time: 1.5544\n",
      "375/388, train_loss: 0.9202, step time: 1.5543\n",
      "376/388, train_loss: 0.9876, step time: 1.5586\n",
      "377/388, train_loss: 0.8107, step time: 1.5541\n",
      "378/388, train_loss: 0.8230, step time: 1.5520\n",
      "379/388, train_loss: 0.8002, step time: 1.5564\n",
      "380/388, train_loss: 0.9337, step time: 1.5539\n",
      "381/388, train_loss: 0.8755, step time: 1.5584\n",
      "382/388, train_loss: 0.8619, step time: 1.5559\n",
      "383/388, train_loss: 0.9383, step time: 1.5513\n",
      "384/388, train_loss: 0.8997, step time: 1.5605\n",
      "385/388, train_loss: 0.7691, step time: 1.5489\n",
      "386/388, train_loss: 0.9364, step time: 1.5509\n",
      "387/388, train_loss: 0.7708, step time: 1.5535\n",
      "388/388, train_loss: 0.7978, step time: 1.5537\n",
      "epoch 2 average loss: 0.8764\n",
      "saved new best metric model\n",
      "current epoch: 2 current mean dice: 0.4102 tc: 0.4398 wt: 0.5738 et: 0.2171\n",
      "best mean dice: 0.4102 at epoch: 2\n",
      "time consuming of epoch 2 is: 710.7567\n",
      "----------\n",
      "epoch 3/100\n",
      "1/388, train_loss: 0.8813, step time: 1.5659\n",
      "2/388, train_loss: 0.8516, step time: 1.5553\n",
      "3/388, train_loss: 0.6861, step time: 1.5531\n",
      "4/388, train_loss: 0.7940, step time: 1.5564\n",
      "5/388, train_loss: 0.7094, step time: 1.5553\n",
      "6/388, train_loss: 0.9106, step time: 1.5532\n",
      "7/388, train_loss: 0.6766, step time: 1.5519\n",
      "8/388, train_loss: 0.9107, step time: 1.5519\n",
      "9/388, train_loss: 0.9437, step time: 1.5561\n",
      "10/388, train_loss: 0.8761, step time: 1.5561\n",
      "11/388, train_loss: 0.7493, step time: 1.5544\n",
      "12/388, train_loss: 0.7504, step time: 1.5550\n",
      "13/388, train_loss: 0.8639, step time: 1.5571\n",
      "14/388, train_loss: 0.9147, step time: 1.5586\n",
      "15/388, train_loss: 0.8295, step time: 1.5517\n",
      "16/388, train_loss: 0.8162, step time: 1.5559\n",
      "17/388, train_loss: 0.8932, step time: 1.5562\n",
      "18/388, train_loss: 0.7075, step time: 1.5629\n",
      "19/388, train_loss: 0.6491, step time: 1.5577\n",
      "20/388, train_loss: 0.6377, step time: 1.5488\n",
      "21/388, train_loss: 0.9060, step time: 1.5549\n",
      "22/388, train_loss: 0.8192, step time: 1.5549\n",
      "23/388, train_loss: 0.9426, step time: 1.5482\n",
      "24/388, train_loss: 0.8283, step time: 1.5541\n",
      "25/388, train_loss: 0.8018, step time: 1.5566\n",
      "26/388, train_loss: 0.9229, step time: 1.5531\n",
      "27/388, train_loss: 0.9440, step time: 1.5536\n",
      "28/388, train_loss: 0.6002, step time: 1.5535\n",
      "29/388, train_loss: 0.8283, step time: 1.5502\n",
      "30/388, train_loss: 0.8633, step time: 1.5531\n",
      "31/388, train_loss: 0.7834, step time: 1.5544\n",
      "32/388, train_loss: 0.9818, step time: 1.5516\n",
      "33/388, train_loss: 0.9382, step time: 1.5568\n",
      "34/388, train_loss: 0.9374, step time: 1.5526\n",
      "35/388, train_loss: 0.8259, step time: 1.5510\n",
      "36/388, train_loss: 0.9867, step time: 1.5570\n",
      "37/388, train_loss: 0.8947, step time: 1.5550\n",
      "38/388, train_loss: 0.8133, step time: 1.5511\n",
      "39/388, train_loss: 0.9722, step time: 1.5548\n",
      "40/388, train_loss: 0.9873, step time: 1.5543\n",
      "41/388, train_loss: 0.8086, step time: 1.5531\n",
      "42/388, train_loss: 0.7915, step time: 1.5523\n",
      "43/388, train_loss: 0.8298, step time: 1.5520\n",
      "44/388, train_loss: 0.8101, step time: 1.5509\n",
      "45/388, train_loss: 0.9316, step time: 1.5552\n",
      "46/388, train_loss: 0.9068, step time: 1.5534\n",
      "47/388, train_loss: 0.7550, step time: 1.5518\n",
      "48/388, train_loss: 0.7397, step time: 1.5535\n",
      "49/388, train_loss: 0.9023, step time: 1.5559\n",
      "50/388, train_loss: 0.6668, step time: 1.5539\n",
      "51/388, train_loss: 0.7403, step time: 1.5561\n",
      "52/388, train_loss: 0.8055, step time: 1.5511\n",
      "53/388, train_loss: 0.6462, step time: 1.5532\n",
      "54/388, train_loss: 0.8918, step time: 1.5564\n",
      "55/388, train_loss: 0.8500, step time: 1.5508\n",
      "56/388, train_loss: 0.7048, step time: 1.5521\n",
      "57/388, train_loss: 0.8318, step time: 1.5606\n",
      "58/388, train_loss: 0.8957, step time: 1.5515\n",
      "59/388, train_loss: 0.8053, step time: 1.5552\n",
      "60/388, train_loss: 0.8428, step time: 1.5539\n",
      "61/388, train_loss: 0.9267, step time: 1.5508\n",
      "62/388, train_loss: 0.8221, step time: 1.5538\n",
      "63/388, train_loss: 0.7340, step time: 1.5534\n",
      "64/388, train_loss: 0.8889, step time: 1.5508\n",
      "65/388, train_loss: 0.6880, step time: 1.5579\n",
      "66/388, train_loss: 0.6690, step time: 1.5506\n",
      "67/388, train_loss: 0.9476, step time: 1.5547\n",
      "68/388, train_loss: 0.8944, step time: 1.5542\n",
      "69/388, train_loss: 0.9711, step time: 1.5531\n",
      "70/388, train_loss: 0.6970, step time: 1.5513\n",
      "71/388, train_loss: 0.8559, step time: 1.5554\n",
      "72/388, train_loss: 0.7476, step time: 1.5494\n",
      "73/388, train_loss: 0.7819, step time: 1.5518\n",
      "74/388, train_loss: 0.8059, step time: 1.5530\n",
      "75/388, train_loss: 0.5844, step time: 1.5530\n",
      "76/388, train_loss: 0.9705, step time: 1.5519\n",
      "77/388, train_loss: 0.8528, step time: 1.5628\n",
      "78/388, train_loss: 0.8117, step time: 1.5516\n",
      "79/388, train_loss: 0.8334, step time: 1.5513\n",
      "80/388, train_loss: 0.8663, step time: 1.5505\n",
      "81/388, train_loss: 0.9698, step time: 1.5510\n",
      "82/388, train_loss: 0.7873, step time: 1.5548\n",
      "83/388, train_loss: 0.9382, step time: 1.5533\n",
      "84/388, train_loss: 0.7904, step time: 1.5546\n",
      "85/388, train_loss: 0.9081, step time: 1.5493\n",
      "86/388, train_loss: 0.8138, step time: 1.5551\n",
      "87/388, train_loss: 0.8571, step time: 1.5538\n",
      "88/388, train_loss: 0.7447, step time: 1.5517\n",
      "89/388, train_loss: 0.8896, step time: 1.5558\n",
      "90/388, train_loss: 0.8360, step time: 1.5571\n",
      "91/388, train_loss: 0.7951, step time: 1.5540\n",
      "92/388, train_loss: 0.9132, step time: 1.5555\n",
      "93/388, train_loss: 0.8149, step time: 1.5499\n",
      "94/388, train_loss: 0.8034, step time: 1.5567\n",
      "95/388, train_loss: 0.9886, step time: 1.5555\n",
      "96/388, train_loss: 0.7371, step time: 1.5496\n",
      "97/388, train_loss: 0.7420, step time: 1.5631\n",
      "98/388, train_loss: 0.8651, step time: 1.5666\n",
      "99/388, train_loss: 0.7652, step time: 1.5571\n",
      "100/388, train_loss: 0.7998, step time: 1.5553\n",
      "101/388, train_loss: 0.7920, step time: 1.5542\n",
      "102/388, train_loss: 0.8541, step time: 1.5547\n",
      "103/388, train_loss: 0.7665, step time: 1.5521\n",
      "104/388, train_loss: 0.6624, step time: 1.5511\n",
      "105/388, train_loss: 0.7143, step time: 1.5537\n",
      "106/388, train_loss: 0.7818, step time: 1.5557\n",
      "107/388, train_loss: 0.8320, step time: 1.5523\n",
      "108/388, train_loss: 0.8192, step time: 1.5555\n",
      "109/388, train_loss: 0.6540, step time: 1.5522\n",
      "110/388, train_loss: 0.9841, step time: 1.5547\n",
      "111/388, train_loss: 0.9771, step time: 1.5675\n",
      "112/388, train_loss: 0.7689, step time: 1.5521\n",
      "113/388, train_loss: 0.9484, step time: 1.5563\n",
      "114/388, train_loss: 0.8832, step time: 1.5538\n",
      "115/388, train_loss: 0.8893, step time: 1.5509\n",
      "116/388, train_loss: 0.8525, step time: 1.5513\n",
      "117/388, train_loss: 0.7362, step time: 1.5511\n",
      "118/388, train_loss: 0.7637, step time: 1.5491\n",
      "119/388, train_loss: 0.7968, step time: 1.5544\n",
      "120/388, train_loss: 0.9110, step time: 1.5588\n",
      "121/388, train_loss: 0.6840, step time: 1.5679\n",
      "122/388, train_loss: 0.8714, step time: 1.5540\n",
      "123/388, train_loss: 0.5866, step time: 1.5487\n",
      "124/388, train_loss: 0.9196, step time: 1.5576\n",
      "125/388, train_loss: 0.9395, step time: 1.5828\n",
      "126/388, train_loss: 0.5333, step time: 1.5523\n",
      "127/388, train_loss: 0.9499, step time: 1.5551\n",
      "128/388, train_loss: 0.8586, step time: 1.5544\n",
      "129/388, train_loss: 0.6649, step time: 1.5576\n",
      "130/388, train_loss: 0.8117, step time: 1.5489\n",
      "131/388, train_loss: 0.6928, step time: 1.5536\n",
      "132/388, train_loss: 0.9627, step time: 1.5573\n",
      "133/388, train_loss: 0.8017, step time: 1.5507\n",
      "134/388, train_loss: 0.7463, step time: 1.5517\n",
      "135/388, train_loss: 0.8362, step time: 1.5573\n",
      "136/388, train_loss: 0.8057, step time: 1.5525\n",
      "137/388, train_loss: 0.7683, step time: 1.5558\n",
      "138/388, train_loss: 0.8248, step time: 1.5557\n",
      "139/388, train_loss: 0.8838, step time: 1.5525\n",
      "140/388, train_loss: 0.8029, step time: 1.5560\n",
      "141/388, train_loss: 0.8222, step time: 1.5488\n",
      "142/388, train_loss: 0.7050, step time: 1.5557\n",
      "143/388, train_loss: 0.8741, step time: 1.5557\n",
      "144/388, train_loss: 0.7389, step time: 1.5528\n",
      "145/388, train_loss: 0.8299, step time: 1.5547\n",
      "146/388, train_loss: 0.5175, step time: 1.5536\n",
      "147/388, train_loss: 0.7810, step time: 1.5486\n",
      "148/388, train_loss: 0.8487, step time: 1.5538\n",
      "149/388, train_loss: 0.5611, step time: 1.5500\n",
      "150/388, train_loss: 0.6982, step time: 1.5527\n",
      "151/388, train_loss: 0.9670, step time: 1.5537\n",
      "152/388, train_loss: 0.7614, step time: 1.5543\n",
      "153/388, train_loss: 0.7912, step time: 1.5531\n",
      "154/388, train_loss: 0.8945, step time: 1.5511\n",
      "155/388, train_loss: 0.8858, step time: 1.5571\n",
      "156/388, train_loss: 0.7494, step time: 1.5522\n",
      "157/388, train_loss: 0.7364, step time: 1.5546\n",
      "158/388, train_loss: 0.6577, step time: 1.5598\n",
      "159/388, train_loss: 0.9335, step time: 1.5506\n",
      "160/388, train_loss: 0.8593, step time: 1.5539\n",
      "161/388, train_loss: 0.8790, step time: 1.5572\n",
      "162/388, train_loss: 0.9294, step time: 1.5527\n",
      "163/388, train_loss: 0.8309, step time: 1.5569\n",
      "164/388, train_loss: 0.9162, step time: 1.5557\n",
      "165/388, train_loss: 0.9139, step time: 1.5536\n",
      "166/388, train_loss: 0.8476, step time: 1.5537\n",
      "167/388, train_loss: 0.8210, step time: 1.5511\n",
      "168/388, train_loss: 0.9482, step time: 1.5549\n",
      "169/388, train_loss: 0.7758, step time: 1.5561\n",
      "170/388, train_loss: 0.9067, step time: 1.5499\n",
      "171/388, train_loss: 0.7524, step time: 1.5568\n",
      "172/388, train_loss: 0.7721, step time: 1.5536\n",
      "173/388, train_loss: 0.7292, step time: 1.5499\n",
      "174/388, train_loss: 0.8388, step time: 1.5557\n",
      "175/388, train_loss: 0.6834, step time: 1.5555\n",
      "176/388, train_loss: 0.7702, step time: 1.5505\n",
      "177/388, train_loss: 0.6478, step time: 1.5573\n",
      "178/388, train_loss: 0.9127, step time: 1.5568\n",
      "179/388, train_loss: 0.8011, step time: 1.5516\n",
      "180/388, train_loss: 0.8028, step time: 1.5562\n",
      "181/388, train_loss: 0.9249, step time: 1.5553\n",
      "182/388, train_loss: 0.8502, step time: 1.5589\n",
      "183/388, train_loss: 0.7074, step time: 1.5583\n",
      "184/388, train_loss: 0.8715, step time: 1.5522\n",
      "185/388, train_loss: 0.7690, step time: 1.5563\n",
      "186/388, train_loss: 0.9605, step time: 1.5563\n",
      "187/388, train_loss: 0.9480, step time: 1.5518\n",
      "188/388, train_loss: 0.7753, step time: 1.5550\n",
      "189/388, train_loss: 0.7996, step time: 1.5519\n",
      "190/388, train_loss: 0.5456, step time: 1.5551\n",
      "191/388, train_loss: 0.8942, step time: 1.5498\n",
      "192/388, train_loss: 0.7476, step time: 1.5515\n",
      "193/388, train_loss: 0.6202, step time: 1.5557\n",
      "194/388, train_loss: 0.7987, step time: 1.5532\n",
      "195/388, train_loss: 0.6712, step time: 1.5535\n",
      "196/388, train_loss: 0.8027, step time: 1.5537\n",
      "197/388, train_loss: 0.7996, step time: 1.5539\n",
      "198/388, train_loss: 0.9738, step time: 1.5565\n",
      "199/388, train_loss: 0.5421, step time: 1.5573\n",
      "200/388, train_loss: 0.7307, step time: 1.5545\n",
      "201/388, train_loss: 0.9621, step time: 1.5543\n",
      "202/388, train_loss: 0.7293, step time: 1.5552\n",
      "203/388, train_loss: 0.8803, step time: 1.5561\n",
      "204/388, train_loss: 0.6379, step time: 1.5556\n",
      "205/388, train_loss: 0.6597, step time: 1.5503\n",
      "206/388, train_loss: 0.7609, step time: 1.5681\n",
      "207/388, train_loss: 0.7956, step time: 1.5519\n",
      "208/388, train_loss: 0.8691, step time: 1.5523\n",
      "209/388, train_loss: 0.9182, step time: 1.5525\n",
      "210/388, train_loss: 0.6935, step time: 1.5568\n",
      "211/388, train_loss: 0.9285, step time: 1.5652\n",
      "212/388, train_loss: 0.7428, step time: 1.5490\n",
      "213/388, train_loss: 0.9023, step time: 1.5619\n",
      "214/388, train_loss: 0.8230, step time: 1.5547\n",
      "215/388, train_loss: 0.9246, step time: 1.5499\n",
      "216/388, train_loss: 0.8270, step time: 1.5528\n",
      "217/388, train_loss: 0.6989, step time: 1.5548\n",
      "218/388, train_loss: 0.8764, step time: 1.5567\n",
      "219/388, train_loss: 0.9636, step time: 1.5532\n",
      "220/388, train_loss: 0.4926, step time: 1.5530\n",
      "221/388, train_loss: 0.6828, step time: 1.5557\n",
      "222/388, train_loss: 0.6728, step time: 1.5603\n",
      "223/388, train_loss: 0.8369, step time: 1.5573\n",
      "224/388, train_loss: 0.7536, step time: 1.5559\n",
      "225/388, train_loss: 0.7620, step time: 1.5513\n",
      "226/388, train_loss: 0.7128, step time: 1.5607\n",
      "227/388, train_loss: 0.8675, step time: 1.5510\n",
      "228/388, train_loss: 0.7371, step time: 1.5501\n",
      "229/388, train_loss: 0.7488, step time: 1.5523\n",
      "230/388, train_loss: 0.7197, step time: 1.5583\n",
      "231/388, train_loss: 0.8202, step time: 1.5477\n",
      "232/388, train_loss: 0.9218, step time: 1.5575\n",
      "233/388, train_loss: 0.7063, step time: 1.5549\n",
      "234/388, train_loss: 0.9466, step time: 1.5574\n",
      "235/388, train_loss: 0.8161, step time: 1.5559\n",
      "236/388, train_loss: 0.9134, step time: 1.5511\n",
      "237/388, train_loss: 0.6671, step time: 1.5576\n",
      "238/388, train_loss: 0.8228, step time: 1.5646\n",
      "239/388, train_loss: 0.6108, step time: 1.5557\n",
      "240/388, train_loss: 0.8036, step time: 1.5544\n",
      "241/388, train_loss: 0.8817, step time: 1.5576\n",
      "242/388, train_loss: 0.9308, step time: 1.5544\n",
      "243/388, train_loss: 0.7900, step time: 1.5563\n",
      "244/388, train_loss: 0.8933, step time: 1.5513\n",
      "245/388, train_loss: 0.9080, step time: 1.5640\n",
      "246/388, train_loss: 0.7812, step time: 1.5594\n",
      "247/388, train_loss: 0.8623, step time: 1.5494\n",
      "248/388, train_loss: 0.8157, step time: 1.5562\n",
      "249/388, train_loss: 0.9861, step time: 1.5528\n",
      "250/388, train_loss: 0.8379, step time: 1.5504\n",
      "251/388, train_loss: 0.6650, step time: 1.5548\n",
      "252/388, train_loss: 0.6318, step time: 1.5560\n",
      "253/388, train_loss: 0.5797, step time: 1.5572\n",
      "254/388, train_loss: 0.8650, step time: 1.5574\n",
      "255/388, train_loss: 0.8773, step time: 1.5739\n",
      "256/388, train_loss: 0.8550, step time: 1.5512\n",
      "257/388, train_loss: 0.9447, step time: 1.5560\n",
      "258/388, train_loss: 0.6709, step time: 1.5589\n",
      "259/388, train_loss: 0.9545, step time: 1.5565\n",
      "260/388, train_loss: 0.8122, step time: 1.5512\n",
      "261/388, train_loss: 0.7089, step time: 1.5564\n",
      "262/388, train_loss: 0.7844, step time: 1.5608\n",
      "263/388, train_loss: 0.8082, step time: 1.5501\n",
      "264/388, train_loss: 0.8801, step time: 1.5531\n",
      "265/388, train_loss: 0.7479, step time: 1.5565\n",
      "266/388, train_loss: 0.9807, step time: 1.5539\n",
      "267/388, train_loss: 0.9401, step time: 1.5558\n",
      "268/388, train_loss: 0.6725, step time: 1.5463\n",
      "269/388, train_loss: 0.7007, step time: 1.5566\n",
      "270/388, train_loss: 0.8745, step time: 1.5573\n",
      "271/388, train_loss: 0.6832, step time: 1.5485\n",
      "272/388, train_loss: 0.8860, step time: 1.5534\n",
      "273/388, train_loss: 0.9158, step time: 1.5548\n",
      "274/388, train_loss: 0.7541, step time: 1.5605\n",
      "275/388, train_loss: 0.6549, step time: 1.5551\n",
      "276/388, train_loss: 0.9137, step time: 1.5506\n",
      "277/388, train_loss: 0.8270, step time: 1.5566\n",
      "278/388, train_loss: 0.8108, step time: 1.5567\n",
      "279/388, train_loss: 0.8495, step time: 1.5504\n",
      "280/388, train_loss: 0.7435, step time: 1.5522\n",
      "281/388, train_loss: 0.8791, step time: 1.5572\n",
      "282/388, train_loss: 0.6030, step time: 1.5573\n",
      "283/388, train_loss: 0.7421, step time: 1.5546\n",
      "284/388, train_loss: 0.6665, step time: 1.5512\n",
      "285/388, train_loss: 0.5812, step time: 1.5586\n",
      "286/388, train_loss: 0.8502, step time: 1.5567\n",
      "287/388, train_loss: 0.9360, step time: 1.5564\n",
      "288/388, train_loss: 0.6557, step time: 1.5584\n",
      "289/388, train_loss: 0.8993, step time: 1.5513\n",
      "290/388, train_loss: 0.5158, step time: 1.5595\n",
      "291/388, train_loss: 0.8523, step time: 1.5515\n",
      "292/388, train_loss: 0.5915, step time: 1.5466\n",
      "293/388, train_loss: 0.6069, step time: 1.5605\n",
      "294/388, train_loss: 0.6632, step time: 1.5588\n",
      "295/388, train_loss: 0.7206, step time: 1.5551\n",
      "296/388, train_loss: 0.7024, step time: 1.5530\n",
      "297/388, train_loss: 0.8102, step time: 1.5537\n",
      "298/388, train_loss: 0.8958, step time: 1.5581\n",
      "299/388, train_loss: 0.6429, step time: 1.5523\n",
      "300/388, train_loss: 0.8300, step time: 1.5592\n",
      "301/388, train_loss: 0.9185, step time: 1.5565\n",
      "302/388, train_loss: 0.8290, step time: 1.5561\n",
      "303/388, train_loss: 0.8492, step time: 1.5565\n",
      "304/388, train_loss: 0.9498, step time: 1.5525\n",
      "305/388, train_loss: 0.7348, step time: 1.5562\n",
      "306/388, train_loss: 0.6076, step time: 1.5429\n",
      "307/388, train_loss: 0.9442, step time: 1.5537\n",
      "308/388, train_loss: 0.8252, step time: 1.5600\n",
      "309/388, train_loss: 0.8504, step time: 1.5562\n",
      "310/388, train_loss: 0.6098, step time: 1.5595\n",
      "311/388, train_loss: 0.6285, step time: 1.5525\n",
      "312/388, train_loss: 0.6332, step time: 1.5510\n",
      "313/388, train_loss: 0.8356, step time: 1.5527\n",
      "314/388, train_loss: 0.5343, step time: 1.5595\n",
      "315/388, train_loss: 0.6839, step time: 1.5504\n",
      "316/388, train_loss: 0.7152, step time: 1.5567\n",
      "317/388, train_loss: 0.8559, step time: 1.5585\n",
      "318/388, train_loss: 0.8289, step time: 1.5665\n",
      "319/388, train_loss: 0.7281, step time: 1.5576\n",
      "320/388, train_loss: 0.8317, step time: 1.5514\n",
      "321/388, train_loss: 0.8828, step time: 1.5531\n",
      "322/388, train_loss: 0.7699, step time: 1.5571\n",
      "323/388, train_loss: 0.6387, step time: 1.5532\n",
      "324/388, train_loss: 0.7547, step time: 1.5540\n",
      "325/388, train_loss: 0.8389, step time: 1.5563\n",
      "326/388, train_loss: 0.7676, step time: 1.5624\n",
      "327/388, train_loss: 0.8232, step time: 1.5517\n",
      "328/388, train_loss: 0.7255, step time: 1.5549\n",
      "329/388, train_loss: 0.5543, step time: 1.5543\n",
      "330/388, train_loss: 0.6526, step time: 1.5574\n",
      "331/388, train_loss: 0.6595, step time: 1.5553\n",
      "332/388, train_loss: 0.7602, step time: 1.5551\n",
      "333/388, train_loss: 0.6189, step time: 1.5608\n",
      "334/388, train_loss: 0.8536, step time: 1.5659\n",
      "335/388, train_loss: 0.8017, step time: 1.5569\n",
      "336/388, train_loss: 0.9580, step time: 1.5529\n",
      "337/388, train_loss: 0.6847, step time: 1.5528\n",
      "338/388, train_loss: 0.8429, step time: 1.5612\n",
      "339/388, train_loss: 0.8852, step time: 1.5575\n",
      "340/388, train_loss: 0.6813, step time: 1.5533\n",
      "341/388, train_loss: 0.7892, step time: 1.5553\n",
      "342/388, train_loss: 0.8706, step time: 1.5588\n",
      "343/388, train_loss: 0.6289, step time: 1.5549\n",
      "344/388, train_loss: 0.8626, step time: 1.5533\n",
      "345/388, train_loss: 0.9207, step time: 1.5518\n",
      "346/388, train_loss: 0.6535, step time: 1.5624\n",
      "347/388, train_loss: 0.6479, step time: 1.5489\n",
      "348/388, train_loss: 0.4798, step time: 1.5548\n",
      "349/388, train_loss: 0.6815, step time: 1.5554\n",
      "350/388, train_loss: 0.9688, step time: 1.5574\n",
      "351/388, train_loss: 0.7243, step time: 1.5550\n",
      "352/388, train_loss: 0.7431, step time: 1.5513\n",
      "353/388, train_loss: 0.7848, step time: 1.5557\n",
      "354/388, train_loss: 0.8887, step time: 1.5574\n",
      "355/388, train_loss: 0.9612, step time: 1.5543\n",
      "356/388, train_loss: 0.6220, step time: 1.5587\n",
      "357/388, train_loss: 0.9106, step time: 1.5535\n",
      "358/388, train_loss: 0.9194, step time: 1.5511\n",
      "359/388, train_loss: 0.9026, step time: 1.5525\n",
      "360/388, train_loss: 0.9220, step time: 1.5535\n",
      "361/388, train_loss: 0.9523, step time: 1.5533\n",
      "362/388, train_loss: 0.5289, step time: 1.5631\n",
      "363/388, train_loss: 0.6087, step time: 1.5522\n",
      "364/388, train_loss: 0.8826, step time: 1.5523\n",
      "365/388, train_loss: 0.6124, step time: 1.5558\n",
      "366/388, train_loss: 0.6216, step time: 1.5602\n",
      "367/388, train_loss: 0.6097, step time: 1.5534\n",
      "368/388, train_loss: 0.7389, step time: 1.5552\n",
      "369/388, train_loss: 0.8966, step time: 1.5510\n",
      "370/388, train_loss: 0.7159, step time: 1.5563\n",
      "371/388, train_loss: 0.7624, step time: 1.5513\n",
      "372/388, train_loss: 0.6796, step time: 1.5525\n",
      "373/388, train_loss: 0.7346, step time: 1.5584\n",
      "374/388, train_loss: 0.6727, step time: 1.5556\n",
      "375/388, train_loss: 0.7254, step time: 1.5517\n",
      "376/388, train_loss: 0.8864, step time: 1.5563\n",
      "377/388, train_loss: 0.5811, step time: 1.5543\n",
      "378/388, train_loss: 0.6054, step time: 1.5621\n",
      "379/388, train_loss: 0.8063, step time: 1.5557\n",
      "380/388, train_loss: 0.7774, step time: 1.5511\n",
      "381/388, train_loss: 0.7624, step time: 1.5542\n",
      "382/388, train_loss: 0.7537, step time: 1.5642\n",
      "383/388, train_loss: 0.7514, step time: 1.5546\n",
      "384/388, train_loss: 0.5426, step time: 1.5566\n",
      "385/388, train_loss: 0.7350, step time: 1.5506\n",
      "386/388, train_loss: 0.9590, step time: 1.5610\n",
      "387/388, train_loss: 0.7696, step time: 1.5539\n",
      "388/388, train_loss: 0.7394, step time: 1.5501\n",
      "epoch 3 average loss: 0.7959\n",
      "saved new best metric model\n",
      "current epoch: 3 current mean dice: 0.5533 tc: 0.5885 wt: 0.7604 et: 0.3109\n",
      "best mean dice: 0.5533 at epoch: 3\n",
      "time consuming of epoch 3 is: 711.7184\n",
      "----------\n",
      "epoch 4/100\n",
      "1/388, train_loss: 0.6798, step time: 1.5755\n",
      "2/388, train_loss: 0.7589, step time: 1.5550\n",
      "3/388, train_loss: 0.7359, step time: 1.5560\n",
      "4/388, train_loss: 0.5109, step time: 1.5541\n",
      "5/388, train_loss: 0.7415, step time: 1.5573\n",
      "6/388, train_loss: 0.5991, step time: 1.5561\n",
      "7/388, train_loss: 0.6901, step time: 1.5641\n",
      "8/388, train_loss: 0.7064, step time: 1.5386\n",
      "9/388, train_loss: 0.7519, step time: 1.5524\n",
      "10/388, train_loss: 0.7154, step time: 1.5554\n",
      "11/388, train_loss: 0.9692, step time: 1.5564\n",
      "12/388, train_loss: 0.8524, step time: 1.5642\n",
      "13/388, train_loss: 0.6723, step time: 1.5540\n",
      "14/388, train_loss: 0.6920, step time: 1.5576\n",
      "15/388, train_loss: 0.7293, step time: 1.5313\n",
      "16/388, train_loss: 0.6776, step time: 1.5294\n",
      "17/388, train_loss: 0.5822, step time: 1.5573\n",
      "18/388, train_loss: 0.7756, step time: 1.5613\n",
      "19/388, train_loss: 0.7567, step time: 1.5378\n",
      "20/388, train_loss: 0.6753, step time: 1.5380\n",
      "21/388, train_loss: 0.6259, step time: 1.5540\n",
      "22/388, train_loss: 0.5938, step time: 1.5561\n",
      "23/388, train_loss: 0.7013, step time: 1.5348\n",
      "24/388, train_loss: 0.6278, step time: 1.5390\n",
      "25/388, train_loss: 0.6281, step time: 1.5543\n",
      "26/388, train_loss: 0.8718, step time: 1.5601\n",
      "27/388, train_loss: 0.8444, step time: 1.5324\n",
      "28/388, train_loss: 0.6872, step time: 1.5600\n",
      "29/388, train_loss: 0.8907, step time: 1.5526\n",
      "30/388, train_loss: 0.7558, step time: 1.5663\n",
      "31/388, train_loss: 0.6828, step time: 1.5322\n",
      "32/388, train_loss: 0.8057, step time: 1.5303\n",
      "33/388, train_loss: 0.5907, step time: 1.5581\n",
      "34/388, train_loss: 0.8754, step time: 1.5593\n",
      "35/388, train_loss: 0.7709, step time: 1.5549\n",
      "36/388, train_loss: 0.6272, step time: 1.5707\n",
      "37/388, train_loss: 0.6588, step time: 1.5592\n",
      "38/388, train_loss: 0.9693, step time: 1.5575\n",
      "39/388, train_loss: 0.4290, step time: 1.5358\n",
      "40/388, train_loss: 0.9792, step time: 1.5656\n",
      "41/388, train_loss: 0.6784, step time: 1.5521\n",
      "42/388, train_loss: 0.8126, step time: 1.5626\n",
      "43/388, train_loss: 0.8337, step time: 1.5538\n",
      "44/388, train_loss: 0.9341, step time: 1.5587\n",
      "45/388, train_loss: 0.5467, step time: 1.5525\n",
      "46/388, train_loss: 0.5991, step time: 1.5568\n",
      "47/388, train_loss: 0.7261, step time: 1.5599\n",
      "48/388, train_loss: 0.5775, step time: 1.5586\n",
      "49/388, train_loss: 0.8147, step time: 1.5579\n",
      "50/388, train_loss: 0.5224, step time: 1.5542\n",
      "51/388, train_loss: 0.5608, step time: 1.5657\n",
      "52/388, train_loss: 0.7281, step time: 1.5584\n",
      "53/388, train_loss: 0.8897, step time: 1.5631\n",
      "54/388, train_loss: 0.8104, step time: 1.5586\n",
      "55/388, train_loss: 0.8907, step time: 1.5579\n",
      "56/388, train_loss: 0.8886, step time: 1.5311\n",
      "57/388, train_loss: 0.6961, step time: 1.5550\n",
      "58/388, train_loss: 0.8496, step time: 1.5576\n",
      "59/388, train_loss: 0.8041, step time: 1.5647\n",
      "60/388, train_loss: 0.9075, step time: 1.5335\n",
      "61/388, train_loss: 0.9582, step time: 1.5563\n",
      "62/388, train_loss: 0.7528, step time: 1.5563\n",
      "63/388, train_loss: 0.8015, step time: 1.5634\n",
      "64/388, train_loss: 0.5653, step time: 1.5362\n",
      "65/388, train_loss: 0.4246, step time: 1.5581\n",
      "66/388, train_loss: 0.5537, step time: 1.5619\n",
      "67/388, train_loss: 0.9048, step time: 1.5586\n",
      "68/388, train_loss: 0.6045, step time: 1.5627\n",
      "69/388, train_loss: 0.8730, step time: 1.5539\n",
      "70/388, train_loss: 0.6994, step time: 1.5533\n",
      "71/388, train_loss: 0.7129, step time: 1.5572\n",
      "72/388, train_loss: 0.5412, step time: 1.5388\n",
      "73/388, train_loss: 0.5929, step time: 1.5533\n",
      "74/388, train_loss: 0.7669, step time: 1.5530\n",
      "75/388, train_loss: 0.5747, step time: 1.5396\n",
      "76/388, train_loss: 0.8081, step time: 1.5498\n",
      "77/388, train_loss: 0.5958, step time: 1.5566\n",
      "78/388, train_loss: 0.7791, step time: 1.5517\n",
      "79/388, train_loss: 0.7558, step time: 1.5572\n",
      "80/388, train_loss: 0.8554, step time: 1.5321\n",
      "81/388, train_loss: 0.9397, step time: 1.5538\n",
      "82/388, train_loss: 0.8282, step time: 1.5546\n",
      "83/388, train_loss: 0.7278, step time: 1.5346\n",
      "84/388, train_loss: 0.7138, step time: 1.5595\n",
      "85/388, train_loss: 0.5291, step time: 1.5517\n",
      "86/388, train_loss: 0.8551, step time: 1.5573\n",
      "87/388, train_loss: 0.6822, step time: 1.5285\n",
      "88/388, train_loss: 0.8050, step time: 1.5572\n",
      "89/388, train_loss: 0.6490, step time: 1.5541\n",
      "90/388, train_loss: 0.8909, step time: 1.5527\n",
      "91/388, train_loss: 0.7355, step time: 1.5337\n",
      "92/388, train_loss: 0.7056, step time: 1.5328\n",
      "93/388, train_loss: 0.9334, step time: 1.5570\n",
      "94/388, train_loss: 0.7368, step time: 1.5552\n",
      "95/388, train_loss: 0.7059, step time: 1.5556\n",
      "96/388, train_loss: 0.9074, step time: 1.5312\n",
      "97/388, train_loss: 0.5293, step time: 1.5576\n",
      "98/388, train_loss: 0.5857, step time: 1.5571\n",
      "99/388, train_loss: 0.8785, step time: 1.5660\n",
      "100/388, train_loss: 0.7337, step time: 1.5596\n",
      "101/388, train_loss: 0.7879, step time: 1.5653\n",
      "102/388, train_loss: 0.8600, step time: 1.5522\n",
      "103/388, train_loss: 0.5360, step time: 1.5343\n",
      "104/388, train_loss: 0.9613, step time: 1.5609\n",
      "105/388, train_loss: 0.6356, step time: 1.5606\n",
      "106/388, train_loss: 0.7497, step time: 1.5534\n",
      "107/388, train_loss: 0.5017, step time: 1.5365\n",
      "108/388, train_loss: 0.4925, step time: 1.5630\n",
      "109/388, train_loss: 0.8649, step time: 1.5575\n",
      "110/388, train_loss: 0.6270, step time: 1.5584\n",
      "111/388, train_loss: 0.9358, step time: 1.5715\n",
      "112/388, train_loss: 0.8678, step time: 1.5320\n",
      "113/388, train_loss: 0.7778, step time: 1.5461\n",
      "114/388, train_loss: 0.7480, step time: 1.5512\n",
      "115/388, train_loss: 0.5538, step time: 1.5329\n",
      "116/388, train_loss: 0.8577, step time: 1.5333\n",
      "117/388, train_loss: 0.7412, step time: 1.5489\n",
      "118/388, train_loss: 0.8617, step time: 1.5459\n",
      "119/388, train_loss: 0.7313, step time: 1.5610\n",
      "120/388, train_loss: 0.4975, step time: 1.5321\n",
      "121/388, train_loss: 0.6511, step time: 1.5576\n",
      "122/388, train_loss: 0.7208, step time: 1.5586\n",
      "123/388, train_loss: 0.9773, step time: 1.5327\n",
      "124/388, train_loss: 0.5786, step time: 1.5415\n",
      "125/388, train_loss: 0.7886, step time: 1.5560\n",
      "126/388, train_loss: 0.5621, step time: 1.5590\n",
      "127/388, train_loss: 0.8770, step time: 1.5582\n",
      "128/388, train_loss: 0.7078, step time: 1.5566\n",
      "129/388, train_loss: 0.5698, step time: 1.5542\n",
      "130/388, train_loss: 0.6102, step time: 1.5521\n",
      "131/388, train_loss: 0.9233, step time: 1.5565\n",
      "132/388, train_loss: 0.8974, step time: 1.5578\n",
      "133/388, train_loss: 0.6000, step time: 1.5583\n",
      "134/388, train_loss: 0.4624, step time: 1.5537\n",
      "135/388, train_loss: 0.6131, step time: 1.5358\n",
      "136/388, train_loss: 0.6663, step time: 1.5599\n",
      "137/388, train_loss: 0.7725, step time: 1.5548\n",
      "138/388, train_loss: 0.8814, step time: 1.5582\n",
      "139/388, train_loss: 0.7358, step time: 1.5348\n",
      "140/388, train_loss: 0.5449, step time: 1.5639\n",
      "141/388, train_loss: 0.5176, step time: 1.5514\n",
      "142/388, train_loss: 0.5886, step time: 1.5576\n",
      "143/388, train_loss: 0.5749, step time: 1.5359\n",
      "144/388, train_loss: 0.8230, step time: 1.5317\n",
      "145/388, train_loss: 0.8227, step time: 1.5530\n",
      "146/388, train_loss: 0.8715, step time: 1.5660\n",
      "147/388, train_loss: 0.7672, step time: 1.5312\n",
      "148/388, train_loss: 0.9511, step time: 1.5660\n",
      "149/388, train_loss: 0.6370, step time: 1.5531\n",
      "150/388, train_loss: 0.7037, step time: 1.5575\n",
      "151/388, train_loss: 0.7467, step time: 1.5544\n",
      "152/388, train_loss: 0.6964, step time: 1.5436\n",
      "153/388, train_loss: 0.6082, step time: 1.5538\n",
      "154/388, train_loss: 0.5709, step time: 1.5559\n",
      "155/388, train_loss: 0.5009, step time: 1.5566\n",
      "156/388, train_loss: 0.7810, step time: 1.5321\n",
      "157/388, train_loss: 0.6324, step time: 1.5455\n",
      "158/388, train_loss: 0.6687, step time: 1.5559\n",
      "159/388, train_loss: 0.8144, step time: 1.5572\n",
      "160/388, train_loss: 0.3781, step time: 1.5777\n",
      "161/388, train_loss: 0.4465, step time: 1.5587\n",
      "162/388, train_loss: 0.9531, step time: 1.5486\n",
      "163/388, train_loss: 0.3939, step time: 1.5368\n",
      "164/388, train_loss: 0.6875, step time: 1.5347\n",
      "165/388, train_loss: 0.5625, step time: 1.5524\n",
      "166/388, train_loss: 0.5807, step time: 1.5568\n",
      "167/388, train_loss: 0.8176, step time: 1.5319\n",
      "168/388, train_loss: 0.7115, step time: 1.5383\n",
      "169/388, train_loss: 0.6557, step time: 1.5494\n",
      "170/388, train_loss: 0.4113, step time: 1.5474\n",
      "171/388, train_loss: 0.7132, step time: 1.5325\n",
      "172/388, train_loss: 0.4766, step time: 1.5320\n",
      "173/388, train_loss: 0.4502, step time: 1.5460\n",
      "174/388, train_loss: 0.5452, step time: 1.5470\n",
      "175/388, train_loss: 0.7813, step time: 1.5302\n",
      "176/388, train_loss: 0.5077, step time: 1.5359\n",
      "177/388, train_loss: 0.5705, step time: 1.5559\n",
      "178/388, train_loss: 0.7552, step time: 1.5459\n",
      "179/388, train_loss: 0.7029, step time: 1.5375\n",
      "180/388, train_loss: 0.5482, step time: 1.5377\n",
      "181/388, train_loss: 0.4984, step time: 1.5419\n",
      "182/388, train_loss: 0.7926, step time: 1.5447\n",
      "183/388, train_loss: 0.6602, step time: 1.5294\n",
      "184/388, train_loss: 0.5760, step time: 1.5319\n",
      "185/388, train_loss: 0.7657, step time: 1.5535\n",
      "186/388, train_loss: 0.6037, step time: 1.5570\n",
      "187/388, train_loss: 0.9102, step time: 1.5350\n",
      "188/388, train_loss: 0.7154, step time: 1.5345\n",
      "189/388, train_loss: 0.8155, step time: 1.5431\n",
      "190/388, train_loss: 0.4540, step time: 1.5546\n",
      "191/388, train_loss: 0.5615, step time: 1.5319\n",
      "192/388, train_loss: 0.7417, step time: 1.5320\n",
      "193/388, train_loss: 0.5241, step time: 1.5601\n",
      "194/388, train_loss: 0.5140, step time: 1.5513\n",
      "195/388, train_loss: 0.8104, step time: 1.5309\n",
      "196/388, train_loss: 0.5956, step time: 1.5332\n",
      "197/388, train_loss: 0.7357, step time: 1.5489\n",
      "198/388, train_loss: 0.5884, step time: 1.5521\n",
      "199/388, train_loss: 0.7593, step time: 1.5483\n",
      "200/388, train_loss: 0.7472, step time: 1.5509\n",
      "201/388, train_loss: 0.5413, step time: 1.5492\n",
      "202/388, train_loss: 0.6849, step time: 1.5437\n",
      "203/388, train_loss: 0.6185, step time: 1.5468\n",
      "204/388, train_loss: 0.9125, step time: 1.5328\n",
      "205/388, train_loss: 0.3907, step time: 1.5487\n",
      "206/388, train_loss: 0.8437, step time: 1.5418\n",
      "207/388, train_loss: 0.5914, step time: 1.5439\n",
      "208/388, train_loss: 0.4742, step time: 1.5383\n",
      "209/388, train_loss: 0.5216, step time: 1.5490\n",
      "210/388, train_loss: 0.5858, step time: 1.5554\n",
      "211/388, train_loss: 0.6825, step time: 1.5442\n",
      "212/388, train_loss: 0.5937, step time: 1.5410\n",
      "213/388, train_loss: 0.7829, step time: 1.5435\n",
      "214/388, train_loss: 0.4844, step time: 1.5481\n",
      "215/388, train_loss: 0.6076, step time: 1.5518\n",
      "216/388, train_loss: 0.4966, step time: 1.5361\n",
      "217/388, train_loss: 0.4518, step time: 1.5524\n",
      "218/388, train_loss: 0.5891, step time: 1.5516\n",
      "219/388, train_loss: 0.6555, step time: 1.5510\n",
      "220/388, train_loss: 0.4273, step time: 1.5328\n",
      "221/388, train_loss: 0.7318, step time: 1.5434\n",
      "222/388, train_loss: 0.8191, step time: 1.5500\n",
      "223/388, train_loss: 0.6914, step time: 1.5416\n",
      "224/388, train_loss: 0.5606, step time: 1.5508\n",
      "225/388, train_loss: 0.5903, step time: 1.5432\n",
      "226/388, train_loss: 0.6618, step time: 1.5638\n",
      "227/388, train_loss: 0.8083, step time: 1.5463\n",
      "228/388, train_loss: 0.4629, step time: 1.5298\n",
      "229/388, train_loss: 0.6327, step time: 1.5568\n",
      "230/388, train_loss: 0.5985, step time: 1.5440\n",
      "231/388, train_loss: 0.3220, step time: 1.5548\n",
      "232/388, train_loss: 0.4421, step time: 1.5362\n",
      "233/388, train_loss: 0.8456, step time: 1.5458\n",
      "234/388, train_loss: 0.6237, step time: 1.5537\n",
      "235/388, train_loss: 0.5892, step time: 1.5485\n",
      "236/388, train_loss: 0.9417, step time: 1.5352\n",
      "237/388, train_loss: 0.7072, step time: 1.5434\n",
      "238/388, train_loss: 0.6271, step time: 1.5435\n",
      "239/388, train_loss: 0.3506, step time: 1.5514\n",
      "240/388, train_loss: 0.8930, step time: 1.5354\n",
      "241/388, train_loss: 0.6622, step time: 1.5441\n",
      "242/388, train_loss: 0.4866, step time: 1.5614\n",
      "243/388, train_loss: 0.3989, step time: 1.5569\n",
      "244/388, train_loss: 0.5251, step time: 1.5368\n",
      "245/388, train_loss: 0.7875, step time: 1.5426\n",
      "246/388, train_loss: 0.6594, step time: 1.5516\n",
      "247/388, train_loss: 0.6619, step time: 1.5468\n",
      "248/388, train_loss: 0.6726, step time: 1.5349\n",
      "249/388, train_loss: 0.5982, step time: 1.5547\n",
      "250/388, train_loss: 0.7601, step time: 1.5432\n",
      "251/388, train_loss: 0.7125, step time: 1.5505\n",
      "252/388, train_loss: 0.5946, step time: 1.5308\n",
      "253/388, train_loss: 0.5345, step time: 1.5664\n",
      "254/388, train_loss: 0.6672, step time: 1.5547\n",
      "255/388, train_loss: 0.8146, step time: 1.5422\n",
      "256/388, train_loss: 0.4258, step time: 1.5333\n",
      "257/388, train_loss: 0.4956, step time: 1.5464\n",
      "258/388, train_loss: 0.5546, step time: 1.5522\n",
      "259/388, train_loss: 0.9161, step time: 1.5507\n",
      "260/388, train_loss: 0.5621, step time: 1.5330\n",
      "261/388, train_loss: 0.5955, step time: 1.5504\n",
      "262/388, train_loss: 0.6926, step time: 1.5458\n",
      "263/388, train_loss: 0.4725, step time: 1.5480\n",
      "264/388, train_loss: 0.4784, step time: 1.5394\n",
      "265/388, train_loss: 0.4186, step time: 1.5453\n",
      "266/388, train_loss: 0.9041, step time: 1.5480\n",
      "267/388, train_loss: 0.4940, step time: 1.5432\n",
      "268/388, train_loss: 0.6094, step time: 1.5510\n",
      "269/388, train_loss: 0.3888, step time: 1.5583\n",
      "270/388, train_loss: 0.2849, step time: 1.5531\n",
      "271/388, train_loss: 0.5584, step time: 1.5467\n",
      "272/388, train_loss: 0.4382, step time: 1.5311\n",
      "273/388, train_loss: 0.9634, step time: 1.5501\n",
      "274/388, train_loss: 0.4988, step time: 1.5525\n",
      "275/388, train_loss: 0.5038, step time: 1.5422\n",
      "276/388, train_loss: 0.7177, step time: 1.5401\n",
      "277/388, train_loss: 0.8168, step time: 1.5450\n",
      "278/388, train_loss: 0.6018, step time: 1.5491\n",
      "279/388, train_loss: 0.9588, step time: 1.5468\n",
      "280/388, train_loss: 0.6249, step time: 1.5351\n",
      "281/388, train_loss: 0.4818, step time: 1.5593\n",
      "282/388, train_loss: 0.5895, step time: 1.5501\n",
      "283/388, train_loss: 0.4581, step time: 1.5446\n",
      "284/388, train_loss: 0.7817, step time: 1.5434\n",
      "285/388, train_loss: 0.8444, step time: 1.5494\n",
      "286/388, train_loss: 0.4989, step time: 1.5526\n",
      "287/388, train_loss: 0.5860, step time: 1.5434\n",
      "288/388, train_loss: 0.4181, step time: 1.5351\n",
      "289/388, train_loss: 0.8091, step time: 1.5499\n",
      "290/388, train_loss: 0.5097, step time: 1.5478\n",
      "291/388, train_loss: 0.4949, step time: 1.5493\n",
      "292/388, train_loss: 0.6445, step time: 1.5323\n",
      "293/388, train_loss: 0.7952, step time: 1.5549\n",
      "294/388, train_loss: 0.5068, step time: 1.5474\n",
      "295/388, train_loss: 0.8861, step time: 1.5485\n",
      "296/388, train_loss: 0.2959, step time: 1.5374\n",
      "297/388, train_loss: 0.4172, step time: 1.5524\n",
      "298/388, train_loss: 0.4384, step time: 1.5487\n",
      "299/388, train_loss: 0.8462, step time: 1.5529\n",
      "300/388, train_loss: 0.5216, step time: 1.5334\n",
      "301/388, train_loss: 0.6899, step time: 1.5475\n",
      "302/388, train_loss: 0.8120, step time: 1.5618\n",
      "303/388, train_loss: 0.8872, step time: 1.5431\n",
      "304/388, train_loss: 0.7593, step time: 1.5354\n",
      "305/388, train_loss: 0.7513, step time: 1.5469\n",
      "306/388, train_loss: 0.6984, step time: 1.5413\n",
      "307/388, train_loss: 0.8831, step time: 1.5531\n",
      "308/388, train_loss: 0.7537, step time: 1.5297\n",
      "309/388, train_loss: 0.6297, step time: 1.5537\n",
      "310/388, train_loss: 0.6472, step time: 1.5529\n",
      "311/388, train_loss: 0.4159, step time: 1.5516\n",
      "312/388, train_loss: 0.2707, step time: 1.5363\n",
      "313/388, train_loss: 0.7254, step time: 1.5385\n",
      "314/388, train_loss: 0.4419, step time: 1.5608\n",
      "315/388, train_loss: 0.6120, step time: 1.5450\n",
      "316/388, train_loss: 0.5302, step time: 1.5399\n",
      "317/388, train_loss: 0.7904, step time: 1.5494\n",
      "318/388, train_loss: 0.5242, step time: 1.5560\n",
      "319/388, train_loss: 0.6985, step time: 1.5493\n",
      "320/388, train_loss: 0.5706, step time: 1.5367\n",
      "321/388, train_loss: 0.3170, step time: 1.5454\n",
      "322/388, train_loss: 0.7751, step time: 1.5474\n",
      "323/388, train_loss: 0.5029, step time: 1.5554\n",
      "324/388, train_loss: 0.7168, step time: 1.5407\n",
      "325/388, train_loss: 0.5227, step time: 1.5613\n",
      "326/388, train_loss: 0.7775, step time: 1.5579\n",
      "327/388, train_loss: 0.5447, step time: 1.5442\n",
      "328/388, train_loss: 0.5062, step time: 1.5342\n",
      "329/388, train_loss: 0.5154, step time: 1.5480\n",
      "330/388, train_loss: 0.7915, step time: 1.5416\n",
      "331/388, train_loss: 0.9000, step time: 1.5508\n",
      "332/388, train_loss: 0.5858, step time: 1.5326\n",
      "333/388, train_loss: 0.6820, step time: 1.5691\n",
      "334/388, train_loss: 0.5686, step time: 1.5486\n",
      "335/388, train_loss: 0.6163, step time: 1.5490\n",
      "336/388, train_loss: 0.4575, step time: 1.5310\n",
      "337/388, train_loss: 0.7952, step time: 1.5538\n",
      "338/388, train_loss: 0.6457, step time: 1.5590\n",
      "339/388, train_loss: 0.7718, step time: 1.5508\n",
      "340/388, train_loss: 0.5885, step time: 1.5375\n",
      "341/388, train_loss: 0.4329, step time: 1.5482\n",
      "342/388, train_loss: 0.5160, step time: 1.5519\n",
      "343/388, train_loss: 0.4711, step time: 1.5478\n",
      "344/388, train_loss: 0.6845, step time: 1.5295\n",
      "345/388, train_loss: 0.7793, step time: 1.5527\n",
      "346/388, train_loss: 0.6122, step time: 1.5432\n",
      "347/388, train_loss: 0.3082, step time: 1.5466\n",
      "348/388, train_loss: 0.7692, step time: 1.5319\n",
      "349/388, train_loss: 0.4907, step time: 1.5407\n",
      "350/388, train_loss: 0.6153, step time: 1.5483\n",
      "351/388, train_loss: 0.4444, step time: 1.5446\n",
      "352/388, train_loss: 0.6963, step time: 1.5340\n",
      "353/388, train_loss: 0.4546, step time: 1.5478\n",
      "354/388, train_loss: 0.4305, step time: 1.5440\n",
      "355/388, train_loss: 0.4717, step time: 1.5487\n",
      "356/388, train_loss: 0.4011, step time: 1.5330\n",
      "357/388, train_loss: 0.7113, step time: 1.5416\n",
      "358/388, train_loss: 0.2903, step time: 1.5496\n",
      "359/388, train_loss: 0.4629, step time: 1.5411\n",
      "360/388, train_loss: 0.8894, step time: 1.5307\n",
      "361/388, train_loss: 0.4527, step time: 1.5478\n",
      "362/388, train_loss: 0.4641, step time: 1.5434\n",
      "363/388, train_loss: 0.6178, step time: 1.5443\n",
      "364/388, train_loss: 0.5662, step time: 1.5390\n",
      "365/388, train_loss: 0.6263, step time: 1.5456\n",
      "366/388, train_loss: 0.4656, step time: 1.5489\n",
      "367/388, train_loss: 0.6472, step time: 1.5455\n",
      "368/388, train_loss: 0.8255, step time: 1.5301\n",
      "369/388, train_loss: 0.6835, step time: 1.5531\n",
      "370/388, train_loss: 0.4662, step time: 1.5445\n",
      "371/388, train_loss: 0.4860, step time: 1.5484\n",
      "372/388, train_loss: 0.6754, step time: 1.5325\n",
      "373/388, train_loss: 0.8808, step time: 1.5541\n",
      "374/388, train_loss: 0.8238, step time: 1.5560\n",
      "375/388, train_loss: 0.5428, step time: 1.5478\n",
      "376/388, train_loss: 0.3605, step time: 1.5361\n",
      "377/388, train_loss: 0.8743, step time: 1.5597\n",
      "378/388, train_loss: 0.4802, step time: 1.5572\n",
      "379/388, train_loss: 0.6056, step time: 1.5482\n",
      "380/388, train_loss: 0.5312, step time: 1.5300\n",
      "381/388, train_loss: 0.5057, step time: 1.5464\n",
      "382/388, train_loss: 0.6637, step time: 1.5514\n",
      "383/388, train_loss: 0.5497, step time: 1.5490\n",
      "384/388, train_loss: 0.5430, step time: 1.5438\n",
      "385/388, train_loss: 0.5540, step time: 1.5405\n",
      "386/388, train_loss: 0.9136, step time: 1.5473\n",
      "387/388, train_loss: 0.6127, step time: 1.5559\n",
      "388/388, train_loss: 0.3808, step time: 1.5301\n",
      "epoch 4 average loss: 0.6592\n",
      "saved new best metric model\n",
      "current epoch: 4 current mean dice: 0.6058 tc: 0.6550 wt: 0.8016 et: 0.3609\n",
      "best mean dice: 0.6058 at epoch: 4\n",
      "time consuming of epoch 4 is: 710.3741\n",
      "----------\n",
      "epoch 5/100\n",
      "1/388, train_loss: 0.4559, step time: 1.5504\n",
      "2/388, train_loss: 0.3670, step time: 1.5313\n",
      "3/388, train_loss: 0.2963, step time: 1.5343\n",
      "4/388, train_loss: 0.8816, step time: 1.5327\n",
      "5/388, train_loss: 0.4676, step time: 1.5318\n",
      "6/388, train_loss: 0.4615, step time: 1.5365\n",
      "7/388, train_loss: 0.6317, step time: 1.5303\n",
      "8/388, train_loss: 0.6865, step time: 1.5306\n",
      "9/388, train_loss: 0.6945, step time: 1.5322\n",
      "10/388, train_loss: 0.5141, step time: 1.5357\n",
      "11/388, train_loss: 0.5729, step time: 1.5367\n",
      "12/388, train_loss: 0.5075, step time: 1.5322\n",
      "13/388, train_loss: 0.7357, step time: 1.5354\n",
      "14/388, train_loss: 0.3828, step time: 1.5569\n",
      "15/388, train_loss: 0.5640, step time: 1.5311\n",
      "16/388, train_loss: 0.5183, step time: 1.5498\n",
      "17/388, train_loss: 0.7143, step time: 1.5302\n",
      "18/388, train_loss: 0.7541, step time: 1.5355\n",
      "19/388, train_loss: 0.4815, step time: 1.5429\n",
      "20/388, train_loss: 0.5271, step time: 1.5311\n",
      "21/388, train_loss: 0.5318, step time: 1.5304\n",
      "22/388, train_loss: 0.4855, step time: 1.5314\n",
      "23/388, train_loss: 0.5275, step time: 1.5321\n",
      "24/388, train_loss: 0.6198, step time: 1.5339\n",
      "25/388, train_loss: 0.4415, step time: 1.5371\n",
      "26/388, train_loss: 0.4798, step time: 1.5309\n",
      "27/388, train_loss: 0.5417, step time: 1.5311\n",
      "28/388, train_loss: 0.3485, step time: 1.5319\n",
      "29/388, train_loss: 0.6343, step time: 1.5301\n",
      "30/388, train_loss: 0.6536, step time: 1.5349\n",
      "31/388, train_loss: 0.5216, step time: 1.5579\n",
      "32/388, train_loss: 0.4814, step time: 1.5575\n",
      "33/388, train_loss: 0.9544, step time: 1.5313\n",
      "34/388, train_loss: 0.2445, step time: 1.5335\n",
      "35/388, train_loss: 0.3998, step time: 1.5309\n",
      "36/388, train_loss: 0.6765, step time: 1.5606\n",
      "37/388, train_loss: 0.6929, step time: 1.5360\n",
      "38/388, train_loss: 0.3977, step time: 1.5327\n",
      "39/388, train_loss: 0.7337, step time: 1.5411\n",
      "40/388, train_loss: 0.6925, step time: 1.5329\n",
      "41/388, train_loss: 0.6753, step time: 1.5435\n",
      "42/388, train_loss: 0.5816, step time: 1.5493\n",
      "43/388, train_loss: 0.6466, step time: 1.5296\n",
      "44/388, train_loss: 0.6313, step time: 1.5282\n",
      "45/388, train_loss: 0.3584, step time: 1.5349\n",
      "46/388, train_loss: 0.4915, step time: 1.5348\n",
      "47/388, train_loss: 0.4445, step time: 1.5354\n",
      "48/388, train_loss: 0.6842, step time: 1.5316\n",
      "49/388, train_loss: 0.2579, step time: 1.5304\n",
      "50/388, train_loss: 0.5745, step time: 1.5315\n",
      "51/388, train_loss: 0.8375, step time: 1.5363\n",
      "52/388, train_loss: 0.3612, step time: 1.5443\n",
      "53/388, train_loss: 0.4888, step time: 1.5294\n",
      "54/388, train_loss: 0.6779, step time: 1.5368\n",
      "55/388, train_loss: 0.5526, step time: 1.5296\n",
      "56/388, train_loss: 0.9298, step time: 1.5349\n",
      "57/388, train_loss: 0.4472, step time: 1.5506\n",
      "58/388, train_loss: 0.5467, step time: 1.5292\n",
      "59/388, train_loss: 0.4843, step time: 1.5288\n",
      "60/388, train_loss: 0.7595, step time: 1.5283\n",
      "61/388, train_loss: 0.4650, step time: 1.5362\n",
      "62/388, train_loss: 0.5176, step time: 1.5379\n",
      "63/388, train_loss: 0.5824, step time: 1.5377\n",
      "64/388, train_loss: 0.4840, step time: 1.5318\n",
      "65/388, train_loss: 0.7158, step time: 1.5298\n",
      "66/388, train_loss: 0.6695, step time: 1.5299\n",
      "67/388, train_loss: 0.2800, step time: 1.5339\n",
      "68/388, train_loss: 0.4414, step time: 1.5399\n",
      "69/388, train_loss: 0.3960, step time: 1.5329\n",
      "70/388, train_loss: 0.4280, step time: 1.5325\n",
      "71/388, train_loss: 0.5811, step time: 1.5303\n",
      "72/388, train_loss: 0.5260, step time: 1.5328\n",
      "73/388, train_loss: 0.7462, step time: 1.5366\n",
      "74/388, train_loss: 0.5081, step time: 1.5366\n",
      "75/388, train_loss: 0.5449, step time: 1.5305\n",
      "76/388, train_loss: 0.5747, step time: 1.5314\n",
      "77/388, train_loss: 0.7793, step time: 1.5323\n",
      "78/388, train_loss: 0.5085, step time: 1.5362\n",
      "79/388, train_loss: 0.7596, step time: 1.5371\n",
      "80/388, train_loss: 0.5669, step time: 1.5340\n",
      "81/388, train_loss: 0.5723, step time: 1.5280\n",
      "82/388, train_loss: 0.3666, step time: 1.5305\n",
      "83/388, train_loss: 0.6652, step time: 1.5295\n",
      "84/388, train_loss: 0.3824, step time: 1.5294\n",
      "85/388, train_loss: 0.4467, step time: 1.5299\n",
      "86/388, train_loss: 0.6797, step time: 1.5363\n",
      "87/388, train_loss: 0.4540, step time: 1.5341\n",
      "88/388, train_loss: 0.6624, step time: 1.5318\n",
      "89/388, train_loss: 0.6450, step time: 1.5353\n",
      "90/388, train_loss: 0.6053, step time: 1.5324\n",
      "91/388, train_loss: 0.4029, step time: 1.5291\n",
      "92/388, train_loss: 0.4521, step time: 1.5468\n",
      "93/388, train_loss: 0.2949, step time: 1.5349\n",
      "94/388, train_loss: 0.4330, step time: 1.5493\n",
      "95/388, train_loss: 0.5846, step time: 1.5341\n",
      "96/388, train_loss: 0.4594, step time: 1.5514\n",
      "97/388, train_loss: 0.8118, step time: 1.5374\n",
      "98/388, train_loss: 0.5509, step time: 1.5414\n",
      "99/388, train_loss: 0.4668, step time: 1.5316\n",
      "100/388, train_loss: 0.6120, step time: 1.5425\n",
      "101/388, train_loss: 0.5521, step time: 1.5344\n",
      "102/388, train_loss: 0.4037, step time: 1.5482\n",
      "103/388, train_loss: 0.5285, step time: 1.5540\n",
      "104/388, train_loss: 0.4874, step time: 1.5341\n",
      "105/388, train_loss: 0.7936, step time: 1.5328\n",
      "106/388, train_loss: 0.4558, step time: 1.5495\n",
      "107/388, train_loss: 0.4075, step time: 1.5372\n",
      "108/388, train_loss: 0.4218, step time: 1.5319\n",
      "109/388, train_loss: 0.5995, step time: 1.5337\n",
      "110/388, train_loss: 0.3798, step time: 1.5475\n",
      "111/388, train_loss: 0.5971, step time: 1.5334\n",
      "112/388, train_loss: 0.5943, step time: 1.5292\n",
      "113/388, train_loss: 0.5123, step time: 1.5300\n",
      "114/388, train_loss: 0.7863, step time: 1.5503\n",
      "115/388, train_loss: 0.3519, step time: 1.5354\n",
      "116/388, train_loss: 0.8388, step time: 1.5342\n",
      "117/388, train_loss: 0.5904, step time: 1.5362\n",
      "118/388, train_loss: 0.4279, step time: 1.5442\n",
      "119/388, train_loss: 0.4932, step time: 1.5347\n",
      "120/388, train_loss: 0.6512, step time: 1.5384\n",
      "121/388, train_loss: 0.8197, step time: 1.5356\n",
      "122/388, train_loss: 0.8994, step time: 1.5455\n",
      "123/388, train_loss: 0.8956, step time: 1.5303\n",
      "124/388, train_loss: 0.4968, step time: 1.5361\n",
      "125/388, train_loss: 0.4914, step time: 1.5369\n",
      "126/388, train_loss: 0.7371, step time: 1.5489\n",
      "127/388, train_loss: 0.5892, step time: 1.5326\n",
      "128/388, train_loss: 0.5386, step time: 1.5299\n",
      "129/388, train_loss: 0.4121, step time: 1.5341\n",
      "130/388, train_loss: 0.4831, step time: 1.5488\n",
      "131/388, train_loss: 0.8233, step time: 1.5375\n",
      "132/388, train_loss: 0.4992, step time: 1.5340\n",
      "133/388, train_loss: 0.2466, step time: 1.5329\n",
      "134/388, train_loss: 0.4481, step time: 1.5412\n",
      "135/388, train_loss: 0.5987, step time: 1.5341\n",
      "136/388, train_loss: 0.6547, step time: 1.5343\n",
      "137/388, train_loss: 0.5339, step time: 1.5379\n",
      "138/388, train_loss: 0.8586, step time: 1.5451\n",
      "139/388, train_loss: 0.6316, step time: 1.5340\n",
      "140/388, train_loss: 0.3475, step time: 1.5340\n",
      "141/388, train_loss: 0.5544, step time: 1.5375\n",
      "142/388, train_loss: 0.7364, step time: 1.5519\n",
      "143/388, train_loss: 0.4105, step time: 1.5378\n",
      "144/388, train_loss: 0.4806, step time: 1.5391\n",
      "145/388, train_loss: 0.5431, step time: 1.5354\n",
      "146/388, train_loss: 0.3775, step time: 1.5521\n",
      "147/388, train_loss: 0.5185, step time: 1.5335\n",
      "148/388, train_loss: 0.6063, step time: 1.5357\n",
      "149/388, train_loss: 0.8357, step time: 1.5345\n",
      "150/388, train_loss: 0.4157, step time: 1.5504\n",
      "151/388, train_loss: 0.5491, step time: 1.5332\n",
      "152/388, train_loss: 0.6761, step time: 1.5338\n",
      "153/388, train_loss: 0.7009, step time: 1.5332\n",
      "154/388, train_loss: 0.6815, step time: 1.5454\n",
      "155/388, train_loss: 0.4856, step time: 1.5381\n",
      "156/388, train_loss: 0.4712, step time: 1.5362\n",
      "157/388, train_loss: 0.7359, step time: 1.5377\n",
      "158/388, train_loss: 0.2655, step time: 1.5459\n",
      "159/388, train_loss: 0.3665, step time: 1.5343\n",
      "160/388, train_loss: 0.5595, step time: 1.5379\n",
      "161/388, train_loss: 0.2210, step time: 1.5336\n",
      "162/388, train_loss: 0.4885, step time: 1.5463\n",
      "163/388, train_loss: 0.4587, step time: 1.5341\n",
      "164/388, train_loss: 0.7860, step time: 1.5338\n",
      "165/388, train_loss: 0.4945, step time: 1.5381\n",
      "166/388, train_loss: 0.2926, step time: 1.5475\n",
      "167/388, train_loss: 0.3264, step time: 1.5316\n",
      "168/388, train_loss: 0.3902, step time: 1.5363\n",
      "169/388, train_loss: 0.3942, step time: 1.5365\n",
      "170/388, train_loss: 0.7352, step time: 1.5496\n",
      "171/388, train_loss: 0.6739, step time: 1.5342\n",
      "172/388, train_loss: 0.4036, step time: 1.5343\n",
      "173/388, train_loss: 0.2976, step time: 1.5367\n",
      "174/388, train_loss: 0.3221, step time: 1.5492\n",
      "175/388, train_loss: 0.3507, step time: 1.5347\n",
      "176/388, train_loss: 0.3652, step time: 1.5348\n",
      "177/388, train_loss: 0.4290, step time: 1.5319\n",
      "178/388, train_loss: 0.2861, step time: 1.5508\n",
      "179/388, train_loss: 0.4232, step time: 1.5371\n",
      "180/388, train_loss: 0.9101, step time: 1.5342\n",
      "181/388, train_loss: 0.4079, step time: 1.5323\n",
      "182/388, train_loss: 0.1960, step time: 1.5442\n",
      "183/388, train_loss: 0.8268, step time: 1.5367\n",
      "184/388, train_loss: 0.3925, step time: 1.5402\n",
      "185/388, train_loss: 0.7966, step time: 1.5381\n",
      "186/388, train_loss: 0.4350, step time: 1.5469\n",
      "187/388, train_loss: 0.7771, step time: 1.5356\n",
      "188/388, train_loss: 0.4862, step time: 1.5361\n",
      "189/388, train_loss: 0.4893, step time: 1.5313\n",
      "190/388, train_loss: 0.4866, step time: 1.5446\n",
      "191/388, train_loss: 0.3856, step time: 1.5355\n",
      "192/388, train_loss: 0.5667, step time: 1.5432\n",
      "193/388, train_loss: 0.4754, step time: 1.5302\n",
      "194/388, train_loss: 0.4190, step time: 1.5412\n",
      "195/388, train_loss: 0.5129, step time: 1.5347\n",
      "196/388, train_loss: 0.2492, step time: 1.5374\n",
      "197/388, train_loss: 0.3276, step time: 1.5376\n",
      "198/388, train_loss: 0.3818, step time: 1.5470\n",
      "199/388, train_loss: 0.8471, step time: 1.5337\n",
      "200/388, train_loss: 0.6934, step time: 1.5355\n",
      "201/388, train_loss: 0.6290, step time: 1.5362\n",
      "202/388, train_loss: 0.3798, step time: 1.5470\n",
      "203/388, train_loss: 0.4701, step time: 1.5310\n",
      "204/388, train_loss: 0.4254, step time: 1.5341\n",
      "205/388, train_loss: 0.4192, step time: 1.5354\n",
      "206/388, train_loss: 0.5153, step time: 1.5462\n",
      "207/388, train_loss: 0.3408, step time: 1.5298\n",
      "208/388, train_loss: 0.4948, step time: 1.5356\n",
      "209/388, train_loss: 0.5157, step time: 1.5386\n",
      "210/388, train_loss: 0.6893, step time: 1.5493\n",
      "211/388, train_loss: 0.4537, step time: 1.5327\n",
      "212/388, train_loss: 0.3815, step time: 1.5314\n",
      "213/388, train_loss: 0.3562, step time: 1.5323\n",
      "214/388, train_loss: 0.5913, step time: 1.5479\n",
      "215/388, train_loss: 0.6828, step time: 1.5504\n",
      "216/388, train_loss: 0.3821, step time: 1.5348\n",
      "217/388, train_loss: 0.7519, step time: 1.5314\n",
      "218/388, train_loss: 0.8045, step time: 1.5468\n",
      "219/388, train_loss: 0.7678, step time: 1.5357\n",
      "220/388, train_loss: 0.8468, step time: 1.5338\n",
      "221/388, train_loss: 0.5168, step time: 1.5341\n",
      "222/388, train_loss: 0.3438, step time: 1.5496\n",
      "223/388, train_loss: 0.5370, step time: 1.5390\n",
      "224/388, train_loss: 0.4537, step time: 1.5337\n",
      "225/388, train_loss: 0.5754, step time: 1.5337\n",
      "226/388, train_loss: 0.3979, step time: 1.5444\n",
      "227/388, train_loss: 0.3076, step time: 1.5382\n",
      "228/388, train_loss: 0.7393, step time: 1.5356\n",
      "229/388, train_loss: 0.7694, step time: 1.5320\n",
      "230/388, train_loss: 0.5860, step time: 1.5506\n",
      "231/388, train_loss: 0.4960, step time: 1.5411\n",
      "232/388, train_loss: 0.8022, step time: 1.5321\n",
      "233/388, train_loss: 0.4664, step time: 1.5375\n",
      "234/388, train_loss: 0.3356, step time: 1.5475\n",
      "235/388, train_loss: 0.2883, step time: 1.5350\n",
      "236/388, train_loss: 0.4981, step time: 1.5326\n",
      "237/388, train_loss: 0.6374, step time: 1.5292\n",
      "238/388, train_loss: 0.3688, step time: 1.5495\n",
      "239/388, train_loss: 0.4555, step time: 1.5367\n",
      "240/388, train_loss: 0.3249, step time: 1.5371\n",
      "241/388, train_loss: 0.9103, step time: 1.5327\n",
      "242/388, train_loss: 0.4458, step time: 1.5451\n",
      "243/388, train_loss: 0.7849, step time: 1.5321\n",
      "244/388, train_loss: 0.3585, step time: 1.5329\n",
      "245/388, train_loss: 0.7552, step time: 1.5332\n",
      "246/388, train_loss: 0.4303, step time: 1.5427\n",
      "247/388, train_loss: 0.6606, step time: 1.5339\n",
      "248/388, train_loss: 0.1949, step time: 1.5426\n",
      "249/388, train_loss: 0.4130, step time: 1.5367\n",
      "250/388, train_loss: 0.4004, step time: 1.5427\n",
      "251/388, train_loss: 0.5278, step time: 1.5485\n",
      "252/388, train_loss: 0.4783, step time: 1.5377\n",
      "253/388, train_loss: 0.3689, step time: 1.5373\n",
      "254/388, train_loss: 0.3735, step time: 1.5527\n",
      "255/388, train_loss: 0.3538, step time: 1.5396\n",
      "256/388, train_loss: 0.7893, step time: 1.5373\n",
      "257/388, train_loss: 0.3159, step time: 1.5363\n",
      "258/388, train_loss: 0.3421, step time: 1.5462\n",
      "259/388, train_loss: 0.5060, step time: 1.5338\n",
      "260/388, train_loss: 0.5653, step time: 1.5363\n",
      "261/388, train_loss: 0.4768, step time: 1.5338\n",
      "262/388, train_loss: 0.5438, step time: 1.5424\n",
      "263/388, train_loss: 0.3931, step time: 1.5347\n",
      "264/388, train_loss: 0.5241, step time: 1.5381\n",
      "265/388, train_loss: 0.4496, step time: 1.5376\n",
      "266/388, train_loss: 0.8740, step time: 1.5473\n",
      "267/388, train_loss: 0.4349, step time: 1.5311\n",
      "268/388, train_loss: 0.2623, step time: 1.5304\n",
      "269/388, train_loss: 0.5033, step time: 1.5609\n",
      "270/388, train_loss: 0.4608, step time: 1.5462\n",
      "271/388, train_loss: 0.3796, step time: 1.5359\n",
      "272/388, train_loss: 0.2211, step time: 1.5321\n",
      "273/388, train_loss: 0.6840, step time: 1.5355\n",
      "274/388, train_loss: 0.5596, step time: 1.5446\n",
      "275/388, train_loss: 0.5683, step time: 1.5451\n",
      "276/388, train_loss: 0.3942, step time: 1.5360\n",
      "277/388, train_loss: 0.5404, step time: 1.5361\n",
      "278/388, train_loss: 0.3481, step time: 1.5481\n",
      "279/388, train_loss: 0.3537, step time: 1.5346\n",
      "280/388, train_loss: 0.3742, step time: 1.5335\n",
      "281/388, train_loss: 0.2551, step time: 1.5394\n",
      "282/388, train_loss: 0.3982, step time: 1.5493\n",
      "283/388, train_loss: 0.2815, step time: 1.5329\n",
      "284/388, train_loss: 0.3352, step time: 1.5303\n",
      "285/388, train_loss: 0.3866, step time: 1.5358\n",
      "286/388, train_loss: 0.9253, step time: 1.5483\n",
      "287/388, train_loss: 0.3654, step time: 1.5342\n",
      "288/388, train_loss: 0.4798, step time: 1.5347\n",
      "289/388, train_loss: 0.6640, step time: 1.5354\n",
      "290/388, train_loss: 0.5998, step time: 1.5556\n",
      "291/388, train_loss: 0.7945, step time: 1.5397\n",
      "292/388, train_loss: 0.4239, step time: 1.5344\n",
      "293/388, train_loss: 0.6202, step time: 1.5389\n",
      "294/388, train_loss: 0.3643, step time: 1.5498\n",
      "295/388, train_loss: 0.4463, step time: 1.5322\n",
      "296/388, train_loss: 0.4208, step time: 1.5342\n",
      "297/388, train_loss: 0.6214, step time: 1.5347\n",
      "298/388, train_loss: 0.6539, step time: 1.5456\n",
      "299/388, train_loss: 0.5406, step time: 1.5360\n",
      "300/388, train_loss: 0.3632, step time: 1.5398\n",
      "301/388, train_loss: 0.5180, step time: 1.5301\n",
      "302/388, train_loss: 0.3335, step time: 1.5489\n",
      "303/388, train_loss: 0.5697, step time: 1.5352\n",
      "304/388, train_loss: 0.3539, step time: 1.5382\n",
      "305/388, train_loss: 0.4248, step time: 1.5312\n",
      "306/388, train_loss: 0.3376, step time: 1.5435\n",
      "307/388, train_loss: 0.6130, step time: 1.5346\n",
      "308/388, train_loss: 0.2913, step time: 1.5370\n",
      "309/388, train_loss: 0.4558, step time: 1.5377\n",
      "310/388, train_loss: 0.6682, step time: 1.5484\n",
      "311/388, train_loss: 0.6340, step time: 1.5324\n",
      "312/388, train_loss: 0.4424, step time: 1.5350\n",
      "313/388, train_loss: 0.5103, step time: 1.5368\n",
      "314/388, train_loss: 0.2817, step time: 1.5517\n",
      "315/388, train_loss: 0.3660, step time: 1.5309\n",
      "316/388, train_loss: 0.3436, step time: 1.5339\n",
      "317/388, train_loss: 0.3854, step time: 1.5338\n",
      "318/388, train_loss: 0.6649, step time: 1.5539\n",
      "319/388, train_loss: 0.6507, step time: 1.5340\n",
      "320/388, train_loss: 0.4828, step time: 1.5364\n",
      "321/388, train_loss: 0.3035, step time: 1.5360\n",
      "322/388, train_loss: 0.4279, step time: 1.5494\n",
      "323/388, train_loss: 0.3166, step time: 1.5350\n",
      "324/388, train_loss: 0.4769, step time: 1.5322\n",
      "325/388, train_loss: 0.6588, step time: 1.5330\n",
      "326/388, train_loss: 0.3257, step time: 1.5421\n",
      "327/388, train_loss: 0.2999, step time: 1.5299\n",
      "328/388, train_loss: 0.4105, step time: 1.5365\n",
      "329/388, train_loss: 0.3028, step time: 1.5393\n",
      "330/388, train_loss: 0.6160, step time: 1.5497\n",
      "331/388, train_loss: 0.3961, step time: 1.5313\n",
      "332/388, train_loss: 0.6331, step time: 1.5373\n",
      "333/388, train_loss: 0.8878, step time: 1.5499\n",
      "334/388, train_loss: 0.4363, step time: 1.5458\n",
      "335/388, train_loss: 0.5643, step time: 1.5312\n",
      "336/388, train_loss: 0.8568, step time: 1.5308\n",
      "337/388, train_loss: 0.3892, step time: 1.5369\n",
      "338/388, train_loss: 0.3292, step time: 1.5473\n",
      "339/388, train_loss: 0.4995, step time: 1.5316\n",
      "340/388, train_loss: 0.4000, step time: 1.5324\n",
      "341/388, train_loss: 0.6969, step time: 1.5370\n",
      "342/388, train_loss: 0.4149, step time: 1.5510\n",
      "343/388, train_loss: 0.4486, step time: 1.5332\n",
      "344/388, train_loss: 0.4025, step time: 1.5315\n",
      "345/388, train_loss: 0.4228, step time: 1.5362\n",
      "346/388, train_loss: 0.4076, step time: 1.5538\n",
      "347/388, train_loss: 0.2203, step time: 1.5366\n",
      "348/388, train_loss: 0.6304, step time: 1.5361\n",
      "349/388, train_loss: 0.5468, step time: 1.5327\n",
      "350/388, train_loss: 0.2066, step time: 1.5454\n",
      "351/388, train_loss: 0.5763, step time: 1.5374\n",
      "352/388, train_loss: 0.4868, step time: 1.5318\n",
      "353/388, train_loss: 0.1775, step time: 1.5311\n",
      "354/388, train_loss: 0.3679, step time: 1.5441\n",
      "355/388, train_loss: 0.2654, step time: 1.5330\n",
      "356/388, train_loss: 0.2043, step time: 1.5316\n",
      "357/388, train_loss: 0.6179, step time: 1.5343\n",
      "358/388, train_loss: 0.5915, step time: 1.5457\n",
      "359/388, train_loss: 0.5739, step time: 1.5388\n",
      "360/388, train_loss: 0.4612, step time: 1.5331\n",
      "361/388, train_loss: 0.3739, step time: 1.5344\n",
      "362/388, train_loss: 0.2924, step time: 1.5474\n",
      "363/388, train_loss: 0.3371, step time: 1.5316\n",
      "364/388, train_loss: 0.1645, step time: 1.5339\n",
      "365/388, train_loss: 0.9097, step time: 1.5371\n",
      "366/388, train_loss: 0.7790, step time: 1.5488\n",
      "367/388, train_loss: 0.4161, step time: 1.5304\n",
      "368/388, train_loss: 0.8829, step time: 1.5337\n",
      "369/388, train_loss: 0.6049, step time: 1.5313\n",
      "370/388, train_loss: 0.6459, step time: 1.5481\n",
      "371/388, train_loss: 0.4733, step time: 1.5355\n",
      "372/388, train_loss: 0.3716, step time: 1.5360\n",
      "373/388, train_loss: 0.5581, step time: 1.5312\n",
      "374/388, train_loss: 0.6099, step time: 1.5482\n",
      "375/388, train_loss: 0.3405, step time: 1.5321\n",
      "376/388, train_loss: 0.5521, step time: 1.5452\n",
      "377/388, train_loss: 0.5109, step time: 1.5341\n",
      "378/388, train_loss: 0.5710, step time: 1.5411\n",
      "379/388, train_loss: 0.5608, step time: 1.5345\n",
      "380/388, train_loss: 0.7613, step time: 1.5489\n",
      "381/388, train_loss: 0.4832, step time: 1.5347\n",
      "382/388, train_loss: 0.2597, step time: 1.5539\n",
      "383/388, train_loss: 0.2989, step time: 1.5348\n",
      "384/388, train_loss: 0.2789, step time: 1.5364\n",
      "385/388, train_loss: 0.6536, step time: 1.5390\n",
      "386/388, train_loss: 0.4797, step time: 1.5571\n",
      "387/388, train_loss: 0.3753, step time: 1.5354\n",
      "388/388, train_loss: 0.6355, step time: 1.5380\n",
      "epoch 5 average loss: 0.5160\n",
      "current epoch: 5 current mean dice: 0.5712 tc: 0.6076 wt: 0.7652 et: 0.3409\n",
      "best mean dice: 0.6058 at epoch: 4\n",
      "time consuming of epoch 5 is: 705.2117\n",
      "----------\n",
      "epoch 6/100\n",
      "1/388, train_loss: 0.5300, step time: 1.5553\n",
      "2/388, train_loss: 0.5937, step time: 1.5314\n",
      "3/388, train_loss: 0.5320, step time: 1.5306\n",
      "4/388, train_loss: 0.5529, step time: 1.5287\n",
      "5/388, train_loss: 0.8554, step time: 1.5330\n",
      "6/388, train_loss: 0.7631, step time: 1.5330\n",
      "7/388, train_loss: 0.3382, step time: 1.5344\n",
      "8/388, train_loss: 0.3835, step time: 1.5471\n",
      "9/388, train_loss: 0.6608, step time: 1.5320\n",
      "10/388, train_loss: 0.3262, step time: 1.5324\n",
      "11/388, train_loss: 0.5203, step time: 1.5306\n",
      "12/388, train_loss: 0.7051, step time: 1.5318\n",
      "13/388, train_loss: 0.2524, step time: 1.5351\n",
      "14/388, train_loss: 0.4434, step time: 1.5354\n",
      "15/388, train_loss: 0.3674, step time: 1.5331\n",
      "16/388, train_loss: 0.8832, step time: 1.5338\n",
      "17/388, train_loss: 0.6494, step time: 1.5314\n",
      "18/388, train_loss: 0.8481, step time: 1.5343\n",
      "19/388, train_loss: 0.2611, step time: 1.5342\n",
      "20/388, train_loss: 0.9109, step time: 1.5339\n",
      "21/388, train_loss: 0.2794, step time: 1.5353\n",
      "22/388, train_loss: 0.2924, step time: 1.5346\n",
      "23/388, train_loss: 0.3074, step time: 1.5350\n",
      "24/388, train_loss: 0.3617, step time: 1.5336\n",
      "25/388, train_loss: 0.5343, step time: 1.5533\n",
      "26/388, train_loss: 0.5726, step time: 1.5357\n",
      "27/388, train_loss: 0.3696, step time: 1.5318\n",
      "28/388, train_loss: 0.2230, step time: 1.5353\n",
      "29/388, train_loss: 0.3146, step time: 1.5328\n",
      "30/388, train_loss: 0.4474, step time: 1.5329\n",
      "31/388, train_loss: 0.4943, step time: 1.5360\n",
      "32/388, train_loss: 0.4753, step time: 1.5337\n",
      "33/388, train_loss: 0.4534, step time: 1.5330\n",
      "34/388, train_loss: 0.4026, step time: 1.5333\n",
      "35/388, train_loss: 0.3820, step time: 1.5302\n",
      "36/388, train_loss: 0.4045, step time: 1.5343\n",
      "37/388, train_loss: 0.3544, step time: 1.5456\n",
      "38/388, train_loss: 0.7486, step time: 1.5490\n",
      "39/388, train_loss: 0.4674, step time: 1.5365\n",
      "40/388, train_loss: 0.3118, step time: 1.5355\n",
      "41/388, train_loss: 0.3396, step time: 1.5300\n",
      "42/388, train_loss: 0.3465, step time: 1.5395\n",
      "43/388, train_loss: 0.3522, step time: 1.5353\n",
      "44/388, train_loss: 0.6759, step time: 1.5475\n",
      "45/388, train_loss: 0.4111, step time: 1.5328\n",
      "46/388, train_loss: 0.2683, step time: 1.5318\n",
      "47/388, train_loss: 0.4335, step time: 1.5352\n",
      "48/388, train_loss: 0.3485, step time: 1.5500\n",
      "49/388, train_loss: 0.3320, step time: 1.5302\n",
      "50/388, train_loss: 0.3173, step time: 1.5326\n",
      "51/388, train_loss: 0.3235, step time: 1.5424\n",
      "52/388, train_loss: 0.3807, step time: 1.5453\n",
      "53/388, train_loss: 0.3755, step time: 1.5378\n",
      "54/388, train_loss: 0.4406, step time: 1.5354\n",
      "55/388, train_loss: 0.1891, step time: 1.5336\n",
      "56/388, train_loss: 0.3851, step time: 1.5432\n",
      "57/388, train_loss: 0.1532, step time: 1.5329\n",
      "58/388, train_loss: 0.4022, step time: 1.5377\n",
      "59/388, train_loss: 0.3944, step time: 1.5346\n",
      "60/388, train_loss: 0.4162, step time: 1.5420\n",
      "61/388, train_loss: 0.2741, step time: 1.5377\n",
      "62/388, train_loss: 0.2791, step time: 1.5389\n",
      "63/388, train_loss: 0.3316, step time: 1.5356\n",
      "64/388, train_loss: 0.4029, step time: 1.5421\n",
      "65/388, train_loss: 0.4278, step time: 1.5358\n",
      "66/388, train_loss: 0.3954, step time: 1.5365\n",
      "67/388, train_loss: 0.8381, step time: 1.5364\n",
      "68/388, train_loss: 0.3233, step time: 1.5398\n",
      "69/388, train_loss: 0.5570, step time: 1.5333\n",
      "70/388, train_loss: 0.4045, step time: 1.5386\n",
      "71/388, train_loss: 0.2465, step time: 1.5354\n",
      "72/388, train_loss: 0.4422, step time: 1.5434\n",
      "73/388, train_loss: 0.5012, step time: 1.5301\n",
      "74/388, train_loss: 0.4504, step time: 1.5628\n",
      "75/388, train_loss: 0.3940, step time: 1.5381\n",
      "76/388, train_loss: 0.3836, step time: 1.5421\n",
      "77/388, train_loss: 0.7318, step time: 1.5342\n",
      "78/388, train_loss: 0.4592, step time: 1.5404\n",
      "79/388, train_loss: 0.5601, step time: 1.5388\n",
      "80/388, train_loss: 0.3285, step time: 1.5449\n",
      "81/388, train_loss: 0.3759, step time: 1.5340\n",
      "82/388, train_loss: 0.3091, step time: 1.5294\n",
      "83/388, train_loss: 0.3351, step time: 1.5337\n",
      "84/388, train_loss: 0.3408, step time: 1.5436\n",
      "85/388, train_loss: 0.2562, step time: 1.5303\n",
      "86/388, train_loss: 0.8736, step time: 1.5343\n",
      "87/388, train_loss: 0.8297, step time: 1.5299\n",
      "88/388, train_loss: 0.5616, step time: 1.5471\n",
      "89/388, train_loss: 0.3079, step time: 1.5373\n",
      "90/388, train_loss: 0.3519, step time: 1.5293\n",
      "91/388, train_loss: 0.4541, step time: 1.5301\n",
      "92/388, train_loss: 0.7053, step time: 1.5451\n",
      "93/388, train_loss: 0.3193, step time: 1.5461\n",
      "94/388, train_loss: 0.3870, step time: 1.5311\n",
      "95/388, train_loss: 0.5958, step time: 1.5322\n",
      "96/388, train_loss: 0.4033, step time: 1.5466\n",
      "97/388, train_loss: 0.6444, step time: 1.5357\n",
      "98/388, train_loss: 0.2362, step time: 1.5332\n",
      "99/388, train_loss: 0.4036, step time: 1.5332\n",
      "100/388, train_loss: 0.5006, step time: 1.5468\n",
      "101/388, train_loss: 0.5639, step time: 1.5344\n",
      "102/388, train_loss: 0.2895, step time: 1.5317\n",
      "103/388, train_loss: 0.4293, step time: 1.5340\n",
      "104/388, train_loss: 0.2625, step time: 1.5419\n",
      "105/388, train_loss: 0.6476, step time: 1.5344\n",
      "106/388, train_loss: 0.7882, step time: 1.5342\n",
      "107/388, train_loss: 0.3541, step time: 1.5361\n",
      "108/388, train_loss: 0.6343, step time: 1.5485\n",
      "109/388, train_loss: 0.3835, step time: 1.5331\n",
      "110/388, train_loss: 0.7432, step time: 1.5637\n",
      "111/388, train_loss: 0.4170, step time: 1.5320\n",
      "112/388, train_loss: 0.4205, step time: 1.5443\n",
      "113/388, train_loss: 0.6217, step time: 1.5367\n",
      "114/388, train_loss: 0.3282, step time: 1.5338\n",
      "115/388, train_loss: 0.7426, step time: 1.5433\n",
      "116/388, train_loss: 0.6582, step time: 1.5450\n",
      "117/388, train_loss: 0.6662, step time: 1.5380\n",
      "118/388, train_loss: 0.6833, step time: 1.5348\n",
      "119/388, train_loss: 0.2965, step time: 1.5360\n",
      "120/388, train_loss: 0.3718, step time: 1.5440\n",
      "121/388, train_loss: 0.3914, step time: 1.5352\n",
      "122/388, train_loss: 0.5075, step time: 1.5320\n",
      "123/388, train_loss: 0.3331, step time: 1.5336\n",
      "124/388, train_loss: 0.7756, step time: 1.5491\n",
      "125/388, train_loss: 0.5265, step time: 1.5288\n",
      "126/388, train_loss: 0.6885, step time: 1.5304\n",
      "127/388, train_loss: 0.3826, step time: 1.5321\n",
      "128/388, train_loss: 0.6374, step time: 1.5479\n",
      "129/388, train_loss: 0.3055, step time: 1.5340\n",
      "130/388, train_loss: 0.4522, step time: 1.5314\n",
      "131/388, train_loss: 0.5886, step time: 1.5332\n",
      "132/388, train_loss: 0.2454, step time: 1.5419\n",
      "133/388, train_loss: 0.4359, step time: 1.5304\n",
      "134/388, train_loss: 0.2898, step time: 1.5302\n",
      "135/388, train_loss: 0.6847, step time: 1.5317\n",
      "136/388, train_loss: 0.3232, step time: 1.5489\n",
      "137/388, train_loss: 0.4186, step time: 1.5341\n",
      "138/388, train_loss: 0.5309, step time: 1.5311\n",
      "139/388, train_loss: 0.1801, step time: 1.5549\n",
      "140/388, train_loss: 0.4545, step time: 1.5463\n",
      "141/388, train_loss: 0.2473, step time: 1.5340\n",
      "142/388, train_loss: 0.4539, step time: 1.5354\n",
      "143/388, train_loss: 0.5429, step time: 1.5328\n",
      "144/388, train_loss: 0.3891, step time: 1.5446\n",
      "145/388, train_loss: 0.4787, step time: 1.5319\n",
      "146/388, train_loss: 0.4671, step time: 1.5344\n",
      "147/388, train_loss: 0.4439, step time: 1.5347\n",
      "148/388, train_loss: 0.5693, step time: 1.5424\n",
      "149/388, train_loss: 0.1750, step time: 1.5303\n",
      "150/388, train_loss: 0.2262, step time: 1.5312\n",
      "151/388, train_loss: 0.5545, step time: 1.5338\n",
      "152/388, train_loss: 0.3471, step time: 1.5494\n",
      "153/388, train_loss: 0.3910, step time: 1.5345\n",
      "154/388, train_loss: 0.2534, step time: 1.5342\n",
      "155/388, train_loss: 0.4097, step time: 1.5317\n",
      "156/388, train_loss: 0.3949, step time: 1.5298\n",
      "157/388, train_loss: 0.4061, step time: 1.5301\n",
      "158/388, train_loss: 0.3895, step time: 1.5363\n",
      "159/388, train_loss: 0.5876, step time: 1.5384\n",
      "160/388, train_loss: 0.6461, step time: 1.5451\n",
      "161/388, train_loss: 0.3569, step time: 1.5321\n",
      "162/388, train_loss: 0.1753, step time: 1.5317\n",
      "163/388, train_loss: 0.4701, step time: 1.5311\n",
      "164/388, train_loss: 0.3517, step time: 1.5477\n",
      "165/388, train_loss: 0.4043, step time: 1.5354\n",
      "166/388, train_loss: 0.8203, step time: 1.5337\n",
      "167/388, train_loss: 0.4567, step time: 1.5326\n",
      "168/388, train_loss: 0.5016, step time: 1.5429\n",
      "169/388, train_loss: 0.6904, step time: 1.5320\n",
      "170/388, train_loss: 0.3850, step time: 1.5397\n",
      "171/388, train_loss: 0.2617, step time: 1.5415\n",
      "172/388, train_loss: 0.1580, step time: 1.5483\n",
      "173/388, train_loss: 0.6051, step time: 1.5302\n",
      "174/388, train_loss: 0.2259, step time: 1.5317\n",
      "175/388, train_loss: 0.2769, step time: 1.5308\n",
      "176/388, train_loss: 0.4583, step time: 1.5483\n",
      "177/388, train_loss: 0.3235, step time: 1.5355\n",
      "178/388, train_loss: 0.3300, step time: 1.5354\n",
      "179/388, train_loss: 0.1786, step time: 1.5318\n",
      "180/388, train_loss: 0.3978, step time: 1.5466\n",
      "181/388, train_loss: 0.3898, step time: 1.5375\n",
      "182/388, train_loss: 0.5068, step time: 1.5373\n",
      "183/388, train_loss: 0.5296, step time: 1.5345\n",
      "184/388, train_loss: 0.3004, step time: 1.5435\n",
      "185/388, train_loss: 0.2575, step time: 1.5333\n",
      "186/388, train_loss: 0.4614, step time: 1.5446\n",
      "187/388, train_loss: 0.3381, step time: 1.5334\n",
      "188/388, train_loss: 0.4016, step time: 1.5410\n",
      "189/388, train_loss: 0.5069, step time: 1.5330\n",
      "190/388, train_loss: 0.4810, step time: 1.5444\n",
      "191/388, train_loss: 0.2339, step time: 1.5648\n",
      "192/388, train_loss: 0.3173, step time: 1.5496\n",
      "193/388, train_loss: 0.2961, step time: 1.5331\n",
      "194/388, train_loss: 0.4971, step time: 1.5305\n",
      "195/388, train_loss: 0.4529, step time: 1.5356\n",
      "196/388, train_loss: 0.4829, step time: 1.5477\n",
      "197/388, train_loss: 0.4063, step time: 1.5419\n",
      "198/388, train_loss: 0.5580, step time: 1.5338\n",
      "199/388, train_loss: 0.2436, step time: 1.5327\n",
      "200/388, train_loss: 0.6982, step time: 1.5444\n",
      "201/388, train_loss: 0.6240, step time: 1.5375\n",
      "202/388, train_loss: 0.3401, step time: 1.5377\n",
      "203/388, train_loss: 0.2685, step time: 1.5360\n",
      "204/388, train_loss: 0.3640, step time: 1.5447\n",
      "205/388, train_loss: 0.3235, step time: 1.5328\n",
      "206/388, train_loss: 0.3244, step time: 1.5384\n",
      "207/388, train_loss: 0.4140, step time: 1.5371\n",
      "208/388, train_loss: 0.4445, step time: 1.5454\n",
      "209/388, train_loss: 0.1388, step time: 1.5320\n",
      "210/388, train_loss: 0.1181, step time: 1.5338\n",
      "211/388, train_loss: 0.3013, step time: 1.5342\n",
      "212/388, train_loss: 0.3652, step time: 1.5529\n",
      "213/388, train_loss: 0.2712, step time: 1.5345\n",
      "214/388, train_loss: 0.4574, step time: 1.5318\n",
      "215/388, train_loss: 0.8445, step time: 1.5361\n",
      "216/388, train_loss: 0.6323, step time: 1.5463\n",
      "217/388, train_loss: 0.3964, step time: 1.5368\n",
      "218/388, train_loss: 0.8301, step time: 1.5350\n",
      "219/388, train_loss: 0.2018, step time: 1.5339\n",
      "220/388, train_loss: 0.4237, step time: 1.5529\n",
      "221/388, train_loss: 0.4079, step time: 1.5368\n",
      "222/388, train_loss: 0.4854, step time: 1.5351\n",
      "223/388, train_loss: 0.3095, step time: 1.5351\n",
      "224/388, train_loss: 0.3928, step time: 1.5484\n",
      "225/388, train_loss: 0.6006, step time: 1.5376\n",
      "226/388, train_loss: 0.4354, step time: 1.5361\n",
      "227/388, train_loss: 0.5702, step time: 1.5359\n",
      "228/388, train_loss: 0.3332, step time: 1.5445\n",
      "229/388, train_loss: 0.5697, step time: 1.5358\n",
      "230/388, train_loss: 0.3709, step time: 1.5413\n",
      "231/388, train_loss: 0.2797, step time: 1.5373\n",
      "232/388, train_loss: 0.4043, step time: 1.5445\n",
      "233/388, train_loss: 0.3251, step time: 1.5320\n",
      "234/388, train_loss: 0.2384, step time: 1.5336\n",
      "235/388, train_loss: 0.2906, step time: 1.5345\n",
      "236/388, train_loss: 0.3033, step time: 1.5487\n",
      "237/388, train_loss: 0.2891, step time: 1.5318\n",
      "238/388, train_loss: 0.3786, step time: 1.5320\n",
      "239/388, train_loss: 0.5893, step time: 1.5355\n",
      "240/388, train_loss: 0.3609, step time: 1.5471\n",
      "241/388, train_loss: 0.4407, step time: 1.5367\n",
      "242/388, train_loss: 0.4314, step time: 1.5342\n",
      "243/388, train_loss: 0.4469, step time: 1.5323\n",
      "244/388, train_loss: 0.3939, step time: 1.5442\n",
      "245/388, train_loss: 0.3082, step time: 1.5365\n",
      "246/388, train_loss: 0.3911, step time: 1.5369\n",
      "247/388, train_loss: 0.2126, step time: 1.5355\n",
      "248/388, train_loss: 0.3492, step time: 1.5424\n",
      "249/388, train_loss: 0.2937, step time: 1.5340\n",
      "250/388, train_loss: 0.5314, step time: 1.5357\n",
      "251/388, train_loss: 0.3278, step time: 1.5349\n",
      "252/388, train_loss: 0.3229, step time: 1.5507\n",
      "253/388, train_loss: 0.2736, step time: 1.5339\n",
      "254/388, train_loss: 0.4002, step time: 1.5355\n",
      "255/388, train_loss: 0.4433, step time: 1.5581\n",
      "256/388, train_loss: 0.2953, step time: 1.5463\n",
      "257/388, train_loss: 0.5304, step time: 1.5327\n",
      "258/388, train_loss: 0.6518, step time: 1.5477\n",
      "259/388, train_loss: 0.3915, step time: 1.5304\n",
      "260/388, train_loss: 0.2779, step time: 1.5449\n",
      "261/388, train_loss: 0.3006, step time: 1.5360\n",
      "262/388, train_loss: 0.4118, step time: 1.5345\n",
      "263/388, train_loss: 0.3783, step time: 1.5332\n",
      "264/388, train_loss: 0.3094, step time: 1.5481\n",
      "265/388, train_loss: 0.3928, step time: 1.5380\n",
      "266/388, train_loss: 0.2703, step time: 1.5332\n",
      "267/388, train_loss: 0.3288, step time: 1.5294\n",
      "268/388, train_loss: 0.6727, step time: 1.5441\n",
      "269/388, train_loss: 0.3042, step time: 1.5328\n",
      "270/388, train_loss: 0.4652, step time: 1.5410\n",
      "271/388, train_loss: 0.6071, step time: 1.5341\n",
      "272/388, train_loss: 0.3563, step time: 1.5446\n",
      "273/388, train_loss: 0.6190, step time: 1.5371\n",
      "274/388, train_loss: 0.3778, step time: 1.5397\n",
      "275/388, train_loss: 0.2460, step time: 1.5362\n",
      "276/388, train_loss: 0.5813, step time: 1.5483\n",
      "277/388, train_loss: 0.3450, step time: 1.5333\n",
      "278/388, train_loss: 0.4740, step time: 1.5352\n",
      "279/388, train_loss: 0.4021, step time: 1.5339\n",
      "280/388, train_loss: 0.4664, step time: 1.5456\n",
      "281/388, train_loss: 0.4393, step time: 1.5346\n",
      "282/388, train_loss: 0.3069, step time: 1.5339\n",
      "283/388, train_loss: 0.2830, step time: 1.5332\n",
      "284/388, train_loss: 0.7124, step time: 1.5581\n",
      "285/388, train_loss: 0.5253, step time: 1.5376\n",
      "286/388, train_loss: 0.6293, step time: 1.5332\n",
      "287/388, train_loss: 0.4943, step time: 1.5330\n",
      "288/388, train_loss: 0.2847, step time: 1.5497\n",
      "289/388, train_loss: 0.4569, step time: 1.5359\n",
      "290/388, train_loss: 0.7980, step time: 1.5364\n",
      "291/388, train_loss: 0.5232, step time: 1.5339\n",
      "292/388, train_loss: 0.4024, step time: 1.5505\n",
      "293/388, train_loss: 0.3530, step time: 1.5383\n",
      "294/388, train_loss: 0.3593, step time: 1.5346\n",
      "295/388, train_loss: 0.5768, step time: 1.5340\n",
      "296/388, train_loss: 0.2210, step time: 1.5462\n",
      "297/388, train_loss: 0.6986, step time: 1.5369\n",
      "298/388, train_loss: 0.3801, step time: 1.5364\n",
      "299/388, train_loss: 0.3408, step time: 1.5344\n",
      "300/388, train_loss: 0.5135, step time: 1.5475\n",
      "301/388, train_loss: 0.4738, step time: 1.5402\n",
      "302/388, train_loss: 0.3271, step time: 1.5366\n",
      "303/388, train_loss: 0.3184, step time: 1.5346\n",
      "304/388, train_loss: 0.3482, step time: 1.5416\n",
      "305/388, train_loss: 0.3481, step time: 1.5324\n",
      "306/388, train_loss: 0.3279, step time: 1.5358\n",
      "307/388, train_loss: 0.3488, step time: 1.5362\n",
      "308/388, train_loss: 0.3254, step time: 1.5465\n",
      "309/388, train_loss: 0.3896, step time: 1.5307\n",
      "310/388, train_loss: 0.4341, step time: 1.5336\n",
      "311/388, train_loss: 0.3688, step time: 1.5328\n",
      "312/388, train_loss: 0.8657, step time: 1.5476\n",
      "313/388, train_loss: 0.2654, step time: 1.5337\n",
      "314/388, train_loss: 0.7208, step time: 1.5464\n",
      "315/388, train_loss: 0.2136, step time: 1.5351\n",
      "316/388, train_loss: 0.3205, step time: 1.5453\n",
      "317/388, train_loss: 0.3313, step time: 1.5383\n",
      "318/388, train_loss: 0.4605, step time: 1.5385\n",
      "319/388, train_loss: 0.2716, step time: 1.5373\n",
      "320/388, train_loss: 0.4196, step time: 1.5470\n",
      "321/388, train_loss: 0.5684, step time: 1.5350\n",
      "322/388, train_loss: 0.1469, step time: 1.5380\n",
      "323/388, train_loss: 0.4257, step time: 1.5351\n",
      "324/388, train_loss: 0.2252, step time: 1.5441\n",
      "325/388, train_loss: 0.3783, step time: 1.5326\n",
      "326/388, train_loss: 0.1764, step time: 1.5392\n",
      "327/388, train_loss: 0.5562, step time: 1.5379\n",
      "328/388, train_loss: 0.3005, step time: 1.5488\n",
      "329/388, train_loss: 0.3883, step time: 1.5350\n",
      "330/388, train_loss: 0.3796, step time: 1.5364\n",
      "331/388, train_loss: 0.4141, step time: 1.5376\n",
      "332/388, train_loss: 0.3237, step time: 1.5467\n",
      "333/388, train_loss: 0.4515, step time: 1.5341\n",
      "334/388, train_loss: 0.3185, step time: 1.5607\n",
      "335/388, train_loss: 0.2774, step time: 1.5347\n",
      "336/388, train_loss: 0.5658, step time: 1.5453\n",
      "337/388, train_loss: 0.4144, step time: 1.5339\n",
      "338/388, train_loss: 0.2356, step time: 1.5415\n",
      "339/388, train_loss: 0.3764, step time: 1.5335\n",
      "340/388, train_loss: 0.3855, step time: 1.5416\n",
      "341/388, train_loss: 0.4570, step time: 1.5342\n",
      "342/388, train_loss: 0.2951, step time: 1.5416\n",
      "343/388, train_loss: 0.2235, step time: 1.5381\n",
      "344/388, train_loss: 0.4740, step time: 1.5460\n",
      "345/388, train_loss: 0.7950, step time: 1.5316\n",
      "346/388, train_loss: 0.4988, step time: 1.5367\n",
      "347/388, train_loss: 0.5143, step time: 1.5365\n",
      "348/388, train_loss: 0.3370, step time: 1.5485\n",
      "349/388, train_loss: 0.3297, step time: 1.5331\n",
      "350/388, train_loss: 0.7813, step time: 1.5307\n",
      "351/388, train_loss: 0.3630, step time: 1.5337\n",
      "352/388, train_loss: 0.3859, step time: 1.5481\n",
      "353/388, train_loss: 0.2983, step time: 1.5394\n",
      "354/388, train_loss: 0.3387, step time: 1.5307\n",
      "355/388, train_loss: 0.3566, step time: 1.5333\n",
      "356/388, train_loss: 0.4132, step time: 1.5514\n",
      "357/388, train_loss: 0.4025, step time: 1.5368\n",
      "358/388, train_loss: 0.4272, step time: 1.5352\n",
      "359/388, train_loss: 0.6206, step time: 1.5375\n",
      "360/388, train_loss: 0.6939, step time: 1.5522\n",
      "361/388, train_loss: 0.7132, step time: 1.5386\n",
      "362/388, train_loss: 0.2762, step time: 1.5357\n",
      "363/388, train_loss: 0.3142, step time: 1.5397\n",
      "364/388, train_loss: 0.6760, step time: 1.5451\n",
      "365/388, train_loss: 0.3728, step time: 1.5312\n",
      "366/388, train_loss: 0.3293, step time: 1.5349\n",
      "367/388, train_loss: 0.3359, step time: 1.5366\n",
      "368/388, train_loss: 0.3664, step time: 1.5491\n",
      "369/388, train_loss: 0.3000, step time: 1.5400\n",
      "370/388, train_loss: 0.4410, step time: 1.5327\n",
      "371/388, train_loss: 0.7280, step time: 1.5420\n",
      "372/388, train_loss: 0.2526, step time: 1.5475\n",
      "373/388, train_loss: 0.4891, step time: 1.5339\n",
      "374/388, train_loss: 0.2680, step time: 1.5335\n",
      "375/388, train_loss: 0.4753, step time: 1.5348\n",
      "376/388, train_loss: 0.3431, step time: 1.5586\n",
      "377/388, train_loss: 0.2078, step time: 1.5349\n",
      "378/388, train_loss: 0.5824, step time: 1.5367\n",
      "379/388, train_loss: 0.3807, step time: 1.5371\n",
      "380/388, train_loss: 0.4011, step time: 1.5541\n",
      "381/388, train_loss: 0.5685, step time: 1.5356\n",
      "382/388, train_loss: 0.1671, step time: 1.5355\n",
      "383/388, train_loss: 0.3273, step time: 1.5357\n",
      "384/388, train_loss: 0.2531, step time: 1.5500\n",
      "385/388, train_loss: 0.3624, step time: 1.5333\n",
      "386/388, train_loss: 0.3259, step time: 1.5337\n",
      "387/388, train_loss: 0.4605, step time: 1.5348\n",
      "388/388, train_loss: 0.1934, step time: 1.5454\n",
      "epoch 6 average loss: 0.4259\n",
      "saved new best metric model\n",
      "current epoch: 6 current mean dice: 0.6195 tc: 0.6694 wt: 0.8072 et: 0.3818\n",
      "best mean dice: 0.6195 at epoch: 6\n",
      "time consuming of epoch 6 is: 704.5760\n",
      "----------\n",
      "epoch 7/100\n",
      "1/388, train_loss: 0.4839, step time: 1.5548\n",
      "2/388, train_loss: 0.3195, step time: 1.5381\n",
      "3/388, train_loss: 0.4127, step time: 1.5360\n",
      "4/388, train_loss: 0.4748, step time: 1.5401\n",
      "5/388, train_loss: 0.3668, step time: 1.5366\n",
      "6/388, train_loss: 0.3013, step time: 1.5345\n",
      "7/388, train_loss: 0.2378, step time: 1.5423\n",
      "8/388, train_loss: 0.5448, step time: 1.5371\n",
      "9/388, train_loss: 0.2659, step time: 1.5314\n",
      "10/388, train_loss: 0.2338, step time: 1.5338\n",
      "11/388, train_loss: 0.3105, step time: 1.5339\n",
      "12/388, train_loss: 0.4344, step time: 1.5317\n",
      "13/388, train_loss: 0.3447, step time: 1.5291\n",
      "14/388, train_loss: 0.4696, step time: 1.5306\n",
      "15/388, train_loss: 0.5922, step time: 1.5352\n",
      "16/388, train_loss: 0.2977, step time: 1.5350\n",
      "17/388, train_loss: 0.3864, step time: 1.5335\n",
      "18/388, train_loss: 0.4021, step time: 1.5327\n",
      "19/388, train_loss: 0.1929, step time: 1.5311\n",
      "20/388, train_loss: 0.3577, step time: 1.5312\n",
      "21/388, train_loss: 0.2834, step time: 1.5321\n",
      "22/388, train_loss: 0.5964, step time: 1.5362\n",
      "23/388, train_loss: 0.2138, step time: 1.5326\n",
      "24/388, train_loss: 0.2239, step time: 1.5371\n",
      "25/388, train_loss: 0.6129, step time: 1.5514\n",
      "26/388, train_loss: 0.3547, step time: 1.5433\n",
      "27/388, train_loss: 0.4139, step time: 1.5319\n",
      "28/388, train_loss: 0.3001, step time: 1.5338\n",
      "29/388, train_loss: 0.2245, step time: 1.5304\n",
      "30/388, train_loss: 0.2096, step time: 1.5366\n",
      "31/388, train_loss: 0.6192, step time: 1.5341\n",
      "32/388, train_loss: 0.3150, step time: 1.5336\n",
      "33/388, train_loss: 0.2082, step time: 1.5319\n",
      "34/388, train_loss: 0.3917, step time: 1.5333\n",
      "35/388, train_loss: 0.3435, step time: 1.5345\n",
      "36/388, train_loss: 0.1729, step time: 1.5377\n",
      "37/388, train_loss: 0.3234, step time: 1.5323\n",
      "38/388, train_loss: 0.5144, step time: 1.5316\n",
      "39/388, train_loss: 0.5249, step time: 1.5313\n",
      "40/388, train_loss: 0.7791, step time: 1.5335\n",
      "41/388, train_loss: 0.5809, step time: 1.5427\n",
      "42/388, train_loss: 0.3133, step time: 1.5368\n",
      "43/388, train_loss: 0.3442, step time: 1.5346\n",
      "44/388, train_loss: 0.3641, step time: 1.5338\n",
      "45/388, train_loss: 0.3154, step time: 1.5324\n",
      "46/388, train_loss: 0.4368, step time: 1.5352\n",
      "47/388, train_loss: 0.2247, step time: 1.5337\n",
      "48/388, train_loss: 0.3207, step time: 1.5357\n",
      "49/388, train_loss: 0.6070, step time: 1.5308\n",
      "50/388, train_loss: 0.3285, step time: 1.5320\n",
      "51/388, train_loss: 0.3197, step time: 1.5298\n",
      "52/388, train_loss: 0.2461, step time: 1.5362\n",
      "53/388, train_loss: 0.3465, step time: 1.5349\n",
      "54/388, train_loss: 0.5590, step time: 1.5351\n",
      "55/388, train_loss: 0.4209, step time: 1.5322\n",
      "56/388, train_loss: 0.3305, step time: 1.5316\n",
      "57/388, train_loss: 0.3143, step time: 1.5310\n",
      "58/388, train_loss: 0.3488, step time: 1.5304\n",
      "59/388, train_loss: 0.4669, step time: 1.5335\n",
      "60/388, train_loss: 0.2729, step time: 1.5345\n",
      "61/388, train_loss: 0.4834, step time: 1.5337\n",
      "62/388, train_loss: 0.3820, step time: 1.5371\n",
      "63/388, train_loss: 0.2298, step time: 1.5304\n",
      "64/388, train_loss: 0.4218, step time: 1.5328\n",
      "65/388, train_loss: 0.3995, step time: 1.5382\n",
      "66/388, train_loss: 0.3385, step time: 1.5368\n",
      "67/388, train_loss: 0.1341, step time: 1.5362\n",
      "68/388, train_loss: 0.4162, step time: 1.5339\n",
      "69/388, train_loss: 0.3824, step time: 1.5340\n",
      "70/388, train_loss: 0.4795, step time: 1.5327\n",
      "71/388, train_loss: 0.3660, step time: 1.5326\n",
      "72/388, train_loss: 0.2589, step time: 1.5361\n",
      "73/388, train_loss: 0.3153, step time: 1.5333\n",
      "74/388, train_loss: 0.3598, step time: 1.5329\n",
      "75/388, train_loss: 0.3327, step time: 1.5332\n",
      "76/388, train_loss: 0.3218, step time: 1.5338\n",
      "77/388, train_loss: 0.3174, step time: 1.5370\n",
      "78/388, train_loss: 0.3204, step time: 1.5345\n",
      "79/388, train_loss: 0.4531, step time: 1.5365\n",
      "80/388, train_loss: 0.3125, step time: 1.5298\n",
      "81/388, train_loss: 0.7167, step time: 1.5306\n",
      "82/388, train_loss: 0.6776, step time: 1.5309\n",
      "83/388, train_loss: 0.5549, step time: 1.5337\n",
      "84/388, train_loss: 0.3775, step time: 1.5328\n",
      "85/388, train_loss: 0.1474, step time: 1.5345\n",
      "86/388, train_loss: 0.3267, step time: 1.5336\n",
      "87/388, train_loss: 0.1425, step time: 1.5329\n",
      "88/388, train_loss: 0.2369, step time: 1.5331\n",
      "89/388, train_loss: 0.2995, step time: 1.5304\n",
      "90/388, train_loss: 0.2883, step time: 1.5311\n",
      "91/388, train_loss: 0.6096, step time: 1.5311\n",
      "92/388, train_loss: 0.3230, step time: 1.5371\n",
      "93/388, train_loss: 0.4143, step time: 1.5378\n",
      "94/388, train_loss: 0.4714, step time: 1.5324\n",
      "95/388, train_loss: 0.2682, step time: 1.5282\n",
      "96/388, train_loss: 0.3445, step time: 1.5371\n",
      "97/388, train_loss: 0.3596, step time: 1.5361\n",
      "98/388, train_loss: 0.4058, step time: 1.5371\n",
      "99/388, train_loss: 0.4384, step time: 1.5384\n",
      "100/388, train_loss: 0.3557, step time: 1.5321\n",
      "101/388, train_loss: 0.3276, step time: 1.5327\n",
      "102/388, train_loss: 0.4905, step time: 1.5302\n",
      "103/388, train_loss: 0.3742, step time: 1.5350\n",
      "104/388, train_loss: 0.2083, step time: 1.5327\n",
      "105/388, train_loss: 0.4662, step time: 1.5366\n",
      "106/388, train_loss: 0.3475, step time: 1.5318\n",
      "107/388, train_loss: 0.3536, step time: 1.5350\n",
      "108/388, train_loss: 0.3925, step time: 1.5373\n",
      "109/388, train_loss: 0.2003, step time: 1.5342\n",
      "110/388, train_loss: 0.1855, step time: 1.5317\n",
      "111/388, train_loss: 0.1926, step time: 1.5280\n",
      "112/388, train_loss: 0.3617, step time: 1.5316\n",
      "113/388, train_loss: 0.1789, step time: 1.5290\n",
      "114/388, train_loss: 0.7596, step time: 1.5332\n",
      "115/388, train_loss: 0.1947, step time: 1.5315\n",
      "116/388, train_loss: 0.3121, step time: 1.5381\n",
      "117/388, train_loss: 0.3998, step time: 1.5307\n",
      "118/388, train_loss: 0.2616, step time: 1.5318\n",
      "119/388, train_loss: 0.2989, step time: 1.5328\n",
      "120/388, train_loss: 0.2313, step time: 1.5349\n",
      "121/388, train_loss: 0.8766, step time: 1.5357\n",
      "122/388, train_loss: 0.2054, step time: 1.5365\n",
      "123/388, train_loss: 0.4654, step time: 1.5342\n",
      "124/388, train_loss: 0.4489, step time: 1.5348\n",
      "125/388, train_loss: 0.5685, step time: 1.5305\n",
      "126/388, train_loss: 0.2411, step time: 1.5293\n",
      "127/388, train_loss: 0.3212, step time: 1.5301\n",
      "128/388, train_loss: 0.2561, step time: 1.5313\n",
      "129/388, train_loss: 0.4001, step time: 1.5320\n",
      "130/388, train_loss: 0.2112, step time: 1.5354\n",
      "131/388, train_loss: 0.1274, step time: 1.5360\n",
      "132/388, train_loss: 0.2905, step time: 1.5327\n",
      "133/388, train_loss: 0.2911, step time: 1.5326\n",
      "134/388, train_loss: 0.1501, step time: 1.5309\n",
      "135/388, train_loss: 0.4613, step time: 1.5311\n",
      "136/388, train_loss: 0.3606, step time: 1.5326\n",
      "137/388, train_loss: 0.2505, step time: 1.5316\n",
      "138/388, train_loss: 0.4014, step time: 1.5351\n",
      "139/388, train_loss: 0.7588, step time: 1.5443\n",
      "140/388, train_loss: 0.4049, step time: 1.5300\n",
      "141/388, train_loss: 0.9113, step time: 1.5327\n",
      "142/388, train_loss: 0.7718, step time: 1.5309\n",
      "143/388, train_loss: 0.3705, step time: 1.5348\n",
      "144/388, train_loss: 0.2937, step time: 1.5369\n",
      "145/388, train_loss: 0.5084, step time: 1.5347\n",
      "146/388, train_loss: 0.1887, step time: 1.5325\n",
      "147/388, train_loss: 0.3660, step time: 1.5292\n",
      "148/388, train_loss: 0.3392, step time: 1.5329\n",
      "149/388, train_loss: 0.6787, step time: 1.5327\n",
      "150/388, train_loss: 0.4711, step time: 1.5413\n",
      "151/388, train_loss: 0.3396, step time: 1.5371\n",
      "152/388, train_loss: 0.6152, step time: 1.5341\n",
      "153/388, train_loss: 0.3169, step time: 1.5313\n",
      "154/388, train_loss: 0.3307, step time: 1.5321\n",
      "155/388, train_loss: 0.8773, step time: 1.5367\n",
      "156/388, train_loss: 0.3675, step time: 1.5339\n",
      "157/388, train_loss: 0.4234, step time: 1.5389\n",
      "158/388, train_loss: 0.2921, step time: 1.5388\n",
      "159/388, train_loss: 0.8703, step time: 1.5328\n",
      "160/388, train_loss: 0.3291, step time: 1.5316\n",
      "161/388, train_loss: 0.5194, step time: 1.5305\n",
      "162/388, train_loss: 0.3937, step time: 1.5358\n",
      "163/388, train_loss: 0.4028, step time: 1.5345\n",
      "164/388, train_loss: 0.2987, step time: 1.5369\n",
      "165/388, train_loss: 0.5025, step time: 1.5319\n",
      "166/388, train_loss: 0.4603, step time: 1.5310\n",
      "167/388, train_loss: 0.3578, step time: 1.5321\n",
      "168/388, train_loss: 0.3642, step time: 1.5318\n",
      "169/388, train_loss: 0.2874, step time: 1.5358\n",
      "170/388, train_loss: 0.6443, step time: 1.5350\n",
      "171/388, train_loss: 0.1752, step time: 1.5369\n",
      "172/388, train_loss: 0.2984, step time: 1.5311\n",
      "173/388, train_loss: 0.3397, step time: 1.5295\n",
      "174/388, train_loss: 0.2126, step time: 1.5339\n",
      "175/388, train_loss: 0.5720, step time: 1.5344\n",
      "176/388, train_loss: 0.2665, step time: 1.5354\n",
      "177/388, train_loss: 0.5006, step time: 1.5324\n",
      "178/388, train_loss: 0.8136, step time: 1.5327\n",
      "179/388, train_loss: 0.2623, step time: 1.5311\n",
      "180/388, train_loss: 0.1567, step time: 1.5302\n",
      "181/388, train_loss: 0.3198, step time: 1.5293\n",
      "182/388, train_loss: 0.3465, step time: 1.5351\n",
      "183/388, train_loss: 0.2695, step time: 1.5333\n",
      "184/388, train_loss: 0.3543, step time: 1.5369\n",
      "185/388, train_loss: 0.7013, step time: 1.5315\n",
      "186/388, train_loss: 0.3313, step time: 1.5314\n",
      "187/388, train_loss: 0.2339, step time: 1.5324\n",
      "188/388, train_loss: 0.4303, step time: 1.5334\n",
      "189/388, train_loss: 0.8877, step time: 1.5382\n",
      "190/388, train_loss: 0.5711, step time: 1.5330\n",
      "191/388, train_loss: 0.4784, step time: 1.5312\n",
      "192/388, train_loss: 0.2969, step time: 1.5309\n",
      "193/388, train_loss: 0.4314, step time: 1.5330\n",
      "194/388, train_loss: 0.3373, step time: 1.5358\n",
      "195/388, train_loss: 0.4401, step time: 1.5299\n",
      "196/388, train_loss: 0.2457, step time: 1.5331\n",
      "197/388, train_loss: 0.3569, step time: 1.5308\n",
      "198/388, train_loss: 0.2695, step time: 1.5279\n",
      "199/388, train_loss: 0.3587, step time: 1.5413\n",
      "200/388, train_loss: 0.3477, step time: 1.5384\n",
      "201/388, train_loss: 0.7112, step time: 1.5405\n",
      "202/388, train_loss: 0.2695, step time: 1.5426\n",
      "203/388, train_loss: 0.2670, step time: 1.5335\n",
      "204/388, train_loss: 0.3324, step time: 1.5384\n",
      "205/388, train_loss: 0.6404, step time: 1.5362\n",
      "206/388, train_loss: 0.3678, step time: 1.5303\n",
      "207/388, train_loss: 0.6298, step time: 1.5313\n",
      "208/388, train_loss: 0.3403, step time: 1.5366\n",
      "209/388, train_loss: 0.4136, step time: 1.5315\n",
      "210/388, train_loss: 0.2883, step time: 1.5393\n",
      "211/388, train_loss: 0.2786, step time: 1.5313\n",
      "212/388, train_loss: 0.4114, step time: 1.5309\n",
      "213/388, train_loss: 0.5621, step time: 1.5345\n",
      "214/388, train_loss: 0.3279, step time: 1.5345\n",
      "215/388, train_loss: 0.3836, step time: 1.5352\n",
      "216/388, train_loss: 0.2929, step time: 1.5307\n",
      "217/388, train_loss: 0.2530, step time: 1.5304\n",
      "218/388, train_loss: 0.4071, step time: 1.5420\n",
      "219/388, train_loss: 0.4881, step time: 1.5329\n",
      "220/388, train_loss: 0.3198, step time: 1.5362\n",
      "221/388, train_loss: 0.4952, step time: 1.5322\n",
      "222/388, train_loss: 0.5255, step time: 1.5314\n",
      "223/388, train_loss: 0.4008, step time: 1.5349\n",
      "224/388, train_loss: 0.3338, step time: 1.5329\n",
      "225/388, train_loss: 0.7504, step time: 1.5366\n",
      "226/388, train_loss: 0.2885, step time: 1.5374\n",
      "227/388, train_loss: 0.5944, step time: 1.5327\n",
      "228/388, train_loss: 0.4472, step time: 1.5315\n",
      "229/388, train_loss: 0.3029, step time: 1.5319\n",
      "230/388, train_loss: 0.2664, step time: 1.5319\n",
      "231/388, train_loss: 0.3598, step time: 1.5357\n",
      "232/388, train_loss: 0.4190, step time: 1.5391\n",
      "233/388, train_loss: 0.4058, step time: 1.5339\n",
      "234/388, train_loss: 0.2910, step time: 1.5343\n",
      "235/388, train_loss: 0.2322, step time: 1.5299\n",
      "236/388, train_loss: 0.3634, step time: 1.5324\n",
      "237/388, train_loss: 0.4775, step time: 1.5330\n",
      "238/388, train_loss: 0.1662, step time: 1.5349\n",
      "239/388, train_loss: 0.5589, step time: 1.5416\n",
      "240/388, train_loss: 0.4279, step time: 1.5308\n",
      "241/388, train_loss: 0.7579, step time: 1.5319\n",
      "242/388, train_loss: 0.2730, step time: 1.5309\n",
      "243/388, train_loss: 0.5191, step time: 1.5319\n",
      "244/388, train_loss: 0.4803, step time: 1.5324\n",
      "245/388, train_loss: 0.2466, step time: 1.5341\n",
      "246/388, train_loss: 0.1786, step time: 1.5335\n",
      "247/388, train_loss: 0.2378, step time: 1.5315\n",
      "248/388, train_loss: 0.2308, step time: 1.5337\n",
      "249/388, train_loss: 0.1518, step time: 1.5298\n",
      "250/388, train_loss: 0.2809, step time: 1.5351\n",
      "251/388, train_loss: 0.2528, step time: 1.5359\n",
      "252/388, train_loss: 0.3039, step time: 1.5345\n",
      "253/388, train_loss: 0.3844, step time: 1.5343\n",
      "254/388, train_loss: 0.5623, step time: 1.5366\n",
      "255/388, train_loss: 0.3611, step time: 1.5342\n",
      "256/388, train_loss: 0.5493, step time: 1.5364\n",
      "257/388, train_loss: 0.6118, step time: 1.5362\n",
      "258/388, train_loss: 0.2838, step time: 1.5357\n",
      "259/388, train_loss: 0.2696, step time: 1.5323\n",
      "260/388, train_loss: 0.2925, step time: 1.5294\n",
      "261/388, train_loss: 0.4678, step time: 1.5309\n",
      "262/388, train_loss: 0.2463, step time: 1.5389\n",
      "263/388, train_loss: 0.2353, step time: 1.5368\n",
      "264/388, train_loss: 0.2592, step time: 1.5354\n",
      "265/388, train_loss: 0.5101, step time: 1.5323\n",
      "266/388, train_loss: 0.2982, step time: 1.5325\n",
      "267/388, train_loss: 0.2618, step time: 1.5320\n",
      "268/388, train_loss: 0.2639, step time: 1.5341\n",
      "269/388, train_loss: 0.5025, step time: 1.5314\n",
      "270/388, train_loss: 0.2055, step time: 1.5307\n",
      "271/388, train_loss: 0.2553, step time: 1.5292\n",
      "272/388, train_loss: 0.1927, step time: 1.5308\n",
      "273/388, train_loss: 0.4029, step time: 1.5345\n",
      "274/388, train_loss: 0.2643, step time: 1.5404\n",
      "275/388, train_loss: 0.3711, step time: 1.5376\n",
      "276/388, train_loss: 0.2038, step time: 1.5325\n",
      "277/388, train_loss: 0.2777, step time: 1.5323\n",
      "278/388, train_loss: 0.3201, step time: 1.5338\n",
      "279/388, train_loss: 0.3610, step time: 1.5335\n",
      "280/388, train_loss: 0.2921, step time: 1.5341\n",
      "281/388, train_loss: 0.4255, step time: 1.5313\n",
      "282/388, train_loss: 0.2741, step time: 1.5327\n",
      "283/388, train_loss: 0.2439, step time: 1.5307\n",
      "284/388, train_loss: 0.4580, step time: 1.5346\n",
      "285/388, train_loss: 0.3736, step time: 1.5399\n",
      "286/388, train_loss: 0.3022, step time: 1.5310\n",
      "287/388, train_loss: 0.3133, step time: 1.5333\n",
      "288/388, train_loss: 0.2413, step time: 1.5300\n",
      "289/388, train_loss: 0.4222, step time: 1.5326\n",
      "290/388, train_loss: 0.4709, step time: 1.5350\n",
      "291/388, train_loss: 0.6252, step time: 1.5347\n",
      "292/388, train_loss: 0.5176, step time: 1.5376\n",
      "293/388, train_loss: 0.2761, step time: 1.5341\n",
      "294/388, train_loss: 0.5545, step time: 1.5312\n",
      "295/388, train_loss: 0.2632, step time: 1.5300\n",
      "296/388, train_loss: 0.5087, step time: 1.5341\n",
      "297/388, train_loss: 0.6395, step time: 1.5336\n",
      "298/388, train_loss: 0.2066, step time: 1.5376\n",
      "299/388, train_loss: 0.2516, step time: 1.5336\n",
      "300/388, train_loss: 0.4448, step time: 1.5546\n",
      "301/388, train_loss: 0.3634, step time: 1.5378\n",
      "302/388, train_loss: 0.4557, step time: 1.5378\n",
      "303/388, train_loss: 0.5111, step time: 1.5342\n",
      "304/388, train_loss: 0.3740, step time: 1.5324\n",
      "305/388, train_loss: 0.3772, step time: 1.5302\n",
      "306/388, train_loss: 0.3013, step time: 1.5311\n",
      "307/388, train_loss: 0.2631, step time: 1.5351\n",
      "308/388, train_loss: 0.2923, step time: 1.5407\n",
      "309/388, train_loss: 0.1945, step time: 1.5321\n",
      "310/388, train_loss: 0.4537, step time: 1.5354\n",
      "311/388, train_loss: 0.4138, step time: 1.5346\n",
      "312/388, train_loss: 0.2034, step time: 1.5302\n",
      "313/388, train_loss: 0.1408, step time: 1.5336\n",
      "314/388, train_loss: 0.3743, step time: 1.5367\n",
      "315/388, train_loss: 0.4448, step time: 1.5368\n",
      "316/388, train_loss: 0.2911, step time: 1.5331\n",
      "317/388, train_loss: 0.5452, step time: 1.5424\n",
      "318/388, train_loss: 0.2380, step time: 1.5317\n",
      "319/388, train_loss: 0.3160, step time: 1.5357\n",
      "320/388, train_loss: 0.4778, step time: 1.5377\n",
      "321/388, train_loss: 0.3032, step time: 1.5353\n",
      "322/388, train_loss: 0.4880, step time: 1.5312\n",
      "323/388, train_loss: 0.3940, step time: 1.5328\n",
      "324/388, train_loss: 0.2122, step time: 1.5312\n",
      "325/388, train_loss: 0.7811, step time: 1.5360\n",
      "326/388, train_loss: 0.1626, step time: 1.5354\n",
      "327/388, train_loss: 0.3164, step time: 1.5345\n",
      "328/388, train_loss: 0.4196, step time: 1.5341\n",
      "329/388, train_loss: 0.2364, step time: 1.5312\n",
      "330/388, train_loss: 0.3419, step time: 1.5329\n",
      "331/388, train_loss: 0.3589, step time: 1.5324\n",
      "332/388, train_loss: 0.3834, step time: 1.5342\n",
      "333/388, train_loss: 0.1483, step time: 1.5359\n",
      "334/388, train_loss: 0.2465, step time: 1.5666\n",
      "335/388, train_loss: 0.6188, step time: 1.5349\n",
      "336/388, train_loss: 0.3103, step time: 1.5366\n",
      "337/388, train_loss: 0.2157, step time: 1.5346\n",
      "338/388, train_loss: 0.3845, step time: 1.5323\n",
      "339/388, train_loss: 0.2681, step time: 1.5326\n",
      "340/388, train_loss: 0.1598, step time: 1.5349\n",
      "341/388, train_loss: 0.2873, step time: 1.5383\n",
      "342/388, train_loss: 0.4630, step time: 1.5343\n",
      "343/388, train_loss: 0.3404, step time: 1.5332\n",
      "344/388, train_loss: 0.3411, step time: 1.5357\n",
      "345/388, train_loss: 0.3080, step time: 1.5320\n",
      "346/388, train_loss: 0.3184, step time: 1.5384\n",
      "347/388, train_loss: 0.3515, step time: 1.5346\n",
      "348/388, train_loss: 0.3279, step time: 1.5352\n",
      "349/388, train_loss: 0.4063, step time: 1.5339\n",
      "350/388, train_loss: 0.3441, step time: 1.5318\n",
      "351/388, train_loss: 0.2571, step time: 1.5325\n",
      "352/388, train_loss: 0.4344, step time: 1.5324\n",
      "353/388, train_loss: 0.3190, step time: 1.5346\n",
      "354/388, train_loss: 0.3331, step time: 1.5365\n",
      "355/388, train_loss: 0.3311, step time: 1.5281\n",
      "356/388, train_loss: 0.2654, step time: 1.5332\n",
      "357/388, train_loss: 0.0912, step time: 1.5318\n",
      "358/388, train_loss: 0.4495, step time: 1.5340\n",
      "359/388, train_loss: 0.3379, step time: 1.5304\n",
      "360/388, train_loss: 0.2934, step time: 1.5322\n",
      "361/388, train_loss: 0.5791, step time: 1.5321\n",
      "362/388, train_loss: 0.7113, step time: 1.5371\n",
      "363/388, train_loss: 0.3606, step time: 1.5335\n",
      "364/388, train_loss: 0.2626, step time: 1.5322\n",
      "365/388, train_loss: 0.2824, step time: 1.5297\n",
      "366/388, train_loss: 0.3044, step time: 1.5334\n",
      "367/388, train_loss: 0.1991, step time: 1.5355\n",
      "368/388, train_loss: 0.2853, step time: 1.5365\n",
      "369/388, train_loss: 0.1449, step time: 1.5353\n",
      "370/388, train_loss: 0.5213, step time: 1.5339\n",
      "371/388, train_loss: 0.3395, step time: 1.5324\n",
      "372/388, train_loss: 0.3984, step time: 1.5318\n",
      "373/388, train_loss: 0.3157, step time: 1.5339\n",
      "374/388, train_loss: 0.3398, step time: 1.5345\n",
      "375/388, train_loss: 0.3055, step time: 1.5429\n",
      "376/388, train_loss: 0.1748, step time: 1.5324\n",
      "377/388, train_loss: 0.5333, step time: 1.5318\n",
      "378/388, train_loss: 0.2811, step time: 1.5334\n",
      "379/388, train_loss: 0.2164, step time: 1.5315\n",
      "380/388, train_loss: 0.5981, step time: 1.5381\n",
      "381/388, train_loss: 0.2930, step time: 1.5370\n",
      "382/388, train_loss: 0.2634, step time: 1.5322\n",
      "383/388, train_loss: 0.5816, step time: 1.5328\n",
      "384/388, train_loss: 0.4183, step time: 1.5348\n",
      "385/388, train_loss: 0.5962, step time: 1.5351\n",
      "386/388, train_loss: 0.3566, step time: 1.5358\n",
      "387/388, train_loss: 0.1963, step time: 1.5356\n",
      "388/388, train_loss: 0.5948, step time: 1.5325\n",
      "epoch 7 average loss: 0.3737\n",
      "saved new best metric model\n",
      "current epoch: 7 current mean dice: 0.6570 tc: 0.6948 wt: 0.8729 et: 0.4033\n",
      "best mean dice: 0.6570 at epoch: 7\n",
      "time consuming of epoch 7 is: 703.2247\n",
      "----------\n",
      "epoch 8/100\n",
      "1/388, train_loss: 0.3003, step time: 1.5487\n",
      "2/388, train_loss: 0.3420, step time: 1.5337\n",
      "3/388, train_loss: 0.2838, step time: 1.5357\n",
      "4/388, train_loss: 0.4170, step time: 1.5325\n",
      "5/388, train_loss: 0.2713, step time: 1.5361\n",
      "6/388, train_loss: 0.4376, step time: 1.5326\n",
      "7/388, train_loss: 0.2795, step time: 1.5329\n",
      "8/388, train_loss: 0.3310, step time: 1.5382\n",
      "9/388, train_loss: 0.3293, step time: 1.5344\n",
      "10/388, train_loss: 0.1124, step time: 1.5336\n",
      "11/388, train_loss: 0.3609, step time: 1.5327\n",
      "12/388, train_loss: 0.2298, step time: 1.5319\n",
      "13/388, train_loss: 0.3221, step time: 1.5463\n",
      "14/388, train_loss: 0.2258, step time: 1.5429\n",
      "15/388, train_loss: 0.2417, step time: 1.5308\n",
      "16/388, train_loss: 0.2522, step time: 1.5311\n",
      "17/388, train_loss: 0.2838, step time: 1.5331\n",
      "18/388, train_loss: 0.2402, step time: 1.5331\n",
      "19/388, train_loss: 0.2448, step time: 1.5337\n",
      "20/388, train_loss: 0.3667, step time: 1.5370\n",
      "21/388, train_loss: 0.5430, step time: 1.5339\n",
      "22/388, train_loss: 0.2311, step time: 1.5335\n",
      "23/388, train_loss: 0.2862, step time: 1.5325\n",
      "24/388, train_loss: 0.3965, step time: 1.5308\n",
      "25/388, train_loss: 0.4039, step time: 1.5306\n",
      "26/388, train_loss: 0.2982, step time: 1.5366\n",
      "27/388, train_loss: 0.2889, step time: 1.5360\n",
      "28/388, train_loss: 0.1965, step time: 1.5349\n",
      "29/388, train_loss: 0.2945, step time: 1.5337\n",
      "30/388, train_loss: 0.2350, step time: 1.5306\n",
      "31/388, train_loss: 0.2281, step time: 1.5347\n",
      "32/388, train_loss: 0.7477, step time: 1.5307\n",
      "33/388, train_loss: 0.3663, step time: 1.5350\n",
      "34/388, train_loss: 0.5913, step time: 1.5373\n",
      "35/388, train_loss: 0.4688, step time: 1.5398\n",
      "36/388, train_loss: 0.3413, step time: 1.5336\n",
      "37/388, train_loss: 0.1357, step time: 1.5316\n",
      "38/388, train_loss: 0.2747, step time: 1.5396\n",
      "39/388, train_loss: 0.3890, step time: 1.5343\n",
      "40/388, train_loss: 0.2996, step time: 1.5356\n",
      "41/388, train_loss: 0.2815, step time: 1.5350\n",
      "42/388, train_loss: 0.1892, step time: 1.5318\n",
      "43/388, train_loss: 0.2701, step time: 1.5329\n",
      "44/388, train_loss: 0.4158, step time: 1.5333\n",
      "45/388, train_loss: 0.3871, step time: 1.5406\n",
      "46/388, train_loss: 0.4722, step time: 1.5363\n",
      "47/388, train_loss: 0.1741, step time: 1.5336\n",
      "48/388, train_loss: 0.4015, step time: 1.5446\n",
      "49/388, train_loss: 0.3134, step time: 1.5337\n",
      "50/388, train_loss: 0.3846, step time: 1.5439\n",
      "51/388, train_loss: 0.2092, step time: 1.5382\n",
      "52/388, train_loss: 0.6348, step time: 1.5342\n",
      "53/388, train_loss: 0.9376, step time: 1.5354\n",
      "54/388, train_loss: 0.3644, step time: 1.5316\n",
      "55/388, train_loss: 0.2568, step time: 1.5396\n",
      "56/388, train_loss: 0.3683, step time: 1.5324\n",
      "57/388, train_loss: 0.3235, step time: 1.5328\n",
      "58/388, train_loss: 0.3240, step time: 1.5323\n",
      "59/388, train_loss: 0.5193, step time: 1.5343\n",
      "60/388, train_loss: 0.2343, step time: 1.5334\n",
      "61/388, train_loss: 0.3197, step time: 1.5336\n",
      "62/388, train_loss: 0.2480, step time: 1.5388\n",
      "63/388, train_loss: 0.2425, step time: 1.5323\n",
      "64/388, train_loss: 0.3054, step time: 1.5330\n",
      "65/388, train_loss: 0.2523, step time: 1.5295\n",
      "66/388, train_loss: 0.1354, step time: 1.5359\n",
      "67/388, train_loss: 0.2409, step time: 1.5372\n",
      "68/388, train_loss: 0.3659, step time: 1.5354\n",
      "69/388, train_loss: 0.2901, step time: 1.5352\n",
      "70/388, train_loss: 0.2967, step time: 1.5354\n",
      "71/388, train_loss: 0.2554, step time: 1.5345\n",
      "72/388, train_loss: 0.2209, step time: 1.5326\n",
      "73/388, train_loss: 0.2114, step time: 1.5348\n",
      "74/388, train_loss: 0.1780, step time: 1.5359\n",
      "75/388, train_loss: 0.3091, step time: 1.5336\n",
      "76/388, train_loss: 0.3574, step time: 1.5340\n",
      "77/388, train_loss: 0.5526, step time: 1.5287\n",
      "78/388, train_loss: 0.2553, step time: 1.5304\n",
      "79/388, train_loss: 0.5247, step time: 1.5301\n",
      "80/388, train_loss: 0.2659, step time: 1.5303\n",
      "81/388, train_loss: 0.3283, step time: 1.5379\n",
      "82/388, train_loss: 0.2704, step time: 1.5362\n",
      "83/388, train_loss: 0.5997, step time: 1.5337\n",
      "84/388, train_loss: 0.6012, step time: 1.5362\n",
      "85/388, train_loss: 0.3010, step time: 1.5322\n",
      "86/388, train_loss: 0.3147, step time: 1.5315\n",
      "87/388, train_loss: 0.2803, step time: 1.5406\n",
      "88/388, train_loss: 0.2854, step time: 1.5357\n",
      "89/388, train_loss: 0.3235, step time: 1.5374\n",
      "90/388, train_loss: 0.2038, step time: 1.5507\n",
      "91/388, train_loss: 0.3381, step time: 1.5448\n",
      "92/388, train_loss: 0.3909, step time: 1.5355\n",
      "93/388, train_loss: 0.4936, step time: 1.5348\n",
      "94/388, train_loss: 0.4005, step time: 1.5366\n",
      "95/388, train_loss: 0.3058, step time: 1.5359\n",
      "96/388, train_loss: 0.4442, step time: 1.5323\n",
      "97/388, train_loss: 0.4001, step time: 1.5305\n",
      "98/388, train_loss: 0.2954, step time: 1.5357\n",
      "99/388, train_loss: 0.4136, step time: 1.5373\n",
      "100/388, train_loss: 0.4454, step time: 1.5368\n",
      "101/388, train_loss: 0.3468, step time: 1.5362\n",
      "102/388, train_loss: 0.3010, step time: 1.5351\n",
      "103/388, train_loss: 0.5720, step time: 1.5347\n",
      "104/388, train_loss: 0.2740, step time: 1.5394\n",
      "105/388, train_loss: 0.4565, step time: 1.5378\n",
      "106/388, train_loss: 0.2586, step time: 1.5352\n",
      "107/388, train_loss: 0.3454, step time: 1.5389\n",
      "108/388, train_loss: 0.6045, step time: 1.5327\n",
      "109/388, train_loss: 0.2756, step time: 1.5339\n",
      "110/388, train_loss: 0.1168, step time: 1.5334\n",
      "111/388, train_loss: 0.6008, step time: 1.5354\n",
      "112/388, train_loss: 0.3455, step time: 1.5348\n",
      "113/388, train_loss: 0.7640, step time: 1.5350\n",
      "114/388, train_loss: 0.1942, step time: 1.5315\n",
      "115/388, train_loss: 0.3197, step time: 1.5292\n",
      "116/388, train_loss: 0.3123, step time: 1.5447\n",
      "117/388, train_loss: 0.5255, step time: 1.5372\n",
      "118/388, train_loss: 0.3456, step time: 1.5477\n",
      "119/388, train_loss: 0.3460, step time: 1.5307\n",
      "120/388, train_loss: 0.5009, step time: 1.5334\n",
      "121/388, train_loss: 0.5652, step time: 1.5315\n",
      "122/388, train_loss: 0.3341, step time: 1.5505\n",
      "123/388, train_loss: 0.2775, step time: 1.5416\n",
      "124/388, train_loss: 0.2369, step time: 1.5347\n",
      "125/388, train_loss: 0.2920, step time: 1.5335\n",
      "126/388, train_loss: 0.2429, step time: 1.5324\n",
      "127/388, train_loss: 0.2777, step time: 1.5372\n",
      "128/388, train_loss: 0.3398, step time: 1.5405\n",
      "129/388, train_loss: 0.4766, step time: 1.5344\n",
      "130/388, train_loss: 0.2293, step time: 1.5338\n",
      "131/388, train_loss: 0.3834, step time: 1.5316\n",
      "132/388, train_loss: 0.5941, step time: 1.5406\n",
      "133/388, train_loss: 0.2536, step time: 1.5327\n",
      "134/388, train_loss: 0.4536, step time: 1.5317\n",
      "135/388, train_loss: 0.5508, step time: 1.5330\n",
      "136/388, train_loss: 0.2324, step time: 1.5377\n",
      "137/388, train_loss: 0.5915, step time: 1.5352\n",
      "138/388, train_loss: 0.2959, step time: 1.5314\n",
      "139/388, train_loss: 0.2025, step time: 1.5286\n",
      "140/388, train_loss: 0.7228, step time: 1.5295\n",
      "141/388, train_loss: 0.5079, step time: 1.5287\n",
      "142/388, train_loss: 0.5009, step time: 1.5382\n",
      "143/388, train_loss: 0.2036, step time: 1.5328\n",
      "144/388, train_loss: 0.5107, step time: 1.5372\n",
      "145/388, train_loss: 0.3522, step time: 1.5339\n",
      "146/388, train_loss: 0.2954, step time: 1.5331\n",
      "147/388, train_loss: 0.3771, step time: 1.5320\n",
      "148/388, train_loss: 0.4017, step time: 1.5346\n",
      "149/388, train_loss: 0.2367, step time: 1.5323\n",
      "150/388, train_loss: 0.3495, step time: 1.5345\n",
      "151/388, train_loss: 0.7179, step time: 1.5324\n",
      "152/388, train_loss: 0.3771, step time: 1.5367\n",
      "153/388, train_loss: 0.4964, step time: 1.5335\n",
      "154/388, train_loss: 0.4304, step time: 1.5318\n",
      "155/388, train_loss: 0.2697, step time: 1.5326\n",
      "156/388, train_loss: 0.3010, step time: 1.5335\n",
      "157/388, train_loss: 0.3920, step time: 1.5344\n",
      "158/388, train_loss: 0.3569, step time: 1.5361\n",
      "159/388, train_loss: 0.2637, step time: 1.5340\n",
      "160/388, train_loss: 0.2892, step time: 1.5325\n",
      "161/388, train_loss: 0.7961, step time: 1.5333\n",
      "162/388, train_loss: 0.1488, step time: 1.5388\n",
      "163/388, train_loss: 0.2344, step time: 1.5334\n",
      "164/388, train_loss: 0.3651, step time: 1.5343\n",
      "165/388, train_loss: 0.5066, step time: 1.5325\n",
      "166/388, train_loss: 0.2936, step time: 1.5340\n",
      "167/388, train_loss: 0.1556, step time: 1.5323\n",
      "168/388, train_loss: 0.1970, step time: 1.5316\n",
      "169/388, train_loss: 0.1805, step time: 1.5293\n",
      "170/388, train_loss: 0.5055, step time: 1.5333\n",
      "171/388, train_loss: 0.2138, step time: 1.5307\n",
      "172/388, train_loss: 0.5849, step time: 1.5393\n",
      "173/388, train_loss: 0.1910, step time: 1.5380\n",
      "174/388, train_loss: 0.2324, step time: 1.5445\n",
      "175/388, train_loss: 0.1240, step time: 1.5345\n",
      "176/388, train_loss: 0.5319, step time: 1.5354\n",
      "177/388, train_loss: 0.2561, step time: 1.5363\n",
      "178/388, train_loss: 0.3308, step time: 1.5449\n",
      "179/388, train_loss: 0.3095, step time: 1.5334\n",
      "180/388, train_loss: 0.2580, step time: 1.5338\n",
      "181/388, train_loss: 0.8426, step time: 1.5331\n",
      "182/388, train_loss: 0.3547, step time: 1.5323\n",
      "183/388, train_loss: 0.2128, step time: 1.5344\n",
      "184/388, train_loss: 0.2320, step time: 1.5350\n",
      "185/388, train_loss: 0.3950, step time: 1.5361\n",
      "186/388, train_loss: 0.2041, step time: 1.5302\n",
      "187/388, train_loss: 0.2852, step time: 1.5310\n",
      "188/388, train_loss: 0.2756, step time: 1.5333\n",
      "189/388, train_loss: 0.1637, step time: 1.5342\n",
      "190/388, train_loss: 0.2311, step time: 1.5357\n",
      "191/388, train_loss: 0.5043, step time: 1.5340\n",
      "192/388, train_loss: 0.1234, step time: 1.5375\n",
      "193/388, train_loss: 0.2198, step time: 1.5295\n",
      "194/388, train_loss: 0.3501, step time: 1.5328\n",
      "195/388, train_loss: 0.2318, step time: 1.5349\n",
      "196/388, train_loss: 0.1176, step time: 1.5354\n",
      "197/388, train_loss: 0.2345, step time: 1.5320\n",
      "198/388, train_loss: 0.4216, step time: 1.5362\n",
      "199/388, train_loss: 0.3283, step time: 1.5558\n",
      "200/388, train_loss: 0.6307, step time: 1.5368\n",
      "201/388, train_loss: 0.4449, step time: 1.5330\n",
      "202/388, train_loss: 0.3240, step time: 1.5341\n",
      "203/388, train_loss: 0.1748, step time: 1.5356\n",
      "204/388, train_loss: 0.2306, step time: 1.5333\n",
      "205/388, train_loss: 0.3110, step time: 1.5482\n",
      "206/388, train_loss: 0.4104, step time: 1.5303\n",
      "207/388, train_loss: 0.1882, step time: 1.5324\n",
      "208/388, train_loss: 0.2172, step time: 1.5364\n",
      "209/388, train_loss: 0.3762, step time: 1.5374\n",
      "210/388, train_loss: 0.3642, step time: 1.5328\n",
      "211/388, train_loss: 0.5056, step time: 1.5340\n",
      "212/388, train_loss: 0.3451, step time: 1.5326\n",
      "213/388, train_loss: 0.3382, step time: 1.5340\n",
      "214/388, train_loss: 0.2934, step time: 1.5346\n",
      "215/388, train_loss: 0.2270, step time: 1.5437\n",
      "216/388, train_loss: 0.2845, step time: 1.5331\n",
      "217/388, train_loss: 0.3257, step time: 1.5350\n",
      "218/388, train_loss: 0.4521, step time: 1.5334\n",
      "219/388, train_loss: 0.3224, step time: 1.5338\n",
      "220/388, train_loss: 0.3750, step time: 1.5354\n",
      "221/388, train_loss: 0.3777, step time: 1.5356\n",
      "222/388, train_loss: 0.4527, step time: 1.5353\n",
      "223/388, train_loss: 0.2215, step time: 1.5344\n",
      "224/388, train_loss: 0.2213, step time: 1.5302\n",
      "225/388, train_loss: 0.5532, step time: 1.5323\n",
      "226/388, train_loss: 0.4163, step time: 1.5428\n",
      "227/388, train_loss: 0.3781, step time: 1.5370\n",
      "228/388, train_loss: 0.2793, step time: 1.5396\n",
      "229/388, train_loss: 0.1272, step time: 1.5346\n",
      "230/388, train_loss: 0.5061, step time: 1.5312\n",
      "231/388, train_loss: 0.1943, step time: 1.5331\n",
      "232/388, train_loss: 0.3328, step time: 1.5417\n",
      "233/388, train_loss: 0.2807, step time: 1.5343\n",
      "234/388, train_loss: 0.1268, step time: 1.5365\n",
      "235/388, train_loss: 0.2860, step time: 1.5323\n",
      "236/388, train_loss: 0.2956, step time: 1.5314\n",
      "237/388, train_loss: 0.2804, step time: 1.5320\n",
      "238/388, train_loss: 0.3367, step time: 1.5307\n",
      "239/388, train_loss: 0.2254, step time: 1.5365\n",
      "240/388, train_loss: 0.2520, step time: 1.5370\n",
      "241/388, train_loss: 0.2807, step time: 1.5308\n",
      "242/388, train_loss: 0.2740, step time: 1.5321\n",
      "243/388, train_loss: 0.2406, step time: 1.5315\n",
      "244/388, train_loss: 0.2109, step time: 1.5318\n",
      "245/388, train_loss: 0.3129, step time: 1.5304\n",
      "246/388, train_loss: 0.2458, step time: 1.5361\n",
      "247/388, train_loss: 0.1659, step time: 1.5357\n",
      "248/388, train_loss: 0.2359, step time: 1.5377\n",
      "249/388, train_loss: 0.2227, step time: 1.5340\n",
      "250/388, train_loss: 0.7234, step time: 1.5357\n",
      "251/388, train_loss: 0.2397, step time: 1.5280\n",
      "252/388, train_loss: 0.3573, step time: 1.5316\n",
      "253/388, train_loss: 0.1733, step time: 1.5310\n",
      "254/388, train_loss: 0.2088, step time: 1.5322\n",
      "255/388, train_loss: 0.4482, step time: 1.5378\n",
      "256/388, train_loss: 0.1910, step time: 1.5353\n",
      "257/388, train_loss: 0.1680, step time: 1.5366\n",
      "258/388, train_loss: 0.2654, step time: 1.5367\n",
      "259/388, train_loss: 0.1897, step time: 1.5299\n",
      "260/388, train_loss: 0.5500, step time: 1.5326\n",
      "261/388, train_loss: 0.4733, step time: 1.5368\n",
      "262/388, train_loss: 0.4861, step time: 1.5359\n",
      "263/388, train_loss: 0.3739, step time: 1.5370\n",
      "264/388, train_loss: 0.3478, step time: 1.5356\n",
      "265/388, train_loss: 0.3286, step time: 1.5355\n",
      "266/388, train_loss: 0.3319, step time: 1.5336\n",
      "267/388, train_loss: 0.3274, step time: 1.5372\n",
      "268/388, train_loss: 0.3511, step time: 1.5372\n",
      "269/388, train_loss: 0.4142, step time: 1.5363\n",
      "270/388, train_loss: 0.1852, step time: 1.5337\n",
      "271/388, train_loss: 0.2486, step time: 1.5317\n",
      "272/388, train_loss: 0.2383, step time: 1.5304\n",
      "273/388, train_loss: 0.5301, step time: 1.5294\n",
      "274/388, train_loss: 0.2357, step time: 1.5343\n",
      "275/388, train_loss: 0.2202, step time: 1.5363\n",
      "276/388, train_loss: 0.2734, step time: 1.5369\n",
      "277/388, train_loss: 0.2300, step time: 1.5344\n",
      "278/388, train_loss: 0.4813, step time: 1.5362\n",
      "279/388, train_loss: 0.2345, step time: 1.5378\n",
      "280/388, train_loss: 0.4788, step time: 1.5399\n",
      "281/388, train_loss: 0.3861, step time: 1.5323\n",
      "282/388, train_loss: 0.5329, step time: 1.5318\n",
      "283/388, train_loss: 0.3392, step time: 1.5394\n",
      "284/388, train_loss: 0.4034, step time: 1.5303\n",
      "285/388, train_loss: 0.3166, step time: 1.5616\n",
      "286/388, train_loss: 0.4999, step time: 1.5604\n",
      "287/388, train_loss: 0.3965, step time: 1.5397\n",
      "288/388, train_loss: 0.1850, step time: 1.5354\n",
      "289/388, train_loss: 0.1524, step time: 1.5345\n",
      "290/388, train_loss: 0.2276, step time: 1.5359\n",
      "291/388, train_loss: 0.2933, step time: 1.5365\n",
      "292/388, train_loss: 0.2578, step time: 1.5371\n",
      "293/388, train_loss: 0.3892, step time: 1.5331\n",
      "294/388, train_loss: 0.4225, step time: 1.5315\n",
      "295/388, train_loss: 0.2617, step time: 1.5339\n",
      "296/388, train_loss: 0.3102, step time: 1.5343\n",
      "297/388, train_loss: 0.2814, step time: 1.5379\n",
      "298/388, train_loss: 0.2516, step time: 1.5352\n",
      "299/388, train_loss: 0.2337, step time: 1.5444\n",
      "300/388, train_loss: 0.3755, step time: 1.5308\n",
      "301/388, train_loss: 0.2906, step time: 1.5314\n",
      "302/388, train_loss: 0.3580, step time: 1.5396\n",
      "303/388, train_loss: 0.7579, step time: 1.5341\n",
      "304/388, train_loss: 0.2816, step time: 1.5327\n",
      "305/388, train_loss: 0.4385, step time: 1.5298\n",
      "306/388, train_loss: 0.8166, step time: 1.5335\n",
      "307/388, train_loss: 0.0935, step time: 1.5362\n",
      "308/388, train_loss: 0.3136, step time: 1.5360\n",
      "309/388, train_loss: 0.3567, step time: 1.5373\n",
      "310/388, train_loss: 0.3958, step time: 1.5454\n",
      "311/388, train_loss: 0.4321, step time: 1.5310\n",
      "312/388, train_loss: 0.3150, step time: 1.5374\n",
      "313/388, train_loss: 0.1917, step time: 1.5355\n",
      "314/388, train_loss: 0.5180, step time: 1.5492\n",
      "315/388, train_loss: 0.4095, step time: 1.5322\n",
      "316/388, train_loss: 0.4232, step time: 1.5302\n",
      "317/388, train_loss: 0.4105, step time: 1.5308\n",
      "318/388, train_loss: 0.2709, step time: 1.5451\n",
      "319/388, train_loss: 0.2523, step time: 1.5374\n",
      "320/388, train_loss: 0.2759, step time: 1.5361\n",
      "321/388, train_loss: 0.3914, step time: 1.5323\n",
      "322/388, train_loss: 0.2699, step time: 1.5439\n",
      "323/388, train_loss: 0.5301, step time: 1.5302\n",
      "324/388, train_loss: 0.3408, step time: 1.5370\n",
      "325/388, train_loss: 0.2195, step time: 1.5371\n",
      "326/388, train_loss: 0.2158, step time: 1.5517\n",
      "327/388, train_loss: 0.2485, step time: 1.5343\n",
      "328/388, train_loss: 0.6122, step time: 1.5343\n",
      "329/388, train_loss: 0.2471, step time: 1.5376\n",
      "330/388, train_loss: 0.1860, step time: 1.5497\n",
      "331/388, train_loss: 0.4265, step time: 1.5325\n",
      "332/388, train_loss: 0.2400, step time: 1.5349\n",
      "333/388, train_loss: 0.2042, step time: 1.5383\n",
      "334/388, train_loss: 0.2773, step time: 1.5498\n",
      "335/388, train_loss: 0.6552, step time: 1.5307\n",
      "336/388, train_loss: 0.2470, step time: 1.5312\n",
      "337/388, train_loss: 0.2421, step time: 1.5306\n",
      "338/388, train_loss: 0.2417, step time: 1.5480\n",
      "339/388, train_loss: 0.1044, step time: 1.5327\n",
      "340/388, train_loss: 0.1917, step time: 1.5314\n",
      "341/388, train_loss: 0.4026, step time: 1.5334\n",
      "342/388, train_loss: 0.2882, step time: 1.5496\n",
      "343/388, train_loss: 0.3792, step time: 1.5362\n",
      "344/388, train_loss: 0.2356, step time: 1.5595\n",
      "345/388, train_loss: 0.2656, step time: 1.5316\n",
      "346/388, train_loss: 0.3749, step time: 1.5496\n",
      "347/388, train_loss: 0.1366, step time: 1.5363\n",
      "348/388, train_loss: 0.1851, step time: 1.5330\n",
      "349/388, train_loss: 0.3026, step time: 1.5342\n",
      "350/388, train_loss: 0.2457, step time: 1.5410\n",
      "351/388, train_loss: 0.3013, step time: 1.5351\n",
      "352/388, train_loss: 0.1402, step time: 1.5341\n",
      "353/388, train_loss: 0.6498, step time: 1.5378\n",
      "354/388, train_loss: 0.2871, step time: 1.5450\n",
      "355/388, train_loss: 0.2650, step time: 1.5328\n",
      "356/388, train_loss: 0.3050, step time: 1.5367\n",
      "357/388, train_loss: 0.5612, step time: 1.5364\n",
      "358/388, train_loss: 0.2889, step time: 1.5446\n",
      "359/388, train_loss: 0.2616, step time: 1.5324\n",
      "360/388, train_loss: 0.3007, step time: 1.5327\n",
      "361/388, train_loss: 0.6901, step time: 1.5307\n",
      "362/388, train_loss: 0.3500, step time: 1.5497\n",
      "363/388, train_loss: 0.2548, step time: 1.5323\n",
      "364/388, train_loss: 0.3190, step time: 1.5345\n",
      "365/388, train_loss: 0.3149, step time: 1.5317\n",
      "366/388, train_loss: 0.5537, step time: 1.5409\n",
      "367/388, train_loss: 0.5821, step time: 1.5287\n",
      "368/388, train_loss: 0.4789, step time: 1.5451\n",
      "369/388, train_loss: 0.2710, step time: 1.5332\n",
      "370/388, train_loss: 0.2961, step time: 1.5435\n",
      "371/388, train_loss: 0.2639, step time: 1.5334\n",
      "372/388, train_loss: 0.3424, step time: 1.5363\n",
      "373/388, train_loss: 0.2316, step time: 1.5375\n",
      "374/388, train_loss: 0.2043, step time: 1.5460\n",
      "375/388, train_loss: 0.2354, step time: 1.5328\n",
      "376/388, train_loss: 0.1720, step time: 1.5351\n",
      "377/388, train_loss: 0.3144, step time: 1.5383\n",
      "378/388, train_loss: 0.7827, step time: 1.5461\n",
      "379/388, train_loss: 0.1315, step time: 1.5325\n",
      "380/388, train_loss: 0.1476, step time: 1.5296\n",
      "381/388, train_loss: 0.5226, step time: 1.5360\n",
      "382/388, train_loss: 0.3701, step time: 1.5479\n",
      "383/388, train_loss: 0.3799, step time: 1.5331\n",
      "384/388, train_loss: 0.4344, step time: 1.5337\n",
      "385/388, train_loss: 0.6747, step time: 1.5341\n",
      "386/388, train_loss: 0.2775, step time: 1.5480\n",
      "387/388, train_loss: 0.3716, step time: 1.5365\n",
      "388/388, train_loss: 0.1875, step time: 1.5349\n",
      "epoch 8 average loss: 0.3378\n",
      "saved new best metric model\n",
      "current epoch: 8 current mean dice: 0.6797 tc: 0.7250 wt: 0.8586 et: 0.4555\n",
      "best mean dice: 0.6797 at epoch: 8\n",
      "time consuming of epoch 8 is: 703.6443\n",
      "----------\n",
      "epoch 9/100\n",
      "1/388, train_loss: 0.2722, step time: 1.5527\n",
      "2/388, train_loss: 0.3533, step time: 1.5368\n",
      "3/388, train_loss: 0.6084, step time: 1.5331\n",
      "4/388, train_loss: 0.2064, step time: 1.5366\n",
      "5/388, train_loss: 0.1491, step time: 1.5336\n",
      "6/388, train_loss: 0.3294, step time: 1.5334\n",
      "7/388, train_loss: 0.2385, step time: 1.5315\n",
      "8/388, train_loss: 0.4760, step time: 1.5345\n",
      "9/388, train_loss: 0.4299, step time: 1.5370\n",
      "10/388, train_loss: 0.3712, step time: 1.5346\n",
      "11/388, train_loss: 0.3526, step time: 1.5341\n",
      "12/388, train_loss: 0.2354, step time: 1.5403\n",
      "13/388, train_loss: 0.2091, step time: 1.5302\n",
      "14/388, train_loss: 0.5206, step time: 1.5344\n",
      "15/388, train_loss: 0.4426, step time: 1.5369\n",
      "16/388, train_loss: 0.2697, step time: 1.5372\n",
      "17/388, train_loss: 0.3217, step time: 1.5349\n",
      "18/388, train_loss: 0.3422, step time: 1.5357\n",
      "19/388, train_loss: 0.4437, step time: 1.5388\n",
      "20/388, train_loss: 0.2403, step time: 1.5330\n",
      "21/388, train_loss: 0.2993, step time: 1.5322\n",
      "22/388, train_loss: 0.1599, step time: 1.5338\n",
      "23/388, train_loss: 0.2676, step time: 1.5311\n",
      "24/388, train_loss: 0.4155, step time: 1.5332\n",
      "25/388, train_loss: 0.3086, step time: 1.5347\n",
      "26/388, train_loss: 0.1930, step time: 1.5377\n",
      "27/388, train_loss: 0.2215, step time: 1.5373\n",
      "28/388, train_loss: 0.2779, step time: 1.5338\n",
      "29/388, train_loss: 0.6049, step time: 1.5338\n",
      "30/388, train_loss: 0.2640, step time: 1.5402\n",
      "31/388, train_loss: 0.5838, step time: 1.5360\n",
      "32/388, train_loss: 0.4930, step time: 1.5371\n",
      "33/388, train_loss: 0.2141, step time: 1.5298\n",
      "34/388, train_loss: 0.3269, step time: 1.5350\n",
      "35/388, train_loss: 0.1529, step time: 1.5315\n",
      "36/388, train_loss: 0.5310, step time: 1.5361\n",
      "37/388, train_loss: 0.3593, step time: 1.5350\n",
      "38/388, train_loss: 0.7714, step time: 1.5342\n",
      "39/388, train_loss: 0.2897, step time: 1.5337\n",
      "40/388, train_loss: 0.3861, step time: 1.5323\n",
      "41/388, train_loss: 0.4873, step time: 1.5360\n",
      "42/388, train_loss: 0.2635, step time: 1.5365\n",
      "43/388, train_loss: 0.3511, step time: 1.5368\n",
      "44/388, train_loss: 0.3161, step time: 1.5345\n",
      "45/388, train_loss: 0.1806, step time: 1.5373\n",
      "46/388, train_loss: 0.4749, step time: 1.5349\n",
      "47/388, train_loss: 0.0992, step time: 1.5342\n",
      "48/388, train_loss: 0.2391, step time: 1.5328\n",
      "49/388, train_loss: 0.4140, step time: 1.5311\n",
      "50/388, train_loss: 0.2044, step time: 1.5325\n",
      "51/388, train_loss: 0.1307, step time: 1.5307\n",
      "52/388, train_loss: 0.6754, step time: 1.5339\n",
      "53/388, train_loss: 0.5298, step time: 1.5363\n",
      "54/388, train_loss: 0.7467, step time: 1.5344\n",
      "55/388, train_loss: 0.3552, step time: 1.5335\n",
      "56/388, train_loss: 0.6309, step time: 1.5312\n",
      "57/388, train_loss: 0.4179, step time: 1.5320\n",
      "58/388, train_loss: 0.2648, step time: 1.5385\n",
      "59/388, train_loss: 0.4563, step time: 1.5332\n",
      "60/388, train_loss: 0.4482, step time: 1.5355\n",
      "61/388, train_loss: 0.2138, step time: 1.5327\n",
      "62/388, train_loss: 0.2623, step time: 1.5364\n",
      "63/388, train_loss: 0.1748, step time: 1.5321\n",
      "64/388, train_loss: 0.2034, step time: 1.5308\n",
      "65/388, train_loss: 0.1495, step time: 1.5314\n",
      "66/388, train_loss: 0.4987, step time: 1.5318\n",
      "67/388, train_loss: 0.2172, step time: 1.5353\n",
      "68/388, train_loss: 0.5417, step time: 1.5360\n",
      "69/388, train_loss: 0.2597, step time: 1.5335\n",
      "70/388, train_loss: 0.2897, step time: 1.5350\n",
      "71/388, train_loss: 0.2499, step time: 1.5312\n",
      "72/388, train_loss: 0.3265, step time: 1.5299\n",
      "73/388, train_loss: 0.4172, step time: 1.5327\n",
      "74/388, train_loss: 0.3317, step time: 1.5333\n",
      "75/388, train_loss: 0.2337, step time: 1.5350\n",
      "76/388, train_loss: 0.2290, step time: 1.5334\n",
      "77/388, train_loss: 0.2591, step time: 1.5333\n",
      "78/388, train_loss: 0.2208, step time: 1.5353\n",
      "79/388, train_loss: 0.2237, step time: 1.5424\n",
      "80/388, train_loss: 0.1607, step time: 1.5305\n",
      "81/388, train_loss: 0.4942, step time: 1.5338\n",
      "82/388, train_loss: 0.2657, step time: 1.5336\n",
      "83/388, train_loss: 0.0949, step time: 1.5311\n",
      "84/388, train_loss: 0.1874, step time: 1.5365\n",
      "85/388, train_loss: 0.1113, step time: 1.5340\n",
      "86/388, train_loss: 0.4911, step time: 1.5329\n",
      "87/388, train_loss: 0.1932, step time: 1.5290\n",
      "88/388, train_loss: 0.3012, step time: 1.5326\n",
      "89/388, train_loss: 0.3606, step time: 1.5306\n",
      "90/388, train_loss: 0.2089, step time: 1.5343\n",
      "91/388, train_loss: 0.5621, step time: 1.5345\n",
      "92/388, train_loss: 0.3062, step time: 1.5374\n",
      "93/388, train_loss: 0.1553, step time: 1.5303\n",
      "94/388, train_loss: 0.2609, step time: 1.5415\n",
      "95/388, train_loss: 0.2632, step time: 1.5373\n",
      "96/388, train_loss: 0.6748, step time: 1.5350\n",
      "97/388, train_loss: 0.3557, step time: 1.5308\n",
      "98/388, train_loss: 0.4837, step time: 1.5321\n",
      "99/388, train_loss: 0.4108, step time: 1.5331\n",
      "100/388, train_loss: 0.2503, step time: 1.5334\n",
      "101/388, train_loss: 0.4600, step time: 1.5338\n",
      "102/388, train_loss: 0.3214, step time: 1.5332\n",
      "103/388, train_loss: 0.1935, step time: 1.5326\n",
      "104/388, train_loss: 0.1464, step time: 1.5316\n",
      "105/388, train_loss: 0.1431, step time: 1.5314\n",
      "106/388, train_loss: 0.2565, step time: 1.5346\n",
      "107/388, train_loss: 0.3641, step time: 1.5346\n",
      "108/388, train_loss: 0.2361, step time: 1.5427\n",
      "109/388, train_loss: 0.2703, step time: 1.5336\n",
      "110/388, train_loss: 0.7674, step time: 1.5291\n",
      "111/388, train_loss: 0.2784, step time: 1.5328\n",
      "112/388, train_loss: 0.1649, step time: 1.5346\n",
      "113/388, train_loss: 0.3405, step time: 1.5433\n",
      "114/388, train_loss: 0.2281, step time: 1.5371\n",
      "115/388, train_loss: 0.0994, step time: 1.5294\n",
      "116/388, train_loss: 0.2377, step time: 1.5314\n",
      "117/388, train_loss: 0.0848, step time: 1.5281\n",
      "118/388, train_loss: 0.2342, step time: 1.5296\n",
      "119/388, train_loss: 0.6840, step time: 1.5526\n",
      "120/388, train_loss: 0.0989, step time: 1.5322\n",
      "121/388, train_loss: 0.2770, step time: 1.5370\n",
      "122/388, train_loss: 0.1137, step time: 1.5337\n",
      "123/388, train_loss: 0.2520, step time: 1.5314\n",
      "124/388, train_loss: 0.1924, step time: 1.5377\n",
      "125/388, train_loss: 0.1491, step time: 1.5326\n",
      "126/388, train_loss: 0.2406, step time: 1.5323\n",
      "127/388, train_loss: 0.6658, step time: 1.5292\n",
      "128/388, train_loss: 0.2466, step time: 1.5303\n",
      "129/388, train_loss: 0.3223, step time: 1.5344\n",
      "130/388, train_loss: 0.1609, step time: 1.5399\n",
      "131/388, train_loss: 0.3715, step time: 1.5351\n",
      "132/388, train_loss: 0.3002, step time: 1.5381\n",
      "133/388, train_loss: 0.3777, step time: 1.5334\n",
      "134/388, train_loss: 0.2027, step time: 1.5321\n",
      "135/388, train_loss: 0.5672, step time: 1.5345\n",
      "136/388, train_loss: 0.4121, step time: 1.5396\n",
      "137/388, train_loss: 0.2339, step time: 1.5324\n",
      "138/388, train_loss: 0.3121, step time: 1.5308\n",
      "139/388, train_loss: 0.3776, step time: 1.5294\n",
      "140/388, train_loss: 0.2921, step time: 1.5318\n",
      "141/388, train_loss: 0.4737, step time: 1.5300\n",
      "142/388, train_loss: 0.2250, step time: 1.5352\n",
      "143/388, train_loss: 0.2508, step time: 1.5344\n",
      "144/388, train_loss: 0.1966, step time: 1.5362\n",
      "145/388, train_loss: 0.4510, step time: 1.5288\n",
      "146/388, train_loss: 0.3671, step time: 1.5309\n",
      "147/388, train_loss: 0.4320, step time: 1.5351\n",
      "148/388, train_loss: 0.4408, step time: 1.5296\n",
      "149/388, train_loss: 0.4885, step time: 1.5312\n",
      "150/388, train_loss: 0.2160, step time: 1.5372\n",
      "151/388, train_loss: 0.3102, step time: 1.5369\n",
      "152/388, train_loss: 0.1676, step time: 1.5299\n",
      "153/388, train_loss: 0.3639, step time: 1.5350\n",
      "154/388, train_loss: 0.2773, step time: 1.5341\n",
      "155/388, train_loss: 0.2668, step time: 1.5298\n",
      "156/388, train_loss: 0.2303, step time: 1.5312\n",
      "157/388, train_loss: 0.2764, step time: 1.5314\n",
      "158/388, train_loss: 0.2531, step time: 1.5309\n",
      "159/388, train_loss: 0.5530, step time: 1.5328\n",
      "160/388, train_loss: 0.3208, step time: 1.5378\n",
      "161/388, train_loss: 0.3351, step time: 1.5315\n",
      "162/388, train_loss: 0.1880, step time: 1.5307\n",
      "163/388, train_loss: 0.3393, step time: 1.5327\n",
      "164/388, train_loss: 0.1951, step time: 1.5331\n",
      "165/388, train_loss: 0.3114, step time: 1.5281\n",
      "166/388, train_loss: 0.1907, step time: 1.5333\n",
      "167/388, train_loss: 0.1481, step time: 1.5369\n",
      "168/388, train_loss: 0.1073, step time: 1.5335\n",
      "169/388, train_loss: 0.1966, step time: 1.5285\n",
      "170/388, train_loss: 0.1156, step time: 1.5332\n",
      "171/388, train_loss: 0.2815, step time: 1.5354\n",
      "172/388, train_loss: 0.1498, step time: 1.5368\n",
      "173/388, train_loss: 0.1909, step time: 1.5325\n",
      "174/388, train_loss: 0.3414, step time: 1.5346\n",
      "175/388, train_loss: 0.1229, step time: 1.5300\n",
      "176/388, train_loss: 0.6867, step time: 1.5320\n",
      "177/388, train_loss: 0.5806, step time: 1.5330\n",
      "178/388, train_loss: 0.2050, step time: 1.5346\n",
      "179/388, train_loss: 0.2683, step time: 1.5338\n",
      "180/388, train_loss: 0.5057, step time: 1.5346\n",
      "181/388, train_loss: 0.3826, step time: 1.5342\n",
      "182/388, train_loss: 0.2686, step time: 1.5335\n",
      "183/388, train_loss: 0.2867, step time: 1.5359\n",
      "184/388, train_loss: 0.2831, step time: 1.5323\n",
      "185/388, train_loss: 0.1967, step time: 1.5392\n",
      "186/388, train_loss: 0.2444, step time: 1.5355\n",
      "187/388, train_loss: 0.2016, step time: 1.5350\n",
      "188/388, train_loss: 0.3137, step time: 1.5454\n",
      "189/388, train_loss: 0.1804, step time: 1.5325\n",
      "190/388, train_loss: 0.2821, step time: 1.5344\n",
      "191/388, train_loss: 0.3799, step time: 1.5359\n",
      "192/388, train_loss: 0.2012, step time: 1.5363\n",
      "193/388, train_loss: 0.3384, step time: 1.5348\n",
      "194/388, train_loss: 0.5255, step time: 1.5297\n",
      "195/388, train_loss: 0.3275, step time: 1.5305\n",
      "196/388, train_loss: 0.2273, step time: 1.5316\n",
      "197/388, train_loss: 0.7047, step time: 1.5317\n",
      "198/388, train_loss: 0.2836, step time: 1.5339\n",
      "199/388, train_loss: 0.3211, step time: 1.5318\n",
      "200/388, train_loss: 0.4163, step time: 1.5355\n",
      "201/388, train_loss: 0.3185, step time: 1.5351\n",
      "202/388, train_loss: 0.2803, step time: 1.5320\n",
      "203/388, train_loss: 0.4021, step time: 1.5307\n",
      "204/388, train_loss: 0.3582, step time: 1.5382\n",
      "205/388, train_loss: 0.2992, step time: 1.5450\n",
      "206/388, train_loss: 0.3405, step time: 1.5311\n",
      "207/388, train_loss: 0.2291, step time: 1.5325\n",
      "208/388, train_loss: 0.2758, step time: 1.5318\n",
      "209/388, train_loss: 0.4018, step time: 1.5311\n",
      "210/388, train_loss: 0.2121, step time: 1.5350\n",
      "211/388, train_loss: 0.3573, step time: 1.5335\n",
      "212/388, train_loss: 0.3044, step time: 1.5363\n",
      "213/388, train_loss: 0.2868, step time: 1.5317\n",
      "214/388, train_loss: 0.1978, step time: 1.5355\n",
      "215/388, train_loss: 0.1905, step time: 1.5302\n",
      "216/388, train_loss: 0.5592, step time: 1.5321\n",
      "217/388, train_loss: 0.1610, step time: 1.5299\n",
      "218/388, train_loss: 0.2802, step time: 1.5344\n",
      "219/388, train_loss: 0.2537, step time: 1.5339\n",
      "220/388, train_loss: 0.3102, step time: 1.5652\n",
      "221/388, train_loss: 0.5585, step time: 1.5324\n",
      "222/388, train_loss: 0.2506, step time: 1.5321\n",
      "223/388, train_loss: 0.1309, step time: 1.5320\n",
      "224/388, train_loss: 0.2962, step time: 1.5354\n",
      "225/388, train_loss: 0.2846, step time: 1.5312\n",
      "226/388, train_loss: 0.6321, step time: 1.5338\n",
      "227/388, train_loss: 0.3238, step time: 1.5343\n",
      "228/388, train_loss: 0.7211, step time: 1.5372\n",
      "229/388, train_loss: 0.2614, step time: 1.5320\n",
      "230/388, train_loss: 0.4659, step time: 1.5301\n",
      "231/388, train_loss: 0.2668, step time: 1.5329\n",
      "232/388, train_loss: 0.2039, step time: 1.5315\n",
      "233/388, train_loss: 0.2523, step time: 1.5359\n",
      "234/388, train_loss: 0.4067, step time: 1.5307\n",
      "235/388, train_loss: 0.5087, step time: 1.5304\n",
      "236/388, train_loss: 0.2601, step time: 1.5317\n",
      "237/388, train_loss: 0.3022, step time: 1.5333\n",
      "238/388, train_loss: 0.2468, step time: 1.5356\n",
      "239/388, train_loss: 0.4968, step time: 1.5351\n",
      "240/388, train_loss: 0.3277, step time: 1.5337\n",
      "241/388, train_loss: 0.4516, step time: 1.5347\n",
      "242/388, train_loss: 0.3225, step time: 1.5344\n",
      "243/388, train_loss: 0.1718, step time: 1.5361\n",
      "244/388, train_loss: 0.2649, step time: 1.5358\n",
      "245/388, train_loss: 0.3962, step time: 1.5327\n",
      "246/388, train_loss: 0.3100, step time: 1.5340\n",
      "247/388, train_loss: 0.3495, step time: 1.5301\n",
      "248/388, train_loss: 0.2210, step time: 1.5316\n",
      "249/388, train_loss: 0.3161, step time: 1.5345\n",
      "250/388, train_loss: 0.2745, step time: 1.5343\n",
      "251/388, train_loss: 0.6398, step time: 1.5370\n",
      "252/388, train_loss: 0.1884, step time: 1.5338\n",
      "253/388, train_loss: 0.2305, step time: 1.5348\n",
      "254/388, train_loss: 0.3233, step time: 1.5327\n",
      "255/388, train_loss: 0.3606, step time: 1.5326\n",
      "256/388, train_loss: 0.3489, step time: 1.5293\n",
      "257/388, train_loss: 0.1653, step time: 1.5335\n",
      "258/388, train_loss: 0.2939, step time: 1.5328\n",
      "259/388, train_loss: 0.6291, step time: 1.5430\n",
      "260/388, train_loss: 0.2360, step time: 1.5366\n",
      "261/388, train_loss: 0.3802, step time: 1.5350\n",
      "262/388, train_loss: 0.3166, step time: 1.5352\n",
      "263/388, train_loss: 0.2565, step time: 1.5310\n",
      "264/388, train_loss: 0.3127, step time: 1.5333\n",
      "265/388, train_loss: 0.2385, step time: 1.5330\n",
      "266/388, train_loss: 0.2426, step time: 1.5368\n",
      "267/388, train_loss: 0.5255, step time: 1.5352\n",
      "268/388, train_loss: 0.7473, step time: 1.5335\n",
      "269/388, train_loss: 0.2161, step time: 1.5332\n",
      "270/388, train_loss: 0.1578, step time: 1.5357\n",
      "271/388, train_loss: 0.3214, step time: 1.5350\n",
      "272/388, train_loss: 0.1499, step time: 1.5308\n",
      "273/388, train_loss: 0.1371, step time: 1.5364\n",
      "274/388, train_loss: 0.1949, step time: 1.5649\n",
      "275/388, train_loss: 0.4876, step time: 1.5318\n",
      "276/388, train_loss: 0.3225, step time: 1.5352\n",
      "277/388, train_loss: 0.4867, step time: 1.5448\n",
      "278/388, train_loss: 0.5765, step time: 1.5349\n",
      "279/388, train_loss: 0.3998, step time: 1.5330\n",
      "280/388, train_loss: 0.2286, step time: 1.5305\n",
      "281/388, train_loss: 0.2984, step time: 1.5334\n",
      "282/388, train_loss: 0.3127, step time: 1.5398\n",
      "283/388, train_loss: 0.3814, step time: 1.5332\n",
      "284/388, train_loss: 0.3400, step time: 1.5361\n",
      "285/388, train_loss: 0.4056, step time: 1.5326\n",
      "286/388, train_loss: 0.2843, step time: 1.5318\n",
      "287/388, train_loss: 0.2225, step time: 1.5315\n",
      "288/388, train_loss: 0.3575, step time: 1.5430\n",
      "289/388, train_loss: 0.2933, step time: 1.5338\n",
      "290/388, train_loss: 0.3803, step time: 1.5367\n",
      "291/388, train_loss: 0.1197, step time: 1.5341\n",
      "292/388, train_loss: 0.2598, step time: 1.5333\n",
      "293/388, train_loss: 0.1716, step time: 1.5315\n",
      "294/388, train_loss: 0.1236, step time: 1.5407\n",
      "295/388, train_loss: 0.3394, step time: 1.5374\n",
      "296/388, train_loss: 0.3042, step time: 1.5345\n",
      "297/388, train_loss: 0.3205, step time: 1.5312\n",
      "298/388, train_loss: 0.2043, step time: 1.5320\n",
      "299/388, train_loss: 0.3213, step time: 1.5311\n",
      "300/388, train_loss: 0.8171, step time: 1.5347\n",
      "301/388, train_loss: 0.1904, step time: 1.5342\n",
      "302/388, train_loss: 0.2648, step time: 1.5341\n",
      "303/388, train_loss: 0.1108, step time: 1.5341\n",
      "304/388, train_loss: 0.2854, step time: 1.5354\n",
      "305/388, train_loss: 0.1670, step time: 1.5333\n",
      "306/388, train_loss: 0.2279, step time: 1.5315\n",
      "307/388, train_loss: 0.2367, step time: 1.5325\n",
      "308/388, train_loss: 0.4214, step time: 1.5337\n",
      "309/388, train_loss: 0.4116, step time: 1.5331\n",
      "310/388, train_loss: 0.3811, step time: 1.5339\n",
      "311/388, train_loss: 0.3997, step time: 1.5358\n",
      "312/388, train_loss: 0.4320, step time: 1.5334\n",
      "313/388, train_loss: 0.1820, step time: 1.5281\n",
      "314/388, train_loss: 0.1234, step time: 1.5306\n",
      "315/388, train_loss: 0.2806, step time: 1.5341\n",
      "316/388, train_loss: 0.3853, step time: 1.5362\n",
      "317/388, train_loss: 0.7346, step time: 1.5353\n",
      "318/388, train_loss: 0.2468, step time: 1.5363\n",
      "319/388, train_loss: 0.6937, step time: 1.5318\n",
      "320/388, train_loss: 0.2366, step time: 1.5304\n",
      "321/388, train_loss: 0.3449, step time: 1.5629\n",
      "322/388, train_loss: 0.3992, step time: 1.5348\n",
      "323/388, train_loss: 0.1603, step time: 1.5320\n",
      "324/388, train_loss: 0.3636, step time: 1.5339\n",
      "325/388, train_loss: 0.2340, step time: 1.5293\n",
      "326/388, train_loss: 0.4260, step time: 1.5366\n",
      "327/388, train_loss: 0.2386, step time: 1.5339\n",
      "328/388, train_loss: 0.2686, step time: 1.5356\n",
      "329/388, train_loss: 0.4008, step time: 1.5329\n",
      "330/388, train_loss: 0.2739, step time: 1.5372\n",
      "331/388, train_loss: 0.4329, step time: 1.5310\n",
      "332/388, train_loss: 0.2649, step time: 1.5412\n",
      "333/388, train_loss: 0.4211, step time: 1.5291\n",
      "334/388, train_loss: 0.1251, step time: 1.5368\n",
      "335/388, train_loss: 0.3052, step time: 1.5351\n",
      "336/388, train_loss: 0.1878, step time: 1.5322\n",
      "337/388, train_loss: 0.3451, step time: 1.5338\n",
      "338/388, train_loss: 0.1523, step time: 1.5332\n",
      "339/388, train_loss: 0.2869, step time: 1.5298\n",
      "340/388, train_loss: 0.4902, step time: 1.5298\n",
      "341/388, train_loss: 0.4853, step time: 1.5323\n",
      "342/388, train_loss: 0.3844, step time: 1.5324\n",
      "343/388, train_loss: 0.4396, step time: 1.5429\n",
      "344/388, train_loss: 0.2671, step time: 1.5344\n",
      "345/388, train_loss: 0.4232, step time: 1.5657\n",
      "346/388, train_loss: 0.1565, step time: 1.5358\n",
      "347/388, train_loss: 0.1147, step time: 1.5320\n",
      "348/388, train_loss: 0.4307, step time: 1.5297\n",
      "349/388, train_loss: 0.2186, step time: 1.5302\n",
      "350/388, train_loss: 0.3870, step time: 1.5322\n",
      "351/388, train_loss: 0.2123, step time: 1.5317\n",
      "352/388, train_loss: 0.2855, step time: 1.5369\n",
      "353/388, train_loss: 0.1555, step time: 1.5354\n",
      "354/388, train_loss: 0.1650, step time: 1.5351\n",
      "355/388, train_loss: 0.4290, step time: 1.5304\n",
      "356/388, train_loss: 0.3405, step time: 1.5297\n",
      "357/388, train_loss: 0.3817, step time: 1.5295\n",
      "358/388, train_loss: 0.4841, step time: 1.5393\n",
      "359/388, train_loss: 0.2492, step time: 1.5341\n",
      "360/388, train_loss: 0.6395, step time: 1.5341\n",
      "361/388, train_loss: 0.1703, step time: 1.5335\n",
      "362/388, train_loss: 0.4214, step time: 1.5349\n",
      "363/388, train_loss: 0.1904, step time: 1.5332\n",
      "364/388, train_loss: 0.2283, step time: 1.5337\n",
      "365/388, train_loss: 0.1830, step time: 1.5314\n",
      "366/388, train_loss: 0.1389, step time: 1.5292\n",
      "367/388, train_loss: 0.2281, step time: 1.5302\n",
      "368/388, train_loss: 0.3026, step time: 1.5337\n",
      "369/388, train_loss: 0.3006, step time: 1.5617\n",
      "370/388, train_loss: 0.2296, step time: 1.5346\n",
      "371/388, train_loss: 0.8233, step time: 1.5289\n",
      "372/388, train_loss: 0.2381, step time: 1.5340\n",
      "373/388, train_loss: 0.1546, step time: 1.5397\n",
      "374/388, train_loss: 0.2857, step time: 1.5353\n",
      "375/388, train_loss: 0.2030, step time: 1.5474\n",
      "376/388, train_loss: 0.1301, step time: 1.5322\n",
      "377/388, train_loss: 0.2580, step time: 1.5309\n",
      "378/388, train_loss: 0.3161, step time: 1.5332\n",
      "379/388, train_loss: 0.2475, step time: 1.5311\n",
      "380/388, train_loss: 0.4323, step time: 1.5395\n",
      "381/388, train_loss: 0.3053, step time: 1.5350\n",
      "382/388, train_loss: 0.3114, step time: 1.5331\n",
      "383/388, train_loss: 0.1493, step time: 1.5305\n",
      "384/388, train_loss: 0.2735, step time: 1.5312\n",
      "385/388, train_loss: 0.1917, step time: 1.5303\n",
      "386/388, train_loss: 0.3120, step time: 1.5343\n",
      "387/388, train_loss: 0.2681, step time: 1.5361\n",
      "388/388, train_loss: 0.3369, step time: 1.5382\n",
      "epoch 9 average loss: 0.3172\n",
      "current epoch: 9 current mean dice: 0.6625 tc: 0.7371 wt: 0.8427 et: 0.4079\n",
      "best mean dice: 0.6797 at epoch: 8\n",
      "time consuming of epoch 9 is: 701.0676\n",
      "----------\n",
      "epoch 10/100\n",
      "1/388, train_loss: 0.3365, step time: 1.5550\n",
      "2/388, train_loss: 0.3468, step time: 1.5393\n",
      "3/388, train_loss: 0.1209, step time: 1.5329\n",
      "4/388, train_loss: 0.2401, step time: 1.5321\n",
      "5/388, train_loss: 0.2161, step time: 1.5365\n",
      "6/388, train_loss: 0.1393, step time: 1.5468\n",
      "7/388, train_loss: 0.2100, step time: 1.5368\n",
      "8/388, train_loss: 0.2476, step time: 1.5336\n",
      "9/388, train_loss: 0.3229, step time: 1.5316\n",
      "10/388, train_loss: 0.1938, step time: 1.5357\n",
      "11/388, train_loss: 0.2467, step time: 1.5341\n",
      "12/388, train_loss: 0.2410, step time: 1.5313\n",
      "13/388, train_loss: 0.2599, step time: 1.5321\n",
      "14/388, train_loss: 0.1386, step time: 1.5307\n",
      "15/388, train_loss: 0.4229, step time: 1.5351\n",
      "16/388, train_loss: 0.2852, step time: 1.5351\n",
      "17/388, train_loss: 0.5360, step time: 1.5309\n",
      "18/388, train_loss: 0.1979, step time: 1.5323\n",
      "19/388, train_loss: 0.5577, step time: 1.5312\n",
      "20/388, train_loss: 0.3591, step time: 1.5315\n",
      "21/388, train_loss: 0.4972, step time: 1.5302\n",
      "22/388, train_loss: 0.2590, step time: 1.5363\n",
      "23/388, train_loss: 0.5016, step time: 1.5358\n",
      "24/388, train_loss: 0.2278, step time: 1.5315\n",
      "25/388, train_loss: 0.6385, step time: 1.5335\n",
      "26/388, train_loss: 0.1479, step time: 1.5335\n",
      "27/388, train_loss: 0.3110, step time: 1.5343\n",
      "28/388, train_loss: 0.3941, step time: 1.5366\n",
      "29/388, train_loss: 0.1788, step time: 1.5365\n",
      "30/388, train_loss: 0.4375, step time: 1.5295\n",
      "31/388, train_loss: 0.2370, step time: 1.5300\n",
      "32/388, train_loss: 0.3322, step time: 1.5315\n",
      "33/388, train_loss: 0.3195, step time: 1.5473\n",
      "34/388, train_loss: 0.3973, step time: 1.5371\n",
      "35/388, train_loss: 0.1760, step time: 1.5334\n",
      "36/388, train_loss: 0.4663, step time: 1.5323\n",
      "37/388, train_loss: 0.2739, step time: 1.5309\n",
      "38/388, train_loss: 0.2954, step time: 1.5405\n",
      "39/388, train_loss: 0.3284, step time: 1.5356\n",
      "40/388, train_loss: 0.2978, step time: 1.5304\n",
      "41/388, train_loss: 0.2108, step time: 1.5327\n",
      "42/388, train_loss: 0.3493, step time: 1.5347\n",
      "43/388, train_loss: 0.3785, step time: 1.5368\n",
      "44/388, train_loss: 0.2315, step time: 1.5357\n",
      "45/388, train_loss: 0.2823, step time: 1.5337\n",
      "46/388, train_loss: 0.3177, step time: 1.5323\n",
      "47/388, train_loss: 0.4663, step time: 1.5325\n",
      "48/388, train_loss: 0.6084, step time: 1.5397\n",
      "49/388, train_loss: 0.1292, step time: 1.5317\n",
      "50/388, train_loss: 0.3642, step time: 1.5335\n",
      "51/388, train_loss: 0.2675, step time: 1.5366\n",
      "52/388, train_loss: 0.2593, step time: 1.5324\n",
      "53/388, train_loss: 0.3580, step time: 1.5313\n",
      "54/388, train_loss: 0.8003, step time: 1.5319\n",
      "55/388, train_loss: 0.2630, step time: 1.5352\n",
      "56/388, train_loss: 0.2680, step time: 1.5373\n",
      "57/388, train_loss: 0.2789, step time: 1.5332\n",
      "58/388, train_loss: 0.2325, step time: 1.5350\n",
      "59/388, train_loss: 0.2256, step time: 1.5308\n",
      "60/388, train_loss: 0.4538, step time: 1.5354\n",
      "61/388, train_loss: 0.2612, step time: 1.5307\n",
      "62/388, train_loss: 0.4083, step time: 1.5383\n",
      "63/388, train_loss: 0.3666, step time: 1.5322\n",
      "64/388, train_loss: 0.3168, step time: 1.5354\n",
      "65/388, train_loss: 0.3717, step time: 1.5330\n",
      "66/388, train_loss: 0.2328, step time: 1.5366\n",
      "67/388, train_loss: 0.2364, step time: 1.5276\n",
      "68/388, train_loss: 0.2039, step time: 1.5340\n",
      "69/388, train_loss: 0.2672, step time: 1.5348\n",
      "70/388, train_loss: 0.4698, step time: 1.5339\n",
      "71/388, train_loss: 0.2315, step time: 1.5342\n",
      "72/388, train_loss: 0.2866, step time: 1.5330\n",
      "73/388, train_loss: 0.2259, step time: 1.5316\n",
      "74/388, train_loss: 0.2249, step time: 1.5326\n",
      "75/388, train_loss: 0.1764, step time: 1.5344\n",
      "76/388, train_loss: 0.7848, step time: 1.5418\n",
      "77/388, train_loss: 0.3395, step time: 1.5386\n",
      "78/388, train_loss: 0.2902, step time: 1.5314\n",
      "79/388, train_loss: 0.2146, step time: 1.5320\n",
      "80/388, train_loss: 0.2416, step time: 1.5334\n",
      "81/388, train_loss: 0.3017, step time: 1.5298\n",
      "82/388, train_loss: 0.4435, step time: 1.5382\n",
      "83/388, train_loss: 0.2848, step time: 1.5372\n",
      "84/388, train_loss: 0.3696, step time: 1.5338\n",
      "85/388, train_loss: 0.3015, step time: 1.5343\n",
      "86/388, train_loss: 0.4659, step time: 1.5323\n",
      "87/388, train_loss: 0.2340, step time: 1.5323\n",
      "88/388, train_loss: 0.2682, step time: 1.5323\n",
      "89/388, train_loss: 0.1952, step time: 1.5496\n",
      "90/388, train_loss: 0.6953, step time: 1.5344\n",
      "91/388, train_loss: 0.1730, step time: 1.5322\n",
      "92/388, train_loss: 0.1105, step time: 1.5350\n",
      "93/388, train_loss: 0.2782, step time: 1.5389\n",
      "94/388, train_loss: 0.1622, step time: 1.5357\n",
      "95/388, train_loss: 0.2933, step time: 1.5326\n",
      "96/388, train_loss: 0.2541, step time: 1.5328\n",
      "97/388, train_loss: 0.2862, step time: 1.5308\n",
      "98/388, train_loss: 0.2306, step time: 1.5303\n",
      "99/388, train_loss: 0.3763, step time: 1.5302\n",
      "100/388, train_loss: 0.4277, step time: 1.5341\n",
      "101/388, train_loss: 0.4213, step time: 1.5346\n",
      "102/388, train_loss: 0.1822, step time: 1.5346\n",
      "103/388, train_loss: 0.2660, step time: 1.5332\n",
      "104/388, train_loss: 0.3235, step time: 1.5350\n",
      "105/388, train_loss: 0.1953, step time: 1.5323\n",
      "106/388, train_loss: 0.1875, step time: 1.5321\n",
      "107/388, train_loss: 0.3392, step time: 1.5345\n",
      "108/388, train_loss: 0.4756, step time: 1.5362\n",
      "109/388, train_loss: 0.2355, step time: 1.5350\n",
      "110/388, train_loss: 0.7050, step time: 1.5347\n",
      "111/388, train_loss: 0.2471, step time: 1.5335\n",
      "112/388, train_loss: 0.4577, step time: 1.5327\n",
      "113/388, train_loss: 0.2590, step time: 1.5325\n",
      "114/388, train_loss: 0.2583, step time: 1.5351\n",
      "115/388, train_loss: 0.1788, step time: 1.5348\n",
      "116/388, train_loss: 0.0824, step time: 1.5321\n",
      "117/388, train_loss: 0.4088, step time: 1.5332\n",
      "118/388, train_loss: 0.2220, step time: 1.5328\n",
      "119/388, train_loss: 0.2715, step time: 1.5327\n",
      "120/388, train_loss: 0.7384, step time: 1.5363\n",
      "121/388, train_loss: 0.3994, step time: 1.5373\n",
      "122/388, train_loss: 0.1638, step time: 1.5339\n",
      "123/388, train_loss: 0.4454, step time: 1.5304\n",
      "124/388, train_loss: 0.1657, step time: 1.5315\n",
      "125/388, train_loss: 0.5151, step time: 1.5310\n",
      "126/388, train_loss: 0.6053, step time: 1.5310\n",
      "127/388, train_loss: 0.1369, step time: 1.5359\n",
      "128/388, train_loss: 0.1570, step time: 1.5339\n",
      "129/388, train_loss: 0.6593, step time: 1.5337\n",
      "130/388, train_loss: 0.4120, step time: 1.5342\n",
      "131/388, train_loss: 0.2977, step time: 1.5316\n",
      "132/388, train_loss: 0.2832, step time: 1.5290\n",
      "133/388, train_loss: 0.2249, step time: 1.5310\n",
      "134/388, train_loss: 0.2015, step time: 1.5418\n",
      "135/388, train_loss: 0.2176, step time: 1.5363\n",
      "136/388, train_loss: 0.0776, step time: 1.5354\n",
      "137/388, train_loss: 0.2148, step time: 1.5323\n",
      "138/388, train_loss: 0.1831, step time: 1.5324\n",
      "139/388, train_loss: 0.1148, step time: 1.5312\n",
      "140/388, train_loss: 0.4333, step time: 1.5340\n",
      "141/388, train_loss: 0.2314, step time: 1.5317\n",
      "142/388, train_loss: 0.1441, step time: 1.5352\n",
      "143/388, train_loss: 0.2319, step time: 1.5335\n",
      "144/388, train_loss: 0.4016, step time: 1.5364\n",
      "145/388, train_loss: 0.2273, step time: 1.5389\n",
      "146/388, train_loss: 0.2638, step time: 1.5319\n",
      "147/388, train_loss: 0.2088, step time: 1.5332\n",
      "148/388, train_loss: 0.1590, step time: 1.5337\n",
      "149/388, train_loss: 0.3870, step time: 1.5339\n",
      "150/388, train_loss: 0.7554, step time: 1.5303\n",
      "151/388, train_loss: 0.3628, step time: 1.5300\n",
      "152/388, train_loss: 0.2322, step time: 1.5316\n",
      "153/388, train_loss: 0.3092, step time: 1.5340\n",
      "154/388, train_loss: 0.1472, step time: 1.5339\n",
      "155/388, train_loss: 0.2933, step time: 1.5347\n",
      "156/388, train_loss: 0.4976, step time: 1.5359\n",
      "157/388, train_loss: 0.3275, step time: 1.5337\n",
      "158/388, train_loss: 0.5810, step time: 1.5325\n",
      "159/388, train_loss: 0.3035, step time: 1.5292\n",
      "160/388, train_loss: 0.2079, step time: 1.5393\n",
      "161/388, train_loss: 0.1976, step time: 1.5411\n",
      "162/388, train_loss: 0.2457, step time: 1.5354\n",
      "163/388, train_loss: 0.5035, step time: 1.5353\n",
      "164/388, train_loss: 0.1891, step time: 1.5312\n",
      "165/388, train_loss: 0.1112, step time: 1.5325\n",
      "166/388, train_loss: 0.2147, step time: 1.5343\n",
      "167/388, train_loss: 0.3133, step time: 1.5349\n",
      "168/388, train_loss: 0.4487, step time: 1.5389\n",
      "169/388, train_loss: 0.3402, step time: 1.5330\n",
      "170/388, train_loss: 0.2194, step time: 1.5338\n",
      "171/388, train_loss: 0.2345, step time: 1.5357\n",
      "172/388, train_loss: 0.2512, step time: 1.5343\n",
      "173/388, train_loss: 0.1890, step time: 1.5333\n",
      "174/388, train_loss: 0.1281, step time: 1.5332\n",
      "175/388, train_loss: 0.1020, step time: 1.5341\n",
      "176/388, train_loss: 0.3180, step time: 1.5488\n",
      "177/388, train_loss: 0.2583, step time: 1.5329\n",
      "178/388, train_loss: 0.3549, step time: 1.5322\n",
      "179/388, train_loss: 0.4916, step time: 1.5330\n",
      "180/388, train_loss: 0.2444, step time: 1.5341\n",
      "181/388, train_loss: 0.1844, step time: 1.5352\n",
      "182/388, train_loss: 0.2851, step time: 1.5441\n",
      "183/388, train_loss: 0.2247, step time: 1.5359\n",
      "184/388, train_loss: 0.1304, step time: 1.5354\n",
      "185/388, train_loss: 0.1592, step time: 1.5326\n",
      "186/388, train_loss: 0.2805, step time: 1.5308\n",
      "187/388, train_loss: 0.7035, step time: 1.5381\n",
      "188/388, train_loss: 0.2891, step time: 1.5424\n",
      "189/388, train_loss: 0.1760, step time: 1.5322\n",
      "190/388, train_loss: 0.3068, step time: 1.5290\n",
      "191/388, train_loss: 0.3127, step time: 1.5303\n",
      "192/388, train_loss: 0.3414, step time: 1.5318\n",
      "193/388, train_loss: 0.0927, step time: 1.5394\n",
      "194/388, train_loss: 0.3259, step time: 1.5357\n",
      "195/388, train_loss: 0.2082, step time: 1.5375\n",
      "196/388, train_loss: 0.2549, step time: 1.5442\n",
      "197/388, train_loss: 0.1026, step time: 1.5312\n",
      "198/388, train_loss: 0.3336, step time: 1.5334\n",
      "199/388, train_loss: 0.2827, step time: 1.5347\n",
      "200/388, train_loss: 0.4123, step time: 1.5346\n",
      "201/388, train_loss: 0.6495, step time: 1.5301\n",
      "202/388, train_loss: 0.2337, step time: 1.5323\n",
      "203/388, train_loss: 0.1927, step time: 1.5407\n",
      "204/388, train_loss: 0.1885, step time: 1.5383\n",
      "205/388, train_loss: 0.2500, step time: 1.5352\n",
      "206/388, train_loss: 0.3259, step time: 1.5361\n",
      "207/388, train_loss: 0.3309, step time: 1.5305\n",
      "208/388, train_loss: 0.2895, step time: 1.5292\n",
      "209/388, train_loss: 0.0990, step time: 1.5335\n",
      "210/388, train_loss: 0.1338, step time: 1.5336\n",
      "211/388, train_loss: 0.3115, step time: 1.5370\n",
      "212/388, train_loss: 0.3496, step time: 1.5323\n",
      "213/388, train_loss: 0.5113, step time: 1.5309\n",
      "214/388, train_loss: 0.5606, step time: 1.5349\n",
      "215/388, train_loss: 0.1756, step time: 1.5349\n",
      "216/388, train_loss: 0.3211, step time: 1.5417\n",
      "217/388, train_loss: 0.3156, step time: 1.5327\n",
      "218/388, train_loss: 0.1301, step time: 1.5319\n",
      "219/388, train_loss: 0.1241, step time: 1.5609\n",
      "220/388, train_loss: 0.7527, step time: 1.5341\n",
      "221/388, train_loss: 0.2633, step time: 1.5344\n",
      "222/388, train_loss: 0.4958, step time: 1.5394\n",
      "223/388, train_loss: 0.1651, step time: 1.5330\n",
      "224/388, train_loss: 0.2307, step time: 1.5345\n",
      "225/388, train_loss: 0.1841, step time: 1.5326\n",
      "226/388, train_loss: 0.4837, step time: 1.5367\n",
      "227/388, train_loss: 0.1005, step time: 1.5343\n",
      "228/388, train_loss: 0.3187, step time: 1.5329\n",
      "229/388, train_loss: 0.3524, step time: 1.5289\n",
      "230/388, train_loss: 0.3387, step time: 1.5331\n",
      "231/388, train_loss: 0.5183, step time: 1.5363\n",
      "232/388, train_loss: 0.1222, step time: 1.5384\n",
      "233/388, train_loss: 0.1785, step time: 1.5328\n",
      "234/388, train_loss: 0.2826, step time: 1.5330\n",
      "235/388, train_loss: 0.3610, step time: 1.5324\n",
      "236/388, train_loss: 0.4880, step time: 1.5309\n",
      "237/388, train_loss: 0.3725, step time: 1.5300\n",
      "238/388, train_loss: 0.4531, step time: 1.5459\n",
      "239/388, train_loss: 0.2208, step time: 1.5331\n",
      "240/388, train_loss: 0.1142, step time: 1.5335\n",
      "241/388, train_loss: 0.2619, step time: 1.5328\n",
      "242/388, train_loss: 0.2264, step time: 1.5326\n",
      "243/388, train_loss: 0.3623, step time: 1.5384\n",
      "244/388, train_loss: 0.3463, step time: 1.5348\n",
      "245/388, train_loss: 0.2199, step time: 1.5334\n",
      "246/388, train_loss: 0.3465, step time: 1.5337\n",
      "247/388, train_loss: 0.2981, step time: 1.5353\n",
      "248/388, train_loss: 0.3202, step time: 1.5443\n",
      "249/388, train_loss: 0.3475, step time: 1.5412\n",
      "250/388, train_loss: 0.4560, step time: 1.5374\n",
      "251/388, train_loss: 0.3745, step time: 1.5332\n",
      "252/388, train_loss: 0.2230, step time: 1.5363\n",
      "253/388, train_loss: 0.1948, step time: 1.5311\n",
      "254/388, train_loss: 0.1692, step time: 1.5307\n",
      "255/388, train_loss: 0.2887, step time: 1.5348\n",
      "256/388, train_loss: 0.2381, step time: 1.5358\n",
      "257/388, train_loss: 0.5332, step time: 1.5336\n",
      "258/388, train_loss: 0.4873, step time: 1.5323\n",
      "259/388, train_loss: 0.2246, step time: 1.5300\n",
      "260/388, train_loss: 0.2300, step time: 1.5348\n",
      "261/388, train_loss: 0.1610, step time: 1.5318\n",
      "262/388, train_loss: 0.1246, step time: 1.5364\n",
      "263/388, train_loss: 0.3661, step time: 1.5348\n",
      "264/388, train_loss: 0.3522, step time: 1.5324\n",
      "265/388, train_loss: 0.4856, step time: 1.5317\n",
      "266/388, train_loss: 0.2298, step time: 1.5328\n",
      "267/388, train_loss: 0.2210, step time: 1.5330\n",
      "268/388, train_loss: 0.4321, step time: 1.5318\n",
      "269/388, train_loss: 0.1561, step time: 1.5354\n",
      "270/388, train_loss: 0.1305, step time: 1.5339\n",
      "271/388, train_loss: 0.0982, step time: 1.5317\n",
      "272/388, train_loss: 0.1753, step time: 1.5321\n",
      "273/388, train_loss: 0.1658, step time: 1.5392\n",
      "274/388, train_loss: 0.6294, step time: 1.5317\n",
      "275/388, train_loss: 0.1714, step time: 1.5310\n",
      "276/388, train_loss: 0.1382, step time: 1.5320\n",
      "277/388, train_loss: 0.2272, step time: 1.5300\n",
      "278/388, train_loss: 0.1191, step time: 1.5517\n",
      "279/388, train_loss: 0.3451, step time: 1.5342\n",
      "280/388, train_loss: 0.1939, step time: 1.5342\n",
      "281/388, train_loss: 0.1930, step time: 1.5303\n",
      "282/388, train_loss: 0.2554, step time: 1.5338\n",
      "283/388, train_loss: 0.1547, step time: 1.5367\n",
      "284/388, train_loss: 0.4284, step time: 1.5352\n",
      "285/388, train_loss: 0.4562, step time: 1.5346\n",
      "286/388, train_loss: 0.1421, step time: 1.5310\n",
      "287/388, train_loss: 0.1521, step time: 1.5327\n",
      "288/388, train_loss: 0.3803, step time: 1.5349\n",
      "289/388, train_loss: 0.3841, step time: 1.5329\n",
      "290/388, train_loss: 0.1440, step time: 1.5387\n",
      "291/388, train_loss: 0.2905, step time: 1.5359\n",
      "292/388, train_loss: 0.2418, step time: 1.5350\n",
      "293/388, train_loss: 0.1814, step time: 1.5326\n",
      "294/388, train_loss: 0.7432, step time: 1.5306\n",
      "295/388, train_loss: 0.1810, step time: 1.5317\n",
      "296/388, train_loss: 0.3329, step time: 1.5362\n",
      "297/388, train_loss: 0.2375, step time: 1.5345\n",
      "298/388, train_loss: 0.2458, step time: 1.5359\n",
      "299/388, train_loss: 0.3308, step time: 1.5333\n",
      "300/388, train_loss: 0.3456, step time: 1.5316\n",
      "301/388, train_loss: 0.2578, step time: 1.5327\n",
      "302/388, train_loss: 0.2034, step time: 1.5321\n",
      "303/388, train_loss: 0.5573, step time: 1.5357\n",
      "304/388, train_loss: 0.2182, step time: 1.5366\n",
      "305/388, train_loss: 0.2770, step time: 1.5476\n",
      "306/388, train_loss: 0.3292, step time: 1.5332\n",
      "307/388, train_loss: 0.4695, step time: 1.5319\n",
      "308/388, train_loss: 0.2454, step time: 1.5336\n",
      "309/388, train_loss: 0.2487, step time: 1.5368\n",
      "310/388, train_loss: 0.3658, step time: 1.5333\n",
      "311/388, train_loss: 0.1256, step time: 1.5346\n",
      "312/388, train_loss: 0.1672, step time: 1.5311\n",
      "313/388, train_loss: 0.2424, step time: 1.5281\n",
      "314/388, train_loss: 0.2852, step time: 1.5314\n",
      "315/388, train_loss: 0.5516, step time: 1.5387\n",
      "316/388, train_loss: 0.2230, step time: 1.5325\n",
      "317/388, train_loss: 0.2738, step time: 1.5326\n",
      "318/388, train_loss: 0.3422, step time: 1.5432\n",
      "319/388, train_loss: 0.3691, step time: 1.5342\n",
      "320/388, train_loss: 0.1637, step time: 1.5319\n",
      "321/388, train_loss: 0.4083, step time: 1.5335\n",
      "322/388, train_loss: 0.1650, step time: 1.5292\n",
      "323/388, train_loss: 0.1769, step time: 1.5356\n",
      "324/388, train_loss: 0.0908, step time: 1.5364\n",
      "325/388, train_loss: 0.6850, step time: 1.5348\n",
      "326/388, train_loss: 0.2782, step time: 1.5334\n",
      "327/388, train_loss: 0.2694, step time: 1.5337\n",
      "328/388, train_loss: 0.2245, step time: 1.5338\n",
      "329/388, train_loss: 0.2179, step time: 1.5340\n",
      "330/388, train_loss: 0.5031, step time: 1.5368\n",
      "331/388, train_loss: 0.2132, step time: 1.5353\n",
      "332/388, train_loss: 0.1520, step time: 1.5338\n",
      "333/388, train_loss: 0.3455, step time: 1.5292\n",
      "334/388, train_loss: 0.1189, step time: 1.5316\n",
      "335/388, train_loss: 0.3642, step time: 1.5316\n",
      "336/388, train_loss: 0.1886, step time: 1.5306\n",
      "337/388, train_loss: 0.3970, step time: 1.5366\n",
      "338/388, train_loss: 0.3016, step time: 1.5372\n",
      "339/388, train_loss: 0.4336, step time: 1.5322\n",
      "340/388, train_loss: 0.2416, step time: 1.5386\n",
      "341/388, train_loss: 0.2845, step time: 1.5311\n",
      "342/388, train_loss: 0.3177, step time: 1.5332\n",
      "343/388, train_loss: 0.1459, step time: 1.5333\n",
      "344/388, train_loss: 0.1968, step time: 1.5363\n",
      "345/388, train_loss: 0.1860, step time: 1.5314\n",
      "346/388, train_loss: 0.2294, step time: 1.5335\n",
      "347/388, train_loss: 0.3421, step time: 1.5338\n",
      "348/388, train_loss: 0.1915, step time: 1.5468\n",
      "349/388, train_loss: 0.1651, step time: 1.5327\n",
      "350/388, train_loss: 0.6065, step time: 1.5307\n",
      "351/388, train_loss: 0.1686, step time: 1.5579\n",
      "352/388, train_loss: 0.2954, step time: 1.5341\n",
      "353/388, train_loss: 0.2539, step time: 1.5303\n",
      "354/388, train_loss: 0.2772, step time: 1.5345\n",
      "355/388, train_loss: 0.5482, step time: 1.5309\n",
      "356/388, train_loss: 0.1815, step time: 1.5416\n",
      "357/388, train_loss: 0.5474, step time: 1.5336\n",
      "358/388, train_loss: 0.1451, step time: 1.5318\n",
      "359/388, train_loss: 0.1303, step time: 1.5317\n",
      "360/388, train_loss: 0.2339, step time: 1.5329\n",
      "361/388, train_loss: 0.1970, step time: 1.5324\n",
      "362/388, train_loss: 0.5614, step time: 1.5348\n",
      "363/388, train_loss: 0.2454, step time: 1.5319\n",
      "364/388, train_loss: 0.1351, step time: 1.5384\n",
      "365/388, train_loss: 0.1729, step time: 1.5336\n",
      "366/388, train_loss: 0.1569, step time: 1.5312\n",
      "367/388, train_loss: 0.2474, step time: 1.5299\n",
      "368/388, train_loss: 0.3956, step time: 1.5315\n",
      "369/388, train_loss: 0.1794, step time: 1.5292\n",
      "370/388, train_loss: 0.2948, step time: 1.5317\n",
      "371/388, train_loss: 0.3646, step time: 1.5329\n",
      "372/388, train_loss: 0.6485, step time: 1.5326\n",
      "373/388, train_loss: 0.1888, step time: 1.5342\n",
      "374/388, train_loss: 0.3740, step time: 1.5321\n",
      "375/388, train_loss: 0.1131, step time: 1.5285\n",
      "376/388, train_loss: 0.1406, step time: 1.5306\n",
      "377/388, train_loss: 0.1114, step time: 1.5327\n",
      "378/388, train_loss: 0.1284, step time: 1.5286\n",
      "379/388, train_loss: 0.2642, step time: 1.5332\n",
      "380/388, train_loss: 0.5861, step time: 1.5348\n",
      "381/388, train_loss: 0.5674, step time: 1.5350\n",
      "382/388, train_loss: 0.1536, step time: 1.5356\n",
      "383/388, train_loss: 0.2103, step time: 1.5276\n",
      "384/388, train_loss: 0.2203, step time: 1.5278\n",
      "385/388, train_loss: 0.1709, step time: 1.5410\n",
      "386/388, train_loss: 0.1455, step time: 1.5343\n",
      "387/388, train_loss: 0.2729, step time: 1.5321\n",
      "388/388, train_loss: 0.5429, step time: 1.5339\n",
      "epoch 10 average loss: 0.2961\n",
      "saved new best metric model\n",
      "current epoch: 10 current mean dice: 0.6990 tc: 0.7384 wt: 0.8705 et: 0.4882\n",
      "best mean dice: 0.6990 at epoch: 10\n",
      "time consuming of epoch 10 is: 702.7454\n",
      "----------\n",
      "epoch 11/100\n",
      "1/388, train_loss: 0.3612, step time: 1.5487\n",
      "2/388, train_loss: 0.2037, step time: 1.5350\n",
      "3/388, train_loss: 0.4793, step time: 1.5355\n",
      "4/388, train_loss: 0.2438, step time: 1.5366\n",
      "5/388, train_loss: 0.1758, step time: 1.5318\n",
      "6/388, train_loss: 0.2161, step time: 1.5296\n",
      "7/388, train_loss: 0.3182, step time: 1.5332\n",
      "8/388, train_loss: 0.3247, step time: 1.5342\n",
      "9/388, train_loss: 0.1934, step time: 1.5304\n",
      "10/388, train_loss: 0.1584, step time: 1.5313\n",
      "11/388, train_loss: 0.4383, step time: 1.5320\n",
      "12/388, train_loss: 0.3031, step time: 1.5351\n",
      "13/388, train_loss: 0.2115, step time: 1.5310\n",
      "14/388, train_loss: 0.2847, step time: 1.5317\n",
      "15/388, train_loss: 0.2587, step time: 1.5359\n",
      "16/388, train_loss: 0.1530, step time: 1.5349\n",
      "17/388, train_loss: 0.2725, step time: 1.5349\n",
      "18/388, train_loss: 0.1647, step time: 1.5304\n",
      "19/388, train_loss: 0.2342, step time: 1.5298\n",
      "20/388, train_loss: 0.2919, step time: 1.5327\n",
      "21/388, train_loss: 0.3947, step time: 1.5310\n",
      "22/388, train_loss: 0.2745, step time: 1.5297\n",
      "23/388, train_loss: 0.1947, step time: 1.5298\n",
      "24/388, train_loss: 0.1195, step time: 1.5579\n",
      "25/388, train_loss: 0.1549, step time: 1.5631\n",
      "26/388, train_loss: 0.3546, step time: 1.5333\n",
      "27/388, train_loss: 0.7519, step time: 1.5373\n",
      "28/388, train_loss: 0.4187, step time: 1.5358\n",
      "29/388, train_loss: 0.4975, step time: 1.5370\n",
      "30/388, train_loss: 0.4756, step time: 1.5302\n",
      "31/388, train_loss: 0.3004, step time: 1.5332\n",
      "32/388, train_loss: 0.1530, step time: 1.5330\n",
      "33/388, train_loss: 0.2575, step time: 1.5326\n",
      "34/388, train_loss: 0.4184, step time: 1.5457\n",
      "35/388, train_loss: 0.2345, step time: 1.5312\n",
      "36/388, train_loss: 0.2461, step time: 1.5320\n",
      "37/388, train_loss: 0.3028, step time: 1.5340\n",
      "38/388, train_loss: 0.1804, step time: 1.5314\n",
      "39/388, train_loss: 0.3139, step time: 1.5334\n",
      "40/388, train_loss: 0.1902, step time: 1.5350\n",
      "41/388, train_loss: 0.3123, step time: 1.5380\n",
      "42/388, train_loss: 0.2234, step time: 1.5335\n",
      "43/388, train_loss: 0.1731, step time: 1.5309\n",
      "44/388, train_loss: 0.6807, step time: 1.5312\n",
      "45/388, train_loss: 0.3822, step time: 1.5327\n",
      "46/388, train_loss: 0.2808, step time: 1.5413\n",
      "47/388, train_loss: 0.1902, step time: 1.5332\n",
      "48/388, train_loss: 0.2308, step time: 1.5310\n",
      "49/388, train_loss: 0.3633, step time: 1.5306\n",
      "50/388, train_loss: 0.2606, step time: 1.5320\n",
      "51/388, train_loss: 0.4173, step time: 1.5304\n",
      "52/388, train_loss: 0.0903, step time: 1.5358\n",
      "53/388, train_loss: 0.2062, step time: 1.5332\n",
      "54/388, train_loss: 0.4020, step time: 1.5370\n",
      "55/388, train_loss: 0.2506, step time: 1.5326\n",
      "56/388, train_loss: 0.1767, step time: 1.5333\n",
      "57/388, train_loss: 0.2629, step time: 1.5304\n",
      "58/388, train_loss: 0.3275, step time: 1.5310\n",
      "59/388, train_loss: 0.1953, step time: 1.5305\n",
      "60/388, train_loss: 0.1793, step time: 1.5406\n",
      "61/388, train_loss: 0.1659, step time: 1.5356\n",
      "62/388, train_loss: 0.2307, step time: 1.5316\n",
      "63/388, train_loss: 0.1294, step time: 1.5334\n",
      "64/388, train_loss: 0.2156, step time: 1.5319\n",
      "65/388, train_loss: 0.1258, step time: 1.5441\n",
      "66/388, train_loss: 0.2652, step time: 1.5354\n",
      "67/388, train_loss: 0.3919, step time: 1.5335\n",
      "68/388, train_loss: 0.1851, step time: 1.5321\n",
      "69/388, train_loss: 0.2480, step time: 1.5297\n",
      "70/388, train_loss: 0.2665, step time: 1.5448\n",
      "71/388, train_loss: 0.4038, step time: 1.5322\n",
      "72/388, train_loss: 0.1805, step time: 1.5334\n",
      "73/388, train_loss: 0.3912, step time: 1.5294\n",
      "74/388, train_loss: 0.2806, step time: 1.5316\n",
      "75/388, train_loss: 0.3021, step time: 1.5327\n",
      "76/388, train_loss: 0.3165, step time: 1.5347\n",
      "77/388, train_loss: 0.5789, step time: 1.5371\n",
      "78/388, train_loss: 0.2438, step time: 1.5379\n",
      "79/388, train_loss: 0.2207, step time: 1.5307\n",
      "80/388, train_loss: 0.4504, step time: 1.5333\n",
      "81/388, train_loss: 0.1004, step time: 1.5351\n",
      "82/388, train_loss: 0.2377, step time: 1.5320\n",
      "83/388, train_loss: 0.2114, step time: 1.5318\n",
      "84/388, train_loss: 0.1596, step time: 1.5386\n",
      "85/388, train_loss: 0.1930, step time: 1.5301\n",
      "86/388, train_loss: 0.2418, step time: 1.5308\n",
      "87/388, train_loss: 0.6003, step time: 1.5338\n",
      "88/388, train_loss: 0.1075, step time: 1.5353\n",
      "89/388, train_loss: 0.1931, step time: 1.5358\n",
      "90/388, train_loss: 0.2252, step time: 1.5320\n",
      "91/388, train_loss: 0.1066, step time: 1.5290\n",
      "92/388, train_loss: 0.2741, step time: 1.5337\n",
      "93/388, train_loss: 0.2794, step time: 1.5297\n",
      "94/388, train_loss: 0.4730, step time: 1.5323\n",
      "95/388, train_loss: 0.1756, step time: 1.5351\n",
      "96/388, train_loss: 0.3501, step time: 1.5406\n",
      "97/388, train_loss: 0.4310, step time: 1.5311\n",
      "98/388, train_loss: 0.2955, step time: 1.5355\n",
      "99/388, train_loss: 0.4130, step time: 1.5357\n",
      "100/388, train_loss: 0.1105, step time: 1.5349\n",
      "101/388, train_loss: 0.3993, step time: 1.5324\n",
      "102/388, train_loss: 0.4609, step time: 1.5374\n",
      "103/388, train_loss: 0.1455, step time: 1.5322\n",
      "104/388, train_loss: 0.2977, step time: 1.5359\n",
      "105/388, train_loss: 0.2820, step time: 1.5370\n",
      "106/388, train_loss: 0.5308, step time: 1.5367\n",
      "107/388, train_loss: 0.4832, step time: 1.5352\n",
      "108/388, train_loss: 0.3795, step time: 1.5581\n",
      "109/388, train_loss: 0.1672, step time: 1.5348\n",
      "110/388, train_loss: 0.2241, step time: 1.5336\n",
      "111/388, train_loss: 0.4133, step time: 1.5362\n",
      "112/388, train_loss: 0.3280, step time: 1.5327\n",
      "113/388, train_loss: 0.1948, step time: 1.5333\n",
      "114/388, train_loss: 0.2062, step time: 1.5315\n",
      "115/388, train_loss: 0.1976, step time: 1.5322\n",
      "116/388, train_loss: 0.1909, step time: 1.5340\n",
      "117/388, train_loss: 0.2899, step time: 1.5379\n",
      "118/388, train_loss: 0.5454, step time: 1.5374\n",
      "119/388, train_loss: 0.1697, step time: 1.5313\n",
      "120/388, train_loss: 0.1541, step time: 1.5279\n",
      "121/388, train_loss: 0.4314, step time: 1.5318\n",
      "122/388, train_loss: 0.3468, step time: 1.5363\n",
      "123/388, train_loss: 0.3860, step time: 1.5333\n",
      "124/388, train_loss: 0.2382, step time: 1.5317\n",
      "125/388, train_loss: 0.1508, step time: 1.5341\n",
      "126/388, train_loss: 0.3215, step time: 1.5418\n",
      "127/388, train_loss: 0.1710, step time: 1.5421\n",
      "128/388, train_loss: 0.7111, step time: 1.5341\n",
      "129/388, train_loss: 0.1992, step time: 1.5327\n",
      "130/388, train_loss: 0.4345, step time: 1.5314\n",
      "131/388, train_loss: 0.3115, step time: 1.5268\n",
      "132/388, train_loss: 0.3414, step time: 1.5674\n",
      "133/388, train_loss: 0.3985, step time: 1.5548\n",
      "134/388, train_loss: 0.2971, step time: 1.5405\n",
      "135/388, train_loss: 0.4563, step time: 1.5546\n",
      "136/388, train_loss: 0.5056, step time: 1.5302\n",
      "137/388, train_loss: 0.2769, step time: 1.5378\n",
      "138/388, train_loss: 0.2737, step time: 1.5532\n",
      "139/388, train_loss: 0.2840, step time: 1.5365\n",
      "140/388, train_loss: 0.3127, step time: 1.5332\n",
      "141/388, train_loss: 0.2782, step time: 1.5305\n",
      "142/388, train_loss: 0.2259, step time: 1.5299\n",
      "143/388, train_loss: 0.2338, step time: 1.5297\n",
      "144/388, train_loss: 0.2176, step time: 1.5328\n",
      "145/388, train_loss: 0.2943, step time: 1.5316\n",
      "146/388, train_loss: 0.1196, step time: 1.5316\n",
      "147/388, train_loss: 0.5337, step time: 1.5364\n",
      "148/388, train_loss: 0.2847, step time: 1.5434\n",
      "149/388, train_loss: 0.2983, step time: 1.5294\n",
      "150/388, train_loss: 0.1550, step time: 1.5279\n",
      "151/388, train_loss: 0.3937, step time: 1.5302\n",
      "152/388, train_loss: 0.1147, step time: 1.5290\n",
      "153/388, train_loss: 0.2796, step time: 1.5318\n",
      "154/388, train_loss: 0.2638, step time: 1.5309\n",
      "155/388, train_loss: 0.3141, step time: 1.5318\n",
      "156/388, train_loss: 0.1327, step time: 1.5325\n",
      "157/388, train_loss: 0.5127, step time: 1.5337\n",
      "158/388, train_loss: 0.3829, step time: 1.5328\n",
      "159/388, train_loss: 0.1888, step time: 1.5295\n",
      "160/388, train_loss: 0.1671, step time: 1.5306\n",
      "161/388, train_loss: 0.2961, step time: 1.5532\n",
      "162/388, train_loss: 0.1548, step time: 1.5321\n",
      "163/388, train_loss: 0.2909, step time: 1.5376\n",
      "164/388, train_loss: 0.2191, step time: 1.5295\n",
      "165/388, train_loss: 0.7020, step time: 1.5308\n",
      "166/388, train_loss: 0.1905, step time: 1.5323\n",
      "167/388, train_loss: 0.1592, step time: 1.5351\n",
      "168/388, train_loss: 0.1782, step time: 1.5341\n",
      "169/388, train_loss: 0.2729, step time: 1.5388\n",
      "170/388, train_loss: 0.2197, step time: 1.5352\n",
      "171/388, train_loss: 0.3376, step time: 1.5296\n",
      "172/388, train_loss: 0.0649, step time: 1.5340\n",
      "173/388, train_loss: 0.5736, step time: 1.5488\n",
      "174/388, train_loss: 0.3752, step time: 1.5412\n",
      "175/388, train_loss: 0.1947, step time: 1.5383\n",
      "176/388, train_loss: 0.3394, step time: 1.5377\n",
      "177/388, train_loss: 0.3050, step time: 1.5439\n",
      "178/388, train_loss: 0.2447, step time: 1.5319\n",
      "179/388, train_loss: 0.2319, step time: 1.5316\n",
      "180/388, train_loss: 0.2060, step time: 1.5493\n",
      "181/388, train_loss: 0.1385, step time: 1.5418\n",
      "182/388, train_loss: 0.1626, step time: 1.5473\n",
      "183/388, train_loss: 0.7015, step time: 1.5363\n",
      "184/388, train_loss: 0.2758, step time: 1.5377\n",
      "185/388, train_loss: 0.1529, step time: 1.5588\n",
      "186/388, train_loss: 0.1877, step time: 1.5323\n",
      "187/388, train_loss: 0.2353, step time: 1.5280\n",
      "188/388, train_loss: 0.0821, step time: 1.5361\n",
      "189/388, train_loss: 0.1594, step time: 1.5396\n",
      "190/388, train_loss: 0.3456, step time: 1.5482\n",
      "191/388, train_loss: 0.4276, step time: 1.5403\n",
      "192/388, train_loss: 0.2911, step time: 1.5419\n",
      "193/388, train_loss: 0.3002, step time: 1.5435\n",
      "194/388, train_loss: 0.1130, step time: 1.5329\n",
      "195/388, train_loss: 0.3720, step time: 1.5407\n",
      "196/388, train_loss: 0.3283, step time: 1.5322\n",
      "197/388, train_loss: 0.2770, step time: 1.5449\n",
      "198/388, train_loss: 0.1985, step time: 1.5341\n",
      "199/388, train_loss: 0.0924, step time: 1.5385\n",
      "200/388, train_loss: 0.1713, step time: 1.5432\n",
      "201/388, train_loss: 0.1419, step time: 1.5320\n",
      "202/388, train_loss: 0.1466, step time: 1.5338\n",
      "203/388, train_loss: 0.2424, step time: 1.5341\n",
      "204/388, train_loss: 0.2431, step time: 1.5294\n",
      "205/388, train_loss: 0.3175, step time: 1.5340\n",
      "206/388, train_loss: 0.1974, step time: 1.5358\n",
      "207/388, train_loss: 0.3645, step time: 1.5418\n",
      "208/388, train_loss: 0.5533, step time: 1.5428\n",
      "209/388, train_loss: 0.2256, step time: 1.5402\n",
      "210/388, train_loss: 0.0716, step time: 1.5354\n",
      "211/388, train_loss: 0.1333, step time: 1.5374\n",
      "212/388, train_loss: 0.2934, step time: 1.5325\n",
      "213/388, train_loss: 0.0850, step time: 1.5341\n",
      "214/388, train_loss: 0.2790, step time: 1.5361\n",
      "215/388, train_loss: 0.1845, step time: 1.5463\n",
      "216/388, train_loss: 0.1197, step time: 1.5333\n",
      "217/388, train_loss: 0.6314, step time: 1.5423\n",
      "218/388, train_loss: 0.1110, step time: 1.5321\n",
      "219/388, train_loss: 0.2858, step time: 1.5362\n",
      "220/388, train_loss: 0.2660, step time: 1.5344\n",
      "221/388, train_loss: 0.1062, step time: 1.5309\n",
      "222/388, train_loss: 0.5432, step time: 1.5331\n",
      "223/388, train_loss: 0.2974, step time: 1.5370\n",
      "224/388, train_loss: 0.7375, step time: 1.5332\n",
      "225/388, train_loss: 0.2137, step time: 1.5419\n",
      "226/388, train_loss: 0.1162, step time: 1.5434\n",
      "227/388, train_loss: 0.2844, step time: 1.5350\n",
      "228/388, train_loss: 0.1967, step time: 1.5365\n",
      "229/388, train_loss: 0.3225, step time: 1.5419\n",
      "230/388, train_loss: 0.2977, step time: 1.5372\n",
      "231/388, train_loss: 0.5194, step time: 1.5361\n",
      "232/388, train_loss: 0.1870, step time: 1.5383\n",
      "233/388, train_loss: 0.1983, step time: 1.5383\n",
      "234/388, train_loss: 0.1851, step time: 1.5347\n",
      "235/388, train_loss: 0.1134, step time: 1.5401\n",
      "236/388, train_loss: 0.2636, step time: 1.5396\n",
      "237/388, train_loss: 0.2740, step time: 1.5324\n",
      "238/388, train_loss: 0.4345, step time: 1.5443\n",
      "239/388, train_loss: 0.4192, step time: 1.5531\n",
      "240/388, train_loss: 0.5303, step time: 1.5323\n",
      "241/388, train_loss: 0.5190, step time: 1.5315\n",
      "242/388, train_loss: 0.3174, step time: 1.5445\n",
      "243/388, train_loss: 0.2642, step time: 1.5396\n",
      "244/388, train_loss: 0.1753, step time: 1.5447\n",
      "245/388, train_loss: 0.2251, step time: 1.5416\n",
      "246/388, train_loss: 0.2164, step time: 1.5450\n",
      "247/388, train_loss: 0.4068, step time: 1.5376\n",
      "248/388, train_loss: 0.2425, step time: 1.5302\n",
      "249/388, train_loss: 0.1262, step time: 1.5327\n",
      "250/388, train_loss: 0.2498, step time: 1.5342\n",
      "251/388, train_loss: 0.2870, step time: 1.5401\n",
      "252/388, train_loss: 0.2068, step time: 1.5445\n",
      "253/388, train_loss: 0.2925, step time: 1.5459\n",
      "254/388, train_loss: 0.1968, step time: 1.5428\n",
      "255/388, train_loss: 0.3414, step time: 1.5346\n",
      "256/388, train_loss: 0.2697, step time: 1.5340\n",
      "257/388, train_loss: 0.2291, step time: 1.5454\n",
      "258/388, train_loss: 0.2401, step time: 1.5445\n",
      "259/388, train_loss: 0.7352, step time: 1.5455\n",
      "260/388, train_loss: 0.4804, step time: 1.5455\n",
      "261/388, train_loss: 0.3943, step time: 1.5403\n",
      "262/388, train_loss: 0.2263, step time: 1.5318\n",
      "263/388, train_loss: 0.1788, step time: 1.5379\n",
      "264/388, train_loss: 0.2078, step time: 1.5498\n",
      "265/388, train_loss: 0.7042, step time: 1.5369\n",
      "266/388, train_loss: 0.3157, step time: 1.5328\n",
      "267/388, train_loss: 0.4218, step time: 1.5427\n",
      "268/388, train_loss: 0.2796, step time: 1.5339\n",
      "269/388, train_loss: 0.3275, step time: 1.5340\n",
      "270/388, train_loss: 0.2877, step time: 1.5379\n",
      "271/388, train_loss: 0.2651, step time: 1.5364\n",
      "272/388, train_loss: 0.3444, step time: 1.5439\n",
      "273/388, train_loss: 0.2263, step time: 1.5418\n",
      "274/388, train_loss: 0.2112, step time: 1.5387\n",
      "275/388, train_loss: 0.1331, step time: 1.5335\n",
      "276/388, train_loss: 0.6549, step time: 1.5319\n",
      "277/388, train_loss: 0.1618, step time: 1.5387\n",
      "278/388, train_loss: 0.1805, step time: 1.5375\n",
      "279/388, train_loss: 0.2095, step time: 1.5312\n",
      "280/388, train_loss: 0.2485, step time: 1.5315\n",
      "281/388, train_loss: 0.3124, step time: 1.5403\n",
      "282/388, train_loss: 0.1793, step time: 1.5388\n",
      "283/388, train_loss: 0.1375, step time: 1.5349\n",
      "284/388, train_loss: 0.2754, step time: 1.5344\n",
      "285/388, train_loss: 0.1111, step time: 1.5404\n",
      "286/388, train_loss: 0.0959, step time: 1.5397\n",
      "287/388, train_loss: 0.1938, step time: 1.5319\n",
      "288/388, train_loss: 0.3232, step time: 1.5400\n",
      "289/388, train_loss: 0.3941, step time: 1.5332\n",
      "290/388, train_loss: 0.1458, step time: 1.5427\n",
      "291/388, train_loss: 0.4271, step time: 1.5360\n",
      "292/388, train_loss: 0.1027, step time: 1.5320\n",
      "293/388, train_loss: 0.3152, step time: 1.5377\n",
      "294/388, train_loss: 0.2531, step time: 1.5367\n",
      "295/388, train_loss: 0.1918, step time: 1.5357\n",
      "296/388, train_loss: 0.1650, step time: 1.5407\n",
      "297/388, train_loss: 0.4132, step time: 1.5378\n",
      "298/388, train_loss: 0.0943, step time: 1.5323\n",
      "299/388, train_loss: 0.3149, step time: 1.5361\n",
      "300/388, train_loss: 0.4970, step time: 1.5389\n",
      "301/388, train_loss: 0.3319, step time: 1.5418\n",
      "302/388, train_loss: 0.0858, step time: 1.5402\n",
      "303/388, train_loss: 0.3567, step time: 1.5365\n",
      "304/388, train_loss: 0.1970, step time: 1.5416\n",
      "305/388, train_loss: 0.1794, step time: 1.5346\n",
      "306/388, train_loss: 0.1911, step time: 1.5466\n",
      "307/388, train_loss: 0.5049, step time: 1.5300\n",
      "308/388, train_loss: 0.3195, step time: 1.5316\n",
      "309/388, train_loss: 0.2851, step time: 1.5363\n",
      "310/388, train_loss: 0.2987, step time: 1.5322\n",
      "311/388, train_loss: 0.4696, step time: 1.5376\n",
      "312/388, train_loss: 0.1632, step time: 1.5336\n",
      "313/388, train_loss: 0.1897, step time: 1.5379\n",
      "314/388, train_loss: 0.1009, step time: 1.5433\n",
      "315/388, train_loss: 0.3979, step time: 1.5366\n",
      "316/388, train_loss: 0.1821, step time: 1.5398\n",
      "317/388, train_loss: 0.1818, step time: 1.5689\n",
      "318/388, train_loss: 0.3916, step time: 1.5297\n",
      "319/388, train_loss: 0.5716, step time: 1.5339\n",
      "320/388, train_loss: 0.6229, step time: 1.5279\n",
      "321/388, train_loss: 0.6917, step time: 1.5338\n",
      "322/388, train_loss: 0.1512, step time: 1.5390\n",
      "323/388, train_loss: 0.2077, step time: 1.5345\n",
      "324/388, train_loss: 0.2162, step time: 1.5436\n",
      "325/388, train_loss: 0.2479, step time: 1.5292\n",
      "326/388, train_loss: 0.1565, step time: 1.5302\n",
      "327/388, train_loss: 0.2005, step time: 1.5432\n",
      "328/388, train_loss: 0.2526, step time: 1.5314\n",
      "329/388, train_loss: 0.6787, step time: 1.5384\n",
      "330/388, train_loss: 0.2652, step time: 1.5359\n",
      "331/388, train_loss: 0.1098, step time: 1.5396\n",
      "332/388, train_loss: 0.2056, step time: 1.5348\n",
      "333/388, train_loss: 0.1800, step time: 1.5328\n",
      "334/388, train_loss: 0.2072, step time: 1.5323\n",
      "335/388, train_loss: 0.1839, step time: 1.5417\n",
      "336/388, train_loss: 0.1558, step time: 1.5443\n",
      "337/388, train_loss: 0.1003, step time: 1.5306\n",
      "338/388, train_loss: 0.2219, step time: 1.5362\n",
      "339/388, train_loss: 0.0586, step time: 1.5317\n",
      "340/388, train_loss: 0.1557, step time: 1.5403\n",
      "341/388, train_loss: 0.4857, step time: 1.5450\n",
      "342/388, train_loss: 0.2022, step time: 1.5388\n",
      "343/388, train_loss: 0.3220, step time: 1.5427\n",
      "344/388, train_loss: 0.2841, step time: 1.5345\n",
      "345/388, train_loss: 0.2335, step time: 1.5345\n",
      "346/388, train_loss: 0.3987, step time: 1.5366\n",
      "347/388, train_loss: 0.2897, step time: 1.5320\n",
      "348/388, train_loss: 0.0762, step time: 1.5330\n",
      "349/388, train_loss: 0.3184, step time: 1.5462\n",
      "350/388, train_loss: 0.3560, step time: 1.5451\n",
      "351/388, train_loss: 0.2962, step time: 1.5375\n",
      "352/388, train_loss: 0.2488, step time: 1.5360\n",
      "353/388, train_loss: 0.3250, step time: 1.5359\n",
      "354/388, train_loss: 0.1485, step time: 1.5332\n",
      "355/388, train_loss: 0.1716, step time: 1.5342\n",
      "356/388, train_loss: 0.1173, step time: 1.5336\n",
      "357/388, train_loss: 0.6941, step time: 1.5304\n",
      "358/388, train_loss: 0.5817, step time: 1.5317\n",
      "359/388, train_loss: 0.2142, step time: 1.5408\n",
      "360/388, train_loss: 0.2256, step time: 1.5459\n",
      "361/388, train_loss: 0.2315, step time: 1.5325\n",
      "362/388, train_loss: 0.3467, step time: 1.5312\n",
      "363/388, train_loss: 0.2338, step time: 1.5327\n",
      "364/388, train_loss: 0.3274, step time: 1.5346\n",
      "365/388, train_loss: 0.3778, step time: 1.5426\n",
      "366/388, train_loss: 0.2411, step time: 1.5355\n",
      "367/388, train_loss: 0.1610, step time: 1.5320\n",
      "368/388, train_loss: 0.1796, step time: 1.5388\n",
      "369/388, train_loss: 0.4571, step time: 1.5373\n",
      "370/388, train_loss: 0.1443, step time: 1.5354\n",
      "371/388, train_loss: 0.1963, step time: 1.5341\n",
      "372/388, train_loss: 0.1881, step time: 1.5309\n",
      "373/388, train_loss: 0.3305, step time: 1.5342\n",
      "374/388, train_loss: 0.1803, step time: 1.5331\n",
      "375/388, train_loss: 0.1547, step time: 1.5334\n",
      "376/388, train_loss: 0.3905, step time: 1.5451\n",
      "377/388, train_loss: 0.2367, step time: 1.5328\n",
      "378/388, train_loss: 0.1393, step time: 1.5321\n",
      "379/388, train_loss: 0.0872, step time: 1.5347\n",
      "380/388, train_loss: 0.4718, step time: 1.5401\n",
      "381/388, train_loss: 0.2028, step time: 1.5353\n",
      "382/388, train_loss: 0.1309, step time: 1.5442\n",
      "383/388, train_loss: 0.6498, step time: 1.5316\n",
      "384/388, train_loss: 0.2079, step time: 1.5304\n",
      "385/388, train_loss: 0.2045, step time: 1.5408\n",
      "386/388, train_loss: 0.2859, step time: 1.5363\n",
      "387/388, train_loss: 0.1736, step time: 1.5369\n",
      "388/388, train_loss: 0.2583, step time: 1.5306\n",
      "epoch 11 average loss: 0.2801\n",
      "saved new best metric model\n",
      "current epoch: 11 current mean dice: 0.7051 tc: 0.7504 wt: 0.8642 et: 0.5007\n",
      "best mean dice: 0.7051 at epoch: 11\n",
      "time consuming of epoch 11 is: 704.7594\n",
      "----------\n",
      "epoch 12/100\n",
      "1/388, train_loss: 0.0956, step time: 1.5460\n",
      "2/388, train_loss: 0.2462, step time: 1.5350\n",
      "3/388, train_loss: 0.3939, step time: 1.5318\n",
      "4/388, train_loss: 0.3013, step time: 1.5351\n",
      "5/388, train_loss: 0.1838, step time: 1.5449\n",
      "6/388, train_loss: 0.1409, step time: 1.5310\n",
      "7/388, train_loss: 0.0908, step time: 1.5352\n",
      "8/388, train_loss: 0.7384, step time: 1.5349\n",
      "9/388, train_loss: 0.2656, step time: 1.5329\n",
      "10/388, train_loss: 0.1176, step time: 1.5385\n",
      "11/388, train_loss: 0.1332, step time: 1.5324\n",
      "12/388, train_loss: 0.0948, step time: 1.5392\n",
      "13/388, train_loss: 0.1752, step time: 1.5411\n",
      "14/388, train_loss: 0.2451, step time: 1.5375\n",
      "15/388, train_loss: 0.3133, step time: 1.5331\n",
      "16/388, train_loss: 0.2815, step time: 1.5290\n",
      "17/388, train_loss: 0.2933, step time: 1.5354\n",
      "18/388, train_loss: 0.2862, step time: 1.5366\n",
      "19/388, train_loss: 0.1927, step time: 1.5354\n",
      "20/388, train_loss: 0.1103, step time: 1.5401\n",
      "21/388, train_loss: 0.7024, step time: 1.5470\n",
      "22/388, train_loss: 0.1093, step time: 1.5465\n",
      "23/388, train_loss: 0.5283, step time: 1.5482\n",
      "24/388, train_loss: 0.1080, step time: 1.5314\n",
      "25/388, train_loss: 0.2464, step time: 1.5414\n",
      "26/388, train_loss: 0.2068, step time: 1.5375\n",
      "27/388, train_loss: 0.1583, step time: 1.5341\n",
      "28/388, train_loss: 0.0985, step time: 1.5328\n",
      "29/388, train_loss: 0.0791, step time: 1.5336\n",
      "30/388, train_loss: 0.5338, step time: 1.5328\n",
      "31/388, train_loss: 0.2443, step time: 1.5356\n",
      "32/388, train_loss: 0.1239, step time: 1.5301\n",
      "33/388, train_loss: 0.1680, step time: 1.5508\n",
      "34/388, train_loss: 0.1245, step time: 1.5334\n",
      "35/388, train_loss: 0.2695, step time: 1.5359\n",
      "36/388, train_loss: 0.1079, step time: 1.5361\n",
      "37/388, train_loss: 0.3234, step time: 1.5377\n",
      "38/388, train_loss: 0.2542, step time: 1.5344\n",
      "39/388, train_loss: 0.1439, step time: 1.5313\n",
      "40/388, train_loss: 0.1629, step time: 1.5338\n",
      "41/388, train_loss: 0.2473, step time: 1.5309\n",
      "42/388, train_loss: 0.2827, step time: 1.5321\n",
      "43/388, train_loss: 0.2913, step time: 1.5410\n",
      "44/388, train_loss: 0.1699, step time: 1.5513\n",
      "45/388, train_loss: 0.3993, step time: 1.5346\n",
      "46/388, train_loss: 0.1337, step time: 1.5338\n",
      "47/388, train_loss: 0.2391, step time: 1.5376\n",
      "48/388, train_loss: 0.6389, step time: 1.5383\n",
      "49/388, train_loss: 0.0679, step time: 1.5358\n",
      "50/388, train_loss: 0.4244, step time: 1.5345\n",
      "51/388, train_loss: 0.1565, step time: 1.5301\n",
      "52/388, train_loss: 0.1680, step time: 1.5340\n",
      "53/388, train_loss: 0.3688, step time: 1.5383\n",
      "54/388, train_loss: 0.1848, step time: 1.5368\n",
      "55/388, train_loss: 0.1494, step time: 1.5320\n",
      "56/388, train_loss: 0.5014, step time: 1.5293\n",
      "57/388, train_loss: 0.3805, step time: 1.5389\n",
      "58/388, train_loss: 0.4078, step time: 1.5422\n",
      "59/388, train_loss: 0.1642, step time: 1.5351\n",
      "60/388, train_loss: 0.3272, step time: 1.5335\n",
      "61/388, train_loss: 0.3015, step time: 1.5306\n",
      "62/388, train_loss: 0.9672, step time: 1.5348\n",
      "63/388, train_loss: 0.4030, step time: 1.5323\n",
      "64/388, train_loss: 0.2571, step time: 1.5339\n",
      "65/388, train_loss: 0.6012, step time: 1.5374\n",
      "66/388, train_loss: 0.2115, step time: 1.5368\n",
      "67/388, train_loss: 0.1692, step time: 1.5334\n",
      "68/388, train_loss: 0.1944, step time: 1.5333\n",
      "69/388, train_loss: 0.1043, step time: 1.5305\n",
      "70/388, train_loss: 0.4181, step time: 1.5312\n",
      "71/388, train_loss: 0.1752, step time: 1.5419\n",
      "72/388, train_loss: 0.2308, step time: 1.5357\n",
      "73/388, train_loss: 0.2804, step time: 1.5331\n",
      "74/388, train_loss: 0.4728, step time: 1.5285\n",
      "75/388, train_loss: 0.1111, step time: 1.5322\n",
      "76/388, train_loss: 0.2497, step time: 1.5409\n",
      "77/388, train_loss: 0.3277, step time: 1.5367\n",
      "78/388, train_loss: 0.1389, step time: 1.5354\n",
      "79/388, train_loss: 0.1230, step time: 1.5340\n",
      "80/388, train_loss: 0.2933, step time: 1.5343\n",
      "81/388, train_loss: 0.3872, step time: 1.5346\n",
      "82/388, train_loss: 0.1915, step time: 1.5334\n",
      "83/388, train_loss: 0.2476, step time: 1.5338\n",
      "84/388, train_loss: 0.3233, step time: 1.5353\n",
      "85/388, train_loss: 0.3068, step time: 1.5323\n",
      "86/388, train_loss: 0.3604, step time: 1.5316\n",
      "87/388, train_loss: 0.2253, step time: 1.5340\n",
      "88/388, train_loss: 0.1718, step time: 1.5318\n",
      "89/388, train_loss: 0.0992, step time: 1.5362\n",
      "90/388, train_loss: 0.4141, step time: 1.5346\n",
      "91/388, train_loss: 0.4584, step time: 1.5336\n",
      "92/388, train_loss: 0.1741, step time: 1.5345\n",
      "93/388, train_loss: 0.1624, step time: 1.5331\n",
      "94/388, train_loss: 0.3951, step time: 1.5383\n",
      "95/388, train_loss: 0.2793, step time: 1.5424\n",
      "96/388, train_loss: 0.2619, step time: 1.5441\n",
      "97/388, train_loss: 0.7261, step time: 1.5328\n",
      "98/388, train_loss: 0.2747, step time: 1.5318\n",
      "99/388, train_loss: 0.1594, step time: 1.5390\n",
      "100/388, train_loss: 0.7107, step time: 1.5347\n",
      "101/388, train_loss: 0.1224, step time: 1.5426\n",
      "102/388, train_loss: 0.2842, step time: 1.5360\n",
      "103/388, train_loss: 0.3645, step time: 1.5337\n",
      "104/388, train_loss: 0.3228, step time: 1.5382\n",
      "105/388, train_loss: 0.2059, step time: 1.5364\n",
      "106/388, train_loss: 0.1269, step time: 1.5380\n",
      "107/388, train_loss: 0.4639, step time: 1.5412\n",
      "108/388, train_loss: 0.1514, step time: 1.5369\n",
      "109/388, train_loss: 0.3296, step time: 1.5415\n",
      "110/388, train_loss: 0.2332, step time: 1.5348\n",
      "111/388, train_loss: 0.2532, step time: 1.5317\n",
      "112/388, train_loss: 0.1796, step time: 1.5322\n",
      "113/388, train_loss: 0.2246, step time: 1.5398\n",
      "114/388, train_loss: 0.1723, step time: 1.5436\n",
      "115/388, train_loss: 0.4155, step time: 1.5314\n",
      "116/388, train_loss: 0.1353, step time: 1.5318\n",
      "117/388, train_loss: 0.1456, step time: 1.5461\n",
      "118/388, train_loss: 0.1197, step time: 1.5354\n",
      "119/388, train_loss: 0.2619, step time: 1.5383\n",
      "120/388, train_loss: 0.1209, step time: 1.5287\n",
      "121/388, train_loss: 0.1487, step time: 1.5331\n",
      "122/388, train_loss: 0.1203, step time: 1.5326\n",
      "123/388, train_loss: 0.7538, step time: 1.5423\n",
      "124/388, train_loss: 0.1425, step time: 1.5369\n",
      "125/388, train_loss: 0.1914, step time: 1.5442\n",
      "126/388, train_loss: 0.1195, step time: 1.5347\n",
      "127/388, train_loss: 0.2808, step time: 1.5398\n",
      "128/388, train_loss: 0.3695, step time: 1.5374\n",
      "129/388, train_loss: 0.4877, step time: 1.5358\n",
      "130/388, train_loss: 0.2047, step time: 1.5338\n",
      "131/388, train_loss: 0.1818, step time: 1.5367\n",
      "132/388, train_loss: 0.1610, step time: 1.5374\n",
      "133/388, train_loss: 0.1405, step time: 1.5431\n",
      "134/388, train_loss: 0.2303, step time: 1.5355\n",
      "135/388, train_loss: 0.2262, step time: 1.5316\n",
      "136/388, train_loss: 0.2512, step time: 1.5326\n",
      "137/388, train_loss: 0.2311, step time: 1.5360\n",
      "138/388, train_loss: 0.3186, step time: 1.5395\n",
      "139/388, train_loss: 0.2611, step time: 1.5415\n",
      "140/388, train_loss: 0.2152, step time: 1.5329\n",
      "141/388, train_loss: 0.1976, step time: 1.5309\n",
      "142/388, train_loss: 0.1900, step time: 1.5358\n",
      "143/388, train_loss: 0.2403, step time: 1.5427\n",
      "144/388, train_loss: 0.2263, step time: 1.5346\n",
      "145/388, train_loss: 0.4011, step time: 1.5342\n",
      "146/388, train_loss: 0.1588, step time: 1.5364\n",
      "147/388, train_loss: 0.2189, step time: 1.5504\n",
      "148/388, train_loss: 0.3582, step time: 1.5329\n",
      "149/388, train_loss: 0.1573, step time: 1.5273\n",
      "150/388, train_loss: 0.3566, step time: 1.5309\n",
      "151/388, train_loss: 0.1385, step time: 1.5350\n",
      "152/388, train_loss: 0.1142, step time: 1.5370\n",
      "153/388, train_loss: 0.1205, step time: 1.5410\n",
      "154/388, train_loss: 0.3552, step time: 1.5371\n",
      "155/388, train_loss: 0.2204, step time: 1.5378\n",
      "156/388, train_loss: 0.2714, step time: 1.5362\n",
      "157/388, train_loss: 0.2377, step time: 1.5381\n",
      "158/388, train_loss: 0.1323, step time: 1.5315\n",
      "159/388, train_loss: 0.1152, step time: 1.5299\n",
      "160/388, train_loss: 0.1664, step time: 1.5325\n",
      "161/388, train_loss: 0.2435, step time: 1.5341\n",
      "162/388, train_loss: 0.5932, step time: 1.5376\n",
      "163/388, train_loss: 0.2313, step time: 1.5507\n",
      "164/388, train_loss: 0.1594, step time: 1.5428\n",
      "165/388, train_loss: 0.4509, step time: 1.5352\n",
      "166/388, train_loss: 0.0917, step time: 1.5369\n",
      "167/388, train_loss: 0.1591, step time: 1.5323\n",
      "168/388, train_loss: 0.2681, step time: 1.5300\n",
      "169/388, train_loss: 0.3950, step time: 1.5314\n",
      "170/388, train_loss: 0.5014, step time: 1.5362\n",
      "171/388, train_loss: 0.1638, step time: 1.5402\n",
      "172/388, train_loss: 0.2128, step time: 1.5413\n",
      "173/388, train_loss: 0.2122, step time: 1.5337\n",
      "174/388, train_loss: 0.2106, step time: 1.5352\n",
      "175/388, train_loss: 0.2148, step time: 1.5441\n",
      "176/388, train_loss: 0.6960, step time: 1.5427\n",
      "177/388, train_loss: 0.3096, step time: 1.5337\n",
      "178/388, train_loss: 0.1680, step time: 1.5294\n",
      "179/388, train_loss: 0.4978, step time: 1.5301\n",
      "180/388, train_loss: 0.3221, step time: 1.5382\n",
      "181/388, train_loss: 0.1227, step time: 1.5355\n",
      "182/388, train_loss: 0.1236, step time: 1.5372\n",
      "183/388, train_loss: 0.4158, step time: 1.5399\n",
      "184/388, train_loss: 0.1135, step time: 1.5355\n",
      "185/388, train_loss: 0.9707, step time: 1.5398\n",
      "186/388, train_loss: 0.1218, step time: 1.5325\n",
      "187/388, train_loss: 0.2522, step time: 1.5307\n",
      "188/388, train_loss: 0.3080, step time: 1.5493\n",
      "189/388, train_loss: 0.6750, step time: 1.5396\n",
      "190/388, train_loss: 0.1665, step time: 1.5318\n",
      "191/388, train_loss: 0.0949, step time: 1.5369\n",
      "192/388, train_loss: 0.1280, step time: 1.5375\n",
      "193/388, train_loss: 0.2366, step time: 1.5383\n",
      "194/388, train_loss: 0.1561, step time: 1.5359\n",
      "195/388, train_loss: 0.6463, step time: 1.5384\n",
      "196/388, train_loss: 0.2072, step time: 1.5376\n",
      "197/388, train_loss: 0.1459, step time: 1.5381\n",
      "198/388, train_loss: 0.3174, step time: 1.5371\n",
      "199/388, train_loss: 0.7044, step time: 1.5375\n",
      "200/388, train_loss: 0.2617, step time: 1.5386\n",
      "201/388, train_loss: 0.1737, step time: 1.5352\n",
      "202/388, train_loss: 0.5447, step time: 1.5407\n",
      "203/388, train_loss: 0.3784, step time: 1.5370\n",
      "204/388, train_loss: 0.1002, step time: 1.5367\n",
      "205/388, train_loss: 0.1133, step time: 1.5356\n",
      "206/388, train_loss: 0.1566, step time: 1.5358\n",
      "207/388, train_loss: 0.1791, step time: 1.5318\n",
      "208/388, train_loss: 0.0758, step time: 1.5328\n",
      "209/388, train_loss: 0.5262, step time: 1.5356\n",
      "210/388, train_loss: 0.1091, step time: 1.5351\n",
      "211/388, train_loss: 0.1154, step time: 1.5428\n",
      "212/388, train_loss: 0.2444, step time: 1.5338\n",
      "213/388, train_loss: 0.2405, step time: 1.5351\n",
      "214/388, train_loss: 0.1411, step time: 1.5331\n",
      "215/388, train_loss: 0.2838, step time: 1.5401\n",
      "216/388, train_loss: 0.1889, step time: 1.5405\n",
      "217/388, train_loss: 0.3442, step time: 1.5335\n",
      "218/388, train_loss: 0.2427, step time: 1.5387\n",
      "219/388, train_loss: 0.1850, step time: 1.5298\n",
      "220/388, train_loss: 0.4271, step time: 1.5398\n",
      "221/388, train_loss: 0.2683, step time: 1.5459\n",
      "222/388, train_loss: 0.3850, step time: 1.5348\n",
      "223/388, train_loss: 0.6406, step time: 1.5372\n",
      "224/388, train_loss: 0.3768, step time: 1.5333\n",
      "225/388, train_loss: 0.3409, step time: 1.5329\n",
      "226/388, train_loss: 0.2423, step time: 1.5368\n",
      "227/388, train_loss: 0.2067, step time: 1.5349\n",
      "228/388, train_loss: 0.2412, step time: 1.5367\n",
      "229/388, train_loss: 0.2597, step time: 1.5360\n",
      "230/388, train_loss: 0.3776, step time: 1.5470\n",
      "231/388, train_loss: 0.2065, step time: 1.5343\n",
      "232/388, train_loss: 0.3131, step time: 1.5349\n",
      "233/388, train_loss: 0.3561, step time: 1.5356\n",
      "234/388, train_loss: 0.1666, step time: 1.5394\n",
      "235/388, train_loss: 0.3503, step time: 1.5371\n",
      "236/388, train_loss: 0.1654, step time: 1.5331\n",
      "237/388, train_loss: 0.4499, step time: 1.5329\n",
      "238/388, train_loss: 0.2328, step time: 1.5365\n",
      "239/388, train_loss: 0.2174, step time: 1.5403\n",
      "240/388, train_loss: 0.3432, step time: 1.5334\n",
      "241/388, train_loss: 0.2924, step time: 1.5320\n",
      "242/388, train_loss: 0.0916, step time: 1.5325\n",
      "243/388, train_loss: 0.1055, step time: 1.5305\n",
      "244/388, train_loss: 0.2262, step time: 1.5469\n",
      "245/388, train_loss: 0.1966, step time: 1.5423\n",
      "246/388, train_loss: 0.2336, step time: 1.5343\n",
      "247/388, train_loss: 0.1275, step time: 1.5325\n",
      "248/388, train_loss: 0.2650, step time: 1.5335\n",
      "249/388, train_loss: 0.4905, step time: 1.5333\n",
      "250/388, train_loss: 0.0509, step time: 1.5356\n",
      "251/388, train_loss: 0.1142, step time: 1.5337\n",
      "252/388, train_loss: 0.2761, step time: 1.5324\n",
      "253/388, train_loss: 0.3803, step time: 1.5329\n",
      "254/388, train_loss: 0.2598, step time: 1.5333\n",
      "255/388, train_loss: 0.1742, step time: 1.5328\n",
      "256/388, train_loss: 0.1933, step time: 1.5372\n",
      "257/388, train_loss: 0.1790, step time: 1.5485\n",
      "258/388, train_loss: 0.2222, step time: 1.5327\n",
      "259/388, train_loss: 0.1273, step time: 1.5324\n",
      "260/388, train_loss: 0.1846, step time: 1.5625\n",
      "261/388, train_loss: 0.3850, step time: 1.5336\n",
      "262/388, train_loss: 0.2129, step time: 1.5317\n",
      "263/388, train_loss: 0.1586, step time: 1.5423\n",
      "264/388, train_loss: 0.2149, step time: 1.5344\n",
      "265/388, train_loss: 0.2773, step time: 1.5344\n",
      "266/388, train_loss: 0.5180, step time: 1.5358\n",
      "267/388, train_loss: 0.2183, step time: 1.5366\n",
      "268/388, train_loss: 0.1775, step time: 1.5312\n",
      "269/388, train_loss: 0.5344, step time: 1.5347\n",
      "270/388, train_loss: 0.2044, step time: 1.5307\n",
      "271/388, train_loss: 0.6118, step time: 1.5419\n",
      "272/388, train_loss: 0.5197, step time: 1.5387\n",
      "273/388, train_loss: 0.2412, step time: 1.5361\n",
      "274/388, train_loss: 0.2346, step time: 1.5356\n",
      "275/388, train_loss: 0.2362, step time: 1.5304\n",
      "276/388, train_loss: 0.1807, step time: 1.5326\n",
      "277/388, train_loss: 0.3116, step time: 1.5402\n",
      "278/388, train_loss: 0.3504, step time: 1.5325\n",
      "279/388, train_loss: 0.2078, step time: 1.5315\n",
      "280/388, train_loss: 0.3359, step time: 1.5334\n",
      "281/388, train_loss: 0.2343, step time: 1.5327\n",
      "282/388, train_loss: 0.3680, step time: 1.5334\n",
      "283/388, train_loss: 0.5747, step time: 1.5314\n",
      "284/388, train_loss: 0.1784, step time: 1.5531\n",
      "285/388, train_loss: 0.1694, step time: 1.5348\n",
      "286/388, train_loss: 0.1540, step time: 1.5323\n",
      "287/388, train_loss: 0.2843, step time: 1.5345\n",
      "288/388, train_loss: 0.3638, step time: 1.5301\n",
      "289/388, train_loss: 0.2413, step time: 1.5353\n",
      "290/388, train_loss: 0.2474, step time: 1.5365\n",
      "291/388, train_loss: 0.2607, step time: 1.5368\n",
      "292/388, train_loss: 0.2856, step time: 1.5338\n",
      "293/388, train_loss: 0.2707, step time: 1.5313\n",
      "294/388, train_loss: 0.4680, step time: 1.5316\n",
      "295/388, train_loss: 0.2075, step time: 1.5286\n",
      "296/388, train_loss: 0.0661, step time: 1.5325\n",
      "297/388, train_loss: 0.6279, step time: 1.5336\n",
      "298/388, train_loss: 0.3795, step time: 1.5454\n",
      "299/388, train_loss: 0.1403, step time: 1.5311\n",
      "300/388, train_loss: 0.3341, step time: 1.5305\n",
      "301/388, train_loss: 0.1530, step time: 1.5598\n",
      "302/388, train_loss: 0.1656, step time: 1.5521\n",
      "303/388, train_loss: 0.0994, step time: 1.5314\n",
      "304/388, train_loss: 0.3097, step time: 1.5307\n",
      "305/388, train_loss: 0.1264, step time: 1.5327\n",
      "306/388, train_loss: 0.1023, step time: 1.5460\n",
      "307/388, train_loss: 0.5246, step time: 1.5323\n",
      "308/388, train_loss: 0.2050, step time: 1.5342\n",
      "309/388, train_loss: 0.3803, step time: 1.5359\n",
      "310/388, train_loss: 0.3178, step time: 1.5369\n",
      "311/388, train_loss: 0.2902, step time: 1.5320\n",
      "312/388, train_loss: 0.2509, step time: 1.5339\n",
      "313/388, train_loss: 0.3579, step time: 1.5338\n",
      "314/388, train_loss: 0.1007, step time: 1.5308\n",
      "315/388, train_loss: 0.2248, step time: 1.5318\n",
      "316/388, train_loss: 0.5542, step time: 1.5534\n",
      "317/388, train_loss: 0.1552, step time: 1.5356\n",
      "318/388, train_loss: 0.2532, step time: 1.5359\n",
      "319/388, train_loss: 0.2331, step time: 1.5316\n",
      "320/388, train_loss: 0.4840, step time: 1.5307\n",
      "321/388, train_loss: 0.1651, step time: 1.5310\n",
      "322/388, train_loss: 0.1307, step time: 1.5323\n",
      "323/388, train_loss: 0.2227, step time: 1.5363\n",
      "324/388, train_loss: 0.1778, step time: 1.5418\n",
      "325/388, train_loss: 0.2454, step time: 1.5297\n",
      "326/388, train_loss: 0.2932, step time: 1.5297\n",
      "327/388, train_loss: 0.2545, step time: 1.5437\n",
      "328/388, train_loss: 0.3254, step time: 1.5386\n",
      "329/388, train_loss: 0.3388, step time: 1.5330\n",
      "330/388, train_loss: 0.3388, step time: 1.5332\n",
      "331/388, train_loss: 0.3171, step time: 1.5290\n",
      "332/388, train_loss: 0.3228, step time: 1.5314\n",
      "333/388, train_loss: 0.1243, step time: 1.5380\n",
      "334/388, train_loss: 0.7496, step time: 1.5427\n",
      "335/388, train_loss: 0.1832, step time: 1.5345\n",
      "336/388, train_loss: 0.2132, step time: 1.5351\n",
      "337/388, train_loss: 0.2812, step time: 1.5327\n",
      "338/388, train_loss: 0.1774, step time: 1.5324\n",
      "339/388, train_loss: 0.3231, step time: 1.5336\n",
      "340/388, train_loss: 0.2065, step time: 1.5330\n",
      "341/388, train_loss: 0.1722, step time: 1.5375\n",
      "342/388, train_loss: 0.3263, step time: 1.5356\n",
      "343/388, train_loss: 0.2475, step time: 1.5319\n",
      "344/388, train_loss: 0.1754, step time: 1.5336\n",
      "345/388, train_loss: 0.1902, step time: 1.5282\n",
      "346/388, train_loss: 0.3888, step time: 1.5329\n",
      "347/388, train_loss: 0.2000, step time: 1.5305\n",
      "348/388, train_loss: 0.3609, step time: 1.5339\n",
      "349/388, train_loss: 0.2297, step time: 1.5348\n",
      "350/388, train_loss: 0.1135, step time: 1.5361\n",
      "351/388, train_loss: 0.0796, step time: 1.5323\n",
      "352/388, train_loss: 0.2118, step time: 1.5371\n",
      "353/388, train_loss: 0.1606, step time: 1.5322\n",
      "354/388, train_loss: 0.1481, step time: 1.5334\n",
      "355/388, train_loss: 0.3549, step time: 1.5319\n",
      "356/388, train_loss: 0.2366, step time: 1.5363\n",
      "357/388, train_loss: 0.2687, step time: 1.5328\n",
      "358/388, train_loss: 0.1807, step time: 1.5335\n",
      "359/388, train_loss: 0.0943, step time: 1.5355\n",
      "360/388, train_loss: 0.2023, step time: 1.5319\n",
      "361/388, train_loss: 0.2489, step time: 1.5348\n",
      "362/388, train_loss: 0.1511, step time: 1.5405\n",
      "363/388, train_loss: 0.2366, step time: 1.5370\n",
      "364/388, train_loss: 0.3701, step time: 1.5356\n",
      "365/388, train_loss: 0.3128, step time: 1.5327\n",
      "366/388, train_loss: 0.3134, step time: 1.5321\n",
      "367/388, train_loss: 0.2648, step time: 1.5313\n",
      "368/388, train_loss: 0.2205, step time: 1.5408\n",
      "369/388, train_loss: 0.0951, step time: 1.5340\n",
      "370/388, train_loss: 0.2696, step time: 1.5345\n",
      "371/388, train_loss: 0.7050, step time: 1.5574\n",
      "372/388, train_loss: 0.3106, step time: 1.5356\n",
      "373/388, train_loss: 0.2208, step time: 1.5371\n",
      "374/388, train_loss: 0.1167, step time: 1.5340\n",
      "375/388, train_loss: 0.2518, step time: 1.5318\n",
      "376/388, train_loss: 0.6119, step time: 1.5367\n",
      "377/388, train_loss: 0.1846, step time: 1.5331\n",
      "378/388, train_loss: 0.3414, step time: 1.5339\n",
      "379/388, train_loss: 0.2031, step time: 1.5312\n",
      "380/388, train_loss: 0.3966, step time: 1.5331\n",
      "381/388, train_loss: 0.2202, step time: 1.5326\n",
      "382/388, train_loss: 0.1834, step time: 1.5367\n",
      "383/388, train_loss: 0.2021, step time: 1.5362\n",
      "384/388, train_loss: 0.1896, step time: 1.5361\n",
      "385/388, train_loss: 0.1668, step time: 1.5353\n",
      "386/388, train_loss: 0.2564, step time: 1.5314\n",
      "387/388, train_loss: 0.1608, step time: 1.5310\n",
      "388/388, train_loss: 0.3364, step time: 1.5336\n",
      "epoch 12 average loss: 0.2666\n",
      "current epoch: 12 current mean dice: 0.6602 tc: 0.6890 wt: 0.8514 et: 0.4403\n",
      "best mean dice: 0.7051 at epoch: 11\n",
      "time consuming of epoch 12 is: 702.9908\n",
      "----------\n",
      "epoch 13/100\n",
      "1/388, train_loss: 0.3095, step time: 1.5407\n",
      "2/388, train_loss: 0.2912, step time: 1.5374\n",
      "3/388, train_loss: 0.1503, step time: 1.5335\n",
      "4/388, train_loss: 0.3553, step time: 1.5345\n",
      "5/388, train_loss: 0.1674, step time: 1.5380\n",
      "6/388, train_loss: 0.1504, step time: 1.5310\n",
      "7/388, train_loss: 0.5091, step time: 1.5330\n",
      "8/388, train_loss: 0.0490, step time: 1.5360\n",
      "9/388, train_loss: 0.1173, step time: 1.5350\n",
      "10/388, train_loss: 0.3623, step time: 1.5358\n",
      "11/388, train_loss: 0.3323, step time: 1.5321\n",
      "12/388, train_loss: 0.1289, step time: 1.5381\n",
      "13/388, train_loss: 0.3687, step time: 1.5356\n",
      "14/388, train_loss: 0.3801, step time: 1.5366\n",
      "15/388, train_loss: 0.2057, step time: 1.5399\n",
      "16/388, train_loss: 0.3145, step time: 1.5307\n",
      "17/388, train_loss: 0.1074, step time: 1.5319\n",
      "18/388, train_loss: 0.4075, step time: 1.5355\n",
      "19/388, train_loss: 0.4519, step time: 1.5344\n",
      "20/388, train_loss: 0.2409, step time: 1.5364\n",
      "21/388, train_loss: 0.2484, step time: 1.5324\n",
      "22/388, train_loss: 0.1526, step time: 1.5327\n",
      "23/388, train_loss: 0.1616, step time: 1.5283\n",
      "24/388, train_loss: 0.1230, step time: 1.5311\n",
      "25/388, train_loss: 0.3196, step time: 1.5305\n",
      "26/388, train_loss: 0.2881, step time: 1.5360\n",
      "27/388, train_loss: 0.2383, step time: 1.5333\n",
      "28/388, train_loss: 0.2058, step time: 1.5305\n",
      "29/388, train_loss: 0.1637, step time: 1.5297\n",
      "30/388, train_loss: 0.1418, step time: 1.5339\n",
      "31/388, train_loss: 0.2014, step time: 1.5333\n",
      "32/388, train_loss: 0.1532, step time: 1.5327\n",
      "33/388, train_loss: 0.2683, step time: 1.5375\n",
      "34/388, train_loss: 0.3659, step time: 1.5340\n",
      "35/388, train_loss: 0.1312, step time: 1.5287\n",
      "36/388, train_loss: 0.1393, step time: 1.5346\n",
      "37/388, train_loss: 0.2361, step time: 1.5342\n",
      "38/388, train_loss: 0.0477, step time: 1.5386\n",
      "39/388, train_loss: 0.1100, step time: 1.5358\n",
      "40/388, train_loss: 0.3159, step time: 1.5368\n",
      "41/388, train_loss: 0.1370, step time: 1.5307\n",
      "42/388, train_loss: 0.1416, step time: 1.5353\n",
      "43/388, train_loss: 0.0932, step time: 1.5347\n",
      "44/388, train_loss: 0.2356, step time: 1.5335\n",
      "45/388, train_loss: 0.4996, step time: 1.5344\n",
      "46/388, train_loss: 0.1781, step time: 1.5323\n",
      "47/388, train_loss: 0.3407, step time: 1.5335\n",
      "48/388, train_loss: 0.1360, step time: 1.5311\n",
      "49/388, train_loss: 0.1153, step time: 1.5335\n",
      "50/388, train_loss: 0.1706, step time: 1.5441\n",
      "51/388, train_loss: 0.2023, step time: 1.5340\n",
      "52/388, train_loss: 0.2721, step time: 1.5362\n",
      "53/388, train_loss: 0.2098, step time: 1.5345\n",
      "54/388, train_loss: 0.1030, step time: 1.5314\n",
      "55/388, train_loss: 0.3034, step time: 1.5300\n",
      "56/388, train_loss: 0.1630, step time: 1.5319\n",
      "57/388, train_loss: 0.1297, step time: 1.5381\n",
      "58/388, train_loss: 0.1159, step time: 1.5341\n",
      "59/388, train_loss: 0.3939, step time: 1.5303\n",
      "60/388, train_loss: 0.4732, step time: 1.5314\n",
      "61/388, train_loss: 0.4145, step time: 1.5295\n",
      "62/388, train_loss: 0.2963, step time: 1.5341\n",
      "63/388, train_loss: 0.2229, step time: 1.5344\n",
      "64/388, train_loss: 0.4002, step time: 1.5363\n",
      "65/388, train_loss: 0.2517, step time: 1.5305\n",
      "66/388, train_loss: 0.2448, step time: 1.5301\n",
      "67/388, train_loss: 0.2529, step time: 1.5317\n",
      "68/388, train_loss: 0.2659, step time: 1.5366\n",
      "69/388, train_loss: 0.1924, step time: 1.5349\n",
      "70/388, train_loss: 0.2128, step time: 1.5307\n",
      "71/388, train_loss: 0.3166, step time: 1.5352\n",
      "72/388, train_loss: 0.2357, step time: 1.5318\n",
      "73/388, train_loss: 0.2596, step time: 1.5309\n",
      "74/388, train_loss: 0.1275, step time: 1.5348\n",
      "75/388, train_loss: 0.4302, step time: 1.5340\n",
      "76/388, train_loss: 0.1619, step time: 1.5333\n",
      "77/388, train_loss: 0.3575, step time: 1.5331\n",
      "78/388, train_loss: 0.2688, step time: 1.5343\n",
      "79/388, train_loss: 0.1666, step time: 1.5296\n",
      "80/388, train_loss: 0.5420, step time: 1.5328\n",
      "81/388, train_loss: 0.7374, step time: 1.5362\n",
      "82/388, train_loss: 0.1926, step time: 1.5342\n",
      "83/388, train_loss: 0.2902, step time: 1.5328\n",
      "84/388, train_loss: 0.2186, step time: 1.5358\n",
      "85/388, train_loss: 0.1071, step time: 1.5307\n",
      "86/388, train_loss: 0.2975, step time: 1.5338\n",
      "87/388, train_loss: 0.3809, step time: 1.5281\n",
      "88/388, train_loss: 0.1505, step time: 1.5384\n",
      "89/388, train_loss: 0.3851, step time: 1.5315\n",
      "90/388, train_loss: 0.1854, step time: 1.5326\n",
      "91/388, train_loss: 0.2101, step time: 1.5309\n",
      "92/388, train_loss: 0.0911, step time: 1.5375\n",
      "93/388, train_loss: 0.5804, step time: 1.5333\n",
      "94/388, train_loss: 0.2690, step time: 1.5366\n",
      "95/388, train_loss: 0.1383, step time: 1.5346\n",
      "96/388, train_loss: 0.4287, step time: 1.5322\n",
      "97/388, train_loss: 0.2161, step time: 1.5333\n",
      "98/388, train_loss: 0.1929, step time: 1.5381\n",
      "99/388, train_loss: 0.2806, step time: 1.5371\n",
      "100/388, train_loss: 0.3680, step time: 1.5376\n",
      "101/388, train_loss: 0.3081, step time: 1.5331\n",
      "102/388, train_loss: 0.1566, step time: 1.5303\n",
      "103/388, train_loss: 0.1102, step time: 1.5302\n",
      "104/388, train_loss: 0.2698, step time: 1.5331\n",
      "105/388, train_loss: 0.3607, step time: 1.5328\n",
      "106/388, train_loss: 0.3785, step time: 1.5389\n",
      "107/388, train_loss: 0.2861, step time: 1.5332\n",
      "108/388, train_loss: 0.3146, step time: 1.5313\n",
      "109/388, train_loss: 0.1573, step time: 1.5309\n",
      "110/388, train_loss: 0.1788, step time: 1.5310\n",
      "111/388, train_loss: 0.3217, step time: 1.5427\n",
      "112/388, train_loss: 0.3230, step time: 1.5330\n",
      "113/388, train_loss: 0.2797, step time: 1.5343\n",
      "114/388, train_loss: 0.4518, step time: 1.5305\n",
      "115/388, train_loss: 0.1955, step time: 1.5311\n",
      "116/388, train_loss: 0.5106, step time: 1.5333\n",
      "117/388, train_loss: 0.2338, step time: 1.5357\n",
      "118/388, train_loss: 0.0838, step time: 1.5366\n",
      "119/388, train_loss: 0.2525, step time: 1.5325\n",
      "120/388, train_loss: 0.6380, step time: 1.5354\n",
      "121/388, train_loss: 0.1048, step time: 1.5315\n",
      "122/388, train_loss: 0.1475, step time: 1.5318\n",
      "123/388, train_loss: 0.2626, step time: 1.5389\n",
      "124/388, train_loss: 0.2748, step time: 1.5327\n",
      "125/388, train_loss: 0.2960, step time: 1.5328\n",
      "126/388, train_loss: 0.3642, step time: 1.5326\n",
      "127/388, train_loss: 0.1087, step time: 1.5330\n",
      "128/388, train_loss: 0.2310, step time: 1.5346\n",
      "129/388, train_loss: 0.5444, step time: 1.5349\n",
      "130/388, train_loss: 0.1419, step time: 1.5365\n",
      "131/388, train_loss: 0.4002, step time: 1.5337\n",
      "132/388, train_loss: 0.2542, step time: 1.5341\n",
      "133/388, train_loss: 0.1370, step time: 1.5309\n",
      "134/388, train_loss: 0.3192, step time: 1.5334\n",
      "135/388, train_loss: 0.1834, step time: 1.5317\n",
      "136/388, train_loss: 0.2068, step time: 1.5422\n",
      "137/388, train_loss: 0.1879, step time: 1.5367\n",
      "138/388, train_loss: 0.1977, step time: 1.5324\n",
      "139/388, train_loss: 0.1616, step time: 1.5332\n",
      "140/388, train_loss: 0.5328, step time: 1.5341\n",
      "141/388, train_loss: 0.0999, step time: 1.5310\n",
      "142/388, train_loss: 0.3320, step time: 1.5307\n",
      "143/388, train_loss: 0.3847, step time: 1.5341\n",
      "144/388, train_loss: 0.1548, step time: 1.5326\n",
      "145/388, train_loss: 0.1925, step time: 1.5288\n",
      "146/388, train_loss: 0.2189, step time: 1.5353\n",
      "147/388, train_loss: 0.1961, step time: 1.5357\n",
      "148/388, train_loss: 0.2203, step time: 1.5370\n",
      "149/388, train_loss: 0.1923, step time: 1.5296\n",
      "150/388, train_loss: 0.1690, step time: 1.5298\n",
      "151/388, train_loss: 0.2003, step time: 1.5298\n",
      "152/388, train_loss: 0.1232, step time: 1.5346\n",
      "153/388, train_loss: 0.2816, step time: 1.5349\n",
      "154/388, train_loss: 0.4813, step time: 1.5332\n",
      "155/388, train_loss: 0.2113, step time: 1.5341\n",
      "156/388, train_loss: 0.1716, step time: 1.5353\n",
      "157/388, train_loss: 0.2010, step time: 1.5427\n",
      "158/388, train_loss: 0.2079, step time: 1.5345\n",
      "159/388, train_loss: 0.2037, step time: 1.5335\n",
      "160/388, train_loss: 0.3298, step time: 1.5339\n",
      "161/388, train_loss: 0.2455, step time: 1.5315\n",
      "162/388, train_loss: 0.1607, step time: 1.5318\n",
      "163/388, train_loss: 0.2693, step time: 1.5489\n",
      "164/388, train_loss: 0.2372, step time: 1.5331\n",
      "165/388, train_loss: 0.5121, step time: 1.5321\n",
      "166/388, train_loss: 0.0830, step time: 1.5330\n",
      "167/388, train_loss: 0.1463, step time: 1.5332\n",
      "168/388, train_loss: 0.1468, step time: 1.5320\n",
      "169/388, train_loss: 0.4010, step time: 1.5326\n",
      "170/388, train_loss: 0.4726, step time: 1.5392\n",
      "171/388, train_loss: 0.1037, step time: 1.5336\n",
      "172/388, train_loss: 0.3339, step time: 1.5361\n",
      "173/388, train_loss: 0.1377, step time: 1.5320\n",
      "174/388, train_loss: 0.1755, step time: 1.5299\n",
      "175/388, train_loss: 0.3186, step time: 1.5277\n",
      "176/388, train_loss: 0.0778, step time: 1.5324\n",
      "177/388, train_loss: 0.7255, step time: 1.5347\n",
      "178/388, train_loss: 0.1718, step time: 1.5318\n",
      "179/388, train_loss: 0.1094, step time: 1.5287\n",
      "180/388, train_loss: 0.1522, step time: 1.5331\n",
      "181/388, train_loss: 0.1506, step time: 1.5321\n",
      "182/388, train_loss: 0.1997, step time: 1.5312\n",
      "183/388, train_loss: 0.1729, step time: 1.5355\n",
      "184/388, train_loss: 0.1990, step time: 1.5360\n",
      "185/388, train_loss: 0.3398, step time: 1.5411\n",
      "186/388, train_loss: 0.0755, step time: 1.5337\n",
      "187/388, train_loss: 0.0954, step time: 1.5324\n",
      "188/388, train_loss: 0.4752, step time: 1.5344\n",
      "189/388, train_loss: 0.1762, step time: 1.5317\n",
      "190/388, train_loss: 0.1124, step time: 1.5335\n",
      "191/388, train_loss: 0.6129, step time: 1.5312\n",
      "192/388, train_loss: 0.1011, step time: 1.5311\n",
      "193/388, train_loss: 0.1790, step time: 1.5325\n",
      "194/388, train_loss: 0.2031, step time: 1.5395\n",
      "195/388, train_loss: 0.1210, step time: 1.5345\n",
      "196/388, train_loss: 0.1103, step time: 1.5323\n",
      "197/388, train_loss: 0.4458, step time: 1.5299\n",
      "198/388, train_loss: 0.2692, step time: 1.5329\n",
      "199/388, train_loss: 0.1836, step time: 1.5323\n",
      "200/388, train_loss: 0.2438, step time: 1.5387\n",
      "201/388, train_loss: 0.2936, step time: 1.5332\n",
      "202/388, train_loss: 0.1789, step time: 1.5339\n",
      "203/388, train_loss: 0.1627, step time: 1.5314\n",
      "204/388, train_loss: 0.1479, step time: 1.5307\n",
      "205/388, train_loss: 0.2173, step time: 1.5289\n",
      "206/388, train_loss: 0.1258, step time: 1.5410\n",
      "207/388, train_loss: 0.2142, step time: 1.5354\n",
      "208/388, train_loss: 0.3012, step time: 1.5368\n",
      "209/388, train_loss: 0.1793, step time: 1.5319\n",
      "210/388, train_loss: 0.4522, step time: 1.5303\n",
      "211/388, train_loss: 0.1578, step time: 1.5332\n",
      "212/388, train_loss: 0.3926, step time: 1.5318\n",
      "213/388, train_loss: 0.1923, step time: 1.5349\n",
      "214/388, train_loss: 0.1478, step time: 1.5355\n",
      "215/388, train_loss: 0.2386, step time: 1.5569\n",
      "216/388, train_loss: 0.2682, step time: 1.5365\n",
      "217/388, train_loss: 0.0938, step time: 1.5338\n",
      "218/388, train_loss: 0.2499, step time: 1.5320\n",
      "219/388, train_loss: 0.3335, step time: 1.5349\n",
      "220/388, train_loss: 0.4653, step time: 1.5342\n",
      "221/388, train_loss: 0.6047, step time: 1.5400\n",
      "222/388, train_loss: 0.5342, step time: 1.5360\n",
      "223/388, train_loss: 0.1171, step time: 1.5291\n",
      "224/388, train_loss: 0.2040, step time: 1.5313\n",
      "225/388, train_loss: 0.6908, step time: 1.5344\n",
      "226/388, train_loss: 0.1916, step time: 1.5366\n",
      "227/388, train_loss: 0.7036, step time: 1.5321\n",
      "228/388, train_loss: 0.2181, step time: 1.5299\n",
      "229/388, train_loss: 0.2814, step time: 1.5555\n",
      "230/388, train_loss: 0.1561, step time: 1.5363\n",
      "231/388, train_loss: 0.1317, step time: 1.5340\n",
      "232/388, train_loss: 0.3013, step time: 1.5303\n",
      "233/388, train_loss: 0.1931, step time: 1.5312\n",
      "234/388, train_loss: 0.1271, step time: 1.5324\n",
      "235/388, train_loss: 0.1487, step time: 1.5346\n",
      "236/388, train_loss: 0.1993, step time: 1.5359\n",
      "237/388, train_loss: 0.1992, step time: 1.5375\n",
      "238/388, train_loss: 0.7297, step time: 1.5337\n",
      "239/388, train_loss: 0.1932, step time: 1.5323\n",
      "240/388, train_loss: 0.3664, step time: 1.5310\n",
      "241/388, train_loss: 0.1939, step time: 1.5447\n",
      "242/388, train_loss: 0.1536, step time: 1.5370\n",
      "243/388, train_loss: 0.2881, step time: 1.5368\n",
      "244/388, train_loss: 0.0947, step time: 1.5317\n",
      "245/388, train_loss: 0.1199, step time: 1.5360\n",
      "246/388, train_loss: 0.3293, step time: 1.5325\n",
      "247/388, train_loss: 0.2819, step time: 1.5353\n",
      "248/388, train_loss: 0.1791, step time: 1.5325\n",
      "249/388, train_loss: 0.5613, step time: 1.5333\n",
      "250/388, train_loss: 0.2033, step time: 1.5312\n",
      "251/388, train_loss: 0.2652, step time: 1.5288\n",
      "252/388, train_loss: 0.0640, step time: 1.5346\n",
      "253/388, train_loss: 0.2888, step time: 1.5376\n",
      "254/388, train_loss: 0.1479, step time: 1.5587\n",
      "255/388, train_loss: 0.1430, step time: 1.5290\n",
      "256/388, train_loss: 0.2087, step time: 1.5318\n",
      "257/388, train_loss: 0.2072, step time: 1.5372\n",
      "258/388, train_loss: 0.2125, step time: 1.5344\n",
      "259/388, train_loss: 0.1132, step time: 1.5308\n",
      "260/388, train_loss: 0.1436, step time: 1.5342\n",
      "261/388, train_loss: 0.2095, step time: 1.5284\n",
      "262/388, train_loss: 0.2934, step time: 1.5303\n",
      "263/388, train_loss: 0.4082, step time: 1.5342\n",
      "264/388, train_loss: 0.2455, step time: 1.5375\n",
      "265/388, train_loss: 0.1503, step time: 1.5353\n",
      "266/388, train_loss: 0.3317, step time: 1.5327\n",
      "267/388, train_loss: 0.2149, step time: 1.5345\n",
      "268/388, train_loss: 0.3600, step time: 1.5321\n",
      "269/388, train_loss: 0.2035, step time: 1.5313\n",
      "270/388, train_loss: 0.2284, step time: 1.5319\n",
      "271/388, train_loss: 0.1138, step time: 1.5341\n",
      "272/388, train_loss: 0.6312, step time: 1.5345\n",
      "273/388, train_loss: 0.7174, step time: 1.5341\n",
      "274/388, train_loss: 0.2914, step time: 1.5303\n",
      "275/388, train_loss: 0.3288, step time: 1.5315\n",
      "276/388, train_loss: 0.1549, step time: 1.5347\n",
      "277/388, train_loss: 0.2650, step time: 1.5417\n",
      "278/388, train_loss: 0.1754, step time: 1.5312\n",
      "279/388, train_loss: 0.2343, step time: 1.5333\n",
      "280/388, train_loss: 0.2098, step time: 1.5343\n",
      "281/388, train_loss: 0.3276, step time: 1.5369\n",
      "282/388, train_loss: 0.0928, step time: 1.5331\n",
      "283/388, train_loss: 0.2649, step time: 1.5325\n",
      "284/388, train_loss: 0.2073, step time: 1.5335\n",
      "285/388, train_loss: 0.1424, step time: 1.5301\n",
      "286/388, train_loss: 0.3746, step time: 1.5294\n",
      "287/388, train_loss: 0.2659, step time: 1.5353\n",
      "288/388, train_loss: 0.2280, step time: 1.5446\n",
      "289/388, train_loss: 0.1857, step time: 1.5541\n",
      "290/388, train_loss: 0.7349, step time: 1.5485\n",
      "291/388, train_loss: 0.2637, step time: 1.5323\n",
      "292/388, train_loss: 0.0966, step time: 1.5299\n",
      "293/388, train_loss: 0.1480, step time: 1.5383\n",
      "294/388, train_loss: 0.4869, step time: 1.5347\n",
      "295/388, train_loss: 0.2016, step time: 1.5368\n",
      "296/388, train_loss: 0.1316, step time: 1.5309\n",
      "297/388, train_loss: 0.2798, step time: 1.5300\n",
      "298/388, train_loss: 0.1983, step time: 1.5308\n",
      "299/388, train_loss: 0.2129, step time: 1.5330\n",
      "300/388, train_loss: 0.1853, step time: 1.5359\n",
      "301/388, train_loss: 0.3215, step time: 1.5352\n",
      "302/388, train_loss: 0.2260, step time: 1.5337\n",
      "303/388, train_loss: 0.2054, step time: 1.5320\n",
      "304/388, train_loss: 0.1448, step time: 1.5321\n",
      "305/388, train_loss: 0.4192, step time: 1.5311\n",
      "306/388, train_loss: 0.2908, step time: 1.5452\n",
      "307/388, train_loss: 0.1789, step time: 1.5328\n",
      "308/388, train_loss: 0.1203, step time: 1.5347\n",
      "309/388, train_loss: 0.2695, step time: 1.5327\n",
      "310/388, train_loss: 0.5623, step time: 1.5309\n",
      "311/388, train_loss: 0.2442, step time: 1.5302\n",
      "312/388, train_loss: 0.1890, step time: 1.5423\n",
      "313/388, train_loss: 0.5484, step time: 1.5309\n",
      "314/388, train_loss: 0.3551, step time: 1.5406\n",
      "315/388, train_loss: 0.2736, step time: 1.5334\n",
      "316/388, train_loss: 0.1880, step time: 1.5315\n",
      "317/388, train_loss: 0.1866, step time: 1.5333\n",
      "318/388, train_loss: 0.1382, step time: 1.5369\n",
      "319/388, train_loss: 0.3853, step time: 1.5388\n",
      "320/388, train_loss: 0.2360, step time: 1.5441\n",
      "321/388, train_loss: 0.1199, step time: 1.5341\n",
      "322/388, train_loss: 0.3998, step time: 1.5329\n",
      "323/388, train_loss: 0.1935, step time: 1.5377\n",
      "324/388, train_loss: 0.3111, step time: 1.5345\n",
      "325/388, train_loss: 0.2405, step time: 1.5324\n",
      "326/388, train_loss: 0.1088, step time: 1.5318\n",
      "327/388, train_loss: 0.1880, step time: 1.5317\n",
      "328/388, train_loss: 0.2529, step time: 1.5401\n",
      "329/388, train_loss: 0.2663, step time: 1.5348\n",
      "330/388, train_loss: 0.1866, step time: 1.5353\n",
      "331/388, train_loss: 0.3452, step time: 1.5331\n",
      "332/388, train_loss: 0.3075, step time: 1.5327\n",
      "333/388, train_loss: 0.2100, step time: 1.5302\n",
      "334/388, train_loss: 0.1441, step time: 1.5333\n",
      "335/388, train_loss: 0.1948, step time: 1.5338\n",
      "336/388, train_loss: 0.1492, step time: 1.5367\n",
      "337/388, train_loss: 0.1911, step time: 1.5341\n",
      "338/388, train_loss: 0.3904, step time: 1.5376\n",
      "339/388, train_loss: 0.2109, step time: 1.5297\n",
      "340/388, train_loss: 0.1163, step time: 1.5347\n",
      "341/388, train_loss: 0.3190, step time: 1.5344\n",
      "342/388, train_loss: 0.0776, step time: 1.5338\n",
      "343/388, train_loss: 0.1506, step time: 1.5322\n",
      "344/388, train_loss: 0.0913, step time: 1.5296\n",
      "345/388, train_loss: 0.3922, step time: 1.5318\n",
      "346/388, train_loss: 0.4586, step time: 1.5327\n",
      "347/388, train_loss: 0.3093, step time: 1.5310\n",
      "348/388, train_loss: 0.1196, step time: 1.5357\n",
      "349/388, train_loss: 0.2348, step time: 1.5351\n",
      "350/388, train_loss: 0.3511, step time: 1.5576\n",
      "351/388, train_loss: 0.2580, step time: 1.5431\n",
      "352/388, train_loss: 0.4300, step time: 1.5324\n",
      "353/388, train_loss: 0.2505, step time: 1.5305\n",
      "354/388, train_loss: 0.0953, step time: 1.5325\n",
      "355/388, train_loss: 0.4732, step time: 1.5311\n",
      "356/388, train_loss: 0.2216, step time: 1.5302\n",
      "357/388, train_loss: 0.1425, step time: 1.5321\n",
      "358/388, train_loss: 0.2081, step time: 1.5327\n",
      "359/388, train_loss: 0.2368, step time: 1.5331\n",
      "360/388, train_loss: 0.1383, step time: 1.5333\n",
      "361/388, train_loss: 0.4815, step time: 1.5299\n",
      "362/388, train_loss: 0.0829, step time: 1.5304\n",
      "363/388, train_loss: 0.2883, step time: 1.5305\n",
      "364/388, train_loss: 0.5494, step time: 1.5311\n",
      "365/388, train_loss: 0.6044, step time: 1.5330\n",
      "366/388, train_loss: 0.3334, step time: 1.5520\n",
      "367/388, train_loss: 0.2098, step time: 1.5332\n",
      "368/388, train_loss: 0.1595, step time: 1.5327\n",
      "369/388, train_loss: 0.3417, step time: 1.5345\n",
      "370/388, train_loss: 0.3025, step time: 1.5332\n",
      "371/388, train_loss: 0.3980, step time: 1.5351\n",
      "372/388, train_loss: 0.5438, step time: 1.5378\n",
      "373/388, train_loss: 0.3313, step time: 1.5303\n",
      "374/388, train_loss: 0.4219, step time: 1.5316\n",
      "375/388, train_loss: 0.1423, step time: 1.5317\n",
      "376/388, train_loss: 0.4182, step time: 1.5343\n",
      "377/388, train_loss: 0.3352, step time: 1.5358\n",
      "378/388, train_loss: 0.1427, step time: 1.5345\n",
      "379/388, train_loss: 0.2301, step time: 1.5316\n",
      "380/388, train_loss: 0.4492, step time: 1.5341\n",
      "381/388, train_loss: 0.2240, step time: 1.5396\n",
      "382/388, train_loss: 0.2211, step time: 1.5357\n",
      "383/388, train_loss: 0.1599, step time: 1.5352\n",
      "384/388, train_loss: 0.3784, step time: 1.5367\n",
      "385/388, train_loss: 0.2049, step time: 1.5323\n",
      "386/388, train_loss: 0.6930, step time: 1.5326\n",
      "387/388, train_loss: 0.1364, step time: 1.5322\n",
      "388/388, train_loss: 0.1670, step time: 1.5380\n",
      "epoch 13 average loss: 0.2579\n",
      "saved new best metric model\n",
      "current epoch: 13 current mean dice: 0.7194 tc: 0.7715 wt: 0.8767 et: 0.5100\n",
      "best mean dice: 0.7194 at epoch: 13\n",
      "time consuming of epoch 13 is: 705.3195\n",
      "----------\n",
      "epoch 14/100\n",
      "1/388, train_loss: 0.1654, step time: 1.5462\n",
      "2/388, train_loss: 0.0823, step time: 1.5366\n",
      "3/388, train_loss: 0.0951, step time: 1.5347\n",
      "4/388, train_loss: 0.1316, step time: 1.5383\n",
      "5/388, train_loss: 0.1628, step time: 1.5347\n",
      "6/388, train_loss: 0.1018, step time: 1.5371\n",
      "7/388, train_loss: 0.2020, step time: 1.5327\n",
      "8/388, train_loss: 0.3029, step time: 1.5369\n",
      "9/388, train_loss: 0.1879, step time: 1.5305\n",
      "10/388, train_loss: 0.1700, step time: 1.5293\n",
      "11/388, train_loss: 0.2726, step time: 1.5284\n",
      "12/388, train_loss: 0.2239, step time: 1.5347\n",
      "13/388, train_loss: 0.1500, step time: 1.5328\n",
      "14/388, train_loss: 0.2553, step time: 1.5367\n",
      "15/388, train_loss: 0.2105, step time: 1.5393\n",
      "16/388, train_loss: 0.1971, step time: 1.5330\n",
      "17/388, train_loss: 0.2024, step time: 1.5329\n",
      "18/388, train_loss: 0.1290, step time: 1.5314\n",
      "19/388, train_loss: 0.1525, step time: 1.5341\n",
      "20/388, train_loss: 0.1765, step time: 1.5363\n",
      "21/388, train_loss: 0.3864, step time: 1.5350\n",
      "22/388, train_loss: 0.2651, step time: 1.5336\n",
      "23/388, train_loss: 0.1133, step time: 1.5299\n",
      "24/388, train_loss: 0.1601, step time: 1.5286\n",
      "25/388, train_loss: 0.2199, step time: 1.5352\n",
      "26/388, train_loss: 0.2321, step time: 1.5350\n",
      "27/388, train_loss: 0.1519, step time: 1.5346\n",
      "28/388, train_loss: 0.4045, step time: 1.5324\n",
      "29/388, train_loss: 0.2193, step time: 1.5334\n",
      "30/388, train_loss: 0.2899, step time: 1.5369\n",
      "31/388, train_loss: 0.3290, step time: 1.5369\n",
      "32/388, train_loss: 0.2586, step time: 1.5309\n",
      "33/388, train_loss: 0.2026, step time: 1.5314\n",
      "34/388, train_loss: 0.3307, step time: 1.5315\n",
      "35/388, train_loss: 0.1958, step time: 1.5308\n",
      "36/388, train_loss: 0.2239, step time: 1.5344\n",
      "37/388, train_loss: 0.2640, step time: 1.5355\n",
      "38/388, train_loss: 0.2920, step time: 1.5363\n",
      "39/388, train_loss: 0.1454, step time: 1.5306\n",
      "40/388, train_loss: 0.3514, step time: 1.5316\n",
      "41/388, train_loss: 0.3520, step time: 1.5326\n",
      "42/388, train_loss: 0.1761, step time: 1.5473\n",
      "43/388, train_loss: 0.1479, step time: 1.5316\n",
      "44/388, train_loss: 0.3177, step time: 1.5307\n",
      "45/388, train_loss: 0.4353, step time: 1.5354\n",
      "46/388, train_loss: 0.1494, step time: 1.5332\n",
      "47/388, train_loss: 0.1144, step time: 1.5343\n",
      "48/388, train_loss: 0.4337, step time: 1.5355\n",
      "49/388, train_loss: 0.4829, step time: 1.5385\n",
      "50/388, train_loss: 0.3603, step time: 1.5307\n",
      "51/388, train_loss: 0.1898, step time: 1.5285\n",
      "52/388, train_loss: 0.2716, step time: 1.5311\n",
      "53/388, train_loss: 0.1416, step time: 1.5309\n",
      "54/388, train_loss: 0.3019, step time: 1.5364\n",
      "55/388, train_loss: 0.5350, step time: 1.5384\n",
      "56/388, train_loss: 0.0755, step time: 1.5360\n",
      "57/388, train_loss: 0.1112, step time: 1.5311\n",
      "58/388, train_loss: 0.2379, step time: 1.5324\n",
      "59/388, train_loss: 0.2138, step time: 1.5312\n",
      "60/388, train_loss: 0.2575, step time: 1.5328\n",
      "61/388, train_loss: 0.0992, step time: 1.5367\n",
      "62/388, train_loss: 0.3849, step time: 1.5366\n",
      "63/388, train_loss: 0.1389, step time: 1.5368\n",
      "64/388, train_loss: 0.7598, step time: 1.5320\n",
      "65/388, train_loss: 0.3132, step time: 1.5340\n",
      "66/388, train_loss: 0.2557, step time: 1.5377\n",
      "67/388, train_loss: 0.3569, step time: 1.5354\n",
      "68/388, train_loss: 0.6794, step time: 1.5308\n",
      "69/388, train_loss: 0.1570, step time: 1.5319\n",
      "70/388, train_loss: 0.2247, step time: 1.5312\n",
      "71/388, train_loss: 0.1797, step time: 1.5437\n",
      "72/388, train_loss: 0.1964, step time: 1.5342\n",
      "73/388, train_loss: 0.1872, step time: 1.5342\n",
      "74/388, train_loss: 0.3376, step time: 1.5363\n",
      "75/388, train_loss: 0.1478, step time: 1.5315\n",
      "76/388, train_loss: 0.2561, step time: 1.5310\n",
      "77/388, train_loss: 0.3935, step time: 1.5322\n",
      "78/388, train_loss: 0.3508, step time: 1.5406\n",
      "79/388, train_loss: 0.3929, step time: 1.5360\n",
      "80/388, train_loss: 0.1454, step time: 1.5339\n",
      "81/388, train_loss: 0.2888, step time: 1.5340\n",
      "82/388, train_loss: 0.2190, step time: 1.5335\n",
      "83/388, train_loss: 0.2328, step time: 1.5308\n",
      "84/388, train_loss: 0.2443, step time: 1.5366\n",
      "85/388, train_loss: 0.8286, step time: 1.5382\n",
      "86/388, train_loss: 0.1836, step time: 1.5303\n",
      "87/388, train_loss: 0.1557, step time: 1.5333\n",
      "88/388, train_loss: 0.5061, step time: 1.5305\n",
      "89/388, train_loss: 0.2512, step time: 1.5297\n",
      "90/388, train_loss: 0.2275, step time: 1.5369\n",
      "91/388, train_loss: 0.1696, step time: 1.5325\n",
      "92/388, train_loss: 0.2306, step time: 1.5356\n",
      "93/388, train_loss: 0.2057, step time: 1.5325\n",
      "94/388, train_loss: 0.2524, step time: 1.5317\n",
      "95/388, train_loss: 0.1024, step time: 1.5302\n",
      "96/388, train_loss: 0.0957, step time: 1.5372\n",
      "97/388, train_loss: 0.0653, step time: 1.5332\n",
      "98/388, train_loss: 0.3524, step time: 1.5336\n",
      "99/388, train_loss: 0.2786, step time: 1.5309\n",
      "100/388, train_loss: 0.0431, step time: 1.5313\n",
      "101/388, train_loss: 0.2651, step time: 1.5324\n",
      "102/388, train_loss: 0.2290, step time: 1.5367\n",
      "103/388, train_loss: 0.1978, step time: 1.5344\n",
      "104/388, train_loss: 0.1639, step time: 1.5349\n",
      "105/388, train_loss: 0.4035, step time: 1.5312\n",
      "106/388, train_loss: 0.2575, step time: 1.5344\n",
      "107/388, train_loss: 0.1404, step time: 1.5306\n",
      "108/388, train_loss: 0.2041, step time: 1.5347\n",
      "109/388, train_loss: 0.2184, step time: 1.5362\n",
      "110/388, train_loss: 0.3821, step time: 1.5290\n",
      "111/388, train_loss: 0.2034, step time: 1.5322\n",
      "112/388, train_loss: 0.3770, step time: 1.5293\n",
      "113/388, train_loss: 0.1652, step time: 1.5374\n",
      "114/388, train_loss: 0.1454, step time: 1.5360\n",
      "115/388, train_loss: 0.1069, step time: 1.5321\n",
      "116/388, train_loss: 0.1901, step time: 1.5337\n",
      "117/388, train_loss: 0.1012, step time: 1.5326\n",
      "118/388, train_loss: 0.0917, step time: 1.5430\n",
      "119/388, train_loss: 0.1282, step time: 1.5366\n",
      "120/388, train_loss: 0.4349, step time: 1.5369\n",
      "121/388, train_loss: 0.2419, step time: 1.5307\n",
      "122/388, train_loss: 0.3281, step time: 1.5339\n",
      "123/388, train_loss: 0.1406, step time: 1.5296\n",
      "124/388, train_loss: 0.6180, step time: 1.5347\n",
      "125/388, train_loss: 0.2824, step time: 1.5369\n",
      "126/388, train_loss: 0.3494, step time: 1.5326\n",
      "127/388, train_loss: 0.1381, step time: 1.5311\n",
      "128/388, train_loss: 0.1272, step time: 1.5328\n",
      "129/388, train_loss: 0.1672, step time: 1.5344\n",
      "130/388, train_loss: 0.5473, step time: 1.5340\n",
      "131/388, train_loss: 0.4967, step time: 1.5332\n",
      "132/388, train_loss: 0.2474, step time: 1.5311\n",
      "133/388, train_loss: 0.2852, step time: 1.5325\n",
      "134/388, train_loss: 0.1856, step time: 1.5363\n",
      "135/388, train_loss: 0.1722, step time: 1.5315\n",
      "136/388, train_loss: 0.2528, step time: 1.5345\n",
      "137/388, train_loss: 0.2477, step time: 1.5336\n",
      "138/388, train_loss: 0.4846, step time: 1.5315\n",
      "139/388, train_loss: 0.1662, step time: 1.5283\n",
      "140/388, train_loss: 0.1820, step time: 1.5319\n",
      "141/388, train_loss: 0.1641, step time: 1.5344\n",
      "142/388, train_loss: 0.3566, step time: 1.5363\n",
      "143/388, train_loss: 0.1906, step time: 1.5371\n",
      "144/388, train_loss: 0.0626, step time: 1.5302\n",
      "145/388, train_loss: 0.3009, step time: 1.5303\n",
      "146/388, train_loss: 0.3788, step time: 1.5339\n",
      "147/388, train_loss: 0.1976, step time: 1.5339\n",
      "148/388, train_loss: 0.2990, step time: 1.5326\n",
      "149/388, train_loss: 0.1947, step time: 1.5347\n",
      "150/388, train_loss: 0.2545, step time: 1.5351\n",
      "151/388, train_loss: 0.1756, step time: 1.5309\n",
      "152/388, train_loss: 0.2106, step time: 1.5334\n",
      "153/388, train_loss: 0.1736, step time: 1.5397\n",
      "154/388, train_loss: 0.1498, step time: 1.5277\n",
      "155/388, train_loss: 0.6403, step time: 1.5312\n",
      "156/388, train_loss: 0.3519, step time: 1.5362\n",
      "157/388, train_loss: 0.2828, step time: 1.5360\n",
      "158/388, train_loss: 0.2077, step time: 1.5338\n",
      "159/388, train_loss: 0.1083, step time: 1.5357\n",
      "160/388, train_loss: 0.2401, step time: 1.5305\n",
      "161/388, train_loss: 0.1901, step time: 1.5304\n",
      "162/388, train_loss: 0.2504, step time: 1.5318\n",
      "163/388, train_loss: 0.5554, step time: 1.5320\n",
      "164/388, train_loss: 0.0910, step time: 1.5360\n",
      "165/388, train_loss: 0.1088, step time: 1.5366\n",
      "166/388, train_loss: 0.3537, step time: 1.5362\n",
      "167/388, train_loss: 0.1073, step time: 1.5299\n",
      "168/388, train_loss: 0.4203, step time: 1.5592\n",
      "169/388, train_loss: 0.0881, step time: 1.5328\n",
      "170/388, train_loss: 0.2529, step time: 1.5344\n",
      "171/388, train_loss: 0.1777, step time: 1.5309\n",
      "172/388, train_loss: 0.4651, step time: 1.5323\n",
      "173/388, train_loss: 0.4357, step time: 1.5418\n",
      "174/388, train_loss: 0.1252, step time: 1.5365\n",
      "175/388, train_loss: 0.1114, step time: 1.5329\n",
      "176/388, train_loss: 0.3424, step time: 1.5321\n",
      "177/388, train_loss: 0.3915, step time: 1.5337\n",
      "178/388, train_loss: 0.1108, step time: 1.5354\n",
      "179/388, train_loss: 0.1449, step time: 1.5428\n",
      "180/388, train_loss: 0.1414, step time: 1.5324\n",
      "181/388, train_loss: 0.1259, step time: 1.5299\n",
      "182/388, train_loss: 0.4332, step time: 1.5324\n",
      "183/388, train_loss: 0.1017, step time: 1.5332\n",
      "184/388, train_loss: 0.6124, step time: 1.5321\n",
      "185/388, train_loss: 0.2809, step time: 1.5395\n",
      "186/388, train_loss: 0.1413, step time: 1.5426\n",
      "187/388, train_loss: 0.2725, step time: 1.5340\n",
      "188/388, train_loss: 0.1919, step time: 1.5345\n",
      "189/388, train_loss: 0.1468, step time: 1.5326\n",
      "190/388, train_loss: 0.1775, step time: 1.5363\n",
      "191/388, train_loss: 0.4872, step time: 1.5449\n",
      "192/388, train_loss: 0.2192, step time: 1.5313\n",
      "193/388, train_loss: 0.1634, step time: 1.5333\n",
      "194/388, train_loss: 0.8344, step time: 1.5304\n",
      "195/388, train_loss: 0.3433, step time: 1.5305\n",
      "196/388, train_loss: 0.1539, step time: 1.5357\n",
      "197/388, train_loss: 0.0989, step time: 1.5322\n",
      "198/388, train_loss: 0.3050, step time: 1.5370\n",
      "199/388, train_loss: 0.1048, step time: 1.5349\n",
      "200/388, train_loss: 0.3854, step time: 1.5341\n",
      "201/388, train_loss: 0.1799, step time: 1.5335\n",
      "202/388, train_loss: 0.1038, step time: 1.5362\n",
      "203/388, train_loss: 0.2326, step time: 1.5344\n",
      "204/388, train_loss: 0.3685, step time: 1.5349\n",
      "205/388, train_loss: 0.0922, step time: 1.5314\n",
      "206/388, train_loss: 0.3660, step time: 1.5309\n",
      "207/388, train_loss: 0.0698, step time: 1.5315\n",
      "208/388, train_loss: 0.1095, step time: 1.5314\n",
      "209/388, train_loss: 0.3474, step time: 1.5375\n",
      "210/388, train_loss: 0.3200, step time: 1.5382\n",
      "211/388, train_loss: 0.2433, step time: 1.5309\n",
      "212/388, train_loss: 0.1939, step time: 1.5319\n",
      "213/388, train_loss: 0.2519, step time: 1.5333\n",
      "214/388, train_loss: 0.1403, step time: 1.5307\n",
      "215/388, train_loss: 0.1505, step time: 1.5364\n",
      "216/388, train_loss: 0.1295, step time: 1.5362\n",
      "217/388, train_loss: 0.4335, step time: 1.5342\n",
      "218/388, train_loss: 0.1674, step time: 1.5325\n",
      "219/388, train_loss: 0.1714, step time: 1.5328\n",
      "220/388, train_loss: 0.1007, step time: 1.5347\n",
      "221/388, train_loss: 0.3525, step time: 1.5346\n",
      "222/388, train_loss: 0.1036, step time: 1.5350\n",
      "223/388, train_loss: 0.1332, step time: 1.5296\n",
      "224/388, train_loss: 0.1771, step time: 1.5389\n",
      "225/388, train_loss: 0.2796, step time: 1.5359\n",
      "226/388, train_loss: 0.2668, step time: 1.5365\n",
      "227/388, train_loss: 0.2068, step time: 1.5337\n",
      "228/388, train_loss: 0.3427, step time: 1.5331\n",
      "229/388, train_loss: 0.2595, step time: 1.5301\n",
      "230/388, train_loss: 0.2972, step time: 1.5311\n",
      "231/388, train_loss: 0.2639, step time: 1.5287\n",
      "232/388, train_loss: 0.4417, step time: 1.5331\n",
      "233/388, train_loss: 0.2096, step time: 1.5351\n",
      "234/388, train_loss: 0.2243, step time: 1.5341\n",
      "235/388, train_loss: 0.3149, step time: 1.5326\n",
      "236/388, train_loss: 0.1661, step time: 1.5344\n",
      "237/388, train_loss: 0.2085, step time: 1.5299\n",
      "238/388, train_loss: 0.1433, step time: 1.5312\n",
      "239/388, train_loss: 0.3481, step time: 1.5352\n",
      "240/388, train_loss: 0.3018, step time: 1.5345\n",
      "241/388, train_loss: 0.2519, step time: 1.5320\n",
      "242/388, train_loss: 0.2352, step time: 1.5365\n",
      "243/388, train_loss: 0.3491, step time: 1.5351\n",
      "244/388, train_loss: 0.2780, step time: 1.5344\n",
      "245/388, train_loss: 0.1029, step time: 1.5332\n",
      "246/388, train_loss: 0.1490, step time: 1.5344\n",
      "247/388, train_loss: 0.2641, step time: 1.5367\n",
      "248/388, train_loss: 0.2047, step time: 1.5387\n",
      "249/388, train_loss: 0.3617, step time: 1.5320\n",
      "250/388, train_loss: 0.2392, step time: 1.5339\n",
      "251/388, train_loss: 0.1724, step time: 1.5422\n",
      "252/388, train_loss: 0.2335, step time: 1.5298\n",
      "253/388, train_loss: 0.1673, step time: 1.5293\n",
      "254/388, train_loss: 0.3596, step time: 1.5461\n",
      "255/388, train_loss: 0.1953, step time: 1.5349\n",
      "256/388, train_loss: 0.2727, step time: 1.5378\n",
      "257/388, train_loss: 0.1205, step time: 1.5294\n",
      "258/388, train_loss: 0.1670, step time: 1.5325\n",
      "259/388, train_loss: 0.1694, step time: 1.5308\n",
      "260/388, train_loss: 0.2879, step time: 1.5356\n",
      "261/388, train_loss: 0.4535, step time: 1.5353\n",
      "262/388, train_loss: 0.1018, step time: 1.5364\n",
      "263/388, train_loss: 0.1663, step time: 1.5392\n",
      "264/388, train_loss: 0.6269, step time: 1.5337\n",
      "265/388, train_loss: 0.6735, step time: 1.5316\n",
      "266/388, train_loss: 0.1121, step time: 1.5311\n",
      "267/388, train_loss: 0.2169, step time: 1.5299\n",
      "268/388, train_loss: 0.6368, step time: 1.5334\n",
      "269/388, train_loss: 0.1976, step time: 1.5364\n",
      "270/388, train_loss: 0.3149, step time: 1.5398\n",
      "271/388, train_loss: 0.1871, step time: 1.5329\n",
      "272/388, train_loss: 0.3834, step time: 1.5310\n",
      "273/388, train_loss: 0.1497, step time: 1.5313\n",
      "274/388, train_loss: 0.1316, step time: 1.5318\n",
      "275/388, train_loss: 0.0963, step time: 1.5350\n",
      "276/388, train_loss: 0.1392, step time: 1.5342\n",
      "277/388, train_loss: 0.0816, step time: 1.5297\n",
      "278/388, train_loss: 0.5086, step time: 1.5290\n",
      "279/388, train_loss: 0.1640, step time: 1.5327\n",
      "280/388, train_loss: 0.1977, step time: 1.5286\n",
      "281/388, train_loss: 0.0412, step time: 1.5355\n",
      "282/388, train_loss: 0.2142, step time: 1.5379\n",
      "283/388, train_loss: 0.1235, step time: 1.5355\n",
      "284/388, train_loss: 0.2028, step time: 1.5348\n",
      "285/388, train_loss: 0.2287, step time: 1.5341\n",
      "286/388, train_loss: 0.4358, step time: 1.5373\n",
      "287/388, train_loss: 0.2455, step time: 1.5342\n",
      "288/388, train_loss: 0.1422, step time: 1.5417\n",
      "289/388, train_loss: 0.1774, step time: 1.5335\n",
      "290/388, train_loss: 0.3036, step time: 1.5345\n",
      "291/388, train_loss: 0.1584, step time: 1.5319\n",
      "292/388, train_loss: 0.2337, step time: 1.5313\n",
      "293/388, train_loss: 0.1884, step time: 1.5319\n",
      "294/388, train_loss: 0.1250, step time: 1.5321\n",
      "295/388, train_loss: 0.2321, step time: 1.5353\n",
      "296/388, train_loss: 0.2994, step time: 1.5354\n",
      "297/388, train_loss: 0.4770, step time: 1.5333\n",
      "298/388, train_loss: 0.1029, step time: 1.5319\n",
      "299/388, train_loss: 0.2704, step time: 1.5305\n",
      "300/388, train_loss: 0.0845, step time: 1.5317\n",
      "301/388, train_loss: 0.2542, step time: 1.5328\n",
      "302/388, train_loss: 0.2672, step time: 1.5354\n",
      "303/388, train_loss: 0.4870, step time: 1.5364\n",
      "304/388, train_loss: 0.3590, step time: 1.5335\n",
      "305/388, train_loss: 0.1225, step time: 1.5329\n",
      "306/388, train_loss: 0.2757, step time: 1.5321\n",
      "307/388, train_loss: 0.2308, step time: 1.5318\n",
      "308/388, train_loss: 0.1129, step time: 1.5355\n",
      "309/388, train_loss: 0.5663, step time: 1.5339\n",
      "310/388, train_loss: 0.2222, step time: 1.5322\n",
      "311/388, train_loss: 0.5766, step time: 1.5312\n",
      "312/388, train_loss: 0.6033, step time: 1.5346\n",
      "313/388, train_loss: 0.4134, step time: 1.5375\n",
      "314/388, train_loss: 0.3883, step time: 1.5382\n",
      "315/388, train_loss: 0.3744, step time: 1.5340\n",
      "316/388, train_loss: 0.5412, step time: 1.5282\n",
      "317/388, train_loss: 0.1461, step time: 1.5324\n",
      "318/388, train_loss: 0.1778, step time: 1.5334\n",
      "319/388, train_loss: 0.1467, step time: 1.5344\n",
      "320/388, train_loss: 0.2568, step time: 1.5352\n",
      "321/388, train_loss: 0.4863, step time: 1.5363\n",
      "322/388, train_loss: 0.1403, step time: 1.5333\n",
      "323/388, train_loss: 0.0913, step time: 1.5395\n",
      "324/388, train_loss: 0.3267, step time: 1.5362\n",
      "325/388, train_loss: 0.1856, step time: 1.5426\n",
      "326/388, train_loss: 0.6371, step time: 1.5356\n",
      "327/388, train_loss: 0.2546, step time: 1.5311\n",
      "328/388, train_loss: 0.1325, step time: 1.5313\n",
      "329/388, train_loss: 0.2184, step time: 1.5369\n",
      "330/388, train_loss: 0.1505, step time: 1.5432\n",
      "331/388, train_loss: 0.1516, step time: 1.5321\n",
      "332/388, train_loss: 0.3380, step time: 1.5334\n",
      "333/388, train_loss: 0.1689, step time: 1.5379\n",
      "334/388, train_loss: 0.1366, step time: 1.5389\n",
      "335/388, train_loss: 0.1075, step time: 1.5308\n",
      "336/388, train_loss: 0.2581, step time: 1.5294\n",
      "337/388, train_loss: 0.3191, step time: 1.5302\n",
      "338/388, train_loss: 0.3624, step time: 1.5365\n",
      "339/388, train_loss: 0.1523, step time: 1.5350\n",
      "340/388, train_loss: 0.2142, step time: 1.5351\n",
      "341/388, train_loss: 0.1308, step time: 1.5321\n",
      "342/388, train_loss: 0.1160, step time: 1.5300\n",
      "343/388, train_loss: 0.1830, step time: 1.5359\n",
      "344/388, train_loss: 0.2138, step time: 1.5444\n",
      "345/388, train_loss: 0.1807, step time: 1.5609\n",
      "346/388, train_loss: 0.0885, step time: 1.5312\n",
      "347/388, train_loss: 0.3438, step time: 1.5347\n",
      "348/388, train_loss: 0.1355, step time: 1.5496\n",
      "349/388, train_loss: 0.2621, step time: 1.5342\n",
      "350/388, train_loss: 0.2867, step time: 1.5321\n",
      "351/388, train_loss: 0.3216, step time: 1.5318\n",
      "352/388, train_loss: 0.3010, step time: 1.5322\n",
      "353/388, train_loss: 0.1781, step time: 1.5413\n",
      "354/388, train_loss: 0.1349, step time: 1.5385\n",
      "355/388, train_loss: 0.1379, step time: 1.5355\n",
      "356/388, train_loss: 0.3632, step time: 1.5314\n",
      "357/388, train_loss: 0.5972, step time: 1.5320\n",
      "358/388, train_loss: 0.1720, step time: 1.5378\n",
      "359/388, train_loss: 0.1806, step time: 1.5366\n",
      "360/388, train_loss: 0.1251, step time: 1.5354\n",
      "361/388, train_loss: 0.5323, step time: 1.5336\n",
      "362/388, train_loss: 0.3564, step time: 1.5376\n",
      "363/388, train_loss: 0.2509, step time: 1.5295\n",
      "364/388, train_loss: 0.4665, step time: 1.5326\n",
      "365/388, train_loss: 0.0997, step time: 1.5302\n",
      "366/388, train_loss: 0.2279, step time: 1.5347\n",
      "367/388, train_loss: 0.3007, step time: 1.5372\n",
      "368/388, train_loss: 0.1250, step time: 1.5333\n",
      "369/388, train_loss: 0.2237, step time: 1.5333\n",
      "370/388, train_loss: 0.2055, step time: 1.5322\n",
      "371/388, train_loss: 0.2642, step time: 1.5294\n",
      "372/388, train_loss: 0.2897, step time: 1.5327\n",
      "373/388, train_loss: 0.2850, step time: 1.5441\n",
      "374/388, train_loss: 0.2138, step time: 1.5584\n",
      "375/388, train_loss: 0.3719, step time: 1.5319\n",
      "376/388, train_loss: 0.3398, step time: 1.5364\n",
      "377/388, train_loss: 0.2524, step time: 1.5383\n",
      "378/388, train_loss: 0.2218, step time: 1.5324\n",
      "379/388, train_loss: 0.1474, step time: 1.5323\n",
      "380/388, train_loss: 0.2742, step time: 1.5310\n",
      "381/388, train_loss: 0.3090, step time: 1.5284\n",
      "382/388, train_loss: 0.1081, step time: 1.5332\n",
      "383/388, train_loss: 0.2048, step time: 1.5364\n",
      "384/388, train_loss: 0.1470, step time: 1.5355\n",
      "385/388, train_loss: 0.1085, step time: 1.5344\n",
      "386/388, train_loss: 0.1951, step time: 1.5434\n",
      "387/388, train_loss: 0.2298, step time: 1.5362\n",
      "388/388, train_loss: 0.1883, step time: 1.5419\n",
      "epoch 14 average loss: 0.2490\n",
      "saved new best metric model\n",
      "current epoch: 14 current mean dice: 0.7222 tc: 0.7710 wt: 0.8813 et: 0.5144\n",
      "best mean dice: 0.7222 at epoch: 14\n",
      "time consuming of epoch 14 is: 702.7413\n",
      "----------\n",
      "epoch 15/100\n",
      "1/388, train_loss: 0.1902, step time: 1.5583\n",
      "2/388, train_loss: 0.0827, step time: 1.5367\n",
      "3/388, train_loss: 0.1363, step time: 1.5344\n",
      "4/388, train_loss: 0.2873, step time: 1.5364\n",
      "5/388, train_loss: 0.4711, step time: 1.5381\n",
      "6/388, train_loss: 0.5917, step time: 1.5309\n",
      "7/388, train_loss: 0.1305, step time: 1.5306\n",
      "8/388, train_loss: 0.1682, step time: 1.5424\n",
      "9/388, train_loss: 0.2586, step time: 1.5312\n",
      "10/388, train_loss: 0.1193, step time: 1.5352\n",
      "11/388, train_loss: 0.2460, step time: 1.5336\n",
      "12/388, train_loss: 0.3323, step time: 1.5348\n",
      "13/388, train_loss: 0.2083, step time: 1.5343\n",
      "14/388, train_loss: 0.2909, step time: 1.5330\n",
      "15/388, train_loss: 0.1623, step time: 1.5360\n",
      "16/388, train_loss: 0.4592, step time: 1.5329\n",
      "17/388, train_loss: 0.4326, step time: 1.5348\n",
      "18/388, train_loss: 0.1295, step time: 1.5373\n",
      "19/388, train_loss: 0.3808, step time: 1.5342\n",
      "20/388, train_loss: 0.1353, step time: 1.5352\n",
      "21/388, train_loss: 0.1993, step time: 1.5318\n",
      "22/388, train_loss: 0.1612, step time: 1.5315\n",
      "23/388, train_loss: 0.1937, step time: 1.5326\n",
      "24/388, train_loss: 0.4081, step time: 1.5340\n",
      "25/388, train_loss: 0.2457, step time: 1.5351\n",
      "26/388, train_loss: 0.3147, step time: 1.5356\n",
      "27/388, train_loss: 0.4059, step time: 1.5330\n",
      "28/388, train_loss: 0.1301, step time: 1.5326\n",
      "29/388, train_loss: 0.1708, step time: 1.5293\n",
      "30/388, train_loss: 0.3329, step time: 1.5312\n",
      "31/388, train_loss: 0.1778, step time: 1.5436\n",
      "32/388, train_loss: 0.3493, step time: 1.5381\n",
      "33/388, train_loss: 0.1890, step time: 1.5317\n",
      "34/388, train_loss: 0.2239, step time: 1.5334\n",
      "35/388, train_loss: 0.6952, step time: 1.5313\n",
      "36/388, train_loss: 0.1682, step time: 1.5307\n",
      "37/388, train_loss: 0.1102, step time: 1.5359\n",
      "38/388, train_loss: 0.3764, step time: 1.5419\n",
      "39/388, train_loss: 0.3889, step time: 1.5462\n",
      "40/388, train_loss: 0.1761, step time: 1.5340\n",
      "41/388, train_loss: 0.1614, step time: 1.5356\n",
      "42/388, train_loss: 0.1241, step time: 1.5616\n",
      "43/388, train_loss: 0.3915, step time: 1.5336\n",
      "44/388, train_loss: 0.1376, step time: 1.5347\n",
      "45/388, train_loss: 0.3279, step time: 1.5327\n",
      "46/388, train_loss: 0.4145, step time: 1.5298\n",
      "47/388, train_loss: 0.1430, step time: 1.5329\n",
      "48/388, train_loss: 0.3880, step time: 1.5329\n",
      "49/388, train_loss: 0.1751, step time: 1.5353\n",
      "50/388, train_loss: 0.3817, step time: 1.5357\n",
      "51/388, train_loss: 0.1560, step time: 1.5368\n",
      "52/388, train_loss: 0.0991, step time: 1.5346\n",
      "53/388, train_loss: 0.2675, step time: 1.5316\n",
      "54/388, train_loss: 0.4464, step time: 1.5319\n",
      "55/388, train_loss: 0.1897, step time: 1.5331\n",
      "56/388, train_loss: 0.1378, step time: 1.5313\n",
      "57/388, train_loss: 0.4138, step time: 1.5361\n",
      "58/388, train_loss: 0.4568, step time: 1.5357\n",
      "59/388, train_loss: 0.3021, step time: 1.5353\n",
      "60/388, train_loss: 0.3306, step time: 1.5349\n",
      "61/388, train_loss: 0.0940, step time: 1.5580\n",
      "62/388, train_loss: 0.1672, step time: 1.5370\n",
      "63/388, train_loss: 0.2436, step time: 1.5357\n",
      "64/388, train_loss: 0.2272, step time: 1.5337\n",
      "65/388, train_loss: 0.3179, step time: 1.5348\n",
      "66/388, train_loss: 0.4986, step time: 1.5317\n",
      "67/388, train_loss: 0.2788, step time: 1.5351\n",
      "68/388, train_loss: 0.4258, step time: 1.5361\n",
      "69/388, train_loss: 0.1749, step time: 1.5383\n",
      "70/388, train_loss: 0.1882, step time: 1.5365\n",
      "71/388, train_loss: 0.2134, step time: 1.5321\n",
      "72/388, train_loss: 0.1903, step time: 1.5353\n",
      "73/388, train_loss: 0.2558, step time: 1.5304\n",
      "74/388, train_loss: 0.3371, step time: 1.5415\n",
      "75/388, train_loss: 0.2055, step time: 1.5361\n",
      "76/388, train_loss: 0.2160, step time: 1.5320\n",
      "77/388, train_loss: 0.6986, step time: 1.5331\n",
      "78/388, train_loss: 0.1144, step time: 1.5335\n",
      "79/388, train_loss: 0.2066, step time: 1.5333\n",
      "80/388, train_loss: 0.1220, step time: 1.5381\n",
      "81/388, train_loss: 0.2265, step time: 1.5356\n",
      "82/388, train_loss: 0.7265, step time: 1.5335\n",
      "83/388, train_loss: 0.1304, step time: 1.5334\n",
      "84/388, train_loss: 0.2082, step time: 1.5364\n",
      "85/388, train_loss: 0.1404, step time: 1.5371\n",
      "86/388, train_loss: 0.1283, step time: 1.5354\n",
      "87/388, train_loss: 0.2152, step time: 1.5346\n",
      "88/388, train_loss: 0.4184, step time: 1.5371\n",
      "89/388, train_loss: 0.2133, step time: 1.5302\n",
      "90/388, train_loss: 0.1220, step time: 1.5298\n",
      "91/388, train_loss: 0.2392, step time: 1.5305\n",
      "92/388, train_loss: 0.1577, step time: 1.5352\n",
      "93/388, train_loss: 0.2154, step time: 1.5561\n",
      "94/388, train_loss: 0.1418, step time: 1.5334\n",
      "95/388, train_loss: 0.3791, step time: 1.5319\n",
      "96/388, train_loss: 0.1925, step time: 1.5312\n",
      "97/388, train_loss: 0.0820, step time: 1.5377\n",
      "98/388, train_loss: 0.1571, step time: 1.5363\n",
      "99/388, train_loss: 0.2084, step time: 1.5326\n",
      "100/388, train_loss: 0.1294, step time: 1.5340\n",
      "101/388, train_loss: 0.1367, step time: 1.5337\n",
      "102/388, train_loss: 0.1343, step time: 1.5340\n",
      "103/388, train_loss: 0.2471, step time: 1.5498\n",
      "104/388, train_loss: 0.1408, step time: 1.5432\n",
      "105/388, train_loss: 0.2584, step time: 1.5320\n",
      "106/388, train_loss: 0.1483, step time: 1.5343\n",
      "107/388, train_loss: 0.1658, step time: 1.5343\n",
      "108/388, train_loss: 0.2962, step time: 1.5363\n",
      "109/388, train_loss: 0.1894, step time: 1.5350\n",
      "110/388, train_loss: 0.2120, step time: 1.5356\n",
      "111/388, train_loss: 0.1732, step time: 1.5323\n",
      "112/388, train_loss: 0.4737, step time: 1.5323\n",
      "113/388, train_loss: 0.5169, step time: 1.5466\n",
      "114/388, train_loss: 0.0947, step time: 1.5364\n",
      "115/388, train_loss: 0.1279, step time: 1.5291\n",
      "116/388, train_loss: 0.1040, step time: 1.5307\n",
      "117/388, train_loss: 0.1365, step time: 1.5306\n",
      "118/388, train_loss: 0.3194, step time: 1.5312\n",
      "119/388, train_loss: 0.1741, step time: 1.5400\n",
      "120/388, train_loss: 0.6146, step time: 1.5370\n",
      "121/388, train_loss: 0.1008, step time: 1.5331\n",
      "122/388, train_loss: 0.1302, step time: 1.5338\n",
      "123/388, train_loss: 0.1159, step time: 1.5303\n",
      "124/388, train_loss: 0.2005, step time: 1.5312\n",
      "125/388, train_loss: 0.3579, step time: 1.5301\n",
      "126/388, train_loss: 0.1689, step time: 1.5339\n",
      "127/388, train_loss: 0.1351, step time: 1.5394\n",
      "128/388, train_loss: 0.1424, step time: 1.5371\n",
      "129/388, train_loss: 0.1497, step time: 1.5354\n",
      "130/388, train_loss: 0.7024, step time: 1.5362\n",
      "131/388, train_loss: 0.1631, step time: 1.5331\n",
      "132/388, train_loss: 0.1241, step time: 1.5334\n",
      "133/388, train_loss: 0.1115, step time: 1.5398\n",
      "134/388, train_loss: 0.3638, step time: 1.5389\n",
      "135/388, train_loss: 0.1297, step time: 1.5380\n",
      "136/388, train_loss: 0.0447, step time: 1.5332\n",
      "137/388, train_loss: 0.2367, step time: 1.5330\n",
      "138/388, train_loss: 0.2422, step time: 1.5320\n",
      "139/388, train_loss: 0.5476, step time: 1.5356\n",
      "140/388, train_loss: 0.1489, step time: 1.5375\n",
      "141/388, train_loss: 0.1184, step time: 1.5325\n",
      "142/388, train_loss: 0.2328, step time: 1.5336\n",
      "143/388, train_loss: 0.2229, step time: 1.5316\n",
      "144/388, train_loss: 0.2529, step time: 1.5330\n",
      "145/388, train_loss: 0.2517, step time: 1.5315\n",
      "146/388, train_loss: 0.1754, step time: 1.5310\n",
      "147/388, train_loss: 0.2533, step time: 1.5356\n",
      "148/388, train_loss: 0.2374, step time: 1.5341\n",
      "149/388, train_loss: 0.2400, step time: 1.5337\n",
      "150/388, train_loss: 0.2590, step time: 1.5328\n",
      "151/388, train_loss: 0.1106, step time: 1.5311\n",
      "152/388, train_loss: 0.1883, step time: 1.5304\n",
      "153/388, train_loss: 0.2365, step time: 1.5331\n",
      "154/388, train_loss: 0.4637, step time: 1.5371\n",
      "155/388, train_loss: 0.0627, step time: 1.5351\n",
      "156/388, train_loss: 0.3384, step time: 1.5347\n",
      "157/388, train_loss: 0.2154, step time: 1.5347\n",
      "158/388, train_loss: 0.3425, step time: 1.5300\n",
      "159/388, train_loss: 0.2496, step time: 1.5325\n",
      "160/388, train_loss: 0.6374, step time: 1.5351\n",
      "161/388, train_loss: 0.1188, step time: 1.5358\n",
      "162/388, train_loss: 0.1648, step time: 1.5365\n",
      "163/388, train_loss: 0.1389, step time: 1.5567\n",
      "164/388, train_loss: 0.3151, step time: 1.5355\n",
      "165/388, train_loss: 0.2566, step time: 1.5346\n",
      "166/388, train_loss: 0.4171, step time: 1.5304\n",
      "167/388, train_loss: 0.3292, step time: 1.5328\n",
      "168/388, train_loss: 0.1111, step time: 1.5291\n",
      "169/388, train_loss: 0.2347, step time: 1.5346\n",
      "170/388, train_loss: 0.2160, step time: 1.5351\n",
      "171/388, train_loss: 0.1363, step time: 1.5353\n",
      "172/388, train_loss: 0.0904, step time: 1.5309\n",
      "173/388, train_loss: 0.1394, step time: 1.5313\n",
      "174/388, train_loss: 0.1907, step time: 1.5368\n",
      "175/388, train_loss: 0.1624, step time: 1.5385\n",
      "176/388, train_loss: 0.2750, step time: 1.5320\n",
      "177/388, train_loss: 0.4117, step time: 1.5320\n",
      "178/388, train_loss: 0.1829, step time: 1.5413\n",
      "179/388, train_loss: 0.2046, step time: 1.5332\n",
      "180/388, train_loss: 0.4713, step time: 1.5382\n",
      "181/388, train_loss: 0.4384, step time: 1.5321\n",
      "182/388, train_loss: 0.1613, step time: 1.5337\n",
      "183/388, train_loss: 0.2710, step time: 1.5403\n",
      "184/388, train_loss: 0.2357, step time: 1.5328\n",
      "185/388, train_loss: 0.2035, step time: 1.5342\n",
      "186/388, train_loss: 0.3100, step time: 1.5331\n",
      "187/388, train_loss: 0.5921, step time: 1.5358\n",
      "188/388, train_loss: 0.2625, step time: 1.5334\n",
      "189/388, train_loss: 0.5398, step time: 1.5335\n",
      "190/388, train_loss: 0.1027, step time: 1.5299\n",
      "191/388, train_loss: 0.5033, step time: 1.5337\n",
      "192/388, train_loss: 0.2264, step time: 1.5317\n",
      "193/388, train_loss: 0.2081, step time: 1.5451\n",
      "194/388, train_loss: 0.1500, step time: 1.5347\n",
      "195/388, train_loss: 0.1514, step time: 1.5332\n",
      "196/388, train_loss: 0.3008, step time: 1.5307\n",
      "197/388, train_loss: 0.0895, step time: 1.5326\n",
      "198/388, train_loss: 0.3424, step time: 1.5383\n",
      "199/388, train_loss: 0.3500, step time: 1.5362\n",
      "200/388, train_loss: 0.1520, step time: 1.5462\n",
      "201/388, train_loss: 0.1948, step time: 1.5339\n",
      "202/388, train_loss: 0.5291, step time: 1.5337\n",
      "203/388, train_loss: 0.2822, step time: 1.5339\n",
      "204/388, train_loss: 0.1803, step time: 1.5345\n",
      "205/388, train_loss: 0.2495, step time: 1.5341\n",
      "206/388, train_loss: 0.1203, step time: 1.5365\n",
      "207/388, train_loss: 0.3476, step time: 1.5293\n",
      "208/388, train_loss: 0.2836, step time: 1.5302\n",
      "209/388, train_loss: 0.2142, step time: 1.5308\n",
      "210/388, train_loss: 0.1268, step time: 1.5333\n",
      "211/388, train_loss: 0.3850, step time: 1.5390\n",
      "212/388, train_loss: 0.1646, step time: 1.5346\n",
      "213/388, train_loss: 0.1723, step time: 1.5357\n",
      "214/388, train_loss: 0.2792, step time: 1.5522\n",
      "215/388, train_loss: 0.2067, step time: 1.5445\n",
      "216/388, train_loss: 0.2066, step time: 1.5382\n",
      "217/388, train_loss: 0.2146, step time: 1.5362\n",
      "218/388, train_loss: 0.1408, step time: 1.5365\n",
      "219/388, train_loss: 0.1069, step time: 1.5361\n",
      "220/388, train_loss: 0.1385, step time: 1.5332\n",
      "221/388, train_loss: 0.1665, step time: 1.5323\n",
      "222/388, train_loss: 0.1137, step time: 1.5372\n",
      "223/388, train_loss: 0.1486, step time: 1.5354\n",
      "224/388, train_loss: 0.0822, step time: 1.5360\n",
      "225/388, train_loss: 0.2013, step time: 1.5310\n",
      "226/388, train_loss: 0.3223, step time: 1.5317\n",
      "227/388, train_loss: 0.3582, step time: 1.5315\n",
      "228/388, train_loss: 0.2166, step time: 1.5314\n",
      "229/388, train_loss: 0.2888, step time: 1.5672\n",
      "230/388, train_loss: 0.1246, step time: 1.5301\n",
      "231/388, train_loss: 0.1446, step time: 1.5293\n",
      "232/388, train_loss: 0.1073, step time: 1.5356\n",
      "233/388, train_loss: 0.2130, step time: 1.5361\n",
      "234/388, train_loss: 0.1912, step time: 1.5370\n",
      "235/388, train_loss: 0.1872, step time: 1.5348\n",
      "236/388, train_loss: 0.2891, step time: 1.5339\n",
      "237/388, train_loss: 0.1427, step time: 1.5318\n",
      "238/388, train_loss: 0.4416, step time: 1.5435\n",
      "239/388, train_loss: 0.3422, step time: 1.5348\n",
      "240/388, train_loss: 0.4618, step time: 1.5305\n",
      "241/388, train_loss: 0.2516, step time: 1.5332\n",
      "242/388, train_loss: 0.4251, step time: 1.5357\n",
      "243/388, train_loss: 0.1498, step time: 1.5394\n",
      "244/388, train_loss: 0.2689, step time: 1.5354\n",
      "245/388, train_loss: 0.3300, step time: 1.5333\n",
      "246/388, train_loss: 0.2779, step time: 1.5305\n",
      "247/388, train_loss: 0.1204, step time: 1.5335\n",
      "248/388, train_loss: 0.2315, step time: 1.5333\n",
      "249/388, train_loss: 0.1576, step time: 1.5358\n",
      "250/388, train_loss: 0.1966, step time: 1.5571\n",
      "251/388, train_loss: 0.0818, step time: 1.5338\n",
      "252/388, train_loss: 0.1806, step time: 1.5353\n",
      "253/388, train_loss: 0.1599, step time: 1.5353\n",
      "254/388, train_loss: 0.2023, step time: 1.5354\n",
      "255/388, train_loss: 0.3081, step time: 1.5314\n",
      "256/388, train_loss: 0.0455, step time: 1.5311\n",
      "257/388, train_loss: 0.2216, step time: 1.5276\n",
      "258/388, train_loss: 0.3582, step time: 1.5395\n",
      "259/388, train_loss: 0.5014, step time: 1.5432\n",
      "260/388, train_loss: 0.5521, step time: 1.5349\n",
      "261/388, train_loss: 0.2782, step time: 1.5336\n",
      "262/388, train_loss: 0.1943, step time: 1.5321\n",
      "263/388, train_loss: 0.3917, step time: 1.5342\n",
      "264/388, train_loss: 0.0623, step time: 1.5336\n",
      "265/388, train_loss: 0.1404, step time: 1.5299\n",
      "266/388, train_loss: 0.1016, step time: 1.5317\n",
      "267/388, train_loss: 0.1332, step time: 1.5340\n",
      "268/388, train_loss: 0.2048, step time: 1.5308\n",
      "269/388, train_loss: 0.1907, step time: 1.5338\n",
      "270/388, train_loss: 0.1316, step time: 1.5416\n",
      "271/388, train_loss: 0.1115, step time: 1.5305\n",
      "272/388, train_loss: 0.3954, step time: 1.5350\n",
      "273/388, train_loss: 0.2289, step time: 1.5317\n",
      "274/388, train_loss: 0.1498, step time: 1.5363\n",
      "275/388, train_loss: 0.4577, step time: 1.5332\n",
      "276/388, train_loss: 0.1545, step time: 1.5328\n",
      "277/388, train_loss: 0.3699, step time: 1.5300\n",
      "278/388, train_loss: 0.2679, step time: 1.5336\n",
      "279/388, train_loss: 0.2799, step time: 1.5340\n",
      "280/388, train_loss: 0.1734, step time: 1.5363\n",
      "281/388, train_loss: 0.5170, step time: 1.5340\n",
      "282/388, train_loss: 0.3027, step time: 1.5316\n",
      "283/388, train_loss: 0.2066, step time: 1.5436\n",
      "284/388, train_loss: 0.2905, step time: 1.5379\n",
      "285/388, train_loss: 0.2837, step time: 1.5356\n",
      "286/388, train_loss: 0.2198, step time: 1.5351\n",
      "287/388, train_loss: 0.1159, step time: 1.5308\n",
      "288/388, train_loss: 0.3028, step time: 1.5338\n",
      "289/388, train_loss: 0.1619, step time: 1.5285\n",
      "290/388, train_loss: 0.0820, step time: 1.5288\n",
      "291/388, train_loss: 0.0755, step time: 1.5341\n",
      "292/388, train_loss: 0.1305, step time: 1.5317\n",
      "293/388, train_loss: 0.6693, step time: 1.5365\n",
      "294/388, train_loss: 0.3387, step time: 1.5358\n",
      "295/388, train_loss: 0.2094, step time: 1.5513\n",
      "296/388, train_loss: 0.1507, step time: 1.5327\n",
      "297/388, train_loss: 0.2936, step time: 1.5356\n",
      "298/388, train_loss: 0.3219, step time: 1.5366\n",
      "299/388, train_loss: 0.2682, step time: 1.5352\n",
      "300/388, train_loss: 0.1503, step time: 1.5338\n",
      "301/388, train_loss: 0.0743, step time: 1.5326\n",
      "302/388, train_loss: 0.3485, step time: 1.5324\n",
      "303/388, train_loss: 0.3001, step time: 1.5408\n",
      "304/388, train_loss: 0.3039, step time: 1.5350\n",
      "305/388, train_loss: 0.2399, step time: 1.5347\n",
      "306/388, train_loss: 0.1816, step time: 1.5351\n",
      "307/388, train_loss: 0.1667, step time: 1.5326\n",
      "308/388, train_loss: 0.2326, step time: 1.5336\n",
      "309/388, train_loss: 0.2193, step time: 1.5328\n",
      "310/388, train_loss: 0.2844, step time: 1.5380\n",
      "311/388, train_loss: 0.2656, step time: 1.5403\n",
      "312/388, train_loss: 0.2870, step time: 1.5343\n",
      "313/388, train_loss: 0.1154, step time: 1.5336\n",
      "314/388, train_loss: 0.5583, step time: 1.5308\n",
      "315/388, train_loss: 0.2475, step time: 1.5282\n",
      "316/388, train_loss: 0.1955, step time: 1.5298\n",
      "317/388, train_loss: 0.4186, step time: 1.5346\n",
      "318/388, train_loss: 0.1286, step time: 1.5593\n",
      "319/388, train_loss: 0.0929, step time: 1.5327\n",
      "320/388, train_loss: 0.2232, step time: 1.5315\n",
      "321/388, train_loss: 0.2701, step time: 1.5332\n",
      "322/388, train_loss: 0.4477, step time: 1.5319\n",
      "323/388, train_loss: 0.3768, step time: 1.5360\n",
      "324/388, train_loss: 0.4801, step time: 1.5355\n",
      "325/388, train_loss: 0.2349, step time: 1.5354\n",
      "326/388, train_loss: 0.1611, step time: 1.5346\n",
      "327/388, train_loss: 0.2407, step time: 1.5348\n",
      "328/388, train_loss: 0.3515, step time: 1.5318\n",
      "329/388, train_loss: 0.4407, step time: 1.5307\n",
      "330/388, train_loss: 0.1448, step time: 1.5335\n",
      "331/388, train_loss: 0.1174, step time: 1.5313\n",
      "332/388, train_loss: 0.1981, step time: 1.5376\n",
      "333/388, train_loss: 0.1963, step time: 1.5472\n",
      "334/388, train_loss: 0.5202, step time: 1.5329\n",
      "335/388, train_loss: 0.1859, step time: 1.5308\n",
      "336/388, train_loss: 0.1294, step time: 1.5343\n",
      "337/388, train_loss: 0.2382, step time: 1.5353\n",
      "338/388, train_loss: 0.1508, step time: 1.5347\n",
      "339/388, train_loss: 0.1695, step time: 1.5371\n",
      "340/388, train_loss: 0.0589, step time: 1.5361\n",
      "341/388, train_loss: 0.2722, step time: 1.5306\n",
      "342/388, train_loss: 0.6838, step time: 1.5311\n",
      "343/388, train_loss: 0.2581, step time: 1.5315\n",
      "344/388, train_loss: 0.1634, step time: 1.5321\n",
      "345/388, train_loss: 0.4493, step time: 1.5337\n",
      "346/388, train_loss: 0.3376, step time: 1.5385\n",
      "347/388, train_loss: 0.4809, step time: 1.5369\n",
      "348/388, train_loss: 0.1110, step time: 1.5337\n",
      "349/388, train_loss: 0.1994, step time: 1.5304\n",
      "350/388, train_loss: 0.1202, step time: 1.5340\n",
      "351/388, train_loss: 0.3711, step time: 1.5291\n",
      "352/388, train_loss: 0.3693, step time: 1.5315\n",
      "353/388, train_loss: 0.1225, step time: 1.5306\n",
      "354/388, train_loss: 0.0978, step time: 1.5341\n",
      "355/388, train_loss: 0.2548, step time: 1.5358\n",
      "356/388, train_loss: 0.1337, step time: 1.5348\n",
      "357/388, train_loss: 0.3315, step time: 1.5330\n",
      "358/388, train_loss: 0.0673, step time: 1.5313\n",
      "359/388, train_loss: 0.2351, step time: 1.5310\n",
      "360/388, train_loss: 0.2807, step time: 1.5360\n",
      "361/388, train_loss: 0.0755, step time: 1.5373\n",
      "362/388, train_loss: 0.2905, step time: 1.5344\n",
      "363/388, train_loss: 0.1346, step time: 1.5358\n",
      "364/388, train_loss: 0.4508, step time: 1.5342\n",
      "365/388, train_loss: 0.6332, step time: 1.5344\n",
      "366/388, train_loss: 0.1372, step time: 1.5385\n",
      "367/388, train_loss: 0.1432, step time: 1.5360\n",
      "368/388, train_loss: 0.3921, step time: 1.5362\n",
      "369/388, train_loss: 0.7430, step time: 1.5327\n",
      "370/388, train_loss: 0.2177, step time: 1.5289\n",
      "371/388, train_loss: 0.2477, step time: 1.5292\n",
      "372/388, train_loss: 0.4574, step time: 1.5358\n",
      "373/388, train_loss: 0.1460, step time: 1.5376\n",
      "374/388, train_loss: 0.1724, step time: 1.5340\n",
      "375/388, train_loss: 0.3808, step time: 1.5327\n",
      "376/388, train_loss: 0.5755, step time: 1.5324\n",
      "377/388, train_loss: 0.1341, step time: 1.5298\n",
      "378/388, train_loss: 0.6401, step time: 1.5329\n",
      "379/388, train_loss: 0.2273, step time: 1.5433\n",
      "380/388, train_loss: 0.0962, step time: 1.5352\n",
      "381/388, train_loss: 0.7540, step time: 1.5353\n",
      "382/388, train_loss: 0.1528, step time: 1.5346\n",
      "383/388, train_loss: 0.1076, step time: 1.5334\n",
      "384/388, train_loss: 0.0899, step time: 1.5379\n",
      "385/388, train_loss: 0.1387, step time: 1.5455\n",
      "386/388, train_loss: 0.2406, step time: 1.5332\n",
      "387/388, train_loss: 0.1099, step time: 1.5313\n",
      "388/388, train_loss: 0.3413, step time: 1.5334\n",
      "epoch 15 average loss: 0.2502\n",
      "current epoch: 15 current mean dice: 0.7204 tc: 0.7690 wt: 0.8770 et: 0.5154\n",
      "best mean dice: 0.7222 at epoch: 14\n",
      "time consuming of epoch 15 is: 702.3243\n",
      "----------\n",
      "epoch 16/100\n",
      "1/388, train_loss: 0.1129, step time: 1.5456\n",
      "2/388, train_loss: 0.2516, step time: 1.5332\n",
      "3/388, train_loss: 0.7077, step time: 1.5366\n",
      "4/388, train_loss: 0.1004, step time: 1.5332\n",
      "5/388, train_loss: 0.5233, step time: 1.5326\n",
      "6/388, train_loss: 0.3772, step time: 1.5351\n",
      "7/388, train_loss: 0.2905, step time: 1.5308\n",
      "8/388, train_loss: 0.1060, step time: 1.5366\n",
      "9/388, train_loss: 0.4011, step time: 1.5303\n",
      "10/388, train_loss: 0.1987, step time: 1.5315\n",
      "11/388, train_loss: 0.3455, step time: 1.5316\n",
      "12/388, train_loss: 0.1579, step time: 1.5308\n",
      "13/388, train_loss: 0.3896, step time: 1.5348\n",
      "14/388, train_loss: 0.3359, step time: 1.5338\n",
      "15/388, train_loss: 0.1908, step time: 1.5348\n",
      "16/388, train_loss: 0.1181, step time: 1.5327\n",
      "17/388, train_loss: 0.2116, step time: 1.5317\n",
      "18/388, train_loss: 0.3874, step time: 1.5321\n",
      "19/388, train_loss: 0.1694, step time: 1.5327\n",
      "20/388, train_loss: 0.4674, step time: 1.5338\n",
      "21/388, train_loss: 0.8997, step time: 1.5362\n",
      "22/388, train_loss: 0.2260, step time: 1.5330\n",
      "23/388, train_loss: 0.1065, step time: 1.5276\n",
      "24/388, train_loss: 0.3317, step time: 1.5328\n",
      "25/388, train_loss: 0.1024, step time: 1.5302\n",
      "26/388, train_loss: 0.4447, step time: 1.5319\n",
      "27/388, train_loss: 0.6151, step time: 1.5332\n",
      "28/388, train_loss: 0.2498, step time: 1.5392\n",
      "29/388, train_loss: 0.1089, step time: 1.5352\n",
      "30/388, train_loss: 0.2484, step time: 1.5350\n",
      "31/388, train_loss: 0.0638, step time: 1.5338\n",
      "32/388, train_loss: 0.3626, step time: 1.5327\n",
      "33/388, train_loss: 0.1844, step time: 1.5337\n",
      "34/388, train_loss: 0.1139, step time: 1.5356\n",
      "35/388, train_loss: 0.1086, step time: 1.5469\n",
      "36/388, train_loss: 0.1356, step time: 1.5323\n",
      "37/388, train_loss: 0.1890, step time: 1.5330\n",
      "38/388, train_loss: 0.1471, step time: 1.5327\n",
      "39/388, train_loss: 0.4436, step time: 1.5304\n",
      "40/388, train_loss: 0.1059, step time: 1.5354\n",
      "41/388, train_loss: 0.1300, step time: 1.5358\n",
      "42/388, train_loss: 0.1924, step time: 1.5335\n",
      "43/388, train_loss: 0.2042, step time: 1.5350\n",
      "44/388, train_loss: 0.2117, step time: 1.5351\n",
      "45/388, train_loss: 0.3696, step time: 1.5320\n",
      "46/388, train_loss: 0.2530, step time: 1.5318\n",
      "47/388, train_loss: 0.4950, step time: 1.5321\n",
      "48/388, train_loss: 0.3347, step time: 1.5354\n",
      "49/388, train_loss: 0.3094, step time: 1.5400\n",
      "50/388, train_loss: 0.2979, step time: 1.5357\n",
      "51/388, train_loss: 0.2371, step time: 1.5330\n",
      "52/388, train_loss: 0.0843, step time: 1.5321\n",
      "53/388, train_loss: 0.3442, step time: 1.5356\n",
      "54/388, train_loss: 0.1146, step time: 1.5351\n",
      "55/388, train_loss: 0.1561, step time: 1.5371\n",
      "56/388, train_loss: 0.1249, step time: 1.5329\n",
      "57/388, train_loss: 0.2775, step time: 1.5285\n",
      "58/388, train_loss: 0.1824, step time: 1.5338\n",
      "59/388, train_loss: 0.1540, step time: 1.5326\n",
      "60/388, train_loss: 0.2557, step time: 1.5356\n",
      "61/388, train_loss: 0.2865, step time: 1.5390\n",
      "62/388, train_loss: 0.0677, step time: 1.5357\n",
      "63/388, train_loss: 0.2427, step time: 1.5351\n",
      "64/388, train_loss: 0.2108, step time: 1.5321\n",
      "65/388, train_loss: 0.2217, step time: 1.5323\n",
      "66/388, train_loss: 0.2352, step time: 1.5340\n",
      "67/388, train_loss: 0.4488, step time: 1.5371\n",
      "68/388, train_loss: 0.5082, step time: 1.5369\n",
      "69/388, train_loss: 0.3497, step time: 1.5341\n",
      "70/388, train_loss: 0.2223, step time: 1.5346\n",
      "71/388, train_loss: 0.2539, step time: 1.5332\n",
      "72/388, train_loss: 0.5701, step time: 1.5347\n",
      "73/388, train_loss: 0.3638, step time: 1.5367\n",
      "74/388, train_loss: 0.1434, step time: 1.5357\n",
      "75/388, train_loss: 0.2581, step time: 1.5335\n",
      "76/388, train_loss: 0.2509, step time: 1.5380\n",
      "77/388, train_loss: 0.3515, step time: 1.5304\n",
      "78/388, train_loss: 0.1344, step time: 1.5297\n",
      "79/388, train_loss: 0.1723, step time: 1.5394\n",
      "80/388, train_loss: 0.4188, step time: 1.5386\n",
      "81/388, train_loss: 0.1080, step time: 1.5353\n",
      "82/388, train_loss: 0.2213, step time: 1.5324\n",
      "83/388, train_loss: 0.2122, step time: 1.5346\n",
      "84/388, train_loss: 0.1702, step time: 1.5319\n",
      "85/388, train_loss: 0.2279, step time: 1.5328\n",
      "86/388, train_loss: 0.5745, step time: 1.5343\n",
      "87/388, train_loss: 0.2362, step time: 1.5381\n",
      "88/388, train_loss: 0.3490, step time: 1.5351\n",
      "89/388, train_loss: 0.0953, step time: 1.5314\n",
      "90/388, train_loss: 0.1922, step time: 1.5302\n",
      "91/388, train_loss: 0.2607, step time: 1.5355\n",
      "92/388, train_loss: 0.5591, step time: 1.5414\n",
      "93/388, train_loss: 0.2246, step time: 1.5363\n",
      "94/388, train_loss: 0.1085, step time: 1.5343\n",
      "95/388, train_loss: 0.2704, step time: 1.5337\n",
      "96/388, train_loss: 0.1342, step time: 1.5313\n",
      "97/388, train_loss: 0.1814, step time: 1.5320\n",
      "98/388, train_loss: 0.1521, step time: 1.5360\n",
      "99/388, train_loss: 0.3068, step time: 1.5366\n",
      "100/388, train_loss: 0.3762, step time: 1.5349\n",
      "101/388, train_loss: 0.4398, step time: 1.5369\n",
      "102/388, train_loss: 0.2562, step time: 1.5340\n",
      "103/388, train_loss: 0.2131, step time: 1.5297\n",
      "104/388, train_loss: 0.3093, step time: 1.5335\n",
      "105/388, train_loss: 0.2022, step time: 1.5323\n",
      "106/388, train_loss: 0.0713, step time: 1.5345\n",
      "107/388, train_loss: 0.1562, step time: 1.5367\n",
      "108/388, train_loss: 0.1020, step time: 1.5342\n",
      "109/388, train_loss: 0.6028, step time: 1.5315\n",
      "110/388, train_loss: 0.0819, step time: 1.5298\n",
      "111/388, train_loss: 0.1656, step time: 1.5331\n",
      "112/388, train_loss: 0.2779, step time: 1.5369\n",
      "113/388, train_loss: 0.1137, step time: 1.5543\n",
      "114/388, train_loss: 0.3362, step time: 1.5311\n",
      "115/388, train_loss: 0.1019, step time: 1.5351\n",
      "116/388, train_loss: 0.2153, step time: 1.5337\n",
      "117/388, train_loss: 0.1264, step time: 1.5349\n",
      "118/388, train_loss: 0.2099, step time: 1.5336\n",
      "119/388, train_loss: 0.1060, step time: 1.5319\n",
      "120/388, train_loss: 0.1866, step time: 1.5327\n",
      "121/388, train_loss: 0.2706, step time: 1.5295\n",
      "122/388, train_loss: 0.1773, step time: 1.5316\n",
      "123/388, train_loss: 0.3551, step time: 1.5347\n",
      "124/388, train_loss: 0.2143, step time: 1.5391\n",
      "125/388, train_loss: 0.1206, step time: 1.5427\n",
      "126/388, train_loss: 0.0850, step time: 1.5362\n",
      "127/388, train_loss: 0.2237, step time: 1.5407\n",
      "128/388, train_loss: 0.3395, step time: 1.5362\n",
      "129/388, train_loss: 0.3130, step time: 1.5337\n",
      "130/388, train_loss: 0.2919, step time: 1.5329\n",
      "131/388, train_loss: 0.2868, step time: 1.5317\n",
      "132/388, train_loss: 0.1187, step time: 1.5336\n",
      "133/388, train_loss: 0.4771, step time: 1.5316\n",
      "134/388, train_loss: 0.2100, step time: 1.5386\n",
      "135/388, train_loss: 0.3618, step time: 1.5326\n",
      "136/388, train_loss: 0.3148, step time: 1.5381\n",
      "137/388, train_loss: 0.3972, step time: 1.5328\n",
      "138/388, train_loss: 0.2720, step time: 1.5346\n",
      "139/388, train_loss: 0.1590, step time: 1.5312\n",
      "140/388, train_loss: 0.1860, step time: 1.5326\n",
      "141/388, train_loss: 0.1521, step time: 1.5472\n",
      "142/388, train_loss: 0.2048, step time: 1.5357\n",
      "143/388, train_loss: 0.1174, step time: 1.5344\n",
      "144/388, train_loss: 0.1615, step time: 1.5310\n",
      "145/388, train_loss: 0.2178, step time: 1.5326\n",
      "146/388, train_loss: 0.2648, step time: 1.5353\n",
      "147/388, train_loss: 0.2580, step time: 1.5342\n",
      "148/388, train_loss: 0.2524, step time: 1.5336\n",
      "149/388, train_loss: 0.1450, step time: 1.5353\n",
      "150/388, train_loss: 0.2066, step time: 1.5315\n",
      "151/388, train_loss: 0.3336, step time: 1.5293\n",
      "152/388, train_loss: 0.2614, step time: 1.5319\n",
      "153/388, train_loss: 0.3133, step time: 1.5347\n",
      "154/388, train_loss: 0.3648, step time: 1.5388\n",
      "155/388, train_loss: 0.1390, step time: 1.5352\n",
      "156/388, train_loss: 0.2607, step time: 1.5405\n",
      "157/388, train_loss: 0.0788, step time: 1.5323\n",
      "158/388, train_loss: 0.2808, step time: 1.5370\n",
      "159/388, train_loss: 0.2243, step time: 1.5424\n",
      "160/388, train_loss: 0.1805, step time: 1.5320\n",
      "161/388, train_loss: 0.1422, step time: 1.5283\n",
      "162/388, train_loss: 0.2945, step time: 1.5316\n",
      "163/388, train_loss: 0.1077, step time: 1.5360\n",
      "164/388, train_loss: 0.2509, step time: 1.5346\n",
      "165/388, train_loss: 0.2054, step time: 1.5352\n",
      "166/388, train_loss: 0.1495, step time: 1.5370\n",
      "167/388, train_loss: 0.0847, step time: 1.5313\n",
      "168/388, train_loss: 0.1772, step time: 1.5317\n",
      "169/388, train_loss: 0.1357, step time: 1.5321\n",
      "170/388, train_loss: 0.1862, step time: 1.5336\n",
      "171/388, train_loss: 0.2279, step time: 1.5385\n",
      "172/388, train_loss: 0.1635, step time: 1.5647\n",
      "173/388, train_loss: 0.0924, step time: 1.5317\n",
      "174/388, train_loss: 0.2444, step time: 1.5303\n",
      "175/388, train_loss: 0.4510, step time: 1.5340\n",
      "176/388, train_loss: 0.1060, step time: 1.5359\n",
      "177/388, train_loss: 0.1648, step time: 1.5334\n",
      "178/388, train_loss: 0.3194, step time: 1.5376\n",
      "179/388, train_loss: 0.4450, step time: 1.5355\n",
      "180/388, train_loss: 0.2128, step time: 1.5335\n",
      "181/388, train_loss: 0.1944, step time: 1.5327\n",
      "182/388, train_loss: 0.0484, step time: 1.5357\n",
      "183/388, train_loss: 0.4203, step time: 1.5378\n",
      "184/388, train_loss: 0.1127, step time: 1.5349\n",
      "185/388, train_loss: 0.0979, step time: 1.5310\n",
      "186/388, train_loss: 0.1299, step time: 1.5356\n",
      "187/388, train_loss: 0.1286, step time: 1.5341\n",
      "188/388, train_loss: 0.2127, step time: 1.5327\n",
      "189/388, train_loss: 0.5664, step time: 1.5336\n",
      "190/388, train_loss: 0.1511, step time: 1.5368\n",
      "191/388, train_loss: 0.4605, step time: 1.5381\n",
      "192/388, train_loss: 0.1904, step time: 1.5308\n",
      "193/388, train_loss: 0.1989, step time: 1.5330\n",
      "194/388, train_loss: 0.1229, step time: 1.5384\n",
      "195/388, train_loss: 0.1276, step time: 1.5345\n",
      "196/388, train_loss: 0.3655, step time: 1.5377\n",
      "197/388, train_loss: 0.1531, step time: 1.5351\n",
      "198/388, train_loss: 0.1085, step time: 1.5308\n",
      "199/388, train_loss: 0.2316, step time: 1.5298\n",
      "200/388, train_loss: 0.2310, step time: 1.5317\n",
      "201/388, train_loss: 0.1145, step time: 1.5326\n",
      "202/388, train_loss: 0.4846, step time: 1.5401\n",
      "203/388, train_loss: 0.2896, step time: 1.5376\n",
      "204/388, train_loss: 0.2122, step time: 1.5336\n",
      "205/388, train_loss: 0.1729, step time: 1.5324\n",
      "206/388, train_loss: 0.0675, step time: 1.5362\n",
      "207/388, train_loss: 0.1398, step time: 1.5312\n",
      "208/388, train_loss: 0.4795, step time: 1.5369\n",
      "209/388, train_loss: 0.0955, step time: 1.5605\n",
      "210/388, train_loss: 0.1419, step time: 1.5361\n",
      "211/388, train_loss: 0.2732, step time: 1.5402\n",
      "212/388, train_loss: 0.3511, step time: 1.5360\n",
      "213/388, train_loss: 0.1557, step time: 1.5339\n",
      "214/388, train_loss: 0.0646, step time: 1.5326\n",
      "215/388, train_loss: 0.2263, step time: 1.5296\n",
      "216/388, train_loss: 0.1429, step time: 1.5549\n",
      "217/388, train_loss: 0.1459, step time: 1.5339\n",
      "218/388, train_loss: 0.1148, step time: 1.5348\n",
      "219/388, train_loss: 0.1816, step time: 1.5423\n",
      "220/388, train_loss: 0.2335, step time: 1.5318\n",
      "221/388, train_loss: 0.1424, step time: 1.5361\n",
      "222/388, train_loss: 0.2268, step time: 1.5350\n",
      "223/388, train_loss: 0.2926, step time: 1.5322\n",
      "224/388, train_loss: 0.2765, step time: 1.5323\n",
      "225/388, train_loss: 0.2300, step time: 1.5366\n",
      "226/388, train_loss: 0.1073, step time: 1.5382\n",
      "227/388, train_loss: 0.2250, step time: 1.5367\n",
      "228/388, train_loss: 0.1296, step time: 1.5329\n",
      "229/388, train_loss: 0.3124, step time: 1.5333\n",
      "230/388, train_loss: 0.1856, step time: 1.5314\n",
      "231/388, train_loss: 0.6501, step time: 1.5403\n",
      "232/388, train_loss: 0.2974, step time: 1.5320\n",
      "233/388, train_loss: 0.2287, step time: 1.5291\n",
      "234/388, train_loss: 0.1651, step time: 1.5333\n",
      "235/388, train_loss: 0.1001, step time: 1.5358\n",
      "236/388, train_loss: 0.1130, step time: 1.5368\n",
      "237/388, train_loss: 0.2702, step time: 1.5349\n",
      "238/388, train_loss: 0.1066, step time: 1.5334\n",
      "239/388, train_loss: 0.3353, step time: 1.5369\n",
      "240/388, train_loss: 0.4398, step time: 1.5359\n",
      "241/388, train_loss: 0.1797, step time: 1.5328\n",
      "242/388, train_loss: 0.0862, step time: 1.5369\n",
      "243/388, train_loss: 0.1455, step time: 1.5353\n",
      "244/388, train_loss: 0.1340, step time: 1.5312\n",
      "245/388, train_loss: 0.2295, step time: 1.5295\n",
      "246/388, train_loss: 0.2220, step time: 1.5306\n",
      "247/388, train_loss: 0.3975, step time: 1.5327\n",
      "248/388, train_loss: 0.1693, step time: 1.5312\n",
      "249/388, train_loss: 0.1667, step time: 1.5354\n",
      "250/388, train_loss: 0.1264, step time: 1.5332\n",
      "251/388, train_loss: 0.4140, step time: 1.5317\n",
      "252/388, train_loss: 0.2452, step time: 1.5348\n",
      "253/388, train_loss: 0.2420, step time: 1.5357\n",
      "254/388, train_loss: 0.1671, step time: 1.5344\n",
      "255/388, train_loss: 0.4954, step time: 1.5367\n",
      "256/388, train_loss: 0.1618, step time: 1.5356\n",
      "257/388, train_loss: 0.2328, step time: 1.5331\n",
      "258/388, train_loss: 0.3089, step time: 1.5340\n",
      "259/388, train_loss: 0.1701, step time: 1.5325\n",
      "260/388, train_loss: 0.1835, step time: 1.5288\n",
      "261/388, train_loss: 0.3540, step time: 1.5297\n",
      "262/388, train_loss: 0.1301, step time: 1.5339\n",
      "263/388, train_loss: 0.2206, step time: 1.5324\n",
      "264/388, train_loss: 0.1730, step time: 1.5376\n",
      "265/388, train_loss: 0.1832, step time: 1.5357\n",
      "266/388, train_loss: 0.1695, step time: 1.5377\n",
      "267/388, train_loss: 0.2452, step time: 1.5321\n",
      "268/388, train_loss: 0.0512, step time: 1.5324\n",
      "269/388, train_loss: 0.2432, step time: 1.5308\n",
      "270/388, train_loss: 0.1706, step time: 1.5344\n",
      "271/388, train_loss: 0.1522, step time: 1.5354\n",
      "272/388, train_loss: 0.3502, step time: 1.5352\n",
      "273/388, train_loss: 0.5643, step time: 1.5361\n",
      "274/388, train_loss: 0.2438, step time: 1.5347\n",
      "275/388, train_loss: 0.2684, step time: 1.5315\n",
      "276/388, train_loss: 0.6940, step time: 1.5367\n",
      "277/388, train_loss: 0.1429, step time: 1.5337\n",
      "278/388, train_loss: 0.3619, step time: 1.5357\n",
      "279/388, train_loss: 0.3390, step time: 1.5303\n",
      "280/388, train_loss: 0.2891, step time: 1.5400\n",
      "281/388, train_loss: 0.0847, step time: 1.5325\n",
      "282/388, train_loss: 0.3809, step time: 1.5366\n",
      "283/388, train_loss: 0.5922, step time: 1.5348\n",
      "284/388, train_loss: 0.4957, step time: 1.5361\n",
      "285/388, train_loss: 0.1207, step time: 1.5352\n",
      "286/388, train_loss: 0.3105, step time: 1.5340\n",
      "287/388, train_loss: 0.2617, step time: 1.5361\n",
      "288/388, train_loss: 0.5750, step time: 1.5348\n",
      "289/388, train_loss: 0.2606, step time: 1.5305\n",
      "290/388, train_loss: 0.3759, step time: 1.5356\n",
      "291/388, train_loss: 0.3475, step time: 1.5312\n",
      "292/388, train_loss: 0.4990, step time: 1.5375\n",
      "293/388, train_loss: 0.1303, step time: 1.5388\n",
      "294/388, train_loss: 0.2143, step time: 1.5355\n",
      "295/388, train_loss: 0.6906, step time: 1.5364\n",
      "296/388, train_loss: 0.1765, step time: 1.5329\n",
      "297/388, train_loss: 0.1990, step time: 1.5337\n",
      "298/388, train_loss: 0.1549, step time: 1.5361\n",
      "299/388, train_loss: 0.3055, step time: 1.5318\n",
      "300/388, train_loss: 0.2199, step time: 1.5321\n",
      "301/388, train_loss: 0.2343, step time: 1.5331\n",
      "302/388, train_loss: 0.0969, step time: 1.5321\n",
      "303/388, train_loss: 0.1208, step time: 1.5403\n",
      "304/388, train_loss: 0.1183, step time: 1.5379\n",
      "305/388, train_loss: 0.2619, step time: 1.5321\n",
      "306/388, train_loss: 0.1539, step time: 1.5350\n",
      "307/388, train_loss: 0.2300, step time: 1.5321\n",
      "308/388, train_loss: 0.1472, step time: 1.5351\n",
      "309/388, train_loss: 0.2995, step time: 1.5404\n",
      "310/388, train_loss: 0.4384, step time: 1.5351\n",
      "311/388, train_loss: 0.1897, step time: 1.5384\n",
      "312/388, train_loss: 0.1804, step time: 1.5412\n",
      "313/388, train_loss: 0.3710, step time: 1.5322\n",
      "314/388, train_loss: 0.1724, step time: 1.5342\n",
      "315/388, train_loss: 0.1120, step time: 1.5324\n",
      "316/388, train_loss: 0.2404, step time: 1.5374\n",
      "317/388, train_loss: 0.0900, step time: 1.5360\n",
      "318/388, train_loss: 0.0809, step time: 1.5376\n",
      "319/388, train_loss: 0.1728, step time: 1.5357\n",
      "320/388, train_loss: 0.2822, step time: 1.5308\n",
      "321/388, train_loss: 0.4198, step time: 1.5329\n",
      "322/388, train_loss: 0.1312, step time: 1.5327\n",
      "323/388, train_loss: 0.0822, step time: 1.5349\n",
      "324/388, train_loss: 0.1816, step time: 1.5451\n",
      "325/388, train_loss: 0.3679, step time: 1.5298\n",
      "326/388, train_loss: 0.2980, step time: 1.5282\n",
      "327/388, train_loss: 0.1326, step time: 1.5345\n",
      "328/388, train_loss: 0.1123, step time: 1.5365\n",
      "329/388, train_loss: 0.1125, step time: 1.5341\n",
      "330/388, train_loss: 0.1747, step time: 1.5382\n",
      "331/388, train_loss: 0.2767, step time: 1.5601\n",
      "332/388, train_loss: 0.1607, step time: 1.5391\n",
      "333/388, train_loss: 0.2250, step time: 1.5343\n",
      "334/388, train_loss: 0.2039, step time: 1.5362\n",
      "335/388, train_loss: 0.1228, step time: 1.5336\n",
      "336/388, train_loss: 0.1224, step time: 1.5375\n",
      "337/388, train_loss: 0.1136, step time: 1.5345\n",
      "338/388, train_loss: 0.1011, step time: 1.5362\n",
      "339/388, train_loss: 0.1677, step time: 1.5381\n",
      "340/388, train_loss: 0.0948, step time: 1.5356\n",
      "341/388, train_loss: 0.1534, step time: 1.5288\n",
      "342/388, train_loss: 0.0813, step time: 1.5320\n",
      "343/388, train_loss: 0.5598, step time: 1.5300\n",
      "344/388, train_loss: 0.2773, step time: 1.5356\n",
      "345/388, train_loss: 0.1946, step time: 1.5389\n",
      "346/388, train_loss: 0.1694, step time: 1.5366\n",
      "347/388, train_loss: 0.2038, step time: 1.5336\n",
      "348/388, train_loss: 0.0903, step time: 1.5324\n",
      "349/388, train_loss: 0.3066, step time: 1.5319\n",
      "350/388, train_loss: 0.1795, step time: 1.5445\n",
      "351/388, train_loss: 0.1526, step time: 1.5353\n",
      "352/388, train_loss: 0.2064, step time: 1.5298\n",
      "353/388, train_loss: 0.2568, step time: 1.5342\n",
      "354/388, train_loss: 0.2331, step time: 1.5352\n",
      "355/388, train_loss: 0.1264, step time: 1.5355\n",
      "356/388, train_loss: 0.5458, step time: 1.5318\n",
      "357/388, train_loss: 0.3192, step time: 1.5344\n",
      "358/388, train_loss: 0.1467, step time: 1.5327\n",
      "359/388, train_loss: 0.0973, step time: 1.5337\n",
      "360/388, train_loss: 0.4266, step time: 1.5334\n",
      "361/388, train_loss: 0.0748, step time: 1.5360\n",
      "362/388, train_loss: 0.1359, step time: 1.5355\n",
      "363/388, train_loss: 0.3606, step time: 1.5348\n",
      "364/388, train_loss: 0.1255, step time: 1.5337\n",
      "365/388, train_loss: 0.3246, step time: 1.5314\n",
      "366/388, train_loss: 0.1051, step time: 1.5427\n",
      "367/388, train_loss: 0.2520, step time: 1.5339\n",
      "368/388, train_loss: 0.2574, step time: 1.5369\n",
      "369/388, train_loss: 0.1624, step time: 1.5346\n",
      "370/388, train_loss: 0.3698, step time: 1.5334\n",
      "371/388, train_loss: 0.1351, step time: 1.5314\n",
      "372/388, train_loss: 0.2833, step time: 1.5363\n",
      "373/388, train_loss: 0.2984, step time: 1.5355\n",
      "374/388, train_loss: 0.2186, step time: 1.5364\n",
      "375/388, train_loss: 0.3387, step time: 1.5347\n",
      "376/388, train_loss: 0.1429, step time: 1.5329\n",
      "377/388, train_loss: 0.4368, step time: 1.5383\n",
      "378/388, train_loss: 0.7028, step time: 1.5381\n",
      "379/388, train_loss: 0.3041, step time: 1.5443\n",
      "380/388, train_loss: 0.3247, step time: 1.5354\n",
      "381/388, train_loss: 0.1710, step time: 1.5330\n",
      "382/388, train_loss: 0.1552, step time: 1.5354\n",
      "383/388, train_loss: 0.3028, step time: 1.5408\n",
      "384/388, train_loss: 0.4761, step time: 1.5348\n",
      "385/388, train_loss: 0.1867, step time: 1.5361\n",
      "386/388, train_loss: 0.2644, step time: 1.5340\n",
      "387/388, train_loss: 0.3263, step time: 1.5301\n",
      "388/388, train_loss: 0.3064, step time: 1.5328\n",
      "epoch 16 average loss: 0.2415\n",
      "current epoch: 16 current mean dice: 0.6838 tc: 0.7315 wt: 0.8579 et: 0.4619\n",
      "best mean dice: 0.7222 at epoch: 14\n",
      "time consuming of epoch 16 is: 702.6135\n",
      "----------\n",
      "epoch 17/100\n",
      "1/388, train_loss: 0.2416, step time: 1.5531\n",
      "2/388, train_loss: 0.2801, step time: 1.5339\n",
      "3/388, train_loss: 0.2229, step time: 1.5326\n",
      "4/388, train_loss: 0.1189, step time: 1.5380\n",
      "5/388, train_loss: 0.3602, step time: 1.5329\n",
      "6/388, train_loss: 0.3438, step time: 1.5351\n",
      "7/388, train_loss: 0.2827, step time: 1.5320\n",
      "8/388, train_loss: 0.1421, step time: 1.5360\n",
      "9/388, train_loss: 0.1442, step time: 1.5375\n",
      "10/388, train_loss: 0.2363, step time: 1.5394\n",
      "11/388, train_loss: 0.4019, step time: 1.5388\n",
      "12/388, train_loss: 0.0570, step time: 1.5356\n",
      "13/388, train_loss: 0.1547, step time: 1.5349\n",
      "14/388, train_loss: 0.1070, step time: 1.5373\n",
      "15/388, train_loss: 0.3559, step time: 1.5407\n",
      "16/388, train_loss: 0.1876, step time: 1.5385\n",
      "17/388, train_loss: 0.2514, step time: 1.5373\n",
      "18/388, train_loss: 0.2538, step time: 1.5355\n",
      "19/388, train_loss: 0.1901, step time: 1.5349\n",
      "20/388, train_loss: 0.1579, step time: 1.5378\n",
      "21/388, train_loss: 0.2876, step time: 1.5364\n",
      "22/388, train_loss: 0.3420, step time: 1.5413\n",
      "23/388, train_loss: 0.3250, step time: 1.5343\n",
      "24/388, train_loss: 0.1227, step time: 1.5341\n",
      "25/388, train_loss: 0.2338, step time: 1.5376\n",
      "26/388, train_loss: 0.2653, step time: 1.5353\n",
      "27/388, train_loss: 0.6044, step time: 1.5330\n",
      "28/388, train_loss: 0.2857, step time: 1.5444\n",
      "29/388, train_loss: 0.2648, step time: 1.5347\n",
      "30/388, train_loss: 0.1797, step time: 1.5376\n",
      "31/388, train_loss: 0.2796, step time: 1.5371\n",
      "32/388, train_loss: 0.5176, step time: 1.5321\n",
      "33/388, train_loss: 0.2537, step time: 1.5310\n",
      "34/388, train_loss: 0.2939, step time: 1.5319\n",
      "35/388, train_loss: 0.1122, step time: 1.5347\n",
      "36/388, train_loss: 0.3163, step time: 1.5447\n",
      "37/388, train_loss: 0.0768, step time: 1.5333\n",
      "38/388, train_loss: 0.3666, step time: 1.5320\n",
      "39/388, train_loss: 0.2915, step time: 1.5351\n",
      "40/388, train_loss: 0.0900, step time: 1.5343\n",
      "41/388, train_loss: 0.2204, step time: 1.5408\n",
      "42/388, train_loss: 0.3745, step time: 1.5375\n",
      "43/388, train_loss: 0.1841, step time: 1.5453\n",
      "44/388, train_loss: 0.2673, step time: 1.5354\n",
      "45/388, train_loss: 0.0900, step time: 1.5384\n",
      "46/388, train_loss: 0.1160, step time: 1.5347\n",
      "47/388, train_loss: 0.1466, step time: 1.5381\n",
      "48/388, train_loss: 0.2427, step time: 1.5326\n",
      "49/388, train_loss: 0.1886, step time: 1.5312\n",
      "50/388, train_loss: 0.1672, step time: 1.5361\n",
      "51/388, train_loss: 0.2082, step time: 1.5363\n",
      "52/388, train_loss: 0.1287, step time: 1.5374\n",
      "53/388, train_loss: 0.6035, step time: 1.5340\n",
      "54/388, train_loss: 0.1510, step time: 1.5453\n",
      "55/388, train_loss: 0.4412, step time: 1.5331\n",
      "56/388, train_loss: 0.1973, step time: 1.5385\n",
      "57/388, train_loss: 0.1101, step time: 1.5365\n",
      "58/388, train_loss: 0.5559, step time: 1.5373\n",
      "59/388, train_loss: 0.1760, step time: 1.5306\n",
      "60/388, train_loss: 0.1160, step time: 1.5325\n",
      "61/388, train_loss: 0.3172, step time: 1.5402\n",
      "62/388, train_loss: 0.1978, step time: 1.5374\n",
      "63/388, train_loss: 0.2253, step time: 1.5396\n",
      "64/388, train_loss: 0.3401, step time: 1.5337\n",
      "65/388, train_loss: 0.2059, step time: 1.5359\n",
      "66/388, train_loss: 0.0781, step time: 1.5389\n",
      "67/388, train_loss: 0.2818, step time: 1.5361\n",
      "68/388, train_loss: 0.1692, step time: 1.5342\n",
      "69/388, train_loss: 0.1599, step time: 1.5320\n",
      "70/388, train_loss: 0.1115, step time: 1.5336\n",
      "71/388, train_loss: 0.1718, step time: 1.5341\n",
      "72/388, train_loss: 0.1562, step time: 1.5402\n",
      "73/388, train_loss: 0.2218, step time: 1.5345\n",
      "74/388, train_loss: 0.1207, step time: 1.5343\n",
      "75/388, train_loss: 0.1271, step time: 1.5332\n",
      "76/388, train_loss: 0.1672, step time: 1.5352\n",
      "77/388, train_loss: 0.2700, step time: 1.5369\n",
      "78/388, train_loss: 0.2573, step time: 1.5353\n",
      "79/388, train_loss: 0.3441, step time: 1.5328\n",
      "80/388, train_loss: 0.2172, step time: 1.5350\n",
      "81/388, train_loss: 0.4032, step time: 1.5328\n",
      "82/388, train_loss: 0.4683, step time: 1.5387\n",
      "83/388, train_loss: 0.3759, step time: 1.5394\n",
      "84/388, train_loss: 0.1107, step time: 1.5334\n",
      "85/388, train_loss: 0.1481, step time: 1.5301\n",
      "86/388, train_loss: 0.1618, step time: 1.5331\n",
      "87/388, train_loss: 0.2260, step time: 1.5313\n",
      "88/388, train_loss: 0.2221, step time: 1.5358\n",
      "89/388, train_loss: 0.2239, step time: 1.5353\n",
      "90/388, train_loss: 0.1335, step time: 1.5316\n",
      "91/388, train_loss: 0.1539, step time: 1.5332\n",
      "92/388, train_loss: 0.3340, step time: 1.5352\n",
      "93/388, train_loss: 0.1827, step time: 1.5371\n",
      "94/388, train_loss: 0.2902, step time: 1.5319\n",
      "95/388, train_loss: 0.2927, step time: 1.5343\n",
      "96/388, train_loss: 0.1451, step time: 1.5312\n",
      "97/388, train_loss: 0.1100, step time: 1.5339\n",
      "98/388, train_loss: 0.2659, step time: 1.5372\n",
      "99/388, train_loss: 0.0785, step time: 1.5352\n",
      "100/388, train_loss: 0.1141, step time: 1.5358\n",
      "101/388, train_loss: 0.2063, step time: 1.5309\n",
      "102/388, train_loss: 0.2623, step time: 1.5332\n",
      "103/388, train_loss: 0.1371, step time: 1.5317\n",
      "104/388, train_loss: 0.5580, step time: 1.5591\n",
      "105/388, train_loss: 0.2539, step time: 1.5386\n",
      "106/388, train_loss: 0.1891, step time: 1.5332\n",
      "107/388, train_loss: 0.1438, step time: 1.5317\n",
      "108/388, train_loss: 0.1870, step time: 1.5358\n",
      "109/388, train_loss: 0.0817, step time: 1.5328\n",
      "110/388, train_loss: 0.2602, step time: 1.5347\n",
      "111/388, train_loss: 0.1048, step time: 1.5340\n",
      "112/388, train_loss: 0.2403, step time: 1.5325\n",
      "113/388, train_loss: 0.3464, step time: 1.5317\n",
      "114/388, train_loss: 0.1766, step time: 1.5370\n",
      "115/388, train_loss: 0.1988, step time: 1.5326\n",
      "116/388, train_loss: 0.2697, step time: 1.5342\n",
      "117/388, train_loss: 0.2742, step time: 1.5334\n",
      "118/388, train_loss: 0.2785, step time: 1.5324\n",
      "119/388, train_loss: 0.0891, step time: 1.5308\n",
      "120/388, train_loss: 0.2461, step time: 1.5440\n",
      "121/388, train_loss: 0.3019, step time: 1.5333\n",
      "122/388, train_loss: 0.1657, step time: 1.5359\n",
      "123/388, train_loss: 0.2409, step time: 1.5343\n",
      "124/388, train_loss: 0.3563, step time: 1.5344\n",
      "125/388, train_loss: 0.1893, step time: 1.5302\n",
      "126/388, train_loss: 0.1539, step time: 1.5326\n",
      "127/388, train_loss: 0.1400, step time: 1.5321\n",
      "128/388, train_loss: 0.2017, step time: 1.5344\n",
      "129/388, train_loss: 0.0405, step time: 1.5356\n",
      "130/388, train_loss: 0.2336, step time: 1.5331\n",
      "131/388, train_loss: 0.2141, step time: 1.5311\n",
      "132/388, train_loss: 0.3065, step time: 1.5310\n",
      "133/388, train_loss: 0.2251, step time: 1.5317\n",
      "134/388, train_loss: 0.2148, step time: 1.5342\n",
      "135/388, train_loss: 0.2925, step time: 1.5371\n",
      "136/388, train_loss: 0.5516, step time: 1.5368\n",
      "137/388, train_loss: 0.4834, step time: 1.5334\n",
      "138/388, train_loss: 0.1394, step time: 1.5334\n",
      "139/388, train_loss: 0.1362, step time: 1.5322\n",
      "140/388, train_loss: 0.1522, step time: 1.5356\n",
      "141/388, train_loss: 0.4032, step time: 1.5301\n",
      "142/388, train_loss: 0.1058, step time: 1.5326\n",
      "143/388, train_loss: 0.2359, step time: 1.5318\n",
      "144/388, train_loss: 0.0909, step time: 1.5327\n",
      "145/388, train_loss: 0.1233, step time: 1.5359\n",
      "146/388, train_loss: 0.1117, step time: 1.5325\n",
      "147/388, train_loss: 0.2092, step time: 1.5341\n",
      "148/388, train_loss: 0.1046, step time: 1.5371\n",
      "149/388, train_loss: 0.1452, step time: 1.5323\n",
      "150/388, train_loss: 0.1601, step time: 1.5351\n",
      "151/388, train_loss: 0.0452, step time: 1.5331\n",
      "152/388, train_loss: 0.1581, step time: 1.5317\n",
      "153/388, train_loss: 0.1800, step time: 1.5408\n",
      "154/388, train_loss: 0.2450, step time: 1.5358\n",
      "155/388, train_loss: 0.3260, step time: 1.5346\n",
      "156/388, train_loss: 0.1634, step time: 1.5345\n",
      "157/388, train_loss: 0.1712, step time: 1.5352\n",
      "158/388, train_loss: 0.3630, step time: 1.5328\n",
      "159/388, train_loss: 0.3047, step time: 1.5343\n",
      "160/388, train_loss: 0.2083, step time: 1.5375\n",
      "161/388, train_loss: 0.4252, step time: 1.5311\n",
      "162/388, train_loss: 0.0972, step time: 1.5292\n",
      "163/388, train_loss: 0.1813, step time: 1.5323\n",
      "164/388, train_loss: 0.2429, step time: 1.5340\n",
      "165/388, train_loss: 0.3095, step time: 1.5327\n",
      "166/388, train_loss: 0.2620, step time: 1.5372\n",
      "167/388, train_loss: 0.2330, step time: 1.5360\n",
      "168/388, train_loss: 0.2961, step time: 1.5336\n",
      "169/388, train_loss: 0.1401, step time: 1.5327\n",
      "170/388, train_loss: 0.2835, step time: 1.5364\n",
      "171/388, train_loss: 0.4015, step time: 1.5359\n",
      "172/388, train_loss: 0.1083, step time: 1.5315\n",
      "173/388, train_loss: 0.3957, step time: 1.5329\n",
      "174/388, train_loss: 0.1853, step time: 1.5329\n",
      "175/388, train_loss: 0.3107, step time: 1.5369\n",
      "176/388, train_loss: 0.2443, step time: 1.5390\n",
      "177/388, train_loss: 0.1423, step time: 1.5340\n",
      "178/388, train_loss: 0.2959, step time: 1.5320\n",
      "179/388, train_loss: 0.1926, step time: 1.5309\n",
      "180/388, train_loss: 0.2146, step time: 1.5333\n",
      "181/388, train_loss: 0.1346, step time: 1.5358\n",
      "182/388, train_loss: 0.1739, step time: 1.5379\n",
      "183/388, train_loss: 0.1245, step time: 1.5328\n",
      "184/388, train_loss: 0.1478, step time: 1.5336\n",
      "185/388, train_loss: 0.1525, step time: 1.5338\n",
      "186/388, train_loss: 0.2285, step time: 1.5307\n",
      "187/388, train_loss: 0.1960, step time: 1.5388\n",
      "188/388, train_loss: 0.1648, step time: 1.5357\n",
      "189/388, train_loss: 0.5363, step time: 1.5352\n",
      "190/388, train_loss: 0.4196, step time: 1.5341\n",
      "191/388, train_loss: 0.1119, step time: 1.5324\n",
      "192/388, train_loss: 0.2309, step time: 1.5308\n",
      "193/388, train_loss: 0.0934, step time: 1.5558\n",
      "194/388, train_loss: 0.1046, step time: 1.5330\n",
      "195/388, train_loss: 0.1690, step time: 1.5337\n",
      "196/388, train_loss: 0.2768, step time: 1.5333\n",
      "197/388, train_loss: 0.2359, step time: 1.5318\n",
      "198/388, train_loss: 0.1230, step time: 1.5371\n",
      "199/388, train_loss: 0.2823, step time: 1.5334\n",
      "200/388, train_loss: 0.5212, step time: 1.5358\n",
      "201/388, train_loss: 0.1498, step time: 1.5339\n",
      "202/388, train_loss: 0.1186, step time: 1.5308\n",
      "203/388, train_loss: 0.1110, step time: 1.5320\n",
      "204/388, train_loss: 0.1844, step time: 1.5356\n",
      "205/388, train_loss: 0.1005, step time: 1.5372\n",
      "206/388, train_loss: 0.2119, step time: 1.5331\n",
      "207/388, train_loss: 0.3318, step time: 1.5345\n",
      "208/388, train_loss: 0.2303, step time: 1.5342\n",
      "209/388, train_loss: 0.1265, step time: 1.5599\n",
      "210/388, train_loss: 0.1065, step time: 1.5354\n",
      "211/388, train_loss: 0.2694, step time: 1.5356\n",
      "212/388, train_loss: 0.1124, step time: 1.5383\n",
      "213/388, train_loss: 0.1955, step time: 1.5338\n",
      "214/388, train_loss: 0.2912, step time: 1.5364\n",
      "215/388, train_loss: 0.3880, step time: 1.5339\n",
      "216/388, train_loss: 0.1592, step time: 1.5369\n",
      "217/388, train_loss: 0.1432, step time: 1.5330\n",
      "218/388, train_loss: 0.1681, step time: 1.5365\n",
      "219/388, train_loss: 0.1459, step time: 1.5343\n",
      "220/388, train_loss: 0.2545, step time: 1.5323\n",
      "221/388, train_loss: 0.1293, step time: 1.5347\n",
      "222/388, train_loss: 0.1449, step time: 1.5356\n",
      "223/388, train_loss: 0.2454, step time: 1.5333\n",
      "224/388, train_loss: 0.1979, step time: 1.5364\n",
      "225/388, train_loss: 0.0577, step time: 1.5280\n",
      "226/388, train_loss: 0.0794, step time: 1.5403\n",
      "227/388, train_loss: 0.3777, step time: 1.5360\n",
      "228/388, train_loss: 0.1123, step time: 1.5345\n",
      "229/388, train_loss: 0.1430, step time: 1.5373\n",
      "230/388, train_loss: 0.5829, step time: 1.5317\n",
      "231/388, train_loss: 0.3313, step time: 1.5332\n",
      "232/388, train_loss: 0.0676, step time: 1.5379\n",
      "233/388, train_loss: 0.1816, step time: 1.5329\n",
      "234/388, train_loss: 0.2118, step time: 1.5340\n",
      "235/388, train_loss: 0.3614, step time: 1.5310\n",
      "236/388, train_loss: 0.2817, step time: 1.5316\n",
      "237/388, train_loss: 0.3379, step time: 1.5346\n",
      "238/388, train_loss: 0.2057, step time: 1.5364\n",
      "239/388, train_loss: 0.1860, step time: 1.5361\n",
      "240/388, train_loss: 0.0785, step time: 1.5338\n",
      "241/388, train_loss: 0.1507, step time: 1.5297\n",
      "242/388, train_loss: 0.3408, step time: 1.5320\n",
      "243/388, train_loss: 0.1735, step time: 1.5307\n",
      "244/388, train_loss: 0.3051, step time: 1.5317\n",
      "245/388, train_loss: 0.6304, step time: 1.5362\n",
      "246/388, train_loss: 0.1662, step time: 1.5363\n",
      "247/388, train_loss: 0.1568, step time: 1.5361\n",
      "248/388, train_loss: 0.1101, step time: 1.5332\n",
      "249/388, train_loss: 0.4529, step time: 1.5330\n",
      "250/388, train_loss: 0.0903, step time: 1.5283\n",
      "251/388, train_loss: 0.2768, step time: 1.5313\n",
      "252/388, train_loss: 0.2172, step time: 1.5361\n",
      "253/388, train_loss: 0.5051, step time: 1.5364\n",
      "254/388, train_loss: 0.2148, step time: 1.5373\n",
      "255/388, train_loss: 0.1303, step time: 1.5322\n",
      "256/388, train_loss: 0.5379, step time: 1.5322\n",
      "257/388, train_loss: 0.1050, step time: 1.5324\n",
      "258/388, train_loss: 0.2731, step time: 1.5395\n",
      "259/388, train_loss: 0.1268, step time: 1.5353\n",
      "260/388, train_loss: 0.7388, step time: 1.5308\n",
      "261/388, train_loss: 0.1910, step time: 1.5316\n",
      "262/388, train_loss: 0.1860, step time: 1.5316\n",
      "263/388, train_loss: 0.2714, step time: 1.5329\n",
      "264/388, train_loss: 0.0600, step time: 1.5341\n",
      "265/388, train_loss: 0.1205, step time: 1.5372\n",
      "266/388, train_loss: 0.1550, step time: 1.5357\n",
      "267/388, train_loss: 0.3133, step time: 1.5355\n",
      "268/388, train_loss: 0.1184, step time: 1.5302\n",
      "269/388, train_loss: 0.1121, step time: 1.5297\n",
      "270/388, train_loss: 0.3299, step time: 1.5346\n",
      "271/388, train_loss: 0.1959, step time: 1.5363\n",
      "272/388, train_loss: 0.2971, step time: 1.5381\n",
      "273/388, train_loss: 0.2370, step time: 1.5302\n",
      "274/388, train_loss: 0.2042, step time: 1.5345\n",
      "275/388, train_loss: 0.2861, step time: 1.5291\n",
      "276/388, train_loss: 0.5272, step time: 1.5344\n",
      "277/388, train_loss: 0.2771, step time: 1.5408\n",
      "278/388, train_loss: 0.4794, step time: 1.5380\n",
      "279/388, train_loss: 0.1206, step time: 1.5356\n",
      "280/388, train_loss: 0.0805, step time: 1.5301\n",
      "281/388, train_loss: 0.2019, step time: 1.5311\n",
      "282/388, train_loss: 0.1763, step time: 1.5300\n",
      "283/388, train_loss: 0.1698, step time: 1.5361\n",
      "284/388, train_loss: 0.2799, step time: 1.5341\n",
      "285/388, train_loss: 0.1171, step time: 1.5434\n",
      "286/388, train_loss: 0.2717, step time: 1.5319\n",
      "287/388, train_loss: 0.2313, step time: 1.5313\n",
      "288/388, train_loss: 0.0683, step time: 1.5315\n",
      "289/388, train_loss: 0.5491, step time: 1.5348\n",
      "290/388, train_loss: 0.1287, step time: 1.5339\n",
      "291/388, train_loss: 0.3866, step time: 1.5372\n",
      "292/388, train_loss: 0.6464, step time: 1.5358\n",
      "293/388, train_loss: 0.2688, step time: 1.5336\n",
      "294/388, train_loss: 0.4575, step time: 1.5389\n",
      "295/388, train_loss: 0.1543, step time: 1.5392\n",
      "296/388, train_loss: 0.1317, step time: 1.5307\n",
      "297/388, train_loss: 0.5726, step time: 1.5302\n",
      "298/388, train_loss: 0.7158, step time: 1.5319\n",
      "299/388, train_loss: 0.0726, step time: 1.5312\n",
      "300/388, train_loss: 0.1910, step time: 1.5355\n",
      "301/388, train_loss: 0.4607, step time: 1.5341\n",
      "302/388, train_loss: 0.1331, step time: 1.5351\n",
      "303/388, train_loss: 0.2936, step time: 1.5306\n",
      "304/388, train_loss: 0.1901, step time: 1.5322\n",
      "305/388, train_loss: 0.3784, step time: 1.5350\n",
      "306/388, train_loss: 0.0743, step time: 1.5346\n",
      "307/388, train_loss: 0.3487, step time: 1.5347\n",
      "308/388, train_loss: 0.3568, step time: 1.5348\n",
      "309/388, train_loss: 0.2921, step time: 1.5335\n",
      "310/388, train_loss: 0.1611, step time: 1.5342\n",
      "311/388, train_loss: 0.3060, step time: 1.5303\n",
      "312/388, train_loss: 0.2899, step time: 1.5369\n",
      "313/388, train_loss: 0.1342, step time: 1.5289\n",
      "314/388, train_loss: 0.5663, step time: 1.5311\n",
      "315/388, train_loss: 0.1703, step time: 1.5379\n",
      "316/388, train_loss: 0.2594, step time: 1.5376\n",
      "317/388, train_loss: 0.2542, step time: 1.5357\n",
      "318/388, train_loss: 0.3512, step time: 1.5337\n",
      "319/388, train_loss: 0.1620, step time: 1.5333\n",
      "320/388, train_loss: 0.1628, step time: 1.5357\n",
      "321/388, train_loss: 0.1799, step time: 1.5355\n",
      "322/388, train_loss: 0.2483, step time: 1.5311\n",
      "323/388, train_loss: 0.3664, step time: 1.5279\n",
      "324/388, train_loss: 0.0798, step time: 1.5313\n",
      "325/388, train_loss: 0.4824, step time: 1.5337\n",
      "326/388, train_loss: 0.2684, step time: 1.5337\n",
      "327/388, train_loss: 0.0883, step time: 1.5376\n",
      "328/388, train_loss: 0.2984, step time: 1.5378\n",
      "329/388, train_loss: 0.3570, step time: 1.5311\n",
      "330/388, train_loss: 0.1461, step time: 1.5305\n",
      "331/388, train_loss: 0.1346, step time: 1.5320\n",
      "332/388, train_loss: 0.1587, step time: 1.5341\n",
      "333/388, train_loss: 0.1207, step time: 1.5345\n",
      "334/388, train_loss: 0.3302, step time: 1.5342\n",
      "335/388, train_loss: 0.1293, step time: 1.5353\n",
      "336/388, train_loss: 0.3437, step time: 1.5328\n",
      "337/388, train_loss: 0.3553, step time: 1.5326\n",
      "338/388, train_loss: 0.2189, step time: 1.5317\n",
      "339/388, train_loss: 0.2705, step time: 1.5327\n",
      "340/388, train_loss: 0.2203, step time: 1.5316\n",
      "341/388, train_loss: 0.1164, step time: 1.5366\n",
      "342/388, train_loss: 0.0848, step time: 1.5359\n",
      "343/388, train_loss: 0.3145, step time: 1.5348\n",
      "344/388, train_loss: 0.1440, step time: 1.5294\n",
      "345/388, train_loss: 0.2267, step time: 1.5306\n",
      "346/388, train_loss: 0.2470, step time: 1.5328\n",
      "347/388, train_loss: 0.1236, step time: 1.5316\n",
      "348/388, train_loss: 0.5384, step time: 1.5377\n",
      "349/388, train_loss: 0.2234, step time: 1.5540\n",
      "350/388, train_loss: 0.1531, step time: 1.5332\n",
      "351/388, train_loss: 0.1505, step time: 1.5307\n",
      "352/388, train_loss: 0.3155, step time: 1.5347\n",
      "353/388, train_loss: 0.1356, step time: 1.5357\n",
      "354/388, train_loss: 0.7221, step time: 1.5375\n",
      "355/388, train_loss: 0.3183, step time: 1.5348\n",
      "356/388, train_loss: 0.4300, step time: 1.5311\n",
      "357/388, train_loss: 0.2328, step time: 1.5330\n",
      "358/388, train_loss: 0.0876, step time: 1.5375\n",
      "359/388, train_loss: 0.1977, step time: 1.5357\n",
      "360/388, train_loss: 0.0583, step time: 1.5345\n",
      "361/388, train_loss: 0.2030, step time: 1.5327\n",
      "362/388, train_loss: 0.1049, step time: 1.5297\n",
      "363/388, train_loss: 0.2405, step time: 1.5301\n",
      "364/388, train_loss: 0.1855, step time: 1.5339\n",
      "365/388, train_loss: 0.5905, step time: 1.5346\n",
      "366/388, train_loss: 0.2028, step time: 1.5355\n",
      "367/388, train_loss: 0.3264, step time: 1.5318\n",
      "368/388, train_loss: 0.1258, step time: 1.5329\n",
      "369/388, train_loss: 0.4530, step time: 1.5333\n",
      "370/388, train_loss: 0.4253, step time: 1.5338\n",
      "371/388, train_loss: 0.1568, step time: 1.5320\n",
      "372/388, train_loss: 0.1151, step time: 1.5359\n",
      "373/388, train_loss: 0.2759, step time: 1.5426\n",
      "374/388, train_loss: 0.1436, step time: 1.5321\n",
      "375/388, train_loss: 0.3125, step time: 1.5328\n",
      "376/388, train_loss: 0.1476, step time: 1.5324\n",
      "377/388, train_loss: 0.2145, step time: 1.5363\n",
      "378/388, train_loss: 0.3848, step time: 1.5372\n",
      "379/388, train_loss: 0.0879, step time: 1.5341\n",
      "380/388, train_loss: 0.1399, step time: 1.5323\n",
      "381/388, train_loss: 0.1399, step time: 1.5309\n",
      "382/388, train_loss: 0.1257, step time: 1.5305\n",
      "383/388, train_loss: 0.4055, step time: 1.5307\n",
      "384/388, train_loss: 0.3791, step time: 1.5372\n",
      "385/388, train_loss: 0.1798, step time: 1.5374\n",
      "386/388, train_loss: 0.1849, step time: 1.5365\n",
      "387/388, train_loss: 0.1721, step time: 1.5318\n",
      "388/388, train_loss: 0.0898, step time: 1.5342\n",
      "epoch 17 average loss: 0.2356\n",
      "current epoch: 17 current mean dice: 0.7103 tc: 0.7569 wt: 0.8679 et: 0.5061\n",
      "best mean dice: 0.7222 at epoch: 14\n",
      "time consuming of epoch 17 is: 702.3589\n",
      "----------\n",
      "epoch 18/100\n",
      "1/388, train_loss: 0.2461, step time: 1.5539\n",
      "2/388, train_loss: 0.7016, step time: 1.5384\n",
      "3/388, train_loss: 0.2163, step time: 1.5377\n",
      "4/388, train_loss: 0.3152, step time: 1.5358\n",
      "5/388, train_loss: 0.3907, step time: 1.5369\n",
      "6/388, train_loss: 0.5055, step time: 1.5312\n",
      "7/388, train_loss: 0.3503, step time: 1.5344\n",
      "8/388, train_loss: 0.2686, step time: 1.5384\n",
      "9/388, train_loss: 0.2213, step time: 1.5344\n",
      "10/388, train_loss: 0.4099, step time: 1.5358\n",
      "11/388, train_loss: 0.1905, step time: 1.5357\n",
      "12/388, train_loss: 0.0833, step time: 1.5429\n",
      "13/388, train_loss: 0.3827, step time: 1.5339\n",
      "14/388, train_loss: 0.1312, step time: 1.5319\n",
      "15/388, train_loss: 0.1191, step time: 1.5336\n",
      "16/388, train_loss: 0.5132, step time: 1.5358\n",
      "17/388, train_loss: 0.2293, step time: 1.5351\n",
      "18/388, train_loss: 0.2667, step time: 1.5362\n",
      "19/388, train_loss: 0.5654, step time: 1.5343\n",
      "20/388, train_loss: 0.2388, step time: 1.5315\n",
      "21/388, train_loss: 0.1368, step time: 1.5332\n",
      "22/388, train_loss: 0.1861, step time: 1.5357\n",
      "23/388, train_loss: 0.2650, step time: 1.5361\n",
      "24/388, train_loss: 0.2363, step time: 1.5364\n",
      "25/388, train_loss: 0.2228, step time: 1.5324\n",
      "26/388, train_loss: 0.2057, step time: 1.5319\n",
      "27/388, train_loss: 0.1270, step time: 1.5313\n",
      "28/388, train_loss: 0.1974, step time: 1.5367\n",
      "29/388, train_loss: 0.1825, step time: 1.5338\n",
      "30/388, train_loss: 0.1773, step time: 1.5343\n",
      "31/388, train_loss: 0.1024, step time: 1.5359\n",
      "32/388, train_loss: 0.1980, step time: 1.5315\n",
      "33/388, train_loss: 0.0815, step time: 1.5368\n",
      "34/388, train_loss: 0.1375, step time: 1.5311\n",
      "35/388, train_loss: 0.3595, step time: 1.5396\n",
      "36/388, train_loss: 0.1841, step time: 1.5380\n",
      "37/388, train_loss: 0.5377, step time: 1.5306\n",
      "38/388, train_loss: 0.1829, step time: 1.5317\n",
      "39/388, train_loss: 0.2776, step time: 1.5345\n",
      "40/388, train_loss: 0.5892, step time: 1.5386\n",
      "41/388, train_loss: 0.4145, step time: 1.5355\n",
      "42/388, train_loss: 0.1930, step time: 1.5365\n",
      "43/388, train_loss: 0.3082, step time: 1.5364\n",
      "44/388, train_loss: 0.6199, step time: 1.5291\n",
      "45/388, train_loss: 0.1595, step time: 1.5333\n",
      "46/388, train_loss: 0.2149, step time: 1.5294\n",
      "47/388, train_loss: 0.2500, step time: 1.5474\n",
      "48/388, train_loss: 0.2576, step time: 1.5393\n",
      "49/388, train_loss: 0.3255, step time: 1.5352\n",
      "50/388, train_loss: 0.2095, step time: 1.5327\n",
      "51/388, train_loss: 0.0866, step time: 1.5323\n",
      "52/388, train_loss: 0.2557, step time: 1.5330\n",
      "53/388, train_loss: 0.5954, step time: 1.5323\n",
      "54/388, train_loss: 0.3042, step time: 1.5392\n",
      "55/388, train_loss: 0.1566, step time: 1.5356\n",
      "56/388, train_loss: 0.2204, step time: 1.5348\n",
      "57/388, train_loss: 0.3399, step time: 1.5328\n",
      "58/388, train_loss: 0.2846, step time: 1.5338\n",
      "59/388, train_loss: 0.2467, step time: 1.5312\n",
      "60/388, train_loss: 0.1520, step time: 1.5347\n",
      "61/388, train_loss: 0.2547, step time: 1.5363\n",
      "62/388, train_loss: 0.2513, step time: 1.5317\n",
      "63/388, train_loss: 0.1873, step time: 1.5303\n",
      "64/388, train_loss: 0.3279, step time: 1.5301\n",
      "65/388, train_loss: 0.2373, step time: 1.5304\n",
      "66/388, train_loss: 0.1580, step time: 1.5356\n",
      "67/388, train_loss: 0.2823, step time: 1.5352\n",
      "68/388, train_loss: 0.1111, step time: 1.5376\n",
      "69/388, train_loss: 0.1350, step time: 1.5352\n",
      "70/388, train_loss: 0.4581, step time: 1.5497\n",
      "71/388, train_loss: 0.2184, step time: 1.5377\n",
      "72/388, train_loss: 0.2426, step time: 1.5374\n",
      "73/388, train_loss: 0.2733, step time: 1.5317\n",
      "74/388, train_loss: 0.1577, step time: 1.5344\n",
      "75/388, train_loss: 0.3013, step time: 1.5337\n",
      "76/388, train_loss: 0.6198, step time: 1.5338\n",
      "77/388, train_loss: 0.2220, step time: 1.5317\n",
      "78/388, train_loss: 0.3032, step time: 1.5378\n",
      "79/388, train_loss: 0.1391, step time: 1.5355\n",
      "80/388, train_loss: 0.1321, step time: 1.5373\n",
      "81/388, train_loss: 0.4345, step time: 1.5337\n",
      "82/388, train_loss: 0.2597, step time: 1.5336\n",
      "83/388, train_loss: 0.2399, step time: 1.5339\n",
      "84/388, train_loss: 0.4149, step time: 1.5444\n",
      "85/388, train_loss: 0.0947, step time: 1.5428\n",
      "86/388, train_loss: 0.1116, step time: 1.5545\n",
      "87/388, train_loss: 0.0817, step time: 1.5375\n",
      "88/388, train_loss: 0.1027, step time: 1.5458\n",
      "89/388, train_loss: 0.5031, step time: 1.5355\n",
      "90/388, train_loss: 0.5224, step time: 1.5428\n",
      "91/388, train_loss: 0.1539, step time: 1.5336\n",
      "92/388, train_loss: 0.2180, step time: 1.5354\n",
      "93/388, train_loss: 0.2267, step time: 1.5337\n",
      "94/388, train_loss: 0.0454, step time: 1.5329\n",
      "95/388, train_loss: 0.3094, step time: 1.5367\n",
      "96/388, train_loss: 0.5074, step time: 1.5382\n",
      "97/388, train_loss: 0.7312, step time: 1.5330\n",
      "98/388, train_loss: 0.2166, step time: 1.5318\n",
      "99/388, train_loss: 0.0577, step time: 1.5429\n",
      "100/388, train_loss: 0.1674, step time: 1.5327\n",
      "101/388, train_loss: 0.3372, step time: 1.5393\n",
      "102/388, train_loss: 0.1218, step time: 1.5360\n",
      "103/388, train_loss: 0.0978, step time: 1.5361\n",
      "104/388, train_loss: 0.3169, step time: 1.5323\n",
      "105/388, train_loss: 0.1699, step time: 1.5366\n",
      "106/388, train_loss: 0.1718, step time: 1.5337\n",
      "107/388, train_loss: 0.2080, step time: 1.5362\n",
      "108/388, train_loss: 0.1294, step time: 1.5329\n",
      "109/388, train_loss: 0.1271, step time: 1.5299\n",
      "110/388, train_loss: 0.1141, step time: 1.5309\n",
      "111/388, train_loss: 0.0891, step time: 1.5410\n",
      "112/388, train_loss: 0.1542, step time: 1.5353\n",
      "113/388, train_loss: 0.4509, step time: 1.5305\n",
      "114/388, train_loss: 0.2853, step time: 1.5324\n",
      "115/388, train_loss: 0.1390, step time: 1.5513\n",
      "116/388, train_loss: 0.2170, step time: 1.5339\n",
      "117/388, train_loss: 0.0846, step time: 1.5297\n",
      "118/388, train_loss: 0.3692, step time: 1.5357\n",
      "119/388, train_loss: 0.3952, step time: 1.5342\n",
      "120/388, train_loss: 0.1372, step time: 1.5341\n",
      "121/388, train_loss: 0.1346, step time: 1.5344\n",
      "122/388, train_loss: 0.1949, step time: 1.5336\n",
      "123/388, train_loss: 0.1097, step time: 1.5340\n",
      "124/388, train_loss: 0.3234, step time: 1.5444\n",
      "125/388, train_loss: 0.5522, step time: 1.5353\n",
      "126/388, train_loss: 0.7067, step time: 1.5311\n",
      "127/388, train_loss: 0.1748, step time: 1.5312\n",
      "128/388, train_loss: 0.0612, step time: 1.5352\n",
      "129/388, train_loss: 0.0412, step time: 1.5301\n",
      "130/388, train_loss: 0.2555, step time: 1.5418\n",
      "131/388, train_loss: 0.3736, step time: 1.5353\n",
      "132/388, train_loss: 0.1352, step time: 1.5322\n",
      "133/388, train_loss: 0.2289, step time: 1.5369\n",
      "134/388, train_loss: 0.2792, step time: 1.5348\n",
      "135/388, train_loss: 0.1193, step time: 1.5318\n",
      "136/388, train_loss: 0.3318, step time: 1.5482\n",
      "137/388, train_loss: 0.1077, step time: 1.5344\n",
      "138/388, train_loss: 0.3438, step time: 1.5345\n",
      "139/388, train_loss: 0.1131, step time: 1.5313\n",
      "140/388, train_loss: 0.2854, step time: 1.5314\n",
      "141/388, train_loss: 0.2327, step time: 1.5302\n",
      "142/388, train_loss: 0.2354, step time: 1.5431\n",
      "143/388, train_loss: 0.0819, step time: 1.5345\n",
      "144/388, train_loss: 0.3281, step time: 1.5319\n",
      "145/388, train_loss: 0.3514, step time: 1.5331\n",
      "146/388, train_loss: 0.1394, step time: 1.5293\n",
      "147/388, train_loss: 0.1529, step time: 1.5324\n",
      "148/388, train_loss: 0.6420, step time: 1.5357\n",
      "149/388, train_loss: 0.1030, step time: 1.5320\n",
      "150/388, train_loss: 0.2471, step time: 1.5368\n",
      "151/388, train_loss: 0.1444, step time: 1.5374\n",
      "152/388, train_loss: 0.4253, step time: 1.5330\n",
      "153/388, train_loss: 0.6734, step time: 1.5314\n",
      "154/388, train_loss: 0.0708, step time: 1.5333\n",
      "155/388, train_loss: 0.1893, step time: 1.5287\n",
      "156/388, train_loss: 0.1664, step time: 1.5362\n",
      "157/388, train_loss: 0.1952, step time: 1.5527\n",
      "158/388, train_loss: 0.1525, step time: 1.5334\n",
      "159/388, train_loss: 0.2065, step time: 1.5426\n",
      "160/388, train_loss: 0.1709, step time: 1.5353\n",
      "161/388, train_loss: 0.1555, step time: 1.5335\n",
      "162/388, train_loss: 0.1238, step time: 1.5363\n",
      "163/388, train_loss: 0.2232, step time: 1.5353\n",
      "164/388, train_loss: 0.1448, step time: 1.5312\n",
      "165/388, train_loss: 0.1056, step time: 1.5313\n",
      "166/388, train_loss: 0.2474, step time: 1.5346\n",
      "167/388, train_loss: 0.1257, step time: 1.5428\n",
      "168/388, train_loss: 0.0925, step time: 1.5330\n",
      "169/388, train_loss: 0.6663, step time: 1.5336\n",
      "170/388, train_loss: 0.3185, step time: 1.5460\n",
      "171/388, train_loss: 0.1274, step time: 1.5359\n",
      "172/388, train_loss: 0.0991, step time: 1.5442\n",
      "173/388, train_loss: 0.1264, step time: 1.5380\n",
      "174/388, train_loss: 0.1832, step time: 1.5360\n",
      "175/388, train_loss: 0.2572, step time: 1.5319\n",
      "176/388, train_loss: 0.2107, step time: 1.5330\n",
      "177/388, train_loss: 0.1422, step time: 1.5322\n",
      "178/388, train_loss: 0.2462, step time: 1.5488\n",
      "179/388, train_loss: 0.1369, step time: 1.5397\n",
      "180/388, train_loss: 0.2147, step time: 1.5307\n",
      "181/388, train_loss: 0.2096, step time: 1.5331\n",
      "182/388, train_loss: 0.1385, step time: 1.5360\n",
      "183/388, train_loss: 0.1949, step time: 1.5358\n",
      "184/388, train_loss: 0.0620, step time: 1.5372\n",
      "185/388, train_loss: 0.3233, step time: 1.5351\n",
      "186/388, train_loss: 0.1340, step time: 1.5331\n",
      "187/388, train_loss: 0.2516, step time: 1.5366\n",
      "188/388, train_loss: 0.1930, step time: 1.5410\n",
      "189/388, train_loss: 0.5461, step time: 1.5373\n",
      "190/388, train_loss: 0.4005, step time: 1.5377\n",
      "191/388, train_loss: 0.1517, step time: 1.5291\n",
      "192/388, train_loss: 0.1682, step time: 1.5355\n",
      "193/388, train_loss: 0.4364, step time: 1.5297\n",
      "194/388, train_loss: 0.3676, step time: 1.5346\n",
      "195/388, train_loss: 0.2429, step time: 1.5331\n",
      "196/388, train_loss: 0.3852, step time: 1.5360\n",
      "197/388, train_loss: 0.1826, step time: 1.5363\n",
      "198/388, train_loss: 0.0776, step time: 1.5336\n",
      "199/388, train_loss: 0.1267, step time: 1.5420\n",
      "200/388, train_loss: 0.2500, step time: 1.5631\n",
      "201/388, train_loss: 0.3112, step time: 1.5315\n",
      "202/388, train_loss: 0.1034, step time: 1.5315\n",
      "203/388, train_loss: 0.3110, step time: 1.5384\n",
      "204/388, train_loss: 0.1693, step time: 1.5388\n",
      "205/388, train_loss: 0.1718, step time: 1.5340\n",
      "206/388, train_loss: 0.2835, step time: 1.5344\n",
      "207/388, train_loss: 0.1500, step time: 1.5341\n",
      "208/388, train_loss: 0.1361, step time: 1.5372\n",
      "209/388, train_loss: 0.3818, step time: 1.5315\n",
      "210/388, train_loss: 0.1497, step time: 1.5509\n",
      "211/388, train_loss: 0.3031, step time: 1.5352\n",
      "212/388, train_loss: 0.1668, step time: 1.5500\n",
      "213/388, train_loss: 0.3431, step time: 1.5306\n",
      "214/388, train_loss: 0.4133, step time: 1.5323\n",
      "215/388, train_loss: 0.3317, step time: 1.5296\n",
      "216/388, train_loss: 0.1443, step time: 1.5328\n",
      "217/388, train_loss: 0.4071, step time: 1.5359\n",
      "218/388, train_loss: 0.3232, step time: 1.5416\n",
      "219/388, train_loss: 0.1985, step time: 1.5344\n",
      "220/388, train_loss: 0.1205, step time: 1.5334\n",
      "221/388, train_loss: 0.1924, step time: 1.5334\n",
      "222/388, train_loss: 0.2531, step time: 1.5348\n",
      "223/388, train_loss: 0.1538, step time: 1.5343\n",
      "224/388, train_loss: 0.1004, step time: 1.5359\n",
      "225/388, train_loss: 0.1197, step time: 1.5288\n",
      "226/388, train_loss: 0.0804, step time: 1.5329\n",
      "227/388, train_loss: 0.1710, step time: 1.5299\n",
      "228/388, train_loss: 0.1009, step time: 1.5320\n",
      "229/388, train_loss: 0.1139, step time: 1.5328\n",
      "230/388, train_loss: 0.1083, step time: 1.5337\n",
      "231/388, train_loss: 0.0992, step time: 1.5358\n",
      "232/388, train_loss: 0.2129, step time: 1.5311\n",
      "233/388, train_loss: 0.1422, step time: 1.5336\n",
      "234/388, train_loss: 0.4563, step time: 1.5332\n",
      "235/388, train_loss: 0.2802, step time: 1.5410\n",
      "236/388, train_loss: 0.4155, step time: 1.5341\n",
      "237/388, train_loss: 0.3161, step time: 1.5326\n",
      "238/388, train_loss: 0.6147, step time: 1.5332\n",
      "239/388, train_loss: 0.1796, step time: 1.5360\n",
      "240/388, train_loss: 0.0820, step time: 1.5354\n",
      "241/388, train_loss: 0.1183, step time: 1.5353\n",
      "242/388, train_loss: 0.0836, step time: 1.5370\n",
      "243/388, train_loss: 0.2710, step time: 1.5320\n",
      "244/388, train_loss: 0.3112, step time: 1.5312\n",
      "245/388, train_loss: 0.1499, step time: 1.5333\n",
      "246/388, train_loss: 0.2147, step time: 1.5389\n",
      "247/388, train_loss: 0.0878, step time: 1.5382\n",
      "248/388, train_loss: 0.2362, step time: 1.5353\n",
      "249/388, train_loss: 0.1286, step time: 1.5354\n",
      "250/388, train_loss: 0.3026, step time: 1.5340\n",
      "251/388, train_loss: 0.2197, step time: 1.5328\n",
      "252/388, train_loss: 0.3618, step time: 1.5319\n",
      "253/388, train_loss: 0.3197, step time: 1.5395\n",
      "254/388, train_loss: 0.1344, step time: 1.5379\n",
      "255/388, train_loss: 0.1705, step time: 1.5398\n",
      "256/388, train_loss: 0.1518, step time: 1.5315\n",
      "257/388, train_loss: 0.1471, step time: 1.5319\n",
      "258/388, train_loss: 0.4448, step time: 1.5319\n",
      "259/388, train_loss: 0.1562, step time: 1.5327\n",
      "260/388, train_loss: 0.0948, step time: 1.5375\n",
      "261/388, train_loss: 0.1020, step time: 1.5342\n",
      "262/388, train_loss: 0.1079, step time: 1.5348\n",
      "263/388, train_loss: 0.1618, step time: 1.5332\n",
      "264/388, train_loss: 0.1795, step time: 1.5329\n",
      "265/388, train_loss: 0.3096, step time: 1.5304\n",
      "266/388, train_loss: 0.3786, step time: 1.5306\n",
      "267/388, train_loss: 0.1724, step time: 1.5328\n",
      "268/388, train_loss: 0.0941, step time: 1.5337\n",
      "269/388, train_loss: 0.2152, step time: 1.5356\n",
      "270/388, train_loss: 0.2784, step time: 1.5352\n",
      "271/388, train_loss: 0.3451, step time: 1.5330\n",
      "272/388, train_loss: 0.1723, step time: 1.5324\n",
      "273/388, train_loss: 0.1654, step time: 1.5323\n",
      "274/388, train_loss: 0.2062, step time: 1.5370\n",
      "275/388, train_loss: 0.1513, step time: 1.5348\n",
      "276/388, train_loss: 0.2873, step time: 1.5351\n",
      "277/388, train_loss: 0.0729, step time: 1.5336\n",
      "278/388, train_loss: 0.1178, step time: 1.5337\n",
      "279/388, train_loss: 0.3061, step time: 1.5453\n",
      "280/388, train_loss: 0.3895, step time: 1.5386\n",
      "281/388, train_loss: 0.3788, step time: 1.5338\n",
      "282/388, train_loss: 0.1134, step time: 1.5326\n",
      "283/388, train_loss: 0.1483, step time: 1.5306\n",
      "284/388, train_loss: 0.0926, step time: 1.5355\n",
      "285/388, train_loss: 0.2966, step time: 1.5354\n",
      "286/388, train_loss: 0.1604, step time: 1.5322\n",
      "287/388, train_loss: 0.0756, step time: 1.5294\n",
      "288/388, train_loss: 0.1673, step time: 1.5307\n",
      "289/388, train_loss: 0.1939, step time: 1.5303\n",
      "290/388, train_loss: 0.1285, step time: 1.5387\n",
      "291/388, train_loss: 0.1903, step time: 1.5353\n",
      "292/388, train_loss: 0.1125, step time: 1.5371\n",
      "293/388, train_loss: 0.1798, step time: 1.5344\n",
      "294/388, train_loss: 0.1971, step time: 1.5331\n",
      "295/388, train_loss: 0.1911, step time: 1.5322\n",
      "296/388, train_loss: 0.1847, step time: 1.5358\n",
      "297/388, train_loss: 0.1604, step time: 1.5340\n",
      "298/388, train_loss: 0.2073, step time: 1.5334\n",
      "299/388, train_loss: 0.1015, step time: 1.5324\n",
      "300/388, train_loss: 0.1699, step time: 1.5321\n",
      "301/388, train_loss: 0.1541, step time: 1.5300\n",
      "302/388, train_loss: 0.1185, step time: 1.5385\n",
      "303/388, train_loss: 0.2366, step time: 1.5383\n",
      "304/388, train_loss: 0.0645, step time: 1.5342\n",
      "305/388, train_loss: 0.3998, step time: 1.5350\n",
      "306/388, train_loss: 0.1139, step time: 1.5363\n",
      "307/388, train_loss: 0.4176, step time: 1.5708\n",
      "308/388, train_loss: 0.4390, step time: 1.5372\n",
      "309/388, train_loss: 0.2371, step time: 1.5340\n",
      "310/388, train_loss: 0.1074, step time: 1.5317\n",
      "311/388, train_loss: 0.1100, step time: 1.5363\n",
      "312/388, train_loss: 0.1980, step time: 1.5383\n",
      "313/388, train_loss: 0.2352, step time: 1.5376\n",
      "314/388, train_loss: 0.3355, step time: 1.5497\n",
      "315/388, train_loss: 0.5348, step time: 1.5333\n",
      "316/388, train_loss: 0.1300, step time: 1.5305\n",
      "317/388, train_loss: 0.2820, step time: 1.5348\n",
      "318/388, train_loss: 0.1487, step time: 1.5338\n",
      "319/388, train_loss: 0.2120, step time: 1.5316\n",
      "320/388, train_loss: 0.2164, step time: 1.5381\n",
      "321/388, train_loss: 0.3092, step time: 1.5414\n",
      "322/388, train_loss: 0.0890, step time: 1.5322\n",
      "323/388, train_loss: 0.1457, step time: 1.5306\n",
      "324/388, train_loss: 0.1117, step time: 1.5413\n",
      "325/388, train_loss: 0.3393, step time: 1.5327\n",
      "326/388, train_loss: 0.2091, step time: 1.5318\n",
      "327/388, train_loss: 0.2413, step time: 1.5402\n",
      "328/388, train_loss: 0.1619, step time: 1.5368\n",
      "329/388, train_loss: 0.4422, step time: 1.5345\n",
      "330/388, train_loss: 0.2550, step time: 1.5380\n",
      "331/388, train_loss: 0.1147, step time: 1.5315\n",
      "332/388, train_loss: 0.2147, step time: 1.5285\n",
      "333/388, train_loss: 0.2402, step time: 1.5300\n",
      "334/388, train_loss: 0.0818, step time: 1.5321\n",
      "335/388, train_loss: 0.3975, step time: 1.5329\n",
      "336/388, train_loss: 0.2890, step time: 1.5371\n",
      "337/388, train_loss: 0.1324, step time: 1.5317\n",
      "338/388, train_loss: 0.1624, step time: 1.5328\n",
      "339/388, train_loss: 0.2241, step time: 1.5300\n",
      "340/388, train_loss: 0.2216, step time: 1.5321\n",
      "341/388, train_loss: 0.3273, step time: 1.5309\n",
      "342/388, train_loss: 0.1854, step time: 1.5374\n",
      "343/388, train_loss: 0.2064, step time: 1.5354\n",
      "344/388, train_loss: 0.2245, step time: 1.5371\n",
      "345/388, train_loss: 0.0529, step time: 1.5342\n",
      "346/388, train_loss: 0.1996, step time: 1.5341\n",
      "347/388, train_loss: 0.3942, step time: 1.5292\n",
      "348/388, train_loss: 0.3628, step time: 1.5306\n",
      "349/388, train_loss: 0.2112, step time: 1.5328\n",
      "350/388, train_loss: 0.1457, step time: 1.5325\n",
      "351/388, train_loss: 0.3120, step time: 1.5388\n",
      "352/388, train_loss: 0.3467, step time: 1.5365\n",
      "353/388, train_loss: 0.1519, step time: 1.5327\n",
      "354/388, train_loss: 0.2264, step time: 1.5331\n",
      "355/388, train_loss: 0.2188, step time: 1.5352\n",
      "356/388, train_loss: 0.1584, step time: 1.5337\n",
      "357/388, train_loss: 0.3271, step time: 1.5366\n",
      "358/388, train_loss: 0.1151, step time: 1.5323\n",
      "359/388, train_loss: 0.0982, step time: 1.5306\n",
      "360/388, train_loss: 0.2160, step time: 1.5309\n",
      "361/388, train_loss: 0.3038, step time: 1.5326\n",
      "362/388, train_loss: 0.1252, step time: 1.5401\n",
      "363/388, train_loss: 0.3159, step time: 1.5289\n",
      "364/388, train_loss: 0.0923, step time: 1.5303\n",
      "365/388, train_loss: 0.1894, step time: 1.5354\n",
      "366/388, train_loss: 0.1162, step time: 1.5353\n",
      "367/388, train_loss: 0.2257, step time: 1.5355\n",
      "368/388, train_loss: 0.2522, step time: 1.5374\n",
      "369/388, train_loss: 0.3005, step time: 1.5332\n",
      "370/388, train_loss: 0.3237, step time: 1.5305\n",
      "371/388, train_loss: 0.4464, step time: 1.5320\n",
      "372/388, train_loss: 0.1897, step time: 1.5322\n",
      "373/388, train_loss: 0.1890, step time: 1.5362\n",
      "374/388, train_loss: 0.0529, step time: 1.5362\n",
      "375/388, train_loss: 0.1918, step time: 1.5367\n",
      "376/388, train_loss: 0.5324, step time: 1.5351\n",
      "377/388, train_loss: 0.3017, step time: 1.5349\n",
      "378/388, train_loss: 0.2683, step time: 1.5514\n",
      "379/388, train_loss: 0.0984, step time: 1.5390\n",
      "380/388, train_loss: 0.4551, step time: 1.5381\n",
      "381/388, train_loss: 0.2085, step time: 1.5328\n",
      "382/388, train_loss: 0.1082, step time: 1.5316\n",
      "383/388, train_loss: 0.1126, step time: 1.5320\n",
      "384/388, train_loss: 0.3762, step time: 1.5354\n",
      "385/388, train_loss: 0.1500, step time: 1.5318\n",
      "386/388, train_loss: 0.1275, step time: 1.5396\n",
      "387/388, train_loss: 0.2088, step time: 1.5354\n",
      "388/388, train_loss: 0.2592, step time: 1.5345\n",
      "epoch 18 average loss: 0.2335\n",
      "saved new best metric model\n",
      "current epoch: 18 current mean dice: 0.7282 tc: 0.7827 wt: 0.8842 et: 0.5176\n",
      "best mean dice: 0.7282 at epoch: 18\n",
      "time consuming of epoch 18 is: 704.6356\n",
      "----------\n",
      "epoch 19/100\n",
      "1/388, train_loss: 0.5770, step time: 1.5464\n",
      "2/388, train_loss: 0.4473, step time: 1.5320\n",
      "3/388, train_loss: 0.1012, step time: 1.5333\n",
      "4/388, train_loss: 0.2369, step time: 1.5388\n",
      "5/388, train_loss: 0.1272, step time: 1.5297\n",
      "6/388, train_loss: 0.1417, step time: 1.5335\n",
      "7/388, train_loss: 0.1954, step time: 1.5313\n",
      "8/388, train_loss: 0.1893, step time: 1.5347\n",
      "9/388, train_loss: 0.2256, step time: 1.5606\n",
      "10/388, train_loss: 0.2501, step time: 1.5574\n",
      "11/388, train_loss: 0.1163, step time: 1.5339\n",
      "12/388, train_loss: 0.2656, step time: 1.5475\n",
      "13/388, train_loss: 0.1230, step time: 1.5342\n",
      "14/388, train_loss: 0.2135, step time: 1.5323\n",
      "15/388, train_loss: 0.4355, step time: 1.5330\n",
      "16/388, train_loss: 0.2178, step time: 1.5336\n",
      "17/388, train_loss: 0.3352, step time: 1.5318\n",
      "18/388, train_loss: 0.1310, step time: 1.5360\n",
      "19/388, train_loss: 0.0668, step time: 1.5297\n",
      "20/388, train_loss: 0.1191, step time: 1.5275\n",
      "21/388, train_loss: 0.2162, step time: 1.5298\n",
      "22/388, train_loss: 0.1423, step time: 1.5328\n",
      "23/388, train_loss: 0.1230, step time: 1.5358\n",
      "24/388, train_loss: 0.2666, step time: 1.5373\n",
      "25/388, train_loss: 0.1638, step time: 1.5343\n",
      "26/388, train_loss: 0.1092, step time: 1.5317\n",
      "27/388, train_loss: 0.1067, step time: 1.5339\n",
      "28/388, train_loss: 0.0773, step time: 1.5330\n",
      "29/388, train_loss: 0.3000, step time: 1.5348\n",
      "30/388, train_loss: 0.2291, step time: 1.5342\n",
      "31/388, train_loss: 0.1795, step time: 1.5355\n",
      "32/388, train_loss: 0.1185, step time: 1.5325\n",
      "33/388, train_loss: 0.2478, step time: 1.5336\n",
      "34/388, train_loss: 0.3241, step time: 1.5322\n",
      "35/388, train_loss: 0.5506, step time: 1.5347\n",
      "36/388, train_loss: 0.1800, step time: 1.5376\n",
      "37/388, train_loss: 0.2747, step time: 1.5318\n",
      "38/388, train_loss: 0.3020, step time: 1.5315\n",
      "39/388, train_loss: 0.0788, step time: 1.5303\n",
      "40/388, train_loss: 0.1892, step time: 1.5309\n",
      "41/388, train_loss: 0.1189, step time: 1.5307\n",
      "42/388, train_loss: 0.1231, step time: 1.5383\n",
      "43/388, train_loss: 0.1462, step time: 1.5332\n",
      "44/388, train_loss: 0.3532, step time: 1.5336\n",
      "45/388, train_loss: 0.2420, step time: 1.5308\n",
      "46/388, train_loss: 0.3230, step time: 1.5397\n",
      "47/388, train_loss: 0.1408, step time: 1.5308\n",
      "48/388, train_loss: 0.1041, step time: 1.5338\n",
      "49/388, train_loss: 0.1679, step time: 1.5289\n",
      "50/388, train_loss: 0.1940, step time: 1.5347\n",
      "51/388, train_loss: 0.1438, step time: 1.5346\n",
      "52/388, train_loss: 0.0826, step time: 1.5392\n",
      "53/388, train_loss: 0.2559, step time: 1.5309\n",
      "54/388, train_loss: 0.0721, step time: 1.5307\n",
      "55/388, train_loss: 0.0979, step time: 1.5623\n",
      "56/388, train_loss: 0.2058, step time: 1.5372\n",
      "57/388, train_loss: 0.2485, step time: 1.5328\n",
      "58/388, train_loss: 0.1503, step time: 1.5330\n",
      "59/388, train_loss: 0.5369, step time: 1.5353\n",
      "60/388, train_loss: 0.2083, step time: 1.5354\n",
      "61/388, train_loss: 0.1532, step time: 1.5347\n",
      "62/388, train_loss: 0.2040, step time: 1.5356\n",
      "63/388, train_loss: 0.1704, step time: 1.5326\n",
      "64/388, train_loss: 0.5128, step time: 1.5320\n",
      "65/388, train_loss: 0.4531, step time: 1.5310\n",
      "66/388, train_loss: 0.1317, step time: 1.5331\n",
      "67/388, train_loss: 0.1028, step time: 1.5352\n",
      "68/388, train_loss: 0.3522, step time: 1.5352\n",
      "69/388, train_loss: 0.3169, step time: 1.5439\n",
      "70/388, train_loss: 0.0927, step time: 1.5320\n",
      "71/388, train_loss: 0.2106, step time: 1.5314\n",
      "72/388, train_loss: 0.1163, step time: 1.5333\n",
      "73/388, train_loss: 0.1637, step time: 1.5325\n",
      "74/388, train_loss: 0.6113, step time: 1.5340\n",
      "75/388, train_loss: 0.2796, step time: 1.5343\n",
      "76/388, train_loss: 0.2185, step time: 1.5337\n",
      "77/388, train_loss: 0.0754, step time: 1.5333\n",
      "78/388, train_loss: 0.2197, step time: 1.5317\n",
      "79/388, train_loss: 0.2210, step time: 1.5322\n",
      "80/388, train_loss: 0.2068, step time: 1.5347\n",
      "81/388, train_loss: 0.1493, step time: 1.5329\n",
      "82/388, train_loss: 0.3773, step time: 1.5349\n",
      "83/388, train_loss: 0.2692, step time: 1.5326\n",
      "84/388, train_loss: 0.2643, step time: 1.5321\n",
      "85/388, train_loss: 0.6042, step time: 1.5297\n",
      "86/388, train_loss: 0.2459, step time: 1.5308\n",
      "87/388, train_loss: 0.2381, step time: 1.5322\n",
      "88/388, train_loss: 0.1869, step time: 1.5331\n",
      "89/388, train_loss: 0.1716, step time: 1.5328\n",
      "90/388, train_loss: 0.1113, step time: 1.5353\n",
      "91/388, train_loss: 0.2668, step time: 1.5332\n",
      "92/388, train_loss: 0.1338, step time: 1.5341\n",
      "93/388, train_loss: 0.3184, step time: 1.5347\n",
      "94/388, train_loss: 0.3913, step time: 1.5365\n",
      "95/388, train_loss: 0.0961, step time: 1.5335\n",
      "96/388, train_loss: 0.2455, step time: 1.5403\n",
      "97/388, train_loss: 0.0790, step time: 1.5391\n",
      "98/388, train_loss: 0.2971, step time: 1.5342\n",
      "99/388, train_loss: 0.3037, step time: 1.5335\n",
      "100/388, train_loss: 0.0980, step time: 1.5370\n",
      "101/388, train_loss: 0.2164, step time: 1.5346\n",
      "102/388, train_loss: 0.2089, step time: 1.5359\n",
      "103/388, train_loss: 0.4464, step time: 1.5294\n",
      "104/388, train_loss: 0.3425, step time: 1.5320\n",
      "105/388, train_loss: 0.1280, step time: 1.5312\n",
      "106/388, train_loss: 0.1017, step time: 1.5315\n",
      "107/388, train_loss: 0.1008, step time: 1.5362\n",
      "108/388, train_loss: 0.0385, step time: 1.5345\n",
      "109/388, train_loss: 0.0816, step time: 1.5369\n",
      "110/388, train_loss: 0.1643, step time: 1.5308\n",
      "111/388, train_loss: 0.2236, step time: 1.5319\n",
      "112/388, train_loss: 0.3330, step time: 1.5350\n",
      "113/388, train_loss: 0.3319, step time: 1.5348\n",
      "114/388, train_loss: 0.2323, step time: 1.5354\n",
      "115/388, train_loss: 0.3904, step time: 1.5298\n",
      "116/388, train_loss: 0.1353, step time: 1.5322\n",
      "117/388, train_loss: 0.4703, step time: 1.5437\n",
      "118/388, train_loss: 0.1495, step time: 1.5333\n",
      "119/388, train_loss: 0.1475, step time: 1.5365\n",
      "120/388, train_loss: 0.1293, step time: 1.5329\n",
      "121/388, train_loss: 0.3003, step time: 1.5370\n",
      "122/388, train_loss: 0.1289, step time: 1.5355\n",
      "123/388, train_loss: 0.0886, step time: 1.5313\n",
      "124/388, train_loss: 0.3137, step time: 1.5335\n",
      "125/388, train_loss: 0.3029, step time: 1.5299\n",
      "126/388, train_loss: 0.2693, step time: 1.5572\n",
      "127/388, train_loss: 0.1583, step time: 1.5321\n",
      "128/388, train_loss: 0.5006, step time: 1.5294\n",
      "129/388, train_loss: 0.3706, step time: 1.5305\n",
      "130/388, train_loss: 0.3770, step time: 1.5302\n",
      "131/388, train_loss: 0.4289, step time: 1.5299\n",
      "132/388, train_loss: 0.2409, step time: 1.5287\n",
      "133/388, train_loss: 0.1261, step time: 1.5301\n",
      "134/388, train_loss: 0.2404, step time: 1.5342\n",
      "135/388, train_loss: 0.1299, step time: 1.5302\n",
      "136/388, train_loss: 0.2031, step time: 1.5317\n",
      "137/388, train_loss: 0.1123, step time: 1.5373\n",
      "138/388, train_loss: 0.1569, step time: 1.5367\n",
      "139/388, train_loss: 0.3864, step time: 1.5350\n",
      "140/388, train_loss: 0.0564, step time: 1.5318\n",
      "141/388, train_loss: 0.1280, step time: 1.5286\n",
      "142/388, train_loss: 0.1345, step time: 1.5311\n",
      "143/388, train_loss: 0.1479, step time: 1.5320\n",
      "144/388, train_loss: 0.1476, step time: 1.5398\n",
      "145/388, train_loss: 0.2084, step time: 1.5337\n",
      "146/388, train_loss: 0.1603, step time: 1.5341\n",
      "147/388, train_loss: 0.2052, step time: 1.5361\n",
      "148/388, train_loss: 0.2405, step time: 1.5304\n",
      "149/388, train_loss: 0.1797, step time: 1.5338\n",
      "150/388, train_loss: 0.3185, step time: 1.5341\n",
      "151/388, train_loss: 0.1341, step time: 1.5325\n",
      "152/388, train_loss: 0.3786, step time: 1.5355\n",
      "153/388, train_loss: 0.1768, step time: 1.5330\n",
      "154/388, train_loss: 0.1187, step time: 1.5334\n",
      "155/388, train_loss: 0.3315, step time: 1.5309\n",
      "156/388, train_loss: 0.1559, step time: 1.5337\n",
      "157/388, train_loss: 0.1100, step time: 1.5615\n",
      "158/388, train_loss: 0.1849, step time: 1.5324\n",
      "159/388, train_loss: 0.3261, step time: 1.5315\n",
      "160/388, train_loss: 0.3202, step time: 1.5336\n",
      "161/388, train_loss: 0.1042, step time: 1.5465\n",
      "162/388, train_loss: 0.0916, step time: 1.5341\n",
      "163/388, train_loss: 0.0986, step time: 1.5330\n",
      "164/388, train_loss: 0.2293, step time: 1.5318\n",
      "165/388, train_loss: 0.2915, step time: 1.5380\n",
      "166/388, train_loss: 0.1910, step time: 1.5364\n",
      "167/388, train_loss: 0.1071, step time: 1.5351\n",
      "168/388, train_loss: 0.3082, step time: 1.5308\n",
      "169/388, train_loss: 0.5599, step time: 1.5332\n",
      "170/388, train_loss: 0.3081, step time: 1.5360\n",
      "171/388, train_loss: 0.0693, step time: 1.5304\n",
      "172/388, train_loss: 0.0889, step time: 1.5314\n",
      "173/388, train_loss: 0.1547, step time: 1.5315\n",
      "174/388, train_loss: 0.1748, step time: 1.5371\n",
      "175/388, train_loss: 0.1943, step time: 1.5353\n",
      "176/388, train_loss: 0.2607, step time: 1.5373\n",
      "177/388, train_loss: 0.1230, step time: 1.5330\n",
      "178/388, train_loss: 0.1000, step time: 1.5310\n",
      "179/388, train_loss: 0.2583, step time: 1.5305\n",
      "180/388, train_loss: 0.1928, step time: 1.5329\n",
      "181/388, train_loss: 0.2710, step time: 1.5383\n",
      "182/388, train_loss: 0.3648, step time: 1.5420\n",
      "183/388, train_loss: 0.1330, step time: 1.5333\n",
      "184/388, train_loss: 0.2074, step time: 1.5306\n",
      "185/388, train_loss: 0.0710, step time: 1.5324\n",
      "186/388, train_loss: 0.3064, step time: 1.5306\n",
      "187/388, train_loss: 0.2958, step time: 1.5331\n",
      "188/388, train_loss: 0.2526, step time: 1.5366\n",
      "189/388, train_loss: 0.1789, step time: 1.5327\n",
      "190/388, train_loss: 0.2926, step time: 1.5316\n",
      "191/388, train_loss: 0.2073, step time: 1.5301\n",
      "192/388, train_loss: 0.2518, step time: 1.5432\n",
      "193/388, train_loss: 0.3734, step time: 1.5305\n",
      "194/388, train_loss: 0.1132, step time: 1.5346\n",
      "195/388, train_loss: 0.1252, step time: 1.5358\n",
      "196/388, train_loss: 0.2086, step time: 1.5325\n",
      "197/388, train_loss: 0.1505, step time: 1.5363\n",
      "198/388, train_loss: 0.5174, step time: 1.5280\n",
      "199/388, train_loss: 0.1250, step time: 1.5326\n",
      "200/388, train_loss: 0.1117, step time: 1.5329\n",
      "201/388, train_loss: 0.1603, step time: 1.5330\n",
      "202/388, train_loss: 0.2484, step time: 1.5347\n",
      "203/388, train_loss: 0.1803, step time: 1.5327\n",
      "204/388, train_loss: 0.2192, step time: 1.5334\n",
      "205/388, train_loss: 0.3933, step time: 1.5305\n",
      "206/388, train_loss: 0.2370, step time: 1.5348\n",
      "207/388, train_loss: 0.1055, step time: 1.5319\n",
      "208/388, train_loss: 0.3692, step time: 1.5337\n",
      "209/388, train_loss: 0.1770, step time: 1.5352\n",
      "210/388, train_loss: 0.2519, step time: 1.5350\n",
      "211/388, train_loss: 0.2328, step time: 1.5327\n",
      "212/388, train_loss: 0.5307, step time: 1.5339\n",
      "213/388, train_loss: 0.1212, step time: 1.5312\n",
      "214/388, train_loss: 0.1254, step time: 1.5315\n",
      "215/388, train_loss: 0.2767, step time: 1.5361\n",
      "216/388, train_loss: 0.1834, step time: 1.5395\n",
      "217/388, train_loss: 0.1563, step time: 1.5326\n",
      "218/388, train_loss: 0.0814, step time: 1.5314\n",
      "219/388, train_loss: 0.2456, step time: 1.5338\n",
      "220/388, train_loss: 0.2637, step time: 1.5349\n",
      "221/388, train_loss: 0.6584, step time: 1.5366\n",
      "222/388, train_loss: 0.2352, step time: 1.5353\n",
      "223/388, train_loss: 0.2360, step time: 1.5332\n",
      "224/388, train_loss: 0.1419, step time: 1.5318\n",
      "225/388, train_loss: 0.2616, step time: 1.5409\n",
      "226/388, train_loss: 0.1018, step time: 1.5340\n",
      "227/388, train_loss: 0.1328, step time: 1.5375\n",
      "228/388, train_loss: 0.0828, step time: 1.5335\n",
      "229/388, train_loss: 0.0952, step time: 1.5328\n",
      "230/388, train_loss: 0.3388, step time: 1.5322\n",
      "231/388, train_loss: 0.6101, step time: 1.5407\n",
      "232/388, train_loss: 0.3247, step time: 1.5593\n",
      "233/388, train_loss: 0.1407, step time: 1.5297\n",
      "234/388, train_loss: 0.2981, step time: 1.5306\n",
      "235/388, train_loss: 0.1654, step time: 1.5321\n",
      "236/388, train_loss: 0.1852, step time: 1.5321\n",
      "237/388, train_loss: 0.0997, step time: 1.5357\n",
      "238/388, train_loss: 0.0688, step time: 1.5356\n",
      "239/388, train_loss: 0.6779, step time: 1.5344\n",
      "240/388, train_loss: 0.1924, step time: 1.5326\n",
      "241/388, train_loss: 0.2611, step time: 1.5311\n",
      "242/388, train_loss: 0.2657, step time: 1.5296\n",
      "243/388, train_loss: 0.3181, step time: 1.5321\n",
      "244/388, train_loss: 0.2962, step time: 1.5379\n",
      "245/388, train_loss: 0.2723, step time: 1.5354\n",
      "246/388, train_loss: 0.2706, step time: 1.5362\n",
      "247/388, train_loss: 0.1159, step time: 1.5330\n",
      "248/388, train_loss: 0.1952, step time: 1.5347\n",
      "249/388, train_loss: 0.2654, step time: 1.5326\n",
      "250/388, train_loss: 0.1729, step time: 1.5323\n",
      "251/388, train_loss: 0.0676, step time: 1.5350\n",
      "252/388, train_loss: 0.1933, step time: 1.5345\n",
      "253/388, train_loss: 0.4359, step time: 1.5335\n",
      "254/388, train_loss: 0.6580, step time: 1.5323\n",
      "255/388, train_loss: 0.1709, step time: 1.5339\n",
      "256/388, train_loss: 0.1048, step time: 1.5304\n",
      "257/388, train_loss: 0.1831, step time: 1.5334\n",
      "258/388, train_loss: 0.2845, step time: 1.5317\n",
      "259/388, train_loss: 0.3179, step time: 1.5345\n",
      "260/388, train_loss: 0.2076, step time: 1.5369\n",
      "261/388, train_loss: 0.7094, step time: 1.5336\n",
      "262/388, train_loss: 0.4579, step time: 1.5303\n",
      "263/388, train_loss: 0.2407, step time: 1.5341\n",
      "264/388, train_loss: 0.4771, step time: 1.5336\n",
      "265/388, train_loss: 0.2586, step time: 1.5304\n",
      "266/388, train_loss: 0.2898, step time: 1.5342\n",
      "267/388, train_loss: 0.1389, step time: 1.5351\n",
      "268/388, train_loss: 0.0706, step time: 1.5340\n",
      "269/388, train_loss: 0.3155, step time: 1.5364\n",
      "270/388, train_loss: 0.4346, step time: 1.5346\n",
      "271/388, train_loss: 0.1906, step time: 1.5333\n",
      "272/388, train_loss: 0.3305, step time: 1.5335\n",
      "273/388, train_loss: 0.1416, step time: 1.5334\n",
      "274/388, train_loss: 0.1578, step time: 1.5324\n",
      "275/388, train_loss: 0.1594, step time: 1.5369\n",
      "276/388, train_loss: 0.2223, step time: 1.5402\n",
      "277/388, train_loss: 0.1266, step time: 1.5288\n",
      "278/388, train_loss: 0.1207, step time: 1.5309\n",
      "279/388, train_loss: 0.1705, step time: 1.5380\n",
      "280/388, train_loss: 0.4570, step time: 1.5358\n",
      "281/388, train_loss: 0.2159, step time: 1.5383\n",
      "282/388, train_loss: 0.1833, step time: 1.5316\n",
      "283/388, train_loss: 0.1676, step time: 1.5521\n",
      "284/388, train_loss: 0.3899, step time: 1.5384\n",
      "285/388, train_loss: 0.1689, step time: 1.5322\n",
      "286/388, train_loss: 0.1500, step time: 1.5318\n",
      "287/388, train_loss: 0.2066, step time: 1.5285\n",
      "288/388, train_loss: 0.1446, step time: 1.5324\n",
      "289/388, train_loss: 0.1815, step time: 1.5350\n",
      "290/388, train_loss: 0.0818, step time: 1.5369\n",
      "291/388, train_loss: 0.5522, step time: 1.5342\n",
      "292/388, train_loss: 0.1284, step time: 1.5337\n",
      "293/388, train_loss: 0.1740, step time: 1.5320\n",
      "294/388, train_loss: 0.1784, step time: 1.5281\n",
      "295/388, train_loss: 0.4837, step time: 1.5283\n",
      "296/388, train_loss: 0.2403, step time: 1.5413\n",
      "297/388, train_loss: 0.4112, step time: 1.5335\n",
      "298/388, train_loss: 0.1422, step time: 1.5372\n",
      "299/388, train_loss: 0.2056, step time: 1.5312\n",
      "300/388, train_loss: 0.2660, step time: 1.5338\n",
      "301/388, train_loss: 0.1759, step time: 1.5294\n",
      "302/388, train_loss: 0.1577, step time: 1.5331\n",
      "303/388, train_loss: 0.2738, step time: 1.5380\n",
      "304/388, train_loss: 0.2400, step time: 1.5364\n",
      "305/388, train_loss: 0.0965, step time: 1.5311\n",
      "306/388, train_loss: 0.1851, step time: 1.5314\n",
      "307/388, train_loss: 0.1322, step time: 1.5294\n",
      "308/388, train_loss: 0.1235, step time: 1.5411\n",
      "309/388, train_loss: 0.2009, step time: 1.5317\n",
      "310/388, train_loss: 0.1974, step time: 1.5353\n",
      "311/388, train_loss: 0.5505, step time: 1.5565\n",
      "312/388, train_loss: 0.0667, step time: 1.5326\n",
      "313/388, train_loss: 0.1386, step time: 1.5342\n",
      "314/388, train_loss: 0.1199, step time: 1.5429\n",
      "315/388, train_loss: 0.7502, step time: 1.5332\n",
      "316/388, train_loss: 0.1376, step time: 1.5344\n",
      "317/388, train_loss: 0.1199, step time: 1.5358\n",
      "318/388, train_loss: 0.1292, step time: 1.5423\n",
      "319/388, train_loss: 0.1532, step time: 1.5331\n",
      "320/388, train_loss: 0.1370, step time: 1.5345\n",
      "321/388, train_loss: 0.1560, step time: 1.5353\n",
      "322/388, train_loss: 0.3863, step time: 1.5344\n",
      "323/388, train_loss: 0.2836, step time: 1.5355\n",
      "324/388, train_loss: 0.4375, step time: 1.5340\n",
      "325/388, train_loss: 0.0985, step time: 1.5322\n",
      "326/388, train_loss: 0.3555, step time: 1.5338\n",
      "327/388, train_loss: 0.3258, step time: 1.5334\n",
      "328/388, train_loss: 0.3754, step time: 1.5361\n",
      "329/388, train_loss: 0.3094, step time: 1.5382\n",
      "330/388, train_loss: 0.1811, step time: 1.5388\n",
      "331/388, train_loss: 0.0730, step time: 1.5316\n",
      "332/388, train_loss: 0.1830, step time: 1.5308\n",
      "333/388, train_loss: 0.2194, step time: 1.5303\n",
      "334/388, train_loss: 0.2105, step time: 1.5308\n",
      "335/388, train_loss: 0.0725, step time: 1.5386\n",
      "336/388, train_loss: 0.2596, step time: 1.5327\n",
      "337/388, train_loss: 0.3310, step time: 1.5313\n",
      "338/388, train_loss: 0.6060, step time: 1.5352\n",
      "339/388, train_loss: 0.0905, step time: 1.5346\n",
      "340/388, train_loss: 0.2421, step time: 1.5346\n",
      "341/388, train_loss: 0.3667, step time: 1.5342\n",
      "342/388, train_loss: 0.1735, step time: 1.5300\n",
      "343/388, train_loss: 0.0739, step time: 1.5340\n",
      "344/388, train_loss: 0.3290, step time: 1.5336\n",
      "345/388, train_loss: 0.1719, step time: 1.5352\n",
      "346/388, train_loss: 0.4741, step time: 1.5337\n",
      "347/388, train_loss: 0.1397, step time: 1.5344\n",
      "348/388, train_loss: 0.1876, step time: 1.5342\n",
      "349/388, train_loss: 0.0894, step time: 1.5287\n",
      "350/388, train_loss: 0.1178, step time: 1.5337\n",
      "351/388, train_loss: 0.1797, step time: 1.5320\n",
      "352/388, train_loss: 0.3542, step time: 1.5365\n",
      "353/388, train_loss: 0.1092, step time: 1.5348\n",
      "354/388, train_loss: 0.3379, step time: 1.5383\n",
      "355/388, train_loss: 0.3492, step time: 1.5306\n",
      "356/388, train_loss: 0.2186, step time: 1.5319\n",
      "357/388, train_loss: 0.3382, step time: 1.5333\n",
      "358/388, train_loss: 0.1515, step time: 1.5304\n",
      "359/388, train_loss: 0.5873, step time: 1.5366\n",
      "360/388, train_loss: 0.2123, step time: 1.5376\n",
      "361/388, train_loss: 0.3048, step time: 1.5368\n",
      "362/388, train_loss: 0.3779, step time: 1.5290\n",
      "363/388, train_loss: 0.2602, step time: 1.5323\n",
      "364/388, train_loss: 0.2358, step time: 1.5332\n",
      "365/388, train_loss: 0.1620, step time: 1.5341\n",
      "366/388, train_loss: 0.1523, step time: 1.5339\n",
      "367/388, train_loss: 0.3264, step time: 1.5362\n",
      "368/388, train_loss: 0.1224, step time: 1.5341\n",
      "369/388, train_loss: 0.2900, step time: 1.5325\n",
      "370/388, train_loss: 0.0605, step time: 1.5321\n",
      "371/388, train_loss: 0.3563, step time: 1.5377\n",
      "372/388, train_loss: 0.1207, step time: 1.5356\n",
      "373/388, train_loss: 0.3604, step time: 1.5379\n",
      "374/388, train_loss: 0.1477, step time: 1.5312\n",
      "375/388, train_loss: 0.1419, step time: 1.5323\n",
      "376/388, train_loss: 0.2091, step time: 1.5319\n",
      "377/388, train_loss: 0.1167, step time: 1.5307\n",
      "378/388, train_loss: 0.0591, step time: 1.5345\n",
      "379/388, train_loss: 0.2254, step time: 1.5332\n",
      "380/388, train_loss: 0.1701, step time: 1.5317\n",
      "381/388, train_loss: 0.2510, step time: 1.5324\n",
      "382/388, train_loss: 0.1722, step time: 1.5339\n",
      "383/388, train_loss: 0.3375, step time: 1.5366\n",
      "384/388, train_loss: 0.2496, step time: 1.5339\n",
      "385/388, train_loss: 0.1224, step time: 1.5331\n",
      "386/388, train_loss: 0.1621, step time: 1.5310\n",
      "387/388, train_loss: 0.0894, step time: 1.5342\n",
      "388/388, train_loss: 0.2097, step time: 1.5349\n",
      "epoch 19 average loss: 0.2293\n",
      "current epoch: 19 current mean dice: 0.7139 tc: 0.7556 wt: 0.8771 et: 0.5091\n",
      "best mean dice: 0.7282 at epoch: 18\n",
      "time consuming of epoch 19 is: 702.9552\n",
      "----------\n",
      "epoch 20/100\n",
      "1/388, train_loss: 0.6456, step time: 1.5408\n",
      "2/388, train_loss: 0.1702, step time: 1.5368\n",
      "3/388, train_loss: 0.1454, step time: 1.5338\n",
      "4/388, train_loss: 0.0567, step time: 1.5368\n",
      "5/388, train_loss: 0.2522, step time: 1.5310\n",
      "6/388, train_loss: 0.3363, step time: 1.5282\n",
      "7/388, train_loss: 0.1389, step time: 1.5297\n",
      "8/388, train_loss: 0.2297, step time: 1.5432\n",
      "9/388, train_loss: 0.1269, step time: 1.5348\n",
      "10/388, train_loss: 0.2396, step time: 1.5352\n",
      "11/388, train_loss: 0.1387, step time: 1.5383\n",
      "12/388, train_loss: 0.2010, step time: 1.5431\n",
      "13/388, train_loss: 0.1115, step time: 1.5279\n",
      "14/388, train_loss: 0.3159, step time: 1.5307\n",
      "15/388, train_loss: 0.1833, step time: 1.5338\n",
      "16/388, train_loss: 0.1471, step time: 1.5476\n",
      "17/388, train_loss: 0.2435, step time: 1.5304\n",
      "18/388, train_loss: 0.1679, step time: 1.5329\n",
      "19/388, train_loss: 0.1283, step time: 1.5320\n",
      "20/388, train_loss: 0.1841, step time: 1.5510\n",
      "21/388, train_loss: 0.1890, step time: 1.5292\n",
      "22/388, train_loss: 0.2632, step time: 1.5329\n",
      "23/388, train_loss: 0.2209, step time: 1.5321\n",
      "24/388, train_loss: 0.1298, step time: 1.5427\n",
      "25/388, train_loss: 0.1116, step time: 1.5322\n",
      "26/388, train_loss: 0.0931, step time: 1.5296\n",
      "27/388, train_loss: 0.1193, step time: 1.5320\n",
      "28/388, train_loss: 0.3557, step time: 1.5435\n",
      "29/388, train_loss: 0.1001, step time: 1.5344\n",
      "30/388, train_loss: 0.1945, step time: 1.5368\n",
      "31/388, train_loss: 0.5261, step time: 1.5318\n",
      "32/388, train_loss: 0.1061, step time: 1.5440\n",
      "33/388, train_loss: 0.4144, step time: 1.5333\n",
      "34/388, train_loss: 0.1259, step time: 1.5356\n",
      "35/388, train_loss: 0.1600, step time: 1.5372\n",
      "36/388, train_loss: 0.1128, step time: 1.5493\n",
      "37/388, train_loss: 0.1394, step time: 1.5290\n",
      "38/388, train_loss: 0.1973, step time: 1.5339\n",
      "39/388, train_loss: 0.1870, step time: 1.5354\n",
      "40/388, train_loss: 0.7526, step time: 1.5467\n",
      "41/388, train_loss: 0.1246, step time: 1.5314\n",
      "42/388, train_loss: 0.2742, step time: 1.5308\n",
      "43/388, train_loss: 0.1307, step time: 1.5313\n",
      "44/388, train_loss: 0.3829, step time: 1.5494\n",
      "45/388, train_loss: 0.3099, step time: 1.5354\n",
      "46/388, train_loss: 0.1355, step time: 1.5427\n",
      "47/388, train_loss: 0.2941, step time: 1.5328\n",
      "48/388, train_loss: 0.0986, step time: 1.5416\n",
      "49/388, train_loss: 0.4472, step time: 1.5336\n",
      "50/388, train_loss: 0.3516, step time: 1.5333\n",
      "51/388, train_loss: 0.0975, step time: 1.5326\n",
      "52/388, train_loss: 0.1808, step time: 1.5421\n",
      "53/388, train_loss: 0.1810, step time: 1.5296\n",
      "54/388, train_loss: 0.3210, step time: 1.5306\n",
      "55/388, train_loss: 0.0731, step time: 1.5341\n",
      "56/388, train_loss: 0.1907, step time: 1.5440\n",
      "57/388, train_loss: 0.1650, step time: 1.5322\n",
      "58/388, train_loss: 0.1964, step time: 1.5328\n",
      "59/388, train_loss: 0.0997, step time: 1.5336\n",
      "60/388, train_loss: 0.2935, step time: 1.5383\n",
      "61/388, train_loss: 0.1977, step time: 1.5295\n",
      "62/388, train_loss: 0.4266, step time: 1.5429\n",
      "63/388, train_loss: 0.2865, step time: 1.5365\n",
      "64/388, train_loss: 0.0693, step time: 1.5417\n",
      "65/388, train_loss: 0.1349, step time: 1.5325\n",
      "66/388, train_loss: 0.1128, step time: 1.5353\n",
      "67/388, train_loss: 0.3174, step time: 1.5334\n",
      "68/388, train_loss: 0.1176, step time: 1.5442\n",
      "69/388, train_loss: 0.2242, step time: 1.5364\n",
      "70/388, train_loss: 0.1940, step time: 1.5321\n",
      "71/388, train_loss: 0.2103, step time: 1.5604\n",
      "72/388, train_loss: 0.2063, step time: 1.5404\n",
      "73/388, train_loss: 0.3375, step time: 1.5350\n",
      "74/388, train_loss: 0.2362, step time: 1.5318\n",
      "75/388, train_loss: 0.6583, step time: 1.5335\n",
      "76/388, train_loss: 0.1119, step time: 1.5442\n",
      "77/388, train_loss: 0.2832, step time: 1.5272\n",
      "78/388, train_loss: 0.2393, step time: 1.5306\n",
      "79/388, train_loss: 0.1071, step time: 1.5331\n",
      "80/388, train_loss: 0.3086, step time: 1.5444\n",
      "81/388, train_loss: 0.0719, step time: 1.5304\n",
      "82/388, train_loss: 0.0926, step time: 1.5289\n",
      "83/388, train_loss: 0.1090, step time: 1.5555\n",
      "84/388, train_loss: 0.3315, step time: 1.5440\n",
      "85/388, train_loss: 0.0995, step time: 1.5317\n",
      "86/388, train_loss: 0.2021, step time: 1.5285\n",
      "87/388, train_loss: 0.2308, step time: 1.5345\n",
      "88/388, train_loss: 0.1293, step time: 1.5425\n",
      "89/388, train_loss: 0.2302, step time: 1.5350\n",
      "90/388, train_loss: 0.1126, step time: 1.5310\n",
      "91/388, train_loss: 0.2402, step time: 1.5312\n",
      "92/388, train_loss: 0.1433, step time: 1.5448\n",
      "93/388, train_loss: 0.1562, step time: 1.5348\n",
      "94/388, train_loss: 0.1472, step time: 1.5328\n",
      "95/388, train_loss: 0.4494, step time: 1.5300\n",
      "96/388, train_loss: 0.1253, step time: 1.5433\n",
      "97/388, train_loss: 0.2744, step time: 1.5316\n",
      "98/388, train_loss: 0.5192, step time: 1.5382\n",
      "99/388, train_loss: 0.1167, step time: 1.5342\n",
      "100/388, train_loss: 0.1553, step time: 1.5412\n",
      "101/388, train_loss: 0.3091, step time: 1.5320\n",
      "102/388, train_loss: 0.0823, step time: 1.5373\n",
      "103/388, train_loss: 0.2110, step time: 1.5315\n",
      "104/388, train_loss: 0.1356, step time: 1.5407\n",
      "105/388, train_loss: 0.2129, step time: 1.5406\n",
      "106/388, train_loss: 0.2108, step time: 1.5354\n",
      "107/388, train_loss: 0.1572, step time: 1.5342\n",
      "108/388, train_loss: 0.1516, step time: 1.5381\n",
      "109/388, train_loss: 0.1465, step time: 1.5356\n",
      "110/388, train_loss: 0.2698, step time: 1.5367\n",
      "111/388, train_loss: 0.1032, step time: 1.5415\n",
      "112/388, train_loss: 0.1511, step time: 1.5433\n",
      "113/388, train_loss: 0.1185, step time: 1.5315\n",
      "114/388, train_loss: 0.1760, step time: 1.5365\n",
      "115/388, train_loss: 0.0865, step time: 1.5332\n",
      "116/388, train_loss: 0.4539, step time: 1.5448\n",
      "117/388, train_loss: 0.1579, step time: 1.5328\n",
      "118/388, train_loss: 0.2235, step time: 1.5611\n",
      "119/388, train_loss: 0.1761, step time: 1.5405\n",
      "120/388, train_loss: 0.0986, step time: 1.5407\n",
      "121/388, train_loss: 0.0839, step time: 1.5351\n",
      "122/388, train_loss: 0.1600, step time: 1.5344\n",
      "123/388, train_loss: 0.1949, step time: 1.5356\n",
      "124/388, train_loss: 0.1665, step time: 1.5465\n",
      "125/388, train_loss: 0.6009, step time: 1.5565\n",
      "126/388, train_loss: 0.0804, step time: 1.5352\n",
      "127/388, train_loss: 0.0590, step time: 1.5341\n",
      "128/388, train_loss: 0.1463, step time: 1.5443\n",
      "129/388, train_loss: 0.2402, step time: 1.5343\n",
      "130/388, train_loss: 0.1691, step time: 1.5318\n",
      "131/388, train_loss: 0.1660, step time: 1.5372\n",
      "132/388, train_loss: 0.1154, step time: 1.5409\n",
      "133/388, train_loss: 0.0854, step time: 1.5316\n",
      "134/388, train_loss: 0.2493, step time: 1.5364\n",
      "135/388, train_loss: 0.1296, step time: 1.5331\n",
      "136/388, train_loss: 0.0882, step time: 1.5392\n",
      "137/388, train_loss: 0.4260, step time: 1.5440\n",
      "138/388, train_loss: 0.1844, step time: 1.5359\n",
      "139/388, train_loss: 0.2386, step time: 1.5310\n",
      "140/388, train_loss: 0.2474, step time: 1.5437\n",
      "141/388, train_loss: 0.2352, step time: 1.5452\n",
      "142/388, train_loss: 0.1406, step time: 1.5353\n",
      "143/388, train_loss: 0.5486, step time: 1.5342\n",
      "144/388, train_loss: 0.0885, step time: 1.5417\n",
      "145/388, train_loss: 0.3295, step time: 1.5420\n",
      "146/388, train_loss: 0.1954, step time: 1.5333\n",
      "147/388, train_loss: 0.1775, step time: 1.5311\n",
      "148/388, train_loss: 0.2191, step time: 1.5461\n",
      "149/388, train_loss: 0.3269, step time: 1.5395\n",
      "150/388, train_loss: 0.0898, step time: 1.5413\n",
      "151/388, train_loss: 0.2996, step time: 1.5380\n",
      "152/388, train_loss: 0.2234, step time: 1.5464\n",
      "153/388, train_loss: 0.3381, step time: 1.5392\n",
      "154/388, train_loss: 0.3971, step time: 1.5346\n",
      "155/388, train_loss: 0.5619, step time: 1.5383\n",
      "156/388, train_loss: 0.2540, step time: 1.5402\n",
      "157/388, train_loss: 0.1788, step time: 1.5441\n",
      "158/388, train_loss: 0.0770, step time: 1.5350\n",
      "159/388, train_loss: 0.2389, step time: 1.5333\n",
      "160/388, train_loss: 0.2412, step time: 1.5396\n",
      "161/388, train_loss: 0.1892, step time: 1.5464\n",
      "162/388, train_loss: 0.1352, step time: 1.5367\n",
      "163/388, train_loss: 0.1300, step time: 1.5340\n",
      "164/388, train_loss: 0.5982, step time: 1.5433\n",
      "165/388, train_loss: 0.6410, step time: 1.5429\n",
      "166/388, train_loss: 0.6449, step time: 1.5295\n",
      "167/388, train_loss: 0.3332, step time: 1.5305\n",
      "168/388, train_loss: 0.2470, step time: 1.5411\n",
      "169/388, train_loss: 0.2928, step time: 1.5440\n",
      "170/388, train_loss: 0.1914, step time: 1.5320\n",
      "171/388, train_loss: 0.1514, step time: 1.5598\n",
      "172/388, train_loss: 0.2413, step time: 1.5405\n",
      "173/388, train_loss: 0.0764, step time: 1.5413\n",
      "174/388, train_loss: 0.3929, step time: 1.5354\n",
      "175/388, train_loss: 0.1363, step time: 1.5451\n",
      "176/388, train_loss: 0.2909, step time: 1.5333\n",
      "177/388, train_loss: 0.1786, step time: 1.5406\n",
      "178/388, train_loss: 0.1455, step time: 1.5385\n",
      "179/388, train_loss: 0.0787, step time: 1.5358\n",
      "180/388, train_loss: 0.2305, step time: 1.5472\n",
      "181/388, train_loss: 0.0871, step time: 1.5389\n",
      "182/388, train_loss: 0.1204, step time: 1.5340\n",
      "183/388, train_loss: 0.2252, step time: 1.5340\n",
      "184/388, train_loss: 0.6688, step time: 1.5391\n",
      "185/388, train_loss: 0.1437, step time: 1.5459\n",
      "186/388, train_loss: 0.1469, step time: 1.5368\n",
      "187/388, train_loss: 0.2650, step time: 1.5329\n",
      "188/388, train_loss: 0.1620, step time: 1.5403\n",
      "189/388, train_loss: 0.2748, step time: 1.5491\n",
      "190/388, train_loss: 0.5869, step time: 1.5326\n",
      "191/388, train_loss: 0.1533, step time: 1.5339\n",
      "192/388, train_loss: 0.1140, step time: 1.5403\n",
      "193/388, train_loss: 0.0727, step time: 1.5475\n",
      "194/388, train_loss: 0.4040, step time: 1.5337\n",
      "195/388, train_loss: 0.2382, step time: 1.5325\n",
      "196/388, train_loss: 0.0807, step time: 1.5413\n",
      "197/388, train_loss: 0.2470, step time: 1.5485\n",
      "198/388, train_loss: 0.3211, step time: 1.5347\n",
      "199/388, train_loss: 0.1887, step time: 1.5351\n",
      "200/388, train_loss: 0.2575, step time: 1.5403\n",
      "201/388, train_loss: 0.1730, step time: 1.5437\n",
      "202/388, train_loss: 0.1487, step time: 1.5364\n",
      "203/388, train_loss: 0.1320, step time: 1.5323\n",
      "204/388, train_loss: 0.3521, step time: 1.5415\n",
      "205/388, train_loss: 0.1874, step time: 1.5464\n",
      "206/388, train_loss: 0.3256, step time: 1.5307\n",
      "207/388, train_loss: 0.0721, step time: 1.5328\n",
      "208/388, train_loss: 0.3240, step time: 1.5426\n",
      "209/388, train_loss: 0.2599, step time: 1.5471\n",
      "210/388, train_loss: 0.4788, step time: 1.5375\n",
      "211/388, train_loss: 0.3066, step time: 1.5337\n",
      "212/388, train_loss: 0.3689, step time: 1.5451\n",
      "213/388, train_loss: 0.0844, step time: 1.5465\n",
      "214/388, train_loss: 0.4227, step time: 1.5433\n",
      "215/388, train_loss: 0.3107, step time: 1.5317\n",
      "216/388, train_loss: 0.1187, step time: 1.5398\n",
      "217/388, train_loss: 0.2345, step time: 1.5421\n",
      "218/388, train_loss: 0.2536, step time: 1.5402\n",
      "219/388, train_loss: 0.3451, step time: 1.5355\n",
      "220/388, train_loss: 0.1452, step time: 1.5450\n",
      "221/388, train_loss: 0.0532, step time: 1.5418\n",
      "222/388, train_loss: 0.1828, step time: 1.5386\n",
      "223/388, train_loss: 0.4622, step time: 1.5355\n",
      "224/388, train_loss: 0.0788, step time: 1.5459\n",
      "225/388, train_loss: 0.1358, step time: 1.5409\n",
      "226/388, train_loss: 0.2444, step time: 1.5331\n",
      "227/388, train_loss: 0.0544, step time: 1.5352\n",
      "228/388, train_loss: 0.0971, step time: 1.5435\n",
      "229/388, train_loss: 0.3627, step time: 1.5427\n",
      "230/388, train_loss: 0.1695, step time: 1.5343\n",
      "231/388, train_loss: 0.1864, step time: 1.5333\n",
      "232/388, train_loss: 0.6800, step time: 1.5457\n",
      "233/388, train_loss: 0.2750, step time: 1.5451\n",
      "234/388, train_loss: 0.3112, step time: 1.5326\n",
      "235/388, train_loss: 0.2436, step time: 1.5339\n",
      "236/388, train_loss: 0.0971, step time: 1.5566\n",
      "237/388, train_loss: 0.3136, step time: 1.5426\n",
      "238/388, train_loss: 0.3350, step time: 1.5336\n",
      "239/388, train_loss: 0.1702, step time: 1.5355\n",
      "240/388, train_loss: 0.1524, step time: 1.5479\n",
      "241/388, train_loss: 0.2224, step time: 1.5377\n",
      "242/388, train_loss: 0.1574, step time: 1.5332\n",
      "243/388, train_loss: 0.2923, step time: 1.5316\n",
      "244/388, train_loss: 0.3974, step time: 1.5442\n",
      "245/388, train_loss: 0.3747, step time: 1.5442\n",
      "246/388, train_loss: 0.2365, step time: 1.5340\n",
      "247/388, train_loss: 0.1674, step time: 1.5334\n",
      "248/388, train_loss: 0.2339, step time: 1.5404\n",
      "249/388, train_loss: 0.1567, step time: 1.5470\n",
      "250/388, train_loss: 0.1887, step time: 1.5349\n",
      "251/388, train_loss: 0.0708, step time: 1.5349\n",
      "252/388, train_loss: 0.2424, step time: 1.5443\n",
      "253/388, train_loss: 0.4842, step time: 1.5398\n",
      "254/388, train_loss: 0.5453, step time: 1.5486\n",
      "255/388, train_loss: 0.3109, step time: 1.5366\n",
      "256/388, train_loss: 0.1734, step time: 1.5402\n",
      "257/388, train_loss: 0.1930, step time: 1.5487\n",
      "258/388, train_loss: 0.1201, step time: 1.5381\n",
      "259/388, train_loss: 0.3867, step time: 1.5335\n",
      "260/388, train_loss: 0.1560, step time: 1.5399\n",
      "261/388, train_loss: 0.2178, step time: 1.5479\n",
      "262/388, train_loss: 0.1880, step time: 1.5355\n",
      "263/388, train_loss: 0.1393, step time: 1.5347\n",
      "264/388, train_loss: 0.0832, step time: 1.5402\n",
      "265/388, train_loss: 0.1027, step time: 1.5495\n",
      "266/388, train_loss: 0.1327, step time: 1.5342\n",
      "267/388, train_loss: 0.1651, step time: 1.5313\n",
      "268/388, train_loss: 0.4433, step time: 1.5398\n",
      "269/388, train_loss: 0.3476, step time: 1.5444\n",
      "270/388, train_loss: 0.5632, step time: 1.5369\n",
      "271/388, train_loss: 0.2533, step time: 1.5368\n",
      "272/388, train_loss: 0.4615, step time: 1.5380\n",
      "273/388, train_loss: 0.2321, step time: 1.5449\n",
      "274/388, train_loss: 0.1491, step time: 1.5358\n",
      "275/388, train_loss: 0.5450, step time: 1.5345\n",
      "276/388, train_loss: 0.3947, step time: 1.5411\n",
      "277/388, train_loss: 0.3658, step time: 1.5428\n",
      "278/388, train_loss: 0.2376, step time: 1.5333\n",
      "279/388, train_loss: 0.4758, step time: 1.5360\n",
      "280/388, train_loss: 0.3855, step time: 1.5456\n",
      "281/388, train_loss: 0.1447, step time: 1.5417\n",
      "282/388, train_loss: 0.2835, step time: 1.5325\n",
      "283/388, train_loss: 0.1030, step time: 1.5371\n",
      "284/388, train_loss: 0.1148, step time: 1.5421\n",
      "285/388, train_loss: 0.1719, step time: 1.5392\n",
      "286/388, train_loss: 0.0993, step time: 1.5325\n",
      "287/388, train_loss: 0.2305, step time: 1.5357\n",
      "288/388, train_loss: 0.2343, step time: 1.5444\n",
      "289/388, train_loss: 0.1704, step time: 1.5426\n",
      "290/388, train_loss: 0.3085, step time: 1.5362\n",
      "291/388, train_loss: 0.5245, step time: 1.5543\n",
      "292/388, train_loss: 0.3190, step time: 1.5408\n",
      "293/388, train_loss: 0.3082, step time: 1.5484\n",
      "294/388, train_loss: 0.3352, step time: 1.5353\n",
      "295/388, train_loss: 0.1671, step time: 1.5343\n",
      "296/388, train_loss: 0.2462, step time: 1.5421\n",
      "297/388, train_loss: 0.1604, step time: 1.5478\n",
      "298/388, train_loss: 0.3072, step time: 1.5352\n",
      "299/388, train_loss: 0.2333, step time: 1.5355\n",
      "300/388, train_loss: 0.2098, step time: 1.5410\n",
      "301/388, train_loss: 0.1386, step time: 1.5467\n",
      "302/388, train_loss: 0.3209, step time: 1.5381\n",
      "303/388, train_loss: 0.0793, step time: 1.5320\n",
      "304/388, train_loss: 0.1275, step time: 1.5417\n",
      "305/388, train_loss: 0.2698, step time: 1.5465\n",
      "306/388, train_loss: 0.1905, step time: 1.5317\n",
      "307/388, train_loss: 0.2443, step time: 1.5319\n",
      "308/388, train_loss: 0.1444, step time: 1.5436\n",
      "309/388, train_loss: 0.2689, step time: 1.5459\n",
      "310/388, train_loss: 0.2463, step time: 1.5429\n",
      "311/388, train_loss: 0.2254, step time: 1.5316\n",
      "312/388, train_loss: 0.4787, step time: 1.5413\n",
      "313/388, train_loss: 0.1757, step time: 1.5450\n",
      "314/388, train_loss: 0.1729, step time: 1.5333\n",
      "315/388, train_loss: 0.1094, step time: 1.5361\n",
      "316/388, train_loss: 0.3072, step time: 1.5384\n",
      "317/388, train_loss: 0.1160, step time: 1.5495\n",
      "318/388, train_loss: 0.3025, step time: 1.5366\n",
      "319/388, train_loss: 0.2502, step time: 1.5319\n",
      "320/388, train_loss: 0.2336, step time: 1.5441\n",
      "321/388, train_loss: 0.2866, step time: 1.5518\n",
      "322/388, train_loss: 0.2118, step time: 1.5332\n",
      "323/388, train_loss: 0.2028, step time: 1.5334\n",
      "324/388, train_loss: 0.0830, step time: 1.5395\n",
      "325/388, train_loss: 0.3902, step time: 1.5459\n",
      "326/388, train_loss: 0.1435, step time: 1.5619\n",
      "327/388, train_loss: 0.1571, step time: 1.5327\n",
      "328/388, train_loss: 0.1620, step time: 1.5439\n",
      "329/388, train_loss: 0.0938, step time: 1.5458\n",
      "330/388, train_loss: 0.1172, step time: 1.5320\n",
      "331/388, train_loss: 0.3245, step time: 1.5608\n",
      "332/388, train_loss: 0.3214, step time: 1.5441\n",
      "333/388, train_loss: 0.0390, step time: 1.5421\n",
      "334/388, train_loss: 0.2132, step time: 1.5341\n",
      "335/388, train_loss: 0.2130, step time: 1.5330\n",
      "336/388, train_loss: 0.0957, step time: 1.5507\n",
      "337/388, train_loss: 0.4099, step time: 1.5371\n",
      "338/388, train_loss: 0.1641, step time: 1.5324\n",
      "339/388, train_loss: 0.3839, step time: 1.5394\n",
      "340/388, train_loss: 0.1644, step time: 1.5472\n",
      "341/388, train_loss: 0.1114, step time: 1.5434\n",
      "342/388, train_loss: 0.1217, step time: 1.5371\n",
      "343/388, train_loss: 0.2120, step time: 1.5368\n",
      "344/388, train_loss: 0.0935, step time: 1.5424\n",
      "345/388, train_loss: 0.2122, step time: 1.5393\n",
      "346/388, train_loss: 0.2169, step time: 1.5341\n",
      "347/388, train_loss: 0.1687, step time: 1.5415\n",
      "348/388, train_loss: 0.1516, step time: 1.5476\n",
      "349/388, train_loss: 0.3554, step time: 1.5408\n",
      "350/388, train_loss: 0.2204, step time: 1.5346\n",
      "351/388, train_loss: 0.2352, step time: 1.5376\n",
      "352/388, train_loss: 0.2438, step time: 1.5480\n",
      "353/388, train_loss: 0.1802, step time: 1.5418\n",
      "354/388, train_loss: 0.0773, step time: 1.5336\n",
      "355/388, train_loss: 0.1021, step time: 1.5350\n",
      "356/388, train_loss: 0.2123, step time: 1.5447\n",
      "357/388, train_loss: 0.6172, step time: 1.5391\n",
      "358/388, train_loss: 0.1424, step time: 1.5321\n",
      "359/388, train_loss: 0.3031, step time: 1.5348\n",
      "360/388, train_loss: 0.2723, step time: 1.5426\n",
      "361/388, train_loss: 0.4771, step time: 1.5418\n",
      "362/388, train_loss: 0.1665, step time: 1.5324\n",
      "363/388, train_loss: 0.2825, step time: 1.5597\n",
      "364/388, train_loss: 0.1623, step time: 1.5460\n",
      "365/388, train_loss: 0.3112, step time: 1.5442\n",
      "366/388, train_loss: 0.3431, step time: 1.5360\n",
      "367/388, train_loss: 0.5951, step time: 1.5350\n",
      "368/388, train_loss: 0.0993, step time: 1.5433\n",
      "369/388, train_loss: 0.3597, step time: 1.5405\n",
      "370/388, train_loss: 0.1387, step time: 1.5331\n",
      "371/388, train_loss: 0.2333, step time: 1.5377\n",
      "372/388, train_loss: 0.1789, step time: 1.5448\n",
      "373/388, train_loss: 0.1329, step time: 1.5416\n",
      "374/388, train_loss: 0.3345, step time: 1.5336\n",
      "375/388, train_loss: 0.1740, step time: 1.5374\n",
      "376/388, train_loss: 0.0880, step time: 1.5451\n",
      "377/388, train_loss: 0.2507, step time: 1.5420\n",
      "378/388, train_loss: 0.1988, step time: 1.5595\n",
      "379/388, train_loss: 0.3140, step time: 1.5335\n",
      "380/388, train_loss: 0.0414, step time: 1.5395\n",
      "381/388, train_loss: 0.0927, step time: 1.5412\n",
      "382/388, train_loss: 0.1595, step time: 1.5338\n",
      "383/388, train_loss: 0.1212, step time: 1.5369\n",
      "384/388, train_loss: 0.1264, step time: 1.5418\n",
      "385/388, train_loss: 0.1616, step time: 1.5378\n",
      "386/388, train_loss: 0.2404, step time: 1.5348\n",
      "387/388, train_loss: 0.0924, step time: 1.5417\n",
      "388/388, train_loss: 0.1437, step time: 1.5435\n",
      "epoch 20 average loss: 0.2283\n",
      "saved new best metric model\n",
      "current epoch: 20 current mean dice: 0.7387 tc: 0.7852 wt: 0.8902 et: 0.5406\n",
      "best mean dice: 0.7387 at epoch: 20\n",
      "time consuming of epoch 20 is: 705.8549\n",
      "----------\n",
      "epoch 21/100\n",
      "1/388, train_loss: 0.1404, step time: 1.5731\n",
      "2/388, train_loss: 0.3479, step time: 1.5307\n",
      "3/388, train_loss: 0.1571, step time: 1.5345\n",
      "4/388, train_loss: 0.1263, step time: 1.5310\n",
      "5/388, train_loss: 0.0386, step time: 1.5368\n",
      "6/388, train_loss: 0.1709, step time: 1.5347\n",
      "7/388, train_loss: 0.2103, step time: 1.5367\n",
      "8/388, train_loss: 0.1219, step time: 1.5591\n",
      "9/388, train_loss: 0.2325, step time: 1.5625\n",
      "10/388, train_loss: 0.3410, step time: 1.5323\n",
      "11/388, train_loss: 0.2261, step time: 1.5586\n",
      "12/388, train_loss: 0.0742, step time: 1.5538\n",
      "13/388, train_loss: 0.3384, step time: 1.5597\n",
      "14/388, train_loss: 0.2089, step time: 1.5573\n",
      "15/388, train_loss: 0.1399, step time: 1.5592\n",
      "16/388, train_loss: 0.1273, step time: 1.5333\n",
      "17/388, train_loss: 0.0860, step time: 1.5552\n",
      "18/388, train_loss: 0.4204, step time: 1.5611\n",
      "19/388, train_loss: 0.1463, step time: 1.5578\n",
      "20/388, train_loss: 0.2645, step time: 1.5348\n",
      "21/388, train_loss: 0.2098, step time: 1.5304\n",
      "22/388, train_loss: 0.3540, step time: 1.5458\n",
      "23/388, train_loss: 0.1560, step time: 1.5560\n",
      "24/388, train_loss: 0.1468, step time: 1.5622\n",
      "25/388, train_loss: 0.3133, step time: 1.5613\n",
      "26/388, train_loss: 0.3530, step time: 1.5429\n",
      "27/388, train_loss: 0.1609, step time: 1.5598\n",
      "28/388, train_loss: 0.1882, step time: 1.5330\n",
      "29/388, train_loss: 0.6513, step time: 1.5567\n",
      "30/388, train_loss: 0.2973, step time: 1.5692\n",
      "31/388, train_loss: 0.3602, step time: 1.5647\n",
      "32/388, train_loss: 0.1021, step time: 1.5612\n",
      "33/388, train_loss: 0.2987, step time: 1.5565\n",
      "34/388, train_loss: 0.1092, step time: 1.5605\n",
      "35/388, train_loss: 0.4048, step time: 1.5594\n",
      "36/388, train_loss: 0.4105, step time: 1.5621\n",
      "37/388, train_loss: 0.5406, step time: 1.5575\n",
      "38/388, train_loss: 0.1506, step time: 1.5610\n",
      "39/388, train_loss: 0.2094, step time: 1.5603\n",
      "40/388, train_loss: 0.2044, step time: 1.5744\n",
      "41/388, train_loss: 0.4174, step time: 1.5610\n",
      "42/388, train_loss: 0.4981, step time: 1.5570\n",
      "43/388, train_loss: 0.1112, step time: 1.5601\n",
      "44/388, train_loss: 0.2746, step time: 1.5603\n",
      "45/388, train_loss: 0.2024, step time: 1.5593\n",
      "46/388, train_loss: 0.1377, step time: 1.5670\n",
      "47/388, train_loss: 0.5324, step time: 1.5584\n",
      "48/388, train_loss: 0.3111, step time: 1.5596\n",
      "49/388, train_loss: 0.1239, step time: 1.5617\n",
      "50/388, train_loss: 0.2084, step time: 1.5394\n",
      "51/388, train_loss: 0.1454, step time: 1.5317\n",
      "52/388, train_loss: 0.1155, step time: 1.5602\n",
      "53/388, train_loss: 0.2069, step time: 1.5573\n",
      "54/388, train_loss: 0.2812, step time: 1.5367\n",
      "55/388, train_loss: 0.1687, step time: 1.5400\n",
      "56/388, train_loss: 0.2836, step time: 1.5347\n",
      "57/388, train_loss: 0.1194, step time: 1.5560\n",
      "58/388, train_loss: 0.3655, step time: 1.5336\n",
      "59/388, train_loss: 0.0934, step time: 1.5510\n",
      "60/388, train_loss: 0.0783, step time: 1.5333\n",
      "61/388, train_loss: 0.1271, step time: 1.5323\n",
      "62/388, train_loss: 0.3150, step time: 1.5705\n",
      "63/388, train_loss: 0.1905, step time: 1.5334\n",
      "64/388, train_loss: 0.1225, step time: 1.5649\n",
      "65/388, train_loss: 0.0958, step time: 1.5586\n",
      "66/388, train_loss: 0.1097, step time: 1.5312\n",
      "67/388, train_loss: 0.2109, step time: 1.5574\n",
      "68/388, train_loss: 0.1462, step time: 1.5619\n",
      "69/388, train_loss: 0.1469, step time: 1.5629\n",
      "70/388, train_loss: 0.0935, step time: 1.5364\n",
      "71/388, train_loss: 0.2379, step time: 1.5356\n",
      "72/388, train_loss: 0.2812, step time: 1.5603\n",
      "73/388, train_loss: 0.1228, step time: 1.5599\n",
      "74/388, train_loss: 0.1144, step time: 1.5566\n",
      "75/388, train_loss: 0.5349, step time: 1.5304\n",
      "76/388, train_loss: 0.1789, step time: 1.5643\n",
      "77/388, train_loss: 0.0523, step time: 1.5599\n",
      "78/388, train_loss: 0.0992, step time: 1.5613\n",
      "79/388, train_loss: 0.4749, step time: 1.5606\n",
      "80/388, train_loss: 0.2365, step time: 1.5566\n",
      "81/388, train_loss: 0.1193, step time: 1.5597\n",
      "82/388, train_loss: 0.0991, step time: 1.5351\n",
      "83/388, train_loss: 0.1049, step time: 1.5369\n",
      "84/388, train_loss: 0.2643, step time: 1.5573\n",
      "85/388, train_loss: 0.0980, step time: 1.5591\n",
      "86/388, train_loss: 0.0663, step time: 1.5383\n",
      "87/388, train_loss: 0.0300, step time: 1.5681\n",
      "88/388, train_loss: 0.2181, step time: 1.5561\n",
      "89/388, train_loss: 0.1445, step time: 1.5612\n",
      "90/388, train_loss: 0.2843, step time: 1.5582\n",
      "91/388, train_loss: 0.2580, step time: 1.5668\n",
      "92/388, train_loss: 0.3620, step time: 1.5354\n",
      "93/388, train_loss: 0.0908, step time: 1.5594\n",
      "94/388, train_loss: 0.4188, step time: 1.5611\n",
      "95/388, train_loss: 0.4421, step time: 1.5743\n",
      "96/388, train_loss: 0.1815, step time: 1.5570\n",
      "97/388, train_loss: 0.1049, step time: 1.5464\n",
      "98/388, train_loss: 0.4035, step time: 1.5374\n",
      "99/388, train_loss: 0.2015, step time: 1.5604\n",
      "100/388, train_loss: 0.1522, step time: 1.5585\n",
      "101/388, train_loss: 0.1040, step time: 1.5554\n",
      "102/388, train_loss: 0.1512, step time: 1.5624\n",
      "103/388, train_loss: 0.1294, step time: 1.5595\n",
      "104/388, train_loss: 0.0629, step time: 1.5603\n",
      "105/388, train_loss: 0.0953, step time: 1.5580\n",
      "106/388, train_loss: 0.2804, step time: 1.5621\n",
      "107/388, train_loss: 0.4861, step time: 1.5588\n",
      "108/388, train_loss: 0.1987, step time: 1.5596\n",
      "109/388, train_loss: 0.5527, step time: 1.5615\n",
      "110/388, train_loss: 0.0771, step time: 1.5582\n",
      "111/388, train_loss: 0.1282, step time: 1.5308\n",
      "112/388, train_loss: 0.1821, step time: 1.5325\n",
      "113/388, train_loss: 0.2145, step time: 1.5377\n",
      "114/388, train_loss: 0.2484, step time: 1.5317\n",
      "115/388, train_loss: 0.8735, step time: 1.5521\n",
      "116/388, train_loss: 0.1354, step time: 1.5651\n",
      "117/388, train_loss: 0.3799, step time: 1.5590\n",
      "118/388, train_loss: 0.1249, step time: 1.5562\n",
      "119/388, train_loss: 0.1014, step time: 1.5283\n",
      "120/388, train_loss: 0.2394, step time: 1.5368\n",
      "121/388, train_loss: 0.4980, step time: 1.5357\n",
      "122/388, train_loss: 0.0913, step time: 1.5643\n",
      "123/388, train_loss: 0.2101, step time: 1.5607\n",
      "124/388, train_loss: 0.4052, step time: 1.5578\n",
      "125/388, train_loss: 0.1825, step time: 1.5631\n",
      "126/388, train_loss: 0.2029, step time: 1.5307\n",
      "127/388, train_loss: 0.5110, step time: 1.5549\n",
      "128/388, train_loss: 0.1959, step time: 1.5360\n",
      "129/388, train_loss: 0.1299, step time: 1.5357\n",
      "130/388, train_loss: 0.1653, step time: 1.5593\n",
      "131/388, train_loss: 0.2106, step time: 1.5577\n",
      "132/388, train_loss: 0.1551, step time: 1.5625\n",
      "133/388, train_loss: 0.2160, step time: 1.5612\n",
      "134/388, train_loss: 0.1546, step time: 1.5623\n",
      "135/388, train_loss: 0.1821, step time: 1.5579\n",
      "136/388, train_loss: 0.2729, step time: 1.5596\n",
      "137/388, train_loss: 0.3813, step time: 1.5630\n",
      "138/388, train_loss: 0.5457, step time: 1.5299\n",
      "139/388, train_loss: 0.1937, step time: 1.5611\n",
      "140/388, train_loss: 0.1169, step time: 1.5639\n",
      "141/388, train_loss: 0.3275, step time: 1.5335\n",
      "142/388, train_loss: 0.1644, step time: 1.5414\n",
      "143/388, train_loss: 0.1104, step time: 1.5377\n",
      "144/388, train_loss: 0.4990, step time: 1.5585\n",
      "145/388, train_loss: 0.2294, step time: 1.5634\n",
      "146/388, train_loss: 0.1948, step time: 1.5583\n",
      "147/388, train_loss: 0.0626, step time: 1.5613\n",
      "148/388, train_loss: 0.1608, step time: 1.5317\n",
      "149/388, train_loss: 0.6731, step time: 1.5626\n",
      "150/388, train_loss: 0.1354, step time: 1.5543\n",
      "151/388, train_loss: 0.1469, step time: 1.5585\n",
      "152/388, train_loss: 0.1633, step time: 1.5580\n",
      "153/388, train_loss: 0.1320, step time: 1.5585\n",
      "154/388, train_loss: 0.1313, step time: 1.5576\n",
      "155/388, train_loss: 0.1934, step time: 1.5363\n",
      "156/388, train_loss: 0.2909, step time: 1.5371\n",
      "157/388, train_loss: 0.3621, step time: 1.5355\n",
      "158/388, train_loss: 0.0557, step time: 1.5582\n",
      "159/388, train_loss: 0.0676, step time: 1.5573\n",
      "160/388, train_loss: 0.5949, step time: 1.5656\n",
      "161/388, train_loss: 0.1723, step time: 1.5349\n",
      "162/388, train_loss: 0.1503, step time: 1.5589\n",
      "163/388, train_loss: 0.1336, step time: 1.5334\n",
      "164/388, train_loss: 0.1945, step time: 1.5352\n",
      "165/388, train_loss: 0.1585, step time: 1.5320\n",
      "166/388, train_loss: 0.3927, step time: 1.5328\n",
      "167/388, train_loss: 0.2625, step time: 1.5295\n",
      "168/388, train_loss: 0.1375, step time: 1.5314\n",
      "169/388, train_loss: 0.2260, step time: 1.5385\n",
      "170/388, train_loss: 0.1296, step time: 1.5393\n",
      "171/388, train_loss: 0.6185, step time: 1.5304\n",
      "172/388, train_loss: 0.2344, step time: 1.5333\n",
      "173/388, train_loss: 0.2167, step time: 1.5326\n",
      "174/388, train_loss: 0.1244, step time: 1.5343\n",
      "175/388, train_loss: 0.0863, step time: 1.5340\n",
      "176/388, train_loss: 0.1214, step time: 1.5349\n",
      "177/388, train_loss: 0.1151, step time: 1.5332\n",
      "178/388, train_loss: 0.2471, step time: 1.5557\n",
      "179/388, train_loss: 0.1744, step time: 1.5312\n",
      "180/388, train_loss: 0.1545, step time: 1.5561\n",
      "181/388, train_loss: 0.4955, step time: 1.5316\n",
      "182/388, train_loss: 0.1032, step time: 1.5323\n",
      "183/388, train_loss: 0.2748, step time: 1.5348\n",
      "184/388, train_loss: 0.1789, step time: 1.5335\n",
      "185/388, train_loss: 0.1835, step time: 1.5344\n",
      "186/388, train_loss: 0.2049, step time: 1.5318\n",
      "187/388, train_loss: 0.3217, step time: 1.5301\n",
      "188/388, train_loss: 0.2167, step time: 1.5326\n",
      "189/388, train_loss: 0.4225, step time: 1.5286\n",
      "190/388, train_loss: 0.1783, step time: 1.5333\n",
      "191/388, train_loss: 0.1365, step time: 1.5399\n",
      "192/388, train_loss: 0.4667, step time: 1.5363\n",
      "193/388, train_loss: 0.1232, step time: 1.5325\n",
      "194/388, train_loss: 0.2205, step time: 1.5318\n",
      "195/388, train_loss: 0.2349, step time: 1.5330\n",
      "196/388, train_loss: 0.1415, step time: 1.5292\n",
      "197/388, train_loss: 0.1692, step time: 1.5369\n",
      "198/388, train_loss: 0.1406, step time: 1.5340\n",
      "199/388, train_loss: 0.3464, step time: 1.5356\n",
      "200/388, train_loss: 0.1409, step time: 1.5306\n",
      "201/388, train_loss: 0.2412, step time: 1.5340\n",
      "202/388, train_loss: 0.2309, step time: 1.5335\n",
      "203/388, train_loss: 0.1582, step time: 1.5352\n",
      "204/388, train_loss: 0.0890, step time: 1.5449\n",
      "205/388, train_loss: 0.0508, step time: 1.5305\n",
      "206/388, train_loss: 0.1702, step time: 1.5597\n",
      "207/388, train_loss: 0.2991, step time: 1.5363\n",
      "208/388, train_loss: 0.3695, step time: 1.5451\n",
      "209/388, train_loss: 0.2042, step time: 1.5306\n",
      "210/388, train_loss: 0.3101, step time: 1.5327\n",
      "211/388, train_loss: 0.0694, step time: 1.5320\n",
      "212/388, train_loss: 0.3026, step time: 1.5427\n",
      "213/388, train_loss: 0.2898, step time: 1.5339\n",
      "214/388, train_loss: 0.4015, step time: 1.5352\n",
      "215/388, train_loss: 0.2258, step time: 1.5325\n",
      "216/388, train_loss: 0.3457, step time: 1.5408\n",
      "217/388, train_loss: 0.1321, step time: 1.5383\n",
      "218/388, train_loss: 0.1357, step time: 1.5332\n",
      "219/388, train_loss: 0.0747, step time: 1.5420\n",
      "220/388, train_loss: 0.6927, step time: 1.5404\n",
      "221/388, train_loss: 0.1441, step time: 1.5315\n",
      "222/388, train_loss: 0.1924, step time: 1.5341\n",
      "223/388, train_loss: 0.2305, step time: 1.5372\n",
      "224/388, train_loss: 0.1609, step time: 1.5321\n",
      "225/388, train_loss: 0.3437, step time: 1.5316\n",
      "226/388, train_loss: 0.1398, step time: 1.5319\n",
      "227/388, train_loss: 0.3236, step time: 1.5367\n",
      "228/388, train_loss: 0.1233, step time: 1.5347\n",
      "229/388, train_loss: 0.3186, step time: 1.5352\n",
      "230/388, train_loss: 0.2141, step time: 1.5322\n",
      "231/388, train_loss: 0.1638, step time: 1.5305\n",
      "232/388, train_loss: 0.1153, step time: 1.5484\n",
      "233/388, train_loss: 0.1094, step time: 1.5374\n",
      "234/388, train_loss: 0.1551, step time: 1.5347\n",
      "235/388, train_loss: 0.1565, step time: 1.5343\n",
      "236/388, train_loss: 0.3383, step time: 1.5411\n",
      "237/388, train_loss: 0.2553, step time: 1.5413\n",
      "238/388, train_loss: 0.1833, step time: 1.5317\n",
      "239/388, train_loss: 0.1823, step time: 1.5358\n",
      "240/388, train_loss: 0.1070, step time: 1.5419\n",
      "241/388, train_loss: 0.2329, step time: 1.5329\n",
      "242/388, train_loss: 0.1533, step time: 1.5310\n",
      "243/388, train_loss: 0.1449, step time: 1.5345\n",
      "244/388, train_loss: 0.2797, step time: 1.5436\n",
      "245/388, train_loss: 0.2239, step time: 1.5336\n",
      "246/388, train_loss: 0.2365, step time: 1.5342\n",
      "247/388, train_loss: 0.2308, step time: 1.5345\n",
      "248/388, train_loss: 0.1259, step time: 1.5407\n",
      "249/388, train_loss: 0.0964, step time: 1.5312\n",
      "250/388, train_loss: 0.1269, step time: 1.5475\n",
      "251/388, train_loss: 0.2555, step time: 1.5348\n",
      "252/388, train_loss: 0.0991, step time: 1.5390\n",
      "253/388, train_loss: 0.1479, step time: 1.5375\n",
      "254/388, train_loss: 0.0903, step time: 1.5467\n",
      "255/388, train_loss: 0.1825, step time: 1.5299\n",
      "256/388, train_loss: 0.1452, step time: 1.5379\n",
      "257/388, train_loss: 0.2202, step time: 1.5317\n",
      "258/388, train_loss: 0.1813, step time: 1.5368\n",
      "259/388, train_loss: 0.6120, step time: 1.5380\n",
      "260/388, train_loss: 0.2242, step time: 1.5464\n",
      "261/388, train_loss: 0.2041, step time: 1.5337\n",
      "262/388, train_loss: 0.3990, step time: 1.5331\n",
      "263/388, train_loss: 0.1122, step time: 1.5353\n",
      "264/388, train_loss: 0.1889, step time: 1.5444\n",
      "265/388, train_loss: 0.0572, step time: 1.5300\n",
      "266/388, train_loss: 0.2330, step time: 1.5334\n",
      "267/388, train_loss: 0.2432, step time: 1.5346\n",
      "268/388, train_loss: 0.2959, step time: 1.5450\n",
      "269/388, train_loss: 0.4128, step time: 1.5317\n",
      "270/388, train_loss: 0.0814, step time: 1.5323\n",
      "271/388, train_loss: 0.1522, step time: 1.5327\n",
      "272/388, train_loss: 0.0918, step time: 1.5455\n",
      "273/388, train_loss: 0.1076, step time: 1.5349\n",
      "274/388, train_loss: 0.2688, step time: 1.5372\n",
      "275/388, train_loss: 0.3317, step time: 1.5317\n",
      "276/388, train_loss: 0.2411, step time: 1.5419\n",
      "277/388, train_loss: 0.2399, step time: 1.5322\n",
      "278/388, train_loss: 0.5409, step time: 1.5359\n",
      "279/388, train_loss: 0.1964, step time: 1.5376\n",
      "280/388, train_loss: 0.3276, step time: 1.5421\n",
      "281/388, train_loss: 0.2512, step time: 1.5320\n",
      "282/388, train_loss: 0.6509, step time: 1.5314\n",
      "283/388, train_loss: 0.1176, step time: 1.5394\n",
      "284/388, train_loss: 0.0725, step time: 1.5409\n",
      "285/388, train_loss: 0.1974, step time: 1.5322\n",
      "286/388, train_loss: 0.1593, step time: 1.5559\n",
      "287/388, train_loss: 0.2486, step time: 1.5330\n",
      "288/388, train_loss: 0.2792, step time: 1.5508\n",
      "289/388, train_loss: 0.3449, step time: 1.5349\n",
      "290/388, train_loss: 0.0700, step time: 1.5348\n",
      "291/388, train_loss: 0.2039, step time: 1.5308\n",
      "292/388, train_loss: 0.4717, step time: 1.5448\n",
      "293/388, train_loss: 0.1596, step time: 1.5356\n",
      "294/388, train_loss: 0.2744, step time: 1.5347\n",
      "295/388, train_loss: 0.4005, step time: 1.5310\n",
      "296/388, train_loss: 0.1339, step time: 1.5345\n",
      "297/388, train_loss: 0.0530, step time: 1.5307\n",
      "298/388, train_loss: 0.1548, step time: 1.5336\n",
      "299/388, train_loss: 0.1791, step time: 1.5328\n",
      "300/388, train_loss: 0.1669, step time: 1.5469\n",
      "301/388, train_loss: 0.1335, step time: 1.5290\n",
      "302/388, train_loss: 0.1115, step time: 1.5351\n",
      "303/388, train_loss: 0.1887, step time: 1.5296\n",
      "304/388, train_loss: 0.3018, step time: 1.5383\n",
      "305/388, train_loss: 0.3482, step time: 1.5360\n",
      "306/388, train_loss: 0.2565, step time: 1.5336\n",
      "307/388, train_loss: 0.5653, step time: 1.5345\n",
      "308/388, train_loss: 0.0941, step time: 1.5374\n",
      "309/388, train_loss: 0.1911, step time: 1.5311\n",
      "310/388, train_loss: 0.0866, step time: 1.5345\n",
      "311/388, train_loss: 0.2707, step time: 1.5337\n",
      "312/388, train_loss: 0.2367, step time: 1.5445\n",
      "313/388, train_loss: 0.0998, step time: 1.5379\n",
      "314/388, train_loss: 0.1813, step time: 1.5307\n",
      "315/388, train_loss: 0.1133, step time: 1.5334\n",
      "316/388, train_loss: 0.2340, step time: 1.5414\n",
      "317/388, train_loss: 0.1660, step time: 1.5319\n",
      "318/388, train_loss: 0.1968, step time: 1.5392\n",
      "319/388, train_loss: 0.1749, step time: 1.5297\n",
      "320/388, train_loss: 0.0879, step time: 1.5413\n",
      "321/388, train_loss: 0.3692, step time: 1.5344\n",
      "322/388, train_loss: 0.4063, step time: 1.5343\n",
      "323/388, train_loss: 0.2195, step time: 1.5365\n",
      "324/388, train_loss: 0.2448, step time: 1.5458\n",
      "325/388, train_loss: 0.1027, step time: 1.5295\n",
      "326/388, train_loss: 0.1007, step time: 1.5349\n",
      "327/388, train_loss: 0.0739, step time: 1.5343\n",
      "328/388, train_loss: 0.2576, step time: 1.5435\n",
      "329/388, train_loss: 0.2022, step time: 1.5335\n",
      "330/388, train_loss: 0.1886, step time: 1.5348\n",
      "331/388, train_loss: 0.2901, step time: 1.5316\n",
      "332/388, train_loss: 0.2932, step time: 1.5445\n",
      "333/388, train_loss: 0.1414, step time: 1.5342\n",
      "334/388, train_loss: 0.3230, step time: 1.5358\n",
      "335/388, train_loss: 0.8759, step time: 1.5317\n",
      "336/388, train_loss: 0.3076, step time: 1.5419\n",
      "337/388, train_loss: 0.1366, step time: 1.5298\n",
      "338/388, train_loss: 0.2782, step time: 1.5352\n",
      "339/388, train_loss: 0.1437, step time: 1.5340\n",
      "340/388, train_loss: 0.3728, step time: 1.5440\n",
      "341/388, train_loss: 0.2069, step time: 1.5326\n",
      "342/388, train_loss: 0.1770, step time: 1.5334\n",
      "343/388, train_loss: 0.0979, step time: 1.5371\n",
      "344/388, train_loss: 0.1503, step time: 1.5425\n",
      "345/388, train_loss: 0.0998, step time: 1.5338\n",
      "346/388, train_loss: 0.2451, step time: 1.5351\n",
      "347/388, train_loss: 0.2836, step time: 1.5345\n",
      "348/388, train_loss: 0.2047, step time: 1.5438\n",
      "349/388, train_loss: 0.3423, step time: 1.5337\n",
      "350/388, train_loss: 0.0853, step time: 1.5370\n",
      "351/388, train_loss: 0.2794, step time: 1.5364\n",
      "352/388, train_loss: 0.0720, step time: 1.5480\n",
      "353/388, train_loss: 0.3058, step time: 1.5314\n",
      "354/388, train_loss: 0.3537, step time: 1.5319\n",
      "355/388, train_loss: 0.2514, step time: 1.5346\n",
      "356/388, train_loss: 0.5740, step time: 1.5467\n",
      "357/388, train_loss: 0.1165, step time: 1.5335\n",
      "358/388, train_loss: 0.1897, step time: 1.5337\n",
      "359/388, train_loss: 0.2591, step time: 1.5316\n",
      "360/388, train_loss: 0.2318, step time: 1.5446\n",
      "361/388, train_loss: 0.1979, step time: 1.5360\n",
      "362/388, train_loss: 0.1066, step time: 1.5337\n",
      "363/388, train_loss: 0.1599, step time: 1.5356\n",
      "364/388, train_loss: 0.0705, step time: 1.5421\n",
      "365/388, train_loss: 0.0809, step time: 1.5345\n",
      "366/388, train_loss: 0.2196, step time: 1.5307\n",
      "367/388, train_loss: 0.1222, step time: 1.5371\n",
      "368/388, train_loss: 0.2273, step time: 1.5443\n",
      "369/388, train_loss: 0.0681, step time: 1.5339\n",
      "370/388, train_loss: 0.2284, step time: 1.5337\n",
      "371/388, train_loss: 0.1475, step time: 1.5312\n",
      "372/388, train_loss: 0.4370, step time: 1.5408\n",
      "373/388, train_loss: 0.0648, step time: 1.5345\n",
      "374/388, train_loss: 0.2097, step time: 1.5362\n",
      "375/388, train_loss: 0.2437, step time: 1.5322\n",
      "376/388, train_loss: 0.1122, step time: 1.5312\n",
      "377/388, train_loss: 0.0946, step time: 1.5316\n",
      "378/388, train_loss: 0.1662, step time: 1.5283\n",
      "379/388, train_loss: 0.1439, step time: 1.5334\n",
      "380/388, train_loss: 0.2747, step time: 1.5446\n",
      "381/388, train_loss: 0.0754, step time: 1.5358\n",
      "382/388, train_loss: 0.5719, step time: 1.5327\n",
      "383/388, train_loss: 0.3738, step time: 1.5355\n",
      "384/388, train_loss: 0.2488, step time: 1.5427\n",
      "385/388, train_loss: 0.1982, step time: 1.5314\n",
      "386/388, train_loss: 0.3604, step time: 1.5333\n",
      "387/388, train_loss: 0.4124, step time: 1.5322\n",
      "388/388, train_loss: 0.1923, step time: 1.5441\n",
      "epoch 21 average loss: 0.2246\n",
      "saved new best metric model\n",
      "current epoch: 21 current mean dice: 0.7401 tc: 0.7857 wt: 0.8943 et: 0.5402\n",
      "best mean dice: 0.7401 at epoch: 21\n",
      "time consuming of epoch 21 is: 708.8586\n",
      "----------\n",
      "epoch 22/100\n",
      "1/388, train_loss: 0.1642, step time: 1.5415\n",
      "2/388, train_loss: 0.1321, step time: 1.5348\n",
      "3/388, train_loss: 0.3080, step time: 1.5355\n",
      "4/388, train_loss: 0.1926, step time: 1.5361\n",
      "5/388, train_loss: 0.2064, step time: 1.5306\n",
      "6/388, train_loss: 0.2105, step time: 1.5369\n",
      "7/388, train_loss: 0.1913, step time: 1.5349\n",
      "8/388, train_loss: 0.2549, step time: 1.5329\n",
      "9/388, train_loss: 0.4706, step time: 1.5319\n",
      "10/388, train_loss: 0.4263, step time: 1.5352\n",
      "11/388, train_loss: 0.3809, step time: 1.5336\n",
      "12/388, train_loss: 0.0914, step time: 1.5295\n",
      "13/388, train_loss: 0.1169, step time: 1.5310\n",
      "14/388, train_loss: 0.2363, step time: 1.5347\n",
      "15/388, train_loss: 0.2300, step time: 1.5322\n",
      "16/388, train_loss: 0.5787, step time: 1.5349\n",
      "17/388, train_loss: 0.1025, step time: 1.5359\n",
      "18/388, train_loss: 0.1171, step time: 1.5297\n",
      "19/388, train_loss: 0.1447, step time: 1.5307\n",
      "20/388, train_loss: 0.1433, step time: 1.5348\n",
      "21/388, train_loss: 0.4875, step time: 1.5279\n",
      "22/388, train_loss: 0.0750, step time: 1.5364\n",
      "23/388, train_loss: 0.2424, step time: 1.5383\n",
      "24/388, train_loss: 0.1819, step time: 1.5315\n",
      "25/388, train_loss: 0.1848, step time: 1.5332\n",
      "26/388, train_loss: 0.1848, step time: 1.5374\n",
      "27/388, train_loss: 0.2978, step time: 1.5368\n",
      "28/388, train_loss: 0.1827, step time: 1.5319\n",
      "29/388, train_loss: 0.1310, step time: 1.5600\n",
      "30/388, train_loss: 0.0792, step time: 1.5376\n",
      "31/388, train_loss: 0.0886, step time: 1.5344\n",
      "32/388, train_loss: 0.4458, step time: 1.5349\n",
      "33/388, train_loss: 0.1818, step time: 1.5328\n",
      "34/388, train_loss: 0.1012, step time: 1.5276\n",
      "35/388, train_loss: 0.6181, step time: 1.5301\n",
      "36/388, train_loss: 0.2991, step time: 1.5303\n",
      "37/388, train_loss: 0.2342, step time: 1.5586\n",
      "38/388, train_loss: 0.5961, step time: 1.5665\n",
      "39/388, train_loss: 0.1567, step time: 1.5289\n",
      "40/388, train_loss: 0.0795, step time: 1.5314\n",
      "41/388, train_loss: 0.5070, step time: 1.5319\n",
      "42/388, train_loss: 0.0994, step time: 1.5370\n",
      "43/388, train_loss: 0.1481, step time: 1.5362\n",
      "44/388, train_loss: 0.2120, step time: 1.5361\n",
      "45/388, train_loss: 0.3265, step time: 1.5345\n",
      "46/388, train_loss: 0.1436, step time: 1.5345\n",
      "47/388, train_loss: 0.1665, step time: 1.5332\n",
      "48/388, train_loss: 0.3673, step time: 1.5372\n",
      "49/388, train_loss: 0.3277, step time: 1.5475\n",
      "50/388, train_loss: 0.1275, step time: 1.5339\n",
      "51/388, train_loss: 0.2361, step time: 1.5309\n",
      "52/388, train_loss: 0.1075, step time: 1.5313\n",
      "53/388, train_loss: 0.2311, step time: 1.5340\n",
      "54/388, train_loss: 0.1572, step time: 1.5333\n",
      "55/388, train_loss: 0.1686, step time: 1.5336\n",
      "56/388, train_loss: 0.1513, step time: 1.5318\n",
      "57/388, train_loss: 0.1395, step time: 1.5352\n",
      "58/388, train_loss: 0.2101, step time: 1.5351\n",
      "59/388, train_loss: 0.1962, step time: 1.5319\n",
      "60/388, train_loss: 0.2216, step time: 1.5313\n",
      "61/388, train_loss: 0.0507, step time: 1.5637\n",
      "62/388, train_loss: 0.1829, step time: 1.5355\n",
      "63/388, train_loss: 0.1748, step time: 1.5321\n",
      "64/388, train_loss: 0.1218, step time: 1.5321\n",
      "65/388, train_loss: 0.1260, step time: 1.5332\n",
      "66/388, train_loss: 0.3127, step time: 1.5704\n",
      "67/388, train_loss: 0.1174, step time: 1.5325\n",
      "68/388, train_loss: 0.1859, step time: 1.5318\n",
      "69/388, train_loss: 0.2581, step time: 1.5350\n",
      "70/388, train_loss: 0.2078, step time: 1.5380\n",
      "71/388, train_loss: 0.1353, step time: 1.5360\n",
      "72/388, train_loss: 0.2892, step time: 1.5330\n",
      "73/388, train_loss: 0.4264, step time: 1.5328\n",
      "74/388, train_loss: 0.3462, step time: 1.5352\n",
      "75/388, train_loss: 0.1926, step time: 1.5363\n",
      "76/388, train_loss: 0.2739, step time: 1.5453\n",
      "77/388, train_loss: 0.1237, step time: 1.5350\n",
      "78/388, train_loss: 0.3200, step time: 1.5369\n",
      "79/388, train_loss: 0.1713, step time: 1.5365\n",
      "80/388, train_loss: 0.4682, step time: 1.5402\n",
      "81/388, train_loss: 0.1681, step time: 1.5356\n",
      "82/388, train_loss: 0.2181, step time: 1.5331\n",
      "83/388, train_loss: 0.5214, step time: 1.5353\n",
      "84/388, train_loss: 0.1184, step time: 1.5423\n",
      "85/388, train_loss: 0.5284, step time: 1.5404\n",
      "86/388, train_loss: 0.1537, step time: 1.5372\n",
      "87/388, train_loss: 0.2588, step time: 1.5322\n",
      "88/388, train_loss: 0.1159, step time: 1.5329\n",
      "89/388, train_loss: 0.1088, step time: 1.5326\n",
      "90/388, train_loss: 0.1003, step time: 1.5361\n",
      "91/388, train_loss: 0.2309, step time: 1.5345\n",
      "92/388, train_loss: 0.0807, step time: 1.5442\n",
      "93/388, train_loss: 0.4781, step time: 1.5355\n",
      "94/388, train_loss: 0.6263, step time: 1.5338\n",
      "95/388, train_loss: 0.0943, step time: 1.5344\n",
      "96/388, train_loss: 0.1467, step time: 1.5368\n",
      "97/388, train_loss: 0.2521, step time: 1.5384\n",
      "98/388, train_loss: 0.0893, step time: 1.5313\n",
      "99/388, train_loss: 0.1354, step time: 1.5321\n",
      "100/388, train_loss: 0.1614, step time: 1.5382\n",
      "101/388, train_loss: 0.1053, step time: 1.5350\n",
      "102/388, train_loss: 0.0710, step time: 1.5346\n",
      "103/388, train_loss: 0.2662, step time: 1.5334\n",
      "104/388, train_loss: 0.1536, step time: 1.5288\n",
      "105/388, train_loss: 0.1148, step time: 1.5320\n",
      "106/388, train_loss: 0.2123, step time: 1.5429\n",
      "107/388, train_loss: 0.2187, step time: 1.5421\n",
      "108/388, train_loss: 0.3300, step time: 1.5332\n",
      "109/388, train_loss: 0.3021, step time: 1.5285\n",
      "110/388, train_loss: 0.1227, step time: 1.5329\n",
      "111/388, train_loss: 0.2038, step time: 1.5351\n",
      "112/388, train_loss: 0.1302, step time: 1.5350\n",
      "113/388, train_loss: 0.1600, step time: 1.5356\n",
      "114/388, train_loss: 0.0772, step time: 1.5325\n",
      "115/388, train_loss: 0.1238, step time: 1.5324\n",
      "116/388, train_loss: 0.3421, step time: 1.5318\n",
      "117/388, train_loss: 0.2091, step time: 1.5325\n",
      "118/388, train_loss: 0.1344, step time: 1.5389\n",
      "119/388, train_loss: 0.2094, step time: 1.5361\n",
      "120/388, train_loss: 0.0698, step time: 1.5374\n",
      "121/388, train_loss: 0.2009, step time: 1.5356\n",
      "122/388, train_loss: 0.2456, step time: 1.5318\n",
      "123/388, train_loss: 0.4186, step time: 1.5321\n",
      "124/388, train_loss: 0.1044, step time: 1.5337\n",
      "125/388, train_loss: 0.1414, step time: 1.5371\n",
      "126/388, train_loss: 0.1769, step time: 1.5367\n",
      "127/388, train_loss: 0.1621, step time: 1.5343\n",
      "128/388, train_loss: 0.3100, step time: 1.5330\n",
      "129/388, train_loss: 0.1736, step time: 1.5317\n",
      "130/388, train_loss: 0.1622, step time: 1.5372\n",
      "131/388, train_loss: 0.2294, step time: 1.5373\n",
      "132/388, train_loss: 0.5699, step time: 1.5341\n",
      "133/388, train_loss: 0.1127, step time: 1.5311\n",
      "134/388, train_loss: 0.1580, step time: 1.5312\n",
      "135/388, train_loss: 0.2731, step time: 1.5353\n",
      "136/388, train_loss: 0.1930, step time: 1.5299\n",
      "137/388, train_loss: 0.1901, step time: 1.5304\n",
      "138/388, train_loss: 0.2868, step time: 1.5307\n",
      "139/388, train_loss: 0.2236, step time: 1.5329\n",
      "140/388, train_loss: 0.0825, step time: 1.5366\n",
      "141/388, train_loss: 0.0992, step time: 1.5358\n",
      "142/388, train_loss: 0.3163, step time: 1.5333\n",
      "143/388, train_loss: 0.1545, step time: 1.5310\n",
      "144/388, train_loss: 0.2651, step time: 1.5327\n",
      "145/388, train_loss: 0.1843, step time: 1.5316\n",
      "146/388, train_loss: 0.2293, step time: 1.5323\n",
      "147/388, train_loss: 0.1130, step time: 1.5344\n",
      "148/388, train_loss: 0.2274, step time: 1.5344\n",
      "149/388, train_loss: 0.1580, step time: 1.5345\n",
      "150/388, train_loss: 0.0764, step time: 1.5363\n",
      "151/388, train_loss: 0.3069, step time: 1.5299\n",
      "152/388, train_loss: 0.2664, step time: 1.5343\n",
      "153/388, train_loss: 0.1061, step time: 1.5305\n",
      "154/388, train_loss: 0.3298, step time: 1.5354\n",
      "155/388, train_loss: 0.2799, step time: 1.5604\n",
      "156/388, train_loss: 0.2168, step time: 1.5310\n",
      "157/388, train_loss: 0.2392, step time: 1.5332\n",
      "158/388, train_loss: 0.2929, step time: 1.5341\n",
      "159/388, train_loss: 0.1847, step time: 1.5358\n",
      "160/388, train_loss: 0.1507, step time: 1.5371\n",
      "161/388, train_loss: 0.1497, step time: 1.5325\n",
      "162/388, train_loss: 0.7201, step time: 1.5350\n",
      "163/388, train_loss: 0.2462, step time: 1.5318\n",
      "164/388, train_loss: 0.6516, step time: 1.5323\n",
      "165/388, train_loss: 0.0862, step time: 1.5360\n",
      "166/388, train_loss: 0.1398, step time: 1.5367\n",
      "167/388, train_loss: 0.3420, step time: 1.5350\n",
      "168/388, train_loss: 0.0746, step time: 1.5316\n",
      "169/388, train_loss: 0.1933, step time: 1.5344\n",
      "170/388, train_loss: 0.1112, step time: 1.5315\n",
      "171/388, train_loss: 0.1983, step time: 1.5319\n",
      "172/388, train_loss: 0.1527, step time: 1.5422\n",
      "173/388, train_loss: 0.1411, step time: 1.5367\n",
      "174/388, train_loss: 0.2011, step time: 1.5318\n",
      "175/388, train_loss: 0.2507, step time: 1.5313\n",
      "176/388, train_loss: 0.2795, step time: 1.5358\n",
      "177/388, train_loss: 0.0800, step time: 1.5351\n",
      "178/388, train_loss: 0.1574, step time: 1.5362\n",
      "179/388, train_loss: 0.0685, step time: 1.5358\n",
      "180/388, train_loss: 0.1795, step time: 1.5336\n",
      "181/388, train_loss: 0.1035, step time: 1.5324\n",
      "182/388, train_loss: 0.3040, step time: 1.5319\n",
      "183/388, train_loss: 0.2404, step time: 1.5403\n",
      "184/388, train_loss: 0.0774, step time: 1.5341\n",
      "185/388, train_loss: 0.2141, step time: 1.5338\n",
      "186/388, train_loss: 0.1000, step time: 1.5338\n",
      "187/388, train_loss: 0.0676, step time: 1.5339\n",
      "188/388, train_loss: 0.1734, step time: 1.5321\n",
      "189/388, train_loss: 0.3336, step time: 1.5598\n",
      "190/388, train_loss: 0.0872, step time: 1.5444\n",
      "191/388, train_loss: 0.1160, step time: 1.5339\n",
      "192/388, train_loss: 0.1554, step time: 1.5315\n",
      "193/388, train_loss: 0.2884, step time: 1.5289\n",
      "194/388, train_loss: 0.1130, step time: 1.5330\n",
      "195/388, train_loss: 0.1652, step time: 1.5362\n",
      "196/388, train_loss: 0.2493, step time: 1.5337\n",
      "197/388, train_loss: 0.4594, step time: 1.5300\n",
      "198/388, train_loss: 0.0984, step time: 1.5325\n",
      "199/388, train_loss: 0.2270, step time: 1.5317\n",
      "200/388, train_loss: 0.1517, step time: 1.5662\n",
      "201/388, train_loss: 0.2132, step time: 1.5329\n",
      "202/388, train_loss: 0.2680, step time: 1.5319\n",
      "203/388, train_loss: 0.0690, step time: 1.5332\n",
      "204/388, train_loss: 0.3681, step time: 1.5347\n",
      "205/388, train_loss: 0.1193, step time: 1.5347\n",
      "206/388, train_loss: 0.2717, step time: 1.5311\n",
      "207/388, train_loss: 0.2979, step time: 1.5304\n",
      "208/388, train_loss: 0.2829, step time: 1.5315\n",
      "209/388, train_loss: 0.3089, step time: 1.5351\n",
      "210/388, train_loss: 0.2168, step time: 1.5366\n",
      "211/388, train_loss: 0.3055, step time: 1.5357\n",
      "212/388, train_loss: 0.3284, step time: 1.5336\n",
      "213/388, train_loss: 0.1426, step time: 1.5315\n",
      "214/388, train_loss: 0.1577, step time: 1.5355\n",
      "215/388, train_loss: 0.6399, step time: 1.5315\n",
      "216/388, train_loss: 0.1452, step time: 1.5325\n",
      "217/388, train_loss: 0.1022, step time: 1.5359\n",
      "218/388, train_loss: 0.1638, step time: 1.5375\n",
      "219/388, train_loss: 0.1760, step time: 1.5471\n",
      "220/388, train_loss: 0.1108, step time: 1.5289\n",
      "221/388, train_loss: 0.0923, step time: 1.5363\n",
      "222/388, train_loss: 0.2361, step time: 1.5370\n",
      "223/388, train_loss: 0.1445, step time: 1.5345\n",
      "224/388, train_loss: 0.1836, step time: 1.5337\n",
      "225/388, train_loss: 0.2098, step time: 1.5326\n",
      "226/388, train_loss: 0.3653, step time: 1.5310\n",
      "227/388, train_loss: 0.2252, step time: 1.5349\n",
      "228/388, train_loss: 0.0898, step time: 1.5400\n",
      "229/388, train_loss: 0.3528, step time: 1.5372\n",
      "230/388, train_loss: 0.0763, step time: 1.5369\n",
      "231/388, train_loss: 0.0657, step time: 1.5433\n",
      "232/388, train_loss: 0.1291, step time: 1.5413\n",
      "233/388, train_loss: 0.0744, step time: 1.5379\n",
      "234/388, train_loss: 0.2745, step time: 1.5365\n",
      "235/388, train_loss: 0.2363, step time: 1.5491\n",
      "236/388, train_loss: 0.2281, step time: 1.5346\n",
      "237/388, train_loss: 0.2423, step time: 1.5353\n",
      "238/388, train_loss: 0.1002, step time: 1.5376\n",
      "239/388, train_loss: 0.1574, step time: 1.5355\n",
      "240/388, train_loss: 0.1494, step time: 1.5337\n",
      "241/388, train_loss: 0.2747, step time: 1.5482\n",
      "242/388, train_loss: 0.1395, step time: 1.5377\n",
      "243/388, train_loss: 0.2143, step time: 1.5352\n",
      "244/388, train_loss: 0.2798, step time: 1.5358\n",
      "245/388, train_loss: 0.3269, step time: 1.5403\n",
      "246/388, train_loss: 0.1938, step time: 1.5314\n",
      "247/388, train_loss: 0.2026, step time: 1.5292\n",
      "248/388, train_loss: 0.2277, step time: 1.5325\n",
      "249/388, train_loss: 0.2299, step time: 1.5348\n",
      "250/388, train_loss: 0.2514, step time: 1.5390\n",
      "251/388, train_loss: 0.1992, step time: 1.5363\n",
      "252/388, train_loss: 0.3212, step time: 1.5436\n",
      "253/388, train_loss: 0.1799, step time: 1.5367\n",
      "254/388, train_loss: 0.1988, step time: 1.5379\n",
      "255/388, train_loss: 0.1920, step time: 1.5382\n",
      "256/388, train_loss: 0.2845, step time: 1.5357\n",
      "257/388, train_loss: 0.1003, step time: 1.5355\n",
      "258/388, train_loss: 0.2835, step time: 1.5389\n",
      "259/388, train_loss: 0.1691, step time: 1.5382\n",
      "260/388, train_loss: 0.2379, step time: 1.5384\n",
      "261/388, train_loss: 0.1438, step time: 1.5311\n",
      "262/388, train_loss: 0.0944, step time: 1.5313\n",
      "263/388, train_loss: 0.0886, step time: 1.5379\n",
      "264/388, train_loss: 0.3199, step time: 1.5355\n",
      "265/388, train_loss: 0.2713, step time: 1.5346\n",
      "266/388, train_loss: 0.0722, step time: 1.5335\n",
      "267/388, train_loss: 0.1299, step time: 1.5282\n",
      "268/388, train_loss: 0.0865, step time: 1.5299\n",
      "269/388, train_loss: 0.3409, step time: 1.5315\n",
      "270/388, train_loss: 0.1690, step time: 1.5447\n",
      "271/388, train_loss: 0.2513, step time: 1.5340\n",
      "272/388, train_loss: 0.3385, step time: 1.5331\n",
      "273/388, train_loss: 0.2151, step time: 1.5310\n",
      "274/388, train_loss: 0.0412, step time: 1.5326\n",
      "275/388, train_loss: 0.3315, step time: 1.5312\n",
      "276/388, train_loss: 0.1363, step time: 1.5380\n",
      "277/388, train_loss: 0.1876, step time: 1.5356\n",
      "278/388, train_loss: 0.4164, step time: 1.5329\n",
      "279/388, train_loss: 0.1402, step time: 1.5426\n",
      "280/388, train_loss: 0.2095, step time: 1.5343\n",
      "281/388, train_loss: 0.2242, step time: 1.5275\n",
      "282/388, train_loss: 0.3306, step time: 1.5454\n",
      "283/388, train_loss: 0.1806, step time: 1.5384\n",
      "284/388, train_loss: 0.1457, step time: 1.5319\n",
      "285/388, train_loss: 0.3888, step time: 1.5340\n",
      "286/388, train_loss: 0.1556, step time: 1.5417\n",
      "287/388, train_loss: 0.2348, step time: 1.5356\n",
      "288/388, train_loss: 0.2823, step time: 1.5348\n",
      "289/388, train_loss: 0.1238, step time: 1.5322\n",
      "290/388, train_loss: 0.0908, step time: 1.5328\n",
      "291/388, train_loss: 0.0858, step time: 1.5382\n",
      "292/388, train_loss: 0.3089, step time: 1.5375\n",
      "293/388, train_loss: 0.2093, step time: 1.5374\n",
      "294/388, train_loss: 0.3018, step time: 1.5361\n",
      "295/388, train_loss: 0.1771, step time: 1.5344\n",
      "296/388, train_loss: 0.0922, step time: 1.5351\n",
      "297/388, train_loss: 0.6124, step time: 1.5396\n",
      "298/388, train_loss: 0.3074, step time: 1.5354\n",
      "299/388, train_loss: 0.0846, step time: 1.5331\n",
      "300/388, train_loss: 0.1788, step time: 1.5458\n",
      "301/388, train_loss: 0.1125, step time: 1.5344\n",
      "302/388, train_loss: 0.4015, step time: 1.5351\n",
      "303/388, train_loss: 0.1372, step time: 1.5323\n",
      "304/388, train_loss: 0.1444, step time: 1.5415\n",
      "305/388, train_loss: 0.1850, step time: 1.5333\n",
      "306/388, train_loss: 0.1550, step time: 1.5669\n",
      "307/388, train_loss: 0.6865, step time: 1.5371\n",
      "308/388, train_loss: 0.4915, step time: 1.5411\n",
      "309/388, train_loss: 0.2220, step time: 1.5372\n",
      "310/388, train_loss: 0.1154, step time: 1.5371\n",
      "311/388, train_loss: 0.1360, step time: 1.5353\n",
      "312/388, train_loss: 0.2824, step time: 1.5385\n",
      "313/388, train_loss: 0.2619, step time: 1.5477\n",
      "314/388, train_loss: 0.7234, step time: 1.5488\n",
      "315/388, train_loss: 0.1528, step time: 1.5320\n",
      "316/388, train_loss: 0.3910, step time: 1.5313\n",
      "317/388, train_loss: 0.1370, step time: 1.5341\n",
      "318/388, train_loss: 0.1474, step time: 1.5337\n",
      "319/388, train_loss: 0.0455, step time: 1.5345\n",
      "320/388, train_loss: 0.2257, step time: 1.5397\n",
      "321/388, train_loss: 0.4849, step time: 1.5328\n",
      "322/388, train_loss: 0.2254, step time: 1.5385\n",
      "323/388, train_loss: 0.1641, step time: 1.5407\n",
      "324/388, train_loss: 0.3856, step time: 1.5347\n",
      "325/388, train_loss: 0.1543, step time: 1.5394\n",
      "326/388, train_loss: 0.1476, step time: 1.5285\n",
      "327/388, train_loss: 0.3056, step time: 1.5331\n",
      "328/388, train_loss: 0.1575, step time: 1.5435\n",
      "329/388, train_loss: 0.7492, step time: 1.5304\n",
      "330/388, train_loss: 0.1600, step time: 1.5358\n",
      "331/388, train_loss: 0.4943, step time: 1.5364\n",
      "332/388, train_loss: 0.2142, step time: 1.5333\n",
      "333/388, train_loss: 0.1963, step time: 1.5325\n",
      "334/388, train_loss: 0.2788, step time: 1.5385\n",
      "335/388, train_loss: 0.2603, step time: 1.5410\n",
      "336/388, train_loss: 0.5639, step time: 1.5358\n",
      "337/388, train_loss: 0.1248, step time: 1.5338\n",
      "338/388, train_loss: 0.3366, step time: 1.5360\n",
      "339/388, train_loss: 0.1937, step time: 1.5521\n",
      "340/388, train_loss: 0.1612, step time: 1.5303\n",
      "341/388, train_loss: 0.1404, step time: 1.5285\n",
      "342/388, train_loss: 0.2380, step time: 1.5354\n",
      "343/388, train_loss: 0.1618, step time: 1.5433\n",
      "344/388, train_loss: 0.1631, step time: 1.5362\n",
      "345/388, train_loss: 0.2134, step time: 1.5330\n",
      "346/388, train_loss: 0.0977, step time: 1.5330\n",
      "347/388, train_loss: 0.5988, step time: 1.5340\n",
      "348/388, train_loss: 0.2094, step time: 1.5334\n",
      "349/388, train_loss: 0.0528, step time: 1.5354\n",
      "350/388, train_loss: 0.0931, step time: 1.5373\n",
      "351/388, train_loss: 0.4415, step time: 1.5324\n",
      "352/388, train_loss: 0.1098, step time: 1.5411\n",
      "353/388, train_loss: 0.5886, step time: 1.5427\n",
      "354/388, train_loss: 0.2052, step time: 1.5326\n",
      "355/388, train_loss: 0.2899, step time: 1.5303\n",
      "356/388, train_loss: 0.1462, step time: 1.5327\n",
      "357/388, train_loss: 0.5501, step time: 1.5372\n",
      "358/388, train_loss: 0.4383, step time: 1.5364\n",
      "359/388, train_loss: 0.0633, step time: 1.5338\n",
      "360/388, train_loss: 0.4533, step time: 1.5343\n",
      "361/388, train_loss: 0.0441, step time: 1.5321\n",
      "362/388, train_loss: 0.1185, step time: 1.5344\n",
      "363/388, train_loss: 0.3411, step time: 1.5347\n",
      "364/388, train_loss: 0.3957, step time: 1.5359\n",
      "365/388, train_loss: 0.3434, step time: 1.5339\n",
      "366/388, train_loss: 0.2408, step time: 1.5362\n",
      "367/388, train_loss: 0.3062, step time: 1.5351\n",
      "368/388, train_loss: 0.1057, step time: 1.5300\n",
      "369/388, train_loss: 0.2250, step time: 1.5315\n",
      "370/388, train_loss: 0.0936, step time: 1.5344\n",
      "371/388, train_loss: 0.2411, step time: 1.5381\n",
      "372/388, train_loss: 0.1626, step time: 1.5343\n",
      "373/388, train_loss: 0.2036, step time: 1.5313\n",
      "374/388, train_loss: 0.1588, step time: 1.5315\n",
      "375/388, train_loss: 0.2866, step time: 1.5318\n",
      "376/388, train_loss: 0.4898, step time: 1.5399\n",
      "377/388, train_loss: 0.1914, step time: 1.5334\n",
      "378/388, train_loss: 0.2105, step time: 1.5325\n",
      "379/388, train_loss: 0.1929, step time: 1.5332\n",
      "380/388, train_loss: 0.1274, step time: 1.5446\n",
      "381/388, train_loss: 0.1045, step time: 1.5340\n",
      "382/388, train_loss: 0.1750, step time: 1.5361\n",
      "383/388, train_loss: 0.1044, step time: 1.5341\n",
      "384/388, train_loss: 0.3782, step time: 1.5349\n",
      "385/388, train_loss: 0.1182, step time: 1.5338\n",
      "386/388, train_loss: 0.2835, step time: 1.5314\n",
      "387/388, train_loss: 0.1265, step time: 1.5314\n",
      "388/388, train_loss: 0.1170, step time: 1.5325\n",
      "epoch 22 average loss: 0.2231\n",
      "current epoch: 22 current mean dice: 0.7197 tc: 0.7542 wt: 0.8862 et: 0.5186\n",
      "best mean dice: 0.7401 at epoch: 21\n",
      "time consuming of epoch 22 is: 701.4467\n",
      "----------\n",
      "epoch 23/100\n",
      "1/388, train_loss: 0.0967, step time: 1.5396\n",
      "2/388, train_loss: 0.1875, step time: 1.5329\n",
      "3/388, train_loss: 0.5198, step time: 1.5337\n",
      "4/388, train_loss: 0.3440, step time: 1.5298\n",
      "5/388, train_loss: 0.2052, step time: 1.5382\n",
      "6/388, train_loss: 0.0833, step time: 1.5308\n",
      "7/388, train_loss: 0.4843, step time: 1.5296\n",
      "8/388, train_loss: 0.1228, step time: 1.5351\n",
      "9/388, train_loss: 0.1044, step time: 1.5330\n",
      "10/388, train_loss: 0.1597, step time: 1.5308\n",
      "11/388, train_loss: 0.1180, step time: 1.5454\n",
      "12/388, train_loss: 0.0774, step time: 1.5339\n",
      "13/388, train_loss: 0.2448, step time: 1.5354\n",
      "14/388, train_loss: 0.1091, step time: 1.5338\n",
      "15/388, train_loss: 0.2739, step time: 1.5329\n",
      "16/388, train_loss: 0.1918, step time: 1.5327\n",
      "17/388, train_loss: 0.1378, step time: 1.5338\n",
      "18/388, train_loss: 0.1366, step time: 1.5333\n",
      "19/388, train_loss: 0.5670, step time: 1.5331\n",
      "20/388, train_loss: 0.2436, step time: 1.5406\n",
      "21/388, train_loss: 0.2117, step time: 1.5398\n",
      "22/388, train_loss: 0.0862, step time: 1.5375\n",
      "23/388, train_loss: 0.2113, step time: 1.5266\n",
      "24/388, train_loss: 0.1328, step time: 1.5376\n",
      "25/388, train_loss: 0.5063, step time: 1.5317\n",
      "26/388, train_loss: 0.3479, step time: 1.5346\n",
      "27/388, train_loss: 0.3362, step time: 1.5361\n",
      "28/388, train_loss: 0.2761, step time: 1.5330\n",
      "29/388, train_loss: 0.2887, step time: 1.5392\n",
      "30/388, train_loss: 0.2336, step time: 1.5370\n",
      "31/388, train_loss: 0.2363, step time: 1.5338\n",
      "32/388, train_loss: 0.1876, step time: 1.5330\n",
      "33/388, train_loss: 0.2623, step time: 1.5395\n",
      "34/388, train_loss: 0.0787, step time: 1.5375\n",
      "35/388, train_loss: 0.1928, step time: 1.5303\n",
      "36/388, train_loss: 0.5186, step time: 1.5311\n",
      "37/388, train_loss: 0.1189, step time: 1.5314\n",
      "38/388, train_loss: 0.1701, step time: 1.5348\n",
      "39/388, train_loss: 0.2632, step time: 1.5338\n",
      "40/388, train_loss: 0.2056, step time: 1.5380\n",
      "41/388, train_loss: 0.3353, step time: 1.5359\n",
      "42/388, train_loss: 0.1953, step time: 1.5326\n",
      "43/388, train_loss: 0.3504, step time: 1.5327\n",
      "44/388, train_loss: 0.3235, step time: 1.5320\n",
      "45/388, train_loss: 0.2891, step time: 1.5354\n",
      "46/388, train_loss: 0.5554, step time: 1.5350\n",
      "47/388, train_loss: 0.1478, step time: 1.5362\n",
      "48/388, train_loss: 0.1640, step time: 1.5307\n",
      "49/388, train_loss: 0.2325, step time: 1.5316\n",
      "50/388, train_loss: 0.1456, step time: 1.5354\n",
      "51/388, train_loss: 0.2551, step time: 1.5410\n",
      "52/388, train_loss: 0.1350, step time: 1.5424\n",
      "53/388, train_loss: 0.1365, step time: 1.5308\n",
      "54/388, train_loss: 0.0805, step time: 1.5301\n",
      "55/388, train_loss: 0.6776, step time: 1.5333\n",
      "56/388, train_loss: 0.1515, step time: 1.5398\n",
      "57/388, train_loss: 0.2061, step time: 1.5345\n",
      "58/388, train_loss: 0.1273, step time: 1.5357\n",
      "59/388, train_loss: 0.1027, step time: 1.5332\n",
      "60/388, train_loss: 0.0762, step time: 1.5329\n",
      "61/388, train_loss: 0.2530, step time: 1.5297\n",
      "62/388, train_loss: 0.3750, step time: 1.5336\n",
      "63/388, train_loss: 0.1889, step time: 1.5345\n",
      "64/388, train_loss: 0.2551, step time: 1.5371\n",
      "65/388, train_loss: 0.2579, step time: 1.5323\n",
      "66/388, train_loss: 0.1654, step time: 1.5376\n",
      "67/388, train_loss: 0.2455, step time: 1.5318\n",
      "68/388, train_loss: 0.1354, step time: 1.5330\n",
      "69/388, train_loss: 0.4569, step time: 1.5354\n",
      "70/388, train_loss: 0.2877, step time: 1.5341\n",
      "71/388, train_loss: 0.1185, step time: 1.5317\n",
      "72/388, train_loss: 0.1157, step time: 1.5346\n",
      "73/388, train_loss: 0.2670, step time: 1.5291\n",
      "74/388, train_loss: 0.2258, step time: 1.5335\n",
      "75/388, train_loss: 0.2649, step time: 1.5352\n",
      "76/388, train_loss: 0.1355, step time: 1.5376\n",
      "77/388, train_loss: 0.1583, step time: 1.5349\n",
      "78/388, train_loss: 0.3590, step time: 1.5357\n",
      "79/388, train_loss: 0.0528, step time: 1.5291\n",
      "80/388, train_loss: 0.1121, step time: 1.5332\n",
      "81/388, train_loss: 0.1769, step time: 1.5318\n",
      "82/388, train_loss: 0.2220, step time: 1.5369\n",
      "83/388, train_loss: 0.1321, step time: 1.5433\n",
      "84/388, train_loss: 0.1642, step time: 1.5327\n",
      "85/388, train_loss: 0.7882, step time: 1.5285\n",
      "86/388, train_loss: 0.4826, step time: 1.5317\n",
      "87/388, train_loss: 0.2341, step time: 1.5334\n",
      "88/388, train_loss: 0.3353, step time: 1.5333\n",
      "89/388, train_loss: 0.0929, step time: 1.5356\n",
      "90/388, train_loss: 0.0675, step time: 1.5370\n",
      "91/388, train_loss: 0.2656, step time: 1.5302\n",
      "92/388, train_loss: 0.1595, step time: 1.5309\n",
      "93/388, train_loss: 0.1134, step time: 1.5296\n",
      "94/388, train_loss: 0.2116, step time: 1.5323\n",
      "95/388, train_loss: 0.1229, step time: 1.5555\n",
      "96/388, train_loss: 0.1451, step time: 1.5328\n",
      "97/388, train_loss: 0.2222, step time: 1.5283\n",
      "98/388, train_loss: 0.0650, step time: 1.5301\n",
      "99/388, train_loss: 0.3484, step time: 1.5320\n",
      "100/388, train_loss: 0.1013, step time: 1.5494\n",
      "101/388, train_loss: 0.4665, step time: 1.5374\n",
      "102/388, train_loss: 0.1815, step time: 1.5345\n",
      "103/388, train_loss: 0.5823, step time: 1.5302\n",
      "104/388, train_loss: 0.2242, step time: 1.5326\n",
      "105/388, train_loss: 0.3055, step time: 1.5327\n",
      "106/388, train_loss: 0.1858, step time: 1.5355\n",
      "107/388, train_loss: 0.0677, step time: 1.5319\n",
      "108/388, train_loss: 0.1160, step time: 1.5371\n",
      "109/388, train_loss: 0.3829, step time: 1.5340\n",
      "110/388, train_loss: 0.1474, step time: 1.5445\n",
      "111/388, train_loss: 0.5290, step time: 1.5371\n",
      "112/388, train_loss: 0.1846, step time: 1.5342\n",
      "113/388, train_loss: 0.1629, step time: 1.5308\n",
      "114/388, train_loss: 0.5239, step time: 1.5314\n",
      "115/388, train_loss: 0.1751, step time: 1.5312\n",
      "116/388, train_loss: 0.1365, step time: 1.5336\n",
      "117/388, train_loss: 0.2240, step time: 1.5349\n",
      "118/388, train_loss: 0.2745, step time: 1.5340\n",
      "119/388, train_loss: 0.3485, step time: 1.5293\n",
      "120/388, train_loss: 0.5585, step time: 1.5336\n",
      "121/388, train_loss: 0.1980, step time: 1.5346\n",
      "122/388, train_loss: 0.1711, step time: 1.5340\n",
      "123/388, train_loss: 0.1174, step time: 1.5362\n",
      "124/388, train_loss: 0.3500, step time: 1.5344\n",
      "125/388, train_loss: 0.1031, step time: 1.5332\n",
      "126/388, train_loss: 0.1011, step time: 1.5308\n",
      "127/388, train_loss: 0.0417, step time: 1.5324\n",
      "128/388, train_loss: 0.4200, step time: 1.5336\n",
      "129/388, train_loss: 0.7223, step time: 1.5357\n",
      "130/388, train_loss: 0.1295, step time: 1.5327\n",
      "131/388, train_loss: 0.1495, step time: 1.5359\n",
      "132/388, train_loss: 0.1770, step time: 1.5336\n",
      "133/388, train_loss: 0.2382, step time: 1.5357\n",
      "134/388, train_loss: 0.4526, step time: 1.5366\n",
      "135/388, train_loss: 0.1066, step time: 1.5382\n",
      "136/388, train_loss: 0.2747, step time: 1.5354\n",
      "137/388, train_loss: 0.1107, step time: 1.5315\n",
      "138/388, train_loss: 0.2275, step time: 1.5318\n",
      "139/388, train_loss: 0.0989, step time: 1.5309\n",
      "140/388, train_loss: 0.1669, step time: 1.5292\n",
      "141/388, train_loss: 0.3036, step time: 1.5320\n",
      "142/388, train_loss: 0.0758, step time: 1.5449\n",
      "143/388, train_loss: 0.1469, step time: 1.5340\n",
      "144/388, train_loss: 0.1190, step time: 1.5275\n",
      "145/388, train_loss: 0.2056, step time: 1.5283\n",
      "146/388, train_loss: 0.1773, step time: 1.5362\n",
      "147/388, train_loss: 0.4884, step time: 1.5319\n",
      "148/388, train_loss: 0.1528, step time: 1.5314\n",
      "149/388, train_loss: 0.2484, step time: 1.5278\n",
      "150/388, train_loss: 0.2324, step time: 1.5327\n",
      "151/388, train_loss: 0.3903, step time: 1.5353\n",
      "152/388, train_loss: 0.1569, step time: 1.5352\n",
      "153/388, train_loss: 0.1732, step time: 1.5331\n",
      "154/388, train_loss: 0.2477, step time: 1.5336\n",
      "155/388, train_loss: 0.5325, step time: 1.5295\n",
      "156/388, train_loss: 0.2825, step time: 1.5315\n",
      "157/388, train_loss: 0.1092, step time: 1.5329\n",
      "158/388, train_loss: 0.1399, step time: 1.5320\n",
      "159/388, train_loss: 0.0899, step time: 1.5331\n",
      "160/388, train_loss: 0.1920, step time: 1.5345\n",
      "161/388, train_loss: 0.3915, step time: 1.5330\n",
      "162/388, train_loss: 0.2401, step time: 1.5298\n",
      "163/388, train_loss: 0.3239, step time: 1.5319\n",
      "164/388, train_loss: 0.2528, step time: 1.5331\n",
      "165/388, train_loss: 0.2215, step time: 1.5322\n",
      "166/388, train_loss: 0.3491, step time: 1.5323\n",
      "167/388, train_loss: 0.3845, step time: 1.5365\n",
      "168/388, train_loss: 0.3214, step time: 1.5301\n",
      "169/388, train_loss: 0.2719, step time: 1.5298\n",
      "170/388, train_loss: 0.4102, step time: 1.5614\n",
      "171/388, train_loss: 0.2794, step time: 1.5424\n",
      "172/388, train_loss: 0.1434, step time: 1.5302\n",
      "173/388, train_loss: 0.5753, step time: 1.5316\n",
      "174/388, train_loss: 0.1880, step time: 1.5430\n",
      "175/388, train_loss: 0.2323, step time: 1.5337\n",
      "176/388, train_loss: 0.2976, step time: 1.5332\n",
      "177/388, train_loss: 0.2361, step time: 1.5327\n",
      "178/388, train_loss: 0.1082, step time: 1.5356\n",
      "179/388, train_loss: 0.3410, step time: 1.5353\n",
      "180/388, train_loss: 0.1268, step time: 1.5364\n",
      "181/388, train_loss: 0.3366, step time: 1.5331\n",
      "182/388, train_loss: 0.3251, step time: 1.5424\n",
      "183/388, train_loss: 0.1734, step time: 1.5321\n",
      "184/388, train_loss: 0.1359, step time: 1.5367\n",
      "185/388, train_loss: 0.2351, step time: 1.5335\n",
      "186/388, train_loss: 0.1669, step time: 1.5332\n",
      "187/388, train_loss: 0.1975, step time: 1.5359\n",
      "188/388, train_loss: 0.1322, step time: 1.5336\n",
      "189/388, train_loss: 0.2553, step time: 1.5355\n",
      "190/388, train_loss: 0.1016, step time: 1.5366\n",
      "191/388, train_loss: 0.1730, step time: 1.5349\n",
      "192/388, train_loss: 0.1796, step time: 1.5323\n",
      "193/388, train_loss: 0.3201, step time: 1.5324\n",
      "194/388, train_loss: 0.0977, step time: 1.5328\n",
      "195/388, train_loss: 0.2497, step time: 1.5291\n",
      "196/388, train_loss: 0.3634, step time: 1.5316\n",
      "197/388, train_loss: 0.1705, step time: 1.5348\n",
      "198/388, train_loss: 0.1286, step time: 1.5363\n",
      "199/388, train_loss: 0.2088, step time: 1.5340\n",
      "200/388, train_loss: 0.2563, step time: 1.5306\n",
      "201/388, train_loss: 0.1126, step time: 1.5320\n",
      "202/388, train_loss: 0.1498, step time: 1.5302\n",
      "203/388, train_loss: 0.2822, step time: 1.5313\n",
      "204/388, train_loss: 0.2327, step time: 1.5356\n",
      "205/388, train_loss: 0.1592, step time: 1.5330\n",
      "206/388, train_loss: 0.1033, step time: 1.5372\n",
      "207/388, train_loss: 0.0597, step time: 1.5334\n",
      "208/388, train_loss: 0.2062, step time: 1.5358\n",
      "209/388, train_loss: 0.3398, step time: 1.5315\n",
      "210/388, train_loss: 0.0302, step time: 1.5296\n",
      "211/388, train_loss: 0.3198, step time: 1.5321\n",
      "212/388, train_loss: 0.2200, step time: 1.5365\n",
      "213/388, train_loss: 0.3339, step time: 1.5338\n",
      "214/388, train_loss: 0.2269, step time: 1.5307\n",
      "215/388, train_loss: 0.0711, step time: 1.5314\n",
      "216/388, train_loss: 0.6433, step time: 1.5325\n",
      "217/388, train_loss: 0.2536, step time: 1.5446\n",
      "218/388, train_loss: 0.2726, step time: 1.5334\n",
      "219/388, train_loss: 0.1658, step time: 1.5297\n",
      "220/388, train_loss: 0.3667, step time: 1.5331\n",
      "221/388, train_loss: 0.6397, step time: 1.5322\n",
      "222/388, train_loss: 0.1415, step time: 1.5336\n",
      "223/388, train_loss: 0.2151, step time: 1.5357\n",
      "224/388, train_loss: 0.2739, step time: 1.5350\n",
      "225/388, train_loss: 0.1160, step time: 1.5337\n",
      "226/388, train_loss: 0.2694, step time: 1.5315\n",
      "227/388, train_loss: 0.0938, step time: 1.5306\n",
      "228/388, train_loss: 0.1988, step time: 1.5308\n",
      "229/388, train_loss: 0.2900, step time: 1.5335\n",
      "230/388, train_loss: 0.2564, step time: 1.5374\n",
      "231/388, train_loss: 0.1110, step time: 1.5356\n",
      "232/388, train_loss: 0.0798, step time: 1.5330\n",
      "233/388, train_loss: 0.1133, step time: 1.5400\n",
      "234/388, train_loss: 0.2561, step time: 1.5330\n",
      "235/388, train_loss: 0.1310, step time: 1.5316\n",
      "236/388, train_loss: 0.0450, step time: 1.5474\n",
      "237/388, train_loss: 0.1898, step time: 1.5318\n",
      "238/388, train_loss: 0.5507, step time: 1.5307\n",
      "239/388, train_loss: 0.2128, step time: 1.5303\n",
      "240/388, train_loss: 0.2292, step time: 1.5309\n",
      "241/388, train_loss: 0.3719, step time: 1.5365\n",
      "242/388, train_loss: 0.1714, step time: 1.5362\n",
      "243/388, train_loss: 0.0958, step time: 1.5361\n",
      "244/388, train_loss: 0.1180, step time: 1.5327\n",
      "245/388, train_loss: 0.3323, step time: 1.5335\n",
      "246/388, train_loss: 0.2048, step time: 1.5352\n",
      "247/388, train_loss: 0.1681, step time: 1.5391\n",
      "248/388, train_loss: 0.2127, step time: 1.5347\n",
      "249/388, train_loss: 0.2033, step time: 1.5320\n",
      "250/388, train_loss: 0.3219, step time: 1.5301\n",
      "251/388, train_loss: 0.1140, step time: 1.5313\n",
      "252/388, train_loss: 0.3340, step time: 1.5325\n",
      "253/388, train_loss: 0.2540, step time: 1.5351\n",
      "254/388, train_loss: 0.1557, step time: 1.5378\n",
      "255/388, train_loss: 0.3419, step time: 1.5353\n",
      "256/388, train_loss: 0.3331, step time: 1.5331\n",
      "257/388, train_loss: 0.1254, step time: 1.5334\n",
      "258/388, train_loss: 0.1503, step time: 1.5323\n",
      "259/388, train_loss: 0.1023, step time: 1.5341\n",
      "260/388, train_loss: 0.1711, step time: 1.5369\n",
      "261/388, train_loss: 0.2070, step time: 1.5349\n",
      "262/388, train_loss: 0.0776, step time: 1.5390\n",
      "263/388, train_loss: 0.1424, step time: 1.5304\n",
      "264/388, train_loss: 0.1086, step time: 1.5473\n",
      "265/388, train_loss: 0.1824, step time: 1.5344\n",
      "266/388, train_loss: 0.0722, step time: 1.5373\n",
      "267/388, train_loss: 0.1258, step time: 1.5358\n",
      "268/388, train_loss: 0.0700, step time: 1.5326\n",
      "269/388, train_loss: 0.1358, step time: 1.5345\n",
      "270/388, train_loss: 0.3195, step time: 1.5310\n",
      "271/388, train_loss: 0.4053, step time: 1.5318\n",
      "272/388, train_loss: 0.0947, step time: 1.5375\n",
      "273/388, train_loss: 0.2341, step time: 1.5357\n",
      "274/388, train_loss: 0.3595, step time: 1.5326\n",
      "275/388, train_loss: 0.2445, step time: 1.5323\n",
      "276/388, train_loss: 0.2520, step time: 1.5297\n",
      "277/388, train_loss: 0.1023, step time: 1.5301\n",
      "278/388, train_loss: 0.2133, step time: 1.5332\n",
      "279/388, train_loss: 0.4660, step time: 1.5353\n",
      "280/388, train_loss: 0.3497, step time: 1.5366\n",
      "281/388, train_loss: 0.2218, step time: 1.5317\n",
      "282/388, train_loss: 0.1548, step time: 1.5312\n",
      "283/388, train_loss: 0.1626, step time: 1.5303\n",
      "284/388, train_loss: 0.1041, step time: 1.5354\n",
      "285/388, train_loss: 0.1428, step time: 1.5372\n",
      "286/388, train_loss: 0.2030, step time: 1.5304\n",
      "287/388, train_loss: 0.1366, step time: 1.5331\n",
      "288/388, train_loss: 0.0994, step time: 1.5331\n",
      "289/388, train_loss: 0.2543, step time: 1.5318\n",
      "290/388, train_loss: 0.1814, step time: 1.5370\n",
      "291/388, train_loss: 0.2884, step time: 1.5351\n",
      "292/388, train_loss: 0.0954, step time: 1.5323\n",
      "293/388, train_loss: 0.5331, step time: 1.5301\n",
      "294/388, train_loss: 0.1346, step time: 1.5337\n",
      "295/388, train_loss: 0.2688, step time: 1.5345\n",
      "296/388, train_loss: 0.2795, step time: 1.5373\n",
      "297/388, train_loss: 0.3821, step time: 1.5331\n",
      "298/388, train_loss: 0.1854, step time: 1.5347\n",
      "299/388, train_loss: 0.1406, step time: 1.5310\n",
      "300/388, train_loss: 0.1773, step time: 1.5331\n",
      "301/388, train_loss: 0.1417, step time: 1.5353\n",
      "302/388, train_loss: 0.2003, step time: 1.5322\n",
      "303/388, train_loss: 0.2648, step time: 1.5288\n",
      "304/388, train_loss: 0.0713, step time: 1.5331\n",
      "305/388, train_loss: 0.2390, step time: 1.5320\n",
      "306/388, train_loss: 0.3132, step time: 1.5321\n",
      "307/388, train_loss: 0.3400, step time: 1.5340\n",
      "308/388, train_loss: 0.3629, step time: 1.5352\n",
      "309/388, train_loss: 0.1974, step time: 1.5336\n",
      "310/388, train_loss: 0.1563, step time: 1.5338\n",
      "311/388, train_loss: 0.1169, step time: 1.5313\n",
      "312/388, train_loss: 0.6730, step time: 1.5305\n",
      "313/388, train_loss: 0.3162, step time: 1.5338\n",
      "314/388, train_loss: 0.2098, step time: 1.5451\n",
      "315/388, train_loss: 0.1457, step time: 1.5342\n",
      "316/388, train_loss: 0.1521, step time: 1.5335\n",
      "317/388, train_loss: 0.1755, step time: 1.5300\n",
      "318/388, train_loss: 0.1808, step time: 1.5317\n",
      "319/388, train_loss: 0.0740, step time: 1.5310\n",
      "320/388, train_loss: 0.1963, step time: 1.5323\n",
      "321/388, train_loss: 0.0867, step time: 1.5358\n",
      "322/388, train_loss: 0.1238, step time: 1.5342\n",
      "323/388, train_loss: 0.1429, step time: 1.5351\n",
      "324/388, train_loss: 0.3523, step time: 1.5332\n",
      "325/388, train_loss: 0.0987, step time: 1.5314\n",
      "326/388, train_loss: 0.0972, step time: 1.5314\n",
      "327/388, train_loss: 0.1579, step time: 1.5313\n",
      "328/388, train_loss: 0.1953, step time: 1.5303\n",
      "329/388, train_loss: 0.2301, step time: 1.5401\n",
      "330/388, train_loss: 0.2762, step time: 1.5378\n",
      "331/388, train_loss: 0.4681, step time: 1.5340\n",
      "332/388, train_loss: 0.3338, step time: 1.5306\n",
      "333/388, train_loss: 0.0975, step time: 1.5349\n",
      "334/388, train_loss: 0.0798, step time: 1.5312\n",
      "335/388, train_loss: 0.1537, step time: 1.5336\n",
      "336/388, train_loss: 0.4740, step time: 1.5442\n",
      "337/388, train_loss: 0.1273, step time: 1.5360\n",
      "338/388, train_loss: 0.1574, step time: 1.5295\n",
      "339/388, train_loss: 0.0982, step time: 1.5307\n",
      "340/388, train_loss: 0.3073, step time: 1.5282\n",
      "341/388, train_loss: 0.2211, step time: 1.5334\n",
      "342/388, train_loss: 0.1870, step time: 1.5339\n",
      "343/388, train_loss: 0.0757, step time: 1.5357\n",
      "344/388, train_loss: 0.0909, step time: 1.5369\n",
      "345/388, train_loss: 0.2013, step time: 1.5342\n",
      "346/388, train_loss: 0.0662, step time: 1.5311\n",
      "347/388, train_loss: 0.3326, step time: 1.5318\n",
      "348/388, train_loss: 0.1252, step time: 1.5483\n",
      "349/388, train_loss: 0.1619, step time: 1.5339\n",
      "350/388, train_loss: 0.2822, step time: 1.5350\n",
      "351/388, train_loss: 0.1694, step time: 1.5369\n",
      "352/388, train_loss: 0.1641, step time: 1.5322\n",
      "353/388, train_loss: 0.5352, step time: 1.5319\n",
      "354/388, train_loss: 0.1165, step time: 1.5294\n",
      "355/388, train_loss: 0.1850, step time: 1.5408\n",
      "356/388, train_loss: 0.2709, step time: 1.5372\n",
      "357/388, train_loss: 0.1505, step time: 1.5407\n",
      "358/388, train_loss: 0.1689, step time: 1.5368\n",
      "359/388, train_loss: 0.0869, step time: 1.5314\n",
      "360/388, train_loss: 0.2047, step time: 1.5326\n",
      "361/388, train_loss: 0.1293, step time: 1.5359\n",
      "362/388, train_loss: 0.2201, step time: 1.5305\n",
      "363/388, train_loss: 0.0856, step time: 1.5341\n",
      "364/388, train_loss: 0.3195, step time: 1.5332\n",
      "365/388, train_loss: 0.1936, step time: 1.5340\n",
      "366/388, train_loss: 0.1712, step time: 1.5288\n",
      "367/388, train_loss: 0.1659, step time: 1.5306\n",
      "368/388, train_loss: 0.2220, step time: 1.5303\n",
      "369/388, train_loss: 0.3350, step time: 1.5296\n",
      "370/388, train_loss: 0.1122, step time: 1.5327\n",
      "371/388, train_loss: 0.2108, step time: 1.5373\n",
      "372/388, train_loss: 0.1969, step time: 1.5350\n",
      "373/388, train_loss: 0.1490, step time: 1.5302\n",
      "374/388, train_loss: 0.0458, step time: 1.5335\n",
      "375/388, train_loss: 0.1917, step time: 1.5314\n",
      "376/388, train_loss: 0.1104, step time: 1.5338\n",
      "377/388, train_loss: 0.4226, step time: 1.5298\n",
      "378/388, train_loss: 0.1468, step time: 1.5340\n",
      "379/388, train_loss: 0.1886, step time: 1.5350\n",
      "380/388, train_loss: 0.1315, step time: 1.5373\n",
      "381/388, train_loss: 0.1978, step time: 1.5341\n",
      "382/388, train_loss: 0.0727, step time: 1.5319\n",
      "383/388, train_loss: 0.0984, step time: 1.5306\n",
      "384/388, train_loss: 0.0794, step time: 1.5329\n",
      "385/388, train_loss: 0.1525, step time: 1.5378\n",
      "386/388, train_loss: 0.4057, step time: 1.5342\n",
      "387/388, train_loss: 0.0799, step time: 1.5340\n",
      "388/388, train_loss: 0.6446, step time: 1.5356\n",
      "epoch 23 average loss: 0.2236\n",
      "saved new best metric model\n",
      "current epoch: 23 current mean dice: 0.7451 tc: 0.7965 wt: 0.8929 et: 0.5459\n",
      "best mean dice: 0.7451 at epoch: 23\n",
      "time consuming of epoch 23 is: 702.4054\n",
      "----------\n",
      "epoch 24/100\n",
      "1/388, train_loss: 0.2310, step time: 1.5483\n",
      "2/388, train_loss: 0.0680, step time: 1.5333\n",
      "3/388, train_loss: 0.3011, step time: 1.5390\n",
      "4/388, train_loss: 0.1837, step time: 1.5353\n",
      "5/388, train_loss: 0.2578, step time: 1.5368\n",
      "6/388, train_loss: 0.1536, step time: 1.5367\n",
      "7/388, train_loss: 0.1197, step time: 1.5312\n",
      "8/388, train_loss: 0.1605, step time: 1.5348\n",
      "9/388, train_loss: 0.2465, step time: 1.5304\n",
      "10/388, train_loss: 0.1703, step time: 1.5333\n",
      "11/388, train_loss: 0.2992, step time: 1.5363\n",
      "12/388, train_loss: 0.1651, step time: 1.5409\n",
      "13/388, train_loss: 0.3318, step time: 1.5325\n",
      "14/388, train_loss: 0.1735, step time: 1.5337\n",
      "15/388, train_loss: 0.1061, step time: 1.5316\n",
      "16/388, train_loss: 0.0937, step time: 1.5352\n",
      "17/388, train_loss: 0.1049, step time: 1.5346\n",
      "18/388, train_loss: 0.1523, step time: 1.5390\n",
      "19/388, train_loss: 0.1519, step time: 1.5318\n",
      "20/388, train_loss: 0.4915, step time: 1.5353\n",
      "21/388, train_loss: 0.2060, step time: 1.5332\n",
      "22/388, train_loss: 0.1066, step time: 1.5330\n",
      "23/388, train_loss: 0.1523, step time: 1.5363\n",
      "24/388, train_loss: 0.0616, step time: 1.5357\n",
      "25/388, train_loss: 0.0802, step time: 1.5357\n",
      "26/388, train_loss: 0.1726, step time: 1.5328\n",
      "27/388, train_loss: 0.1207, step time: 1.5384\n",
      "28/388, train_loss: 0.1188, step time: 1.5363\n",
      "29/388, train_loss: 0.1838, step time: 1.5315\n",
      "30/388, train_loss: 0.2520, step time: 1.5367\n",
      "31/388, train_loss: 0.1818, step time: 1.5353\n",
      "32/388, train_loss: 0.1006, step time: 1.5369\n",
      "33/388, train_loss: 0.1451, step time: 1.5340\n",
      "34/388, train_loss: 0.1252, step time: 1.5357\n",
      "35/388, train_loss: 0.5485, step time: 1.5382\n",
      "36/388, train_loss: 0.1117, step time: 1.5351\n",
      "37/388, train_loss: 0.3301, step time: 1.5387\n",
      "38/388, train_loss: 0.1504, step time: 1.5373\n",
      "39/388, train_loss: 0.1627, step time: 1.5339\n",
      "40/388, train_loss: 0.3917, step time: 1.5297\n",
      "41/388, train_loss: 0.3032, step time: 1.5282\n",
      "42/388, train_loss: 0.1972, step time: 1.5350\n",
      "43/388, train_loss: 0.5883, step time: 1.5353\n",
      "44/388, train_loss: 0.1645, step time: 1.5379\n",
      "45/388, train_loss: 0.1874, step time: 1.5357\n",
      "46/388, train_loss: 0.1647, step time: 1.5344\n",
      "47/388, train_loss: 0.3466, step time: 1.5307\n",
      "48/388, train_loss: 0.6046, step time: 1.5337\n",
      "49/388, train_loss: 0.2323, step time: 1.5353\n",
      "50/388, train_loss: 0.1818, step time: 1.5366\n",
      "51/388, train_loss: 0.0984, step time: 1.5341\n",
      "52/388, train_loss: 0.1455, step time: 1.5383\n",
      "53/388, train_loss: 0.3126, step time: 1.5345\n",
      "54/388, train_loss: 0.3524, step time: 1.5352\n",
      "55/388, train_loss: 0.0574, step time: 1.5300\n",
      "56/388, train_loss: 0.2211, step time: 1.5364\n",
      "57/388, train_loss: 0.5952, step time: 1.5377\n",
      "58/388, train_loss: 0.1033, step time: 1.5390\n",
      "59/388, train_loss: 0.1202, step time: 1.5351\n",
      "60/388, train_loss: 0.2208, step time: 1.5327\n",
      "61/388, train_loss: 0.0325, step time: 1.5310\n",
      "62/388, train_loss: 0.1075, step time: 1.5318\n",
      "63/388, train_loss: 0.5064, step time: 1.5370\n",
      "64/388, train_loss: 0.1401, step time: 1.5334\n",
      "65/388, train_loss: 0.1102, step time: 1.5343\n",
      "66/388, train_loss: 0.2406, step time: 1.5314\n",
      "67/388, train_loss: 0.3488, step time: 1.5335\n",
      "68/388, train_loss: 0.1075, step time: 1.5347\n",
      "69/388, train_loss: 0.0800, step time: 1.5346\n",
      "70/388, train_loss: 0.2201, step time: 1.5367\n",
      "71/388, train_loss: 0.0366, step time: 1.5343\n",
      "72/388, train_loss: 0.2811, step time: 1.5332\n",
      "73/388, train_loss: 0.1080, step time: 1.5328\n",
      "74/388, train_loss: 0.1428, step time: 1.5321\n",
      "75/388, train_loss: 0.2091, step time: 1.5368\n",
      "76/388, train_loss: 0.1296, step time: 1.5381\n",
      "77/388, train_loss: 0.1275, step time: 1.5325\n",
      "78/388, train_loss: 0.2248, step time: 1.5409\n",
      "79/388, train_loss: 0.1139, step time: 1.5324\n",
      "80/388, train_loss: 0.2616, step time: 1.5360\n",
      "81/388, train_loss: 0.1362, step time: 1.5318\n",
      "82/388, train_loss: 0.1062, step time: 1.5329\n",
      "83/388, train_loss: 0.1008, step time: 1.5336\n",
      "84/388, train_loss: 0.1506, step time: 1.5381\n",
      "85/388, train_loss: 0.3824, step time: 1.5367\n",
      "86/388, train_loss: 0.1852, step time: 1.5399\n",
      "87/388, train_loss: 0.2826, step time: 1.5324\n",
      "88/388, train_loss: 0.0558, step time: 1.5307\n",
      "89/388, train_loss: 0.2773, step time: 1.5328\n",
      "90/388, train_loss: 0.2133, step time: 1.5411\n",
      "91/388, train_loss: 0.2258, step time: 1.5318\n",
      "92/388, train_loss: 0.1531, step time: 1.5344\n",
      "93/388, train_loss: 0.2539, step time: 1.5323\n",
      "94/388, train_loss: 0.3369, step time: 1.5356\n",
      "95/388, train_loss: 0.2114, step time: 1.5402\n",
      "96/388, train_loss: 0.2184, step time: 1.5384\n",
      "97/388, train_loss: 0.1965, step time: 1.5385\n",
      "98/388, train_loss: 0.1252, step time: 1.5360\n",
      "99/388, train_loss: 0.2994, step time: 1.5371\n",
      "100/388, train_loss: 0.1741, step time: 1.5325\n",
      "101/388, train_loss: 0.1576, step time: 1.5359\n",
      "102/388, train_loss: 0.1671, step time: 1.5333\n",
      "103/388, train_loss: 0.4168, step time: 1.5393\n",
      "104/388, train_loss: 0.4580, step time: 1.5478\n",
      "105/388, train_loss: 0.0809, step time: 1.5325\n",
      "106/388, train_loss: 0.7641, step time: 1.5338\n",
      "107/388, train_loss: 0.2622, step time: 1.5321\n",
      "108/388, train_loss: 0.0861, step time: 1.5403\n",
      "109/388, train_loss: 0.2649, step time: 1.5361\n",
      "110/388, train_loss: 0.1471, step time: 1.5344\n",
      "111/388, train_loss: 0.1627, step time: 1.5307\n",
      "112/388, train_loss: 0.2334, step time: 1.5376\n",
      "113/388, train_loss: 0.1539, step time: 1.5310\n",
      "114/388, train_loss: 0.1356, step time: 1.5344\n",
      "115/388, train_loss: 0.2439, step time: 1.5381\n",
      "116/388, train_loss: 0.0979, step time: 1.5404\n",
      "117/388, train_loss: 0.1814, step time: 1.5429\n",
      "118/388, train_loss: 0.1270, step time: 1.5347\n",
      "119/388, train_loss: 0.4411, step time: 1.5376\n",
      "120/388, train_loss: 0.2269, step time: 1.5363\n",
      "121/388, train_loss: 0.1590, step time: 1.5357\n",
      "122/388, train_loss: 0.0711, step time: 1.5333\n",
      "123/388, train_loss: 0.1698, step time: 1.5325\n",
      "124/388, train_loss: 0.2173, step time: 1.5310\n",
      "125/388, train_loss: 0.4178, step time: 1.5308\n",
      "126/388, train_loss: 0.6381, step time: 1.5344\n",
      "127/388, train_loss: 0.1052, step time: 1.5393\n",
      "128/388, train_loss: 0.2997, step time: 1.5436\n",
      "129/388, train_loss: 0.1075, step time: 1.5323\n",
      "130/388, train_loss: 0.1049, step time: 1.5336\n",
      "131/388, train_loss: 0.2505, step time: 1.5344\n",
      "132/388, train_loss: 0.0994, step time: 1.5345\n",
      "133/388, train_loss: 0.3938, step time: 1.5501\n",
      "134/388, train_loss: 0.3269, step time: 1.5322\n",
      "135/388, train_loss: 0.2586, step time: 1.5316\n",
      "136/388, train_loss: 0.2209, step time: 1.5303\n",
      "137/388, train_loss: 0.2104, step time: 1.5330\n",
      "138/388, train_loss: 0.0728, step time: 1.5363\n",
      "139/388, train_loss: 0.0685, step time: 1.5437\n",
      "140/388, train_loss: 0.2847, step time: 1.5336\n",
      "141/388, train_loss: 0.1355, step time: 1.5322\n",
      "142/388, train_loss: 0.1446, step time: 1.5351\n",
      "143/388, train_loss: 0.1130, step time: 1.5351\n",
      "144/388, train_loss: 0.1998, step time: 1.5451\n",
      "145/388, train_loss: 0.3290, step time: 1.5328\n",
      "146/388, train_loss: 0.6743, step time: 1.5309\n",
      "147/388, train_loss: 0.1100, step time: 1.5309\n",
      "148/388, train_loss: 0.1571, step time: 1.5323\n",
      "149/388, train_loss: 0.3430, step time: 1.5349\n",
      "150/388, train_loss: 0.5021, step time: 1.5375\n",
      "151/388, train_loss: 0.0373, step time: 1.5451\n",
      "152/388, train_loss: 0.1680, step time: 1.5310\n",
      "153/388, train_loss: 0.1179, step time: 1.5320\n",
      "154/388, train_loss: 0.3329, step time: 1.5321\n",
      "155/388, train_loss: 0.1531, step time: 1.5365\n",
      "156/388, train_loss: 0.1986, step time: 1.5368\n",
      "157/388, train_loss: 0.0875, step time: 1.5361\n",
      "158/388, train_loss: 0.3552, step time: 1.5345\n",
      "159/388, train_loss: 0.3278, step time: 1.5328\n",
      "160/388, train_loss: 0.3780, step time: 1.5342\n",
      "161/388, train_loss: 0.2674, step time: 1.5398\n",
      "162/388, train_loss: 0.1091, step time: 1.5358\n",
      "163/388, train_loss: 0.1031, step time: 1.5334\n",
      "164/388, train_loss: 0.1489, step time: 1.5332\n",
      "165/388, train_loss: 0.5427, step time: 1.5326\n",
      "166/388, train_loss: 0.2891, step time: 1.5354\n",
      "167/388, train_loss: 0.3789, step time: 1.5354\n",
      "168/388, train_loss: 0.7311, step time: 1.5356\n",
      "169/388, train_loss: 0.1822, step time: 1.5370\n",
      "170/388, train_loss: 0.3279, step time: 1.5406\n",
      "171/388, train_loss: 0.1633, step time: 1.5364\n",
      "172/388, train_loss: 0.2411, step time: 1.5366\n",
      "173/388, train_loss: 0.2600, step time: 1.5374\n",
      "174/388, train_loss: 0.0830, step time: 1.5325\n",
      "175/388, train_loss: 0.1638, step time: 1.5317\n",
      "176/388, train_loss: 0.1083, step time: 1.5303\n",
      "177/388, train_loss: 0.1624, step time: 1.5316\n",
      "178/388, train_loss: 0.2097, step time: 1.5354\n",
      "179/388, train_loss: 0.1522, step time: 1.5341\n",
      "180/388, train_loss: 0.1771, step time: 1.5331\n",
      "181/388, train_loss: 0.1427, step time: 1.5315\n",
      "182/388, train_loss: 0.1133, step time: 1.5316\n",
      "183/388, train_loss: 0.2065, step time: 1.5335\n",
      "184/388, train_loss: 0.1459, step time: 1.5314\n",
      "185/388, train_loss: 0.1198, step time: 1.5353\n",
      "186/388, train_loss: 0.5370, step time: 1.5368\n",
      "187/388, train_loss: 0.3391, step time: 1.5356\n",
      "188/388, train_loss: 0.1904, step time: 1.5377\n",
      "189/388, train_loss: 0.1151, step time: 1.5341\n",
      "190/388, train_loss: 0.3297, step time: 1.5350\n",
      "191/388, train_loss: 0.0764, step time: 1.5324\n",
      "192/388, train_loss: 0.0520, step time: 1.5368\n",
      "193/388, train_loss: 0.2352, step time: 1.5338\n",
      "194/388, train_loss: 0.2963, step time: 1.5348\n",
      "195/388, train_loss: 0.1760, step time: 1.5368\n",
      "196/388, train_loss: 0.1790, step time: 1.5310\n",
      "197/388, train_loss: 0.3055, step time: 1.5306\n",
      "198/388, train_loss: 0.1560, step time: 1.5312\n",
      "199/388, train_loss: 0.0987, step time: 1.5309\n",
      "200/388, train_loss: 0.1161, step time: 1.5335\n",
      "201/388, train_loss: 0.0608, step time: 1.5365\n",
      "202/388, train_loss: 0.3491, step time: 1.5348\n",
      "203/388, train_loss: 0.1590, step time: 1.5321\n",
      "204/388, train_loss: 0.0741, step time: 1.5353\n",
      "205/388, train_loss: 0.2224, step time: 1.5318\n",
      "206/388, train_loss: 0.0970, step time: 1.5301\n",
      "207/388, train_loss: 0.1633, step time: 1.5305\n",
      "208/388, train_loss: 0.2381, step time: 1.5410\n",
      "209/388, train_loss: 0.4156, step time: 1.5341\n",
      "210/388, train_loss: 0.1650, step time: 1.5331\n",
      "211/388, train_loss: 0.1328, step time: 1.5325\n",
      "212/388, train_loss: 0.2203, step time: 1.5331\n",
      "213/388, train_loss: 0.2753, step time: 1.5320\n",
      "214/388, train_loss: 0.2487, step time: 1.5312\n",
      "215/388, train_loss: 0.0831, step time: 1.5354\n",
      "216/388, train_loss: 0.2568, step time: 1.5386\n",
      "217/388, train_loss: 0.1833, step time: 1.5359\n",
      "218/388, train_loss: 0.1595, step time: 1.5430\n",
      "219/388, train_loss: 0.2142, step time: 1.5348\n",
      "220/388, train_loss: 0.2207, step time: 1.5342\n",
      "221/388, train_loss: 0.2334, step time: 1.5371\n",
      "222/388, train_loss: 0.1508, step time: 1.5372\n",
      "223/388, train_loss: 0.2743, step time: 1.5328\n",
      "224/388, train_loss: 0.1210, step time: 1.5303\n",
      "225/388, train_loss: 0.1184, step time: 1.5301\n",
      "226/388, train_loss: 0.1589, step time: 1.5320\n",
      "227/388, train_loss: 0.2369, step time: 1.5330\n",
      "228/388, train_loss: 0.5387, step time: 1.5345\n",
      "229/388, train_loss: 0.3949, step time: 1.5348\n",
      "230/388, train_loss: 0.1324, step time: 1.5372\n",
      "231/388, train_loss: 0.1179, step time: 1.5339\n",
      "232/388, train_loss: 0.3739, step time: 1.5421\n",
      "233/388, train_loss: 0.2342, step time: 1.5320\n",
      "234/388, train_loss: 0.2838, step time: 1.5344\n",
      "235/388, train_loss: 0.3311, step time: 1.5347\n",
      "236/388, train_loss: 0.1007, step time: 1.5359\n",
      "237/388, train_loss: 0.1369, step time: 1.5368\n",
      "238/388, train_loss: 0.2299, step time: 1.5355\n",
      "239/388, train_loss: 0.2799, step time: 1.5336\n",
      "240/388, train_loss: 0.1958, step time: 1.5327\n",
      "241/388, train_loss: 0.1838, step time: 1.5350\n",
      "242/388, train_loss: 0.1722, step time: 1.5457\n",
      "243/388, train_loss: 0.3518, step time: 1.5378\n",
      "244/388, train_loss: 0.2587, step time: 1.5362\n",
      "245/388, train_loss: 0.2634, step time: 1.5342\n",
      "246/388, train_loss: 0.1429, step time: 1.5351\n",
      "247/388, train_loss: 0.3311, step time: 1.5326\n",
      "248/388, train_loss: 0.4557, step time: 1.5362\n",
      "249/388, train_loss: 0.5230, step time: 1.5341\n",
      "250/388, train_loss: 0.2454, step time: 1.5399\n",
      "251/388, train_loss: 0.4876, step time: 1.5341\n",
      "252/388, train_loss: 0.2665, step time: 1.5326\n",
      "253/388, train_loss: 0.1524, step time: 1.5342\n",
      "254/388, train_loss: 0.0592, step time: 1.5388\n",
      "255/388, train_loss: 0.1074, step time: 1.5359\n",
      "256/388, train_loss: 0.0859, step time: 1.5381\n",
      "257/388, train_loss: 0.4745, step time: 1.5316\n",
      "258/388, train_loss: 0.3287, step time: 1.5327\n",
      "259/388, train_loss: 0.2057, step time: 1.5295\n",
      "260/388, train_loss: 0.1986, step time: 1.5309\n",
      "261/388, train_loss: 0.1625, step time: 1.5351\n",
      "262/388, train_loss: 0.2649, step time: 1.5358\n",
      "263/388, train_loss: 0.4221, step time: 1.5368\n",
      "264/388, train_loss: 0.1196, step time: 1.5359\n",
      "265/388, train_loss: 0.1760, step time: 1.5359\n",
      "266/388, train_loss: 0.0953, step time: 1.5324\n",
      "267/388, train_loss: 0.4105, step time: 1.5292\n",
      "268/388, train_loss: 0.3197, step time: 1.5340\n",
      "269/388, train_loss: 0.3531, step time: 1.5365\n",
      "270/388, train_loss: 0.1735, step time: 1.5377\n",
      "271/388, train_loss: 0.1839, step time: 1.5342\n",
      "272/388, train_loss: 0.2205, step time: 1.5339\n",
      "273/388, train_loss: 0.1372, step time: 1.5329\n",
      "274/388, train_loss: 0.2548, step time: 1.5329\n",
      "275/388, train_loss: 0.0772, step time: 1.5330\n",
      "276/388, train_loss: 0.1733, step time: 1.5314\n",
      "277/388, train_loss: 0.2441, step time: 1.5340\n",
      "278/388, train_loss: 0.2665, step time: 1.5354\n",
      "279/388, train_loss: 0.1012, step time: 1.5353\n",
      "280/388, train_loss: 0.0756, step time: 1.5311\n",
      "281/388, train_loss: 0.1204, step time: 1.5322\n",
      "282/388, train_loss: 0.3526, step time: 1.5318\n",
      "283/388, train_loss: 0.1017, step time: 1.5327\n",
      "284/388, train_loss: 0.3003, step time: 1.5384\n",
      "285/388, train_loss: 0.1883, step time: 1.5350\n",
      "286/388, train_loss: 0.1457, step time: 1.5322\n",
      "287/388, train_loss: 0.3046, step time: 1.5347\n",
      "288/388, train_loss: 0.0986, step time: 1.5293\n",
      "289/388, train_loss: 0.1301, step time: 1.5286\n",
      "290/388, train_loss: 0.2924, step time: 1.5330\n",
      "291/388, train_loss: 0.0677, step time: 1.5308\n",
      "292/388, train_loss: 0.2433, step time: 1.5377\n",
      "293/388, train_loss: 0.3549, step time: 1.5331\n",
      "294/388, train_loss: 0.1189, step time: 1.5345\n",
      "295/388, train_loss: 0.2520, step time: 1.5354\n",
      "296/388, train_loss: 0.4186, step time: 1.5340\n",
      "297/388, train_loss: 0.2822, step time: 1.5341\n",
      "298/388, train_loss: 0.1570, step time: 1.5321\n",
      "299/388, train_loss: 0.1178, step time: 1.5325\n",
      "300/388, train_loss: 0.1997, step time: 1.5312\n",
      "301/388, train_loss: 0.1231, step time: 1.5342\n",
      "302/388, train_loss: 0.3137, step time: 1.5347\n",
      "303/388, train_loss: 0.1718, step time: 1.5350\n",
      "304/388, train_loss: 0.1765, step time: 1.5368\n",
      "305/388, train_loss: 0.0714, step time: 1.5328\n",
      "306/388, train_loss: 0.1045, step time: 1.5320\n",
      "307/388, train_loss: 0.5174, step time: 1.5319\n",
      "308/388, train_loss: 0.0925, step time: 1.5371\n",
      "309/388, train_loss: 0.1552, step time: 1.5359\n",
      "310/388, train_loss: 0.1992, step time: 1.5484\n",
      "311/388, train_loss: 0.2419, step time: 1.5324\n",
      "312/388, train_loss: 0.1135, step time: 1.5306\n",
      "313/388, train_loss: 0.0910, step time: 1.5275\n",
      "314/388, train_loss: 0.2577, step time: 1.5416\n",
      "315/388, train_loss: 0.1301, step time: 1.5406\n",
      "316/388, train_loss: 0.3295, step time: 1.5356\n",
      "317/388, train_loss: 0.1044, step time: 1.5303\n",
      "318/388, train_loss: 0.2167, step time: 1.5339\n",
      "319/388, train_loss: 0.0587, step time: 1.5302\n",
      "320/388, train_loss: 0.2473, step time: 1.5467\n",
      "321/388, train_loss: 0.1126, step time: 1.5355\n",
      "322/388, train_loss: 0.1234, step time: 1.5379\n",
      "323/388, train_loss: 0.2093, step time: 1.5355\n",
      "324/388, train_loss: 0.2235, step time: 1.5341\n",
      "325/388, train_loss: 0.2128, step time: 1.5360\n",
      "326/388, train_loss: 0.0840, step time: 1.5326\n",
      "327/388, train_loss: 0.3251, step time: 1.5354\n",
      "328/388, train_loss: 0.0956, step time: 1.5317\n",
      "329/388, train_loss: 0.1252, step time: 1.5345\n",
      "330/388, train_loss: 0.0839, step time: 1.5339\n",
      "331/388, train_loss: 0.1888, step time: 1.5340\n",
      "332/388, train_loss: 0.1998, step time: 1.5338\n",
      "333/388, train_loss: 0.1247, step time: 1.5330\n",
      "334/388, train_loss: 0.0945, step time: 1.5365\n",
      "335/388, train_loss: 0.1494, step time: 1.5360\n",
      "336/388, train_loss: 0.3297, step time: 1.5404\n",
      "337/388, train_loss: 0.1348, step time: 1.5335\n",
      "338/388, train_loss: 0.0720, step time: 1.5337\n",
      "339/388, train_loss: 0.1240, step time: 1.5339\n",
      "340/388, train_loss: 0.1553, step time: 1.5308\n",
      "341/388, train_loss: 0.1927, step time: 1.5315\n",
      "342/388, train_loss: 0.1136, step time: 1.5366\n",
      "343/388, train_loss: 0.1731, step time: 1.5370\n",
      "344/388, train_loss: 0.2487, step time: 1.5343\n",
      "345/388, train_loss: 0.1348, step time: 1.5334\n",
      "346/388, train_loss: 0.1499, step time: 1.5290\n",
      "347/388, train_loss: 0.2482, step time: 1.5312\n",
      "348/388, train_loss: 0.2446, step time: 1.5332\n",
      "349/388, train_loss: 0.1048, step time: 1.5311\n",
      "350/388, train_loss: 0.3499, step time: 1.5331\n",
      "351/388, train_loss: 0.1759, step time: 1.5332\n",
      "352/388, train_loss: 0.1040, step time: 1.5342\n",
      "353/388, train_loss: 0.1220, step time: 1.5353\n",
      "354/388, train_loss: 0.6173, step time: 1.5315\n",
      "355/388, train_loss: 0.1242, step time: 1.5283\n",
      "356/388, train_loss: 0.4352, step time: 1.5342\n",
      "357/388, train_loss: 0.3040, step time: 1.5318\n",
      "358/388, train_loss: 0.0850, step time: 1.5319\n",
      "359/388, train_loss: 0.1496, step time: 1.5376\n",
      "360/388, train_loss: 0.2837, step time: 1.5384\n",
      "361/388, train_loss: 0.2707, step time: 1.5348\n",
      "362/388, train_loss: 0.0999, step time: 1.5326\n",
      "363/388, train_loss: 0.1637, step time: 1.5328\n",
      "364/388, train_loss: 0.1918, step time: 1.5322\n",
      "365/388, train_loss: 0.1427, step time: 1.5325\n",
      "366/388, train_loss: 0.1741, step time: 1.5376\n",
      "367/388, train_loss: 0.3176, step time: 1.5344\n",
      "368/388, train_loss: 0.4395, step time: 1.5347\n",
      "369/388, train_loss: 0.3100, step time: 1.5353\n",
      "370/388, train_loss: 0.2477, step time: 1.5326\n",
      "371/388, train_loss: 0.0788, step time: 1.5304\n",
      "372/388, train_loss: 0.1521, step time: 1.5330\n",
      "373/388, train_loss: 0.2583, step time: 1.5332\n",
      "374/388, train_loss: 0.2093, step time: 1.5388\n",
      "375/388, train_loss: 0.1398, step time: 1.5355\n",
      "376/388, train_loss: 0.1083, step time: 1.5366\n",
      "377/388, train_loss: 0.1489, step time: 1.5304\n",
      "378/388, train_loss: 0.0646, step time: 1.5311\n",
      "379/388, train_loss: 0.0969, step time: 1.5318\n",
      "380/388, train_loss: 0.2523, step time: 1.5328\n",
      "381/388, train_loss: 0.1938, step time: 1.5340\n",
      "382/388, train_loss: 0.0915, step time: 1.5368\n",
      "383/388, train_loss: 0.3796, step time: 1.5367\n",
      "384/388, train_loss: 0.3615, step time: 1.5368\n",
      "385/388, train_loss: 0.3137, step time: 1.5351\n",
      "386/388, train_loss: 0.2643, step time: 1.5313\n",
      "387/388, train_loss: 0.1817, step time: 1.5297\n",
      "388/388, train_loss: 0.0926, step time: 1.5318\n",
      "epoch 24 average loss: 0.2132\n",
      "current epoch: 24 current mean dice: 0.7347 tc: 0.7965 wt: 0.8875 et: 0.5202\n",
      "best mean dice: 0.7451 at epoch: 23\n",
      "time consuming of epoch 24 is: 701.1522\n",
      "----------\n",
      "epoch 25/100\n",
      "1/388, train_loss: 0.3468, step time: 1.5654\n",
      "2/388, train_loss: 0.1124, step time: 1.5349\n",
      "3/388, train_loss: 0.2111, step time: 1.5344\n",
      "4/388, train_loss: 0.1937, step time: 1.5321\n",
      "5/388, train_loss: 0.1412, step time: 1.5317\n",
      "6/388, train_loss: 0.3117, step time: 1.5319\n",
      "7/388, train_loss: 0.2968, step time: 1.5327\n",
      "8/388, train_loss: 0.1494, step time: 1.5370\n",
      "9/388, train_loss: 0.3394, step time: 1.5338\n",
      "10/388, train_loss: 0.1628, step time: 1.5360\n",
      "11/388, train_loss: 0.3269, step time: 1.5332\n",
      "12/388, train_loss: 0.2086, step time: 1.5362\n",
      "13/388, train_loss: 0.1307, step time: 1.5339\n",
      "14/388, train_loss: 0.0345, step time: 1.5358\n",
      "15/388, train_loss: 0.2889, step time: 1.5356\n",
      "16/388, train_loss: 0.0884, step time: 1.5348\n",
      "17/388, train_loss: 0.4119, step time: 1.5321\n",
      "18/388, train_loss: 0.0901, step time: 1.5290\n",
      "19/388, train_loss: 0.0666, step time: 1.5308\n",
      "20/388, train_loss: 0.3984, step time: 1.5318\n",
      "21/388, train_loss: 0.1011, step time: 1.5329\n",
      "22/388, train_loss: 0.3436, step time: 1.5356\n",
      "23/388, train_loss: 0.1720, step time: 1.5354\n",
      "24/388, train_loss: 0.0986, step time: 1.5347\n",
      "25/388, train_loss: 0.1713, step time: 1.5336\n",
      "26/388, train_loss: 0.4105, step time: 1.5321\n",
      "27/388, train_loss: 0.0760, step time: 1.5343\n",
      "28/388, train_loss: 0.1214, step time: 1.5336\n",
      "29/388, train_loss: 0.0852, step time: 1.5348\n",
      "30/388, train_loss: 0.1413, step time: 1.5329\n",
      "31/388, train_loss: 0.3204, step time: 1.5304\n",
      "32/388, train_loss: 0.0972, step time: 1.5323\n",
      "33/388, train_loss: 0.1478, step time: 1.5313\n",
      "34/388, train_loss: 0.1512, step time: 1.5512\n",
      "35/388, train_loss: 0.1051, step time: 1.5327\n",
      "36/388, train_loss: 0.0780, step time: 1.5343\n",
      "37/388, train_loss: 0.1393, step time: 1.5370\n",
      "38/388, train_loss: 0.1151, step time: 1.5348\n",
      "39/388, train_loss: 0.1559, step time: 1.5396\n",
      "40/388, train_loss: 0.1394, step time: 1.5338\n",
      "41/388, train_loss: 0.3111, step time: 1.5350\n",
      "42/388, train_loss: 0.1730, step time: 1.5353\n",
      "43/388, train_loss: 0.4384, step time: 1.5365\n",
      "44/388, train_loss: 0.1760, step time: 1.5462\n",
      "45/388, train_loss: 0.3096, step time: 1.5314\n",
      "46/388, train_loss: 0.2180, step time: 1.5331\n",
      "47/388, train_loss: 0.1895, step time: 1.5308\n",
      "48/388, train_loss: 0.1129, step time: 1.5407\n",
      "49/388, train_loss: 0.0614, step time: 1.5358\n",
      "50/388, train_loss: 0.1284, step time: 1.5371\n",
      "51/388, train_loss: 0.2024, step time: 1.5342\n",
      "52/388, train_loss: 0.1095, step time: 1.5340\n",
      "53/388, train_loss: 0.1836, step time: 1.5352\n",
      "54/388, train_loss: 0.1067, step time: 1.5383\n",
      "55/388, train_loss: 0.0857, step time: 1.5379\n",
      "56/388, train_loss: 0.1009, step time: 1.5409\n",
      "57/388, train_loss: 0.0584, step time: 1.5288\n",
      "58/388, train_loss: 0.2238, step time: 1.5336\n",
      "59/388, train_loss: 0.0551, step time: 1.5324\n",
      "60/388, train_loss: 0.1487, step time: 1.5320\n",
      "61/388, train_loss: 0.2800, step time: 1.5314\n",
      "62/388, train_loss: 0.1451, step time: 1.5375\n",
      "63/388, train_loss: 0.0945, step time: 1.5390\n",
      "64/388, train_loss: 0.2240, step time: 1.5358\n",
      "65/388, train_loss: 0.3498, step time: 1.5310\n",
      "66/388, train_loss: 0.2452, step time: 1.5345\n",
      "67/388, train_loss: 0.3339, step time: 1.5350\n",
      "68/388, train_loss: 0.1929, step time: 1.5315\n",
      "69/388, train_loss: 0.4471, step time: 1.5368\n",
      "70/388, train_loss: 0.6631, step time: 1.5339\n",
      "71/388, train_loss: 0.1500, step time: 1.5295\n",
      "72/388, train_loss: 0.1121, step time: 1.5304\n",
      "73/388, train_loss: 0.6430, step time: 1.5335\n",
      "74/388, train_loss: 0.1499, step time: 1.5440\n",
      "75/388, train_loss: 0.0690, step time: 1.5378\n",
      "76/388, train_loss: 0.2581, step time: 1.5315\n",
      "77/388, train_loss: 0.1507, step time: 1.5299\n",
      "78/388, train_loss: 0.3218, step time: 1.5319\n",
      "79/388, train_loss: 0.3131, step time: 1.5364\n",
      "80/388, train_loss: 0.0687, step time: 1.5334\n",
      "81/388, train_loss: 0.2341, step time: 1.5345\n",
      "82/388, train_loss: 0.3337, step time: 1.5375\n",
      "83/388, train_loss: 0.0969, step time: 1.5339\n",
      "84/388, train_loss: 0.1026, step time: 1.5323\n",
      "85/388, train_loss: 0.0382, step time: 1.5298\n",
      "86/388, train_loss: 0.2502, step time: 1.5299\n",
      "87/388, train_loss: 0.2150, step time: 1.5343\n",
      "88/388, train_loss: 0.1182, step time: 1.5342\n",
      "89/388, train_loss: 0.1836, step time: 1.5601\n",
      "90/388, train_loss: 0.0809, step time: 1.5345\n",
      "91/388, train_loss: 0.1699, step time: 1.5358\n",
      "92/388, train_loss: 0.3974, step time: 1.5351\n",
      "93/388, train_loss: 0.1053, step time: 1.5341\n",
      "94/388, train_loss: 0.5348, step time: 1.5344\n",
      "95/388, train_loss: 0.1123, step time: 1.5303\n",
      "96/388, train_loss: 0.0879, step time: 1.5305\n",
      "97/388, train_loss: 0.3326, step time: 1.5324\n",
      "98/388, train_loss: 0.1331, step time: 1.5329\n",
      "99/388, train_loss: 0.2616, step time: 1.5367\n",
      "100/388, train_loss: 0.2350, step time: 1.5331\n",
      "101/388, train_loss: 0.2312, step time: 1.5310\n",
      "102/388, train_loss: 0.2298, step time: 1.5326\n",
      "103/388, train_loss: 0.2190, step time: 1.5320\n",
      "104/388, train_loss: 0.0978, step time: 1.5311\n",
      "105/388, train_loss: 0.2973, step time: 1.5284\n",
      "106/388, train_loss: 0.1206, step time: 1.5367\n",
      "107/388, train_loss: 0.2889, step time: 1.5587\n",
      "108/388, train_loss: 0.2309, step time: 1.5597\n",
      "109/388, train_loss: 0.2416, step time: 1.5310\n",
      "110/388, train_loss: 0.1755, step time: 1.5324\n",
      "111/388, train_loss: 0.1350, step time: 1.5346\n",
      "112/388, train_loss: 0.2061, step time: 1.5362\n",
      "113/388, train_loss: 0.1957, step time: 1.5368\n",
      "114/388, train_loss: 0.0957, step time: 1.5477\n",
      "115/388, train_loss: 0.1064, step time: 1.5327\n",
      "116/388, train_loss: 0.2630, step time: 1.5334\n",
      "117/388, train_loss: 0.0772, step time: 1.5339\n",
      "118/388, train_loss: 0.0723, step time: 1.5327\n",
      "119/388, train_loss: 0.1567, step time: 1.5371\n",
      "120/388, train_loss: 0.1887, step time: 1.5354\n",
      "121/388, train_loss: 0.1348, step time: 1.5352\n",
      "122/388, train_loss: 0.3569, step time: 1.5313\n",
      "123/388, train_loss: 0.1209, step time: 1.5330\n",
      "124/388, train_loss: 0.1550, step time: 1.5312\n",
      "125/388, train_loss: 0.3824, step time: 1.5412\n",
      "126/388, train_loss: 0.1061, step time: 1.5398\n",
      "127/388, train_loss: 0.2387, step time: 1.5348\n",
      "128/388, train_loss: 0.1505, step time: 1.5341\n",
      "129/388, train_loss: 0.1279, step time: 1.5308\n",
      "130/388, train_loss: 0.0902, step time: 1.5333\n",
      "131/388, train_loss: 0.3770, step time: 1.5350\n",
      "132/388, train_loss: 0.1804, step time: 1.5352\n",
      "133/388, train_loss: 0.2316, step time: 1.5469\n",
      "134/388, train_loss: 0.2691, step time: 1.5331\n",
      "135/388, train_loss: 0.1206, step time: 1.5305\n",
      "136/388, train_loss: 0.1675, step time: 1.5347\n",
      "137/388, train_loss: 0.3832, step time: 1.5308\n",
      "138/388, train_loss: 0.1378, step time: 1.5367\n",
      "139/388, train_loss: 0.3076, step time: 1.5434\n",
      "140/388, train_loss: 0.2723, step time: 1.5323\n",
      "141/388, train_loss: 0.1140, step time: 1.5326\n",
      "142/388, train_loss: 0.3130, step time: 1.5303\n",
      "143/388, train_loss: 0.3868, step time: 1.5303\n",
      "144/388, train_loss: 0.1968, step time: 1.5310\n",
      "145/388, train_loss: 0.0919, step time: 1.5357\n",
      "146/388, train_loss: 0.5082, step time: 1.5337\n",
      "147/388, train_loss: 0.1620, step time: 1.5330\n",
      "148/388, train_loss: 0.1483, step time: 1.5313\n",
      "149/388, train_loss: 0.1366, step time: 1.5307\n",
      "150/388, train_loss: 0.1626, step time: 1.5328\n",
      "151/388, train_loss: 0.4231, step time: 1.5322\n",
      "152/388, train_loss: 0.4455, step time: 1.5352\n",
      "153/388, train_loss: 0.1019, step time: 1.5376\n",
      "154/388, train_loss: 0.2592, step time: 1.5329\n",
      "155/388, train_loss: 0.0853, step time: 1.5314\n",
      "156/388, train_loss: 0.1998, step time: 1.5335\n",
      "157/388, train_loss: 0.3975, step time: 1.5320\n",
      "158/388, train_loss: 0.3058, step time: 1.5320\n",
      "159/388, train_loss: 0.0837, step time: 1.5360\n",
      "160/388, train_loss: 0.1711, step time: 1.5358\n",
      "161/388, train_loss: 0.1056, step time: 1.5323\n",
      "162/388, train_loss: 0.2150, step time: 1.5308\n",
      "163/388, train_loss: 0.1853, step time: 1.5316\n",
      "164/388, train_loss: 0.2057, step time: 1.5334\n",
      "165/388, train_loss: 0.1628, step time: 1.5361\n",
      "166/388, train_loss: 0.3109, step time: 1.5373\n",
      "167/388, train_loss: 0.2833, step time: 1.5361\n",
      "168/388, train_loss: 0.2089, step time: 1.5371\n",
      "169/388, train_loss: 0.5900, step time: 1.5329\n",
      "170/388, train_loss: 0.1005, step time: 1.5303\n",
      "171/388, train_loss: 0.1416, step time: 1.5382\n",
      "172/388, train_loss: 0.6160, step time: 1.5368\n",
      "173/388, train_loss: 0.2871, step time: 1.5356\n",
      "174/388, train_loss: 0.0946, step time: 1.5331\n",
      "175/388, train_loss: 0.5388, step time: 1.5306\n",
      "176/388, train_loss: 0.0827, step time: 1.5328\n",
      "177/388, train_loss: 0.4602, step time: 1.5309\n",
      "178/388, train_loss: 0.5164, step time: 1.5337\n",
      "179/388, train_loss: 0.3045, step time: 1.5321\n",
      "180/388, train_loss: 0.2696, step time: 1.5358\n",
      "181/388, train_loss: 0.1825, step time: 1.5321\n",
      "182/388, train_loss: 0.1148, step time: 1.5335\n",
      "183/388, train_loss: 0.0840, step time: 1.5340\n",
      "184/388, train_loss: 0.0819, step time: 1.5329\n",
      "185/388, train_loss: 0.1245, step time: 1.5330\n",
      "186/388, train_loss: 0.2601, step time: 1.5322\n",
      "187/388, train_loss: 0.1107, step time: 1.5322\n",
      "188/388, train_loss: 0.1964, step time: 1.5349\n",
      "189/388, train_loss: 0.2333, step time: 1.5344\n",
      "190/388, train_loss: 0.4924, step time: 1.5356\n",
      "191/388, train_loss: 0.1254, step time: 1.5344\n",
      "192/388, train_loss: 0.1339, step time: 1.5333\n",
      "193/388, train_loss: 0.2775, step time: 1.5333\n",
      "194/388, train_loss: 0.1549, step time: 1.5315\n",
      "195/388, train_loss: 0.1398, step time: 1.5304\n",
      "196/388, train_loss: 0.2520, step time: 1.5340\n",
      "197/388, train_loss: 0.1878, step time: 1.5310\n",
      "198/388, train_loss: 0.1872, step time: 1.5327\n",
      "199/388, train_loss: 0.2200, step time: 1.5379\n",
      "200/388, train_loss: 0.2750, step time: 1.5351\n",
      "201/388, train_loss: 0.2586, step time: 1.5384\n",
      "202/388, train_loss: 0.1838, step time: 1.5332\n",
      "203/388, train_loss: 0.2227, step time: 1.5310\n",
      "204/388, train_loss: 0.1932, step time: 1.5337\n",
      "205/388, train_loss: 0.2161, step time: 1.5591\n",
      "206/388, train_loss: 0.1609, step time: 1.5309\n",
      "207/388, train_loss: 0.1083, step time: 1.5504\n",
      "208/388, train_loss: 0.1697, step time: 1.5339\n",
      "209/388, train_loss: 0.1736, step time: 1.5328\n",
      "210/388, train_loss: 0.2164, step time: 1.5326\n",
      "211/388, train_loss: 0.1636, step time: 1.5305\n",
      "212/388, train_loss: 0.2318, step time: 1.5368\n",
      "213/388, train_loss: 0.0907, step time: 1.5339\n",
      "214/388, train_loss: 0.4832, step time: 1.5366\n",
      "215/388, train_loss: 0.0530, step time: 1.5338\n",
      "216/388, train_loss: 0.2445, step time: 1.5321\n",
      "217/388, train_loss: 0.0659, step time: 1.5332\n",
      "218/388, train_loss: 0.1405, step time: 1.5315\n",
      "219/388, train_loss: 0.6355, step time: 1.5429\n",
      "220/388, train_loss: 0.2354, step time: 1.5394\n",
      "221/388, train_loss: 0.1319, step time: 1.5344\n",
      "222/388, train_loss: 0.0935, step time: 1.5316\n",
      "223/388, train_loss: 0.2835, step time: 1.5335\n",
      "224/388, train_loss: 0.3190, step time: 1.5324\n",
      "225/388, train_loss: 0.1496, step time: 1.5368\n",
      "226/388, train_loss: 0.1223, step time: 1.5338\n",
      "227/388, train_loss: 0.1358, step time: 1.5345\n",
      "228/388, train_loss: 0.0880, step time: 1.5365\n",
      "229/388, train_loss: 0.6328, step time: 1.5321\n",
      "230/388, train_loss: 0.2574, step time: 1.5318\n",
      "231/388, train_loss: 0.1685, step time: 1.5327\n",
      "232/388, train_loss: 0.2163, step time: 1.5322\n",
      "233/388, train_loss: 0.3031, step time: 1.5353\n",
      "234/388, train_loss: 0.3630, step time: 1.5355\n",
      "235/388, train_loss: 0.1046, step time: 1.5351\n",
      "236/388, train_loss: 0.1978, step time: 1.5370\n",
      "237/388, train_loss: 0.3888, step time: 1.5352\n",
      "238/388, train_loss: 0.1400, step time: 1.5337\n",
      "239/388, train_loss: 0.1814, step time: 1.5485\n",
      "240/388, train_loss: 0.2353, step time: 1.5559\n",
      "241/388, train_loss: 0.1846, step time: 1.5324\n",
      "242/388, train_loss: 0.1934, step time: 1.5391\n",
      "243/388, train_loss: 0.0985, step time: 1.5340\n",
      "244/388, train_loss: 0.2103, step time: 1.5350\n",
      "245/388, train_loss: 0.2279, step time: 1.5356\n",
      "246/388, train_loss: 0.1794, step time: 1.5304\n",
      "247/388, train_loss: 0.0989, step time: 1.5336\n",
      "248/388, train_loss: 0.3497, step time: 1.5329\n",
      "249/388, train_loss: 0.2858, step time: 1.5311\n",
      "250/388, train_loss: 0.1462, step time: 1.5379\n",
      "251/388, train_loss: 0.2011, step time: 1.5348\n",
      "252/388, train_loss: 0.1282, step time: 1.5570\n",
      "253/388, train_loss: 0.3082, step time: 1.5319\n",
      "254/388, train_loss: 0.4936, step time: 1.5329\n",
      "255/388, train_loss: 0.1826, step time: 1.5320\n",
      "256/388, train_loss: 0.5899, step time: 1.5360\n",
      "257/388, train_loss: 0.0888, step time: 1.5365\n",
      "258/388, train_loss: 0.1927, step time: 1.5380\n",
      "259/388, train_loss: 0.1955, step time: 1.5379\n",
      "260/388, train_loss: 0.2867, step time: 1.5334\n",
      "261/388, train_loss: 0.1090, step time: 1.5316\n",
      "262/388, train_loss: 0.3273, step time: 1.5346\n",
      "263/388, train_loss: 0.1403, step time: 1.5397\n",
      "264/388, train_loss: 0.1580, step time: 1.5371\n",
      "265/388, train_loss: 0.1693, step time: 1.5370\n",
      "266/388, train_loss: 0.0580, step time: 1.5338\n",
      "267/388, train_loss: 0.1532, step time: 1.5339\n",
      "268/388, train_loss: 0.3450, step time: 1.5290\n",
      "269/388, train_loss: 0.1716, step time: 1.5324\n",
      "270/388, train_loss: 0.0779, step time: 1.5430\n",
      "271/388, train_loss: 0.0937, step time: 1.5325\n",
      "272/388, train_loss: 0.1631, step time: 1.5330\n",
      "273/388, train_loss: 0.2597, step time: 1.5414\n",
      "274/388, train_loss: 0.1289, step time: 1.5314\n",
      "275/388, train_loss: 0.2502, step time: 1.5608\n",
      "276/388, train_loss: 0.2231, step time: 1.5369\n",
      "277/388, train_loss: 0.2736, step time: 1.5336\n",
      "278/388, train_loss: 0.2647, step time: 1.5309\n",
      "279/388, train_loss: 0.1529, step time: 1.5341\n",
      "280/388, train_loss: 0.1004, step time: 1.5338\n",
      "281/388, train_loss: 0.1887, step time: 1.5427\n",
      "282/388, train_loss: 0.1577, step time: 1.5321\n",
      "283/388, train_loss: 0.3195, step time: 1.5288\n",
      "284/388, train_loss: 0.0868, step time: 1.5292\n",
      "285/388, train_loss: 0.6111, step time: 1.5331\n",
      "286/388, train_loss: 0.1245, step time: 1.5391\n",
      "287/388, train_loss: 0.2038, step time: 1.5377\n",
      "288/388, train_loss: 0.1101, step time: 1.5301\n",
      "289/388, train_loss: 0.1312, step time: 1.5295\n",
      "290/388, train_loss: 0.2143, step time: 1.5329\n",
      "291/388, train_loss: 0.1858, step time: 1.5325\n",
      "292/388, train_loss: 0.2636, step time: 1.5460\n",
      "293/388, train_loss: 0.3089, step time: 1.5356\n",
      "294/388, train_loss: 0.2711, step time: 1.5371\n",
      "295/388, train_loss: 0.2685, step time: 1.5348\n",
      "296/388, train_loss: 0.1815, step time: 1.5322\n",
      "297/388, train_loss: 0.2833, step time: 1.5297\n",
      "298/388, train_loss: 0.7975, step time: 1.5338\n",
      "299/388, train_loss: 0.1501, step time: 1.5346\n",
      "300/388, train_loss: 0.3771, step time: 1.5347\n",
      "301/388, train_loss: 0.2721, step time: 1.5584\n",
      "302/388, train_loss: 0.1295, step time: 1.5601\n",
      "303/388, train_loss: 0.1781, step time: 1.5615\n",
      "304/388, train_loss: 0.3500, step time: 1.5575\n",
      "305/388, train_loss: 0.1121, step time: 1.5368\n",
      "306/388, train_loss: 0.2493, step time: 1.5365\n",
      "307/388, train_loss: 0.3860, step time: 1.5338\n",
      "308/388, train_loss: 0.1903, step time: 1.5324\n",
      "309/388, train_loss: 0.1886, step time: 1.5317\n",
      "310/388, train_loss: 0.3078, step time: 1.5302\n",
      "311/388, train_loss: 0.2283, step time: 1.5317\n",
      "312/388, train_loss: 0.0920, step time: 1.5374\n",
      "313/388, train_loss: 0.1742, step time: 1.5560\n",
      "314/388, train_loss: 0.2881, step time: 1.5581\n",
      "315/388, train_loss: 0.4837, step time: 1.5391\n",
      "316/388, train_loss: 0.2509, step time: 1.5463\n",
      "317/388, train_loss: 0.1126, step time: 1.5344\n",
      "318/388, train_loss: 0.3951, step time: 1.5315\n",
      "319/388, train_loss: 0.1621, step time: 1.5306\n",
      "320/388, train_loss: 0.0704, step time: 1.5360\n",
      "321/388, train_loss: 0.1768, step time: 1.5350\n",
      "322/388, train_loss: 0.2318, step time: 1.5360\n",
      "323/388, train_loss: 0.1574, step time: 1.5543\n",
      "324/388, train_loss: 0.1179, step time: 1.5291\n",
      "325/388, train_loss: 0.3067, step time: 1.5352\n",
      "326/388, train_loss: 0.1113, step time: 1.5340\n",
      "327/388, train_loss: 0.1049, step time: 1.5379\n",
      "328/388, train_loss: 0.5572, step time: 1.5335\n",
      "329/388, train_loss: 0.1040, step time: 1.5310\n",
      "330/388, train_loss: 0.0671, step time: 1.5406\n",
      "331/388, train_loss: 0.2557, step time: 1.5356\n",
      "332/388, train_loss: 0.1192, step time: 1.5399\n",
      "333/388, train_loss: 0.1109, step time: 1.5372\n",
      "334/388, train_loss: 0.1392, step time: 1.5338\n",
      "335/388, train_loss: 0.2811, step time: 1.5321\n",
      "336/388, train_loss: 0.2062, step time: 1.5313\n",
      "337/388, train_loss: 0.0998, step time: 1.5362\n",
      "338/388, train_loss: 0.2759, step time: 1.5352\n",
      "339/388, train_loss: 0.1860, step time: 1.5343\n",
      "340/388, train_loss: 0.0420, step time: 1.5321\n",
      "341/388, train_loss: 0.1903, step time: 1.5284\n",
      "342/388, train_loss: 0.0724, step time: 1.5299\n",
      "343/388, train_loss: 0.1017, step time: 1.5353\n",
      "344/388, train_loss: 0.2862, step time: 1.5378\n",
      "345/388, train_loss: 0.1408, step time: 1.5327\n",
      "346/388, train_loss: 0.1425, step time: 1.5320\n",
      "347/388, train_loss: 0.1639, step time: 1.5315\n",
      "348/388, train_loss: 0.4582, step time: 1.5317\n",
      "349/388, train_loss: 0.2921, step time: 1.5332\n",
      "350/388, train_loss: 0.5303, step time: 1.5321\n",
      "351/388, train_loss: 0.1840, step time: 1.5327\n",
      "352/388, train_loss: 0.0921, step time: 1.5329\n",
      "353/388, train_loss: 0.1561, step time: 1.5341\n",
      "354/388, train_loss: 0.1632, step time: 1.5335\n",
      "355/388, train_loss: 0.1787, step time: 1.5327\n",
      "356/388, train_loss: 0.2727, step time: 1.5343\n",
      "357/388, train_loss: 0.2503, step time: 1.5345\n",
      "358/388, train_loss: 0.1836, step time: 1.5316\n",
      "359/388, train_loss: 0.2034, step time: 1.5373\n",
      "360/388, train_loss: 0.1212, step time: 1.5373\n",
      "361/388, train_loss: 0.1923, step time: 1.5352\n",
      "362/388, train_loss: 0.1566, step time: 1.5308\n",
      "363/388, train_loss: 0.1184, step time: 1.5323\n",
      "364/388, train_loss: 0.2546, step time: 1.5328\n",
      "365/388, train_loss: 0.1189, step time: 1.5346\n",
      "366/388, train_loss: 0.1154, step time: 1.5353\n",
      "367/388, train_loss: 0.1664, step time: 1.5349\n",
      "368/388, train_loss: 0.1432, step time: 1.5360\n",
      "369/388, train_loss: 0.1405, step time: 1.5320\n",
      "370/388, train_loss: 0.2458, step time: 1.5312\n",
      "371/388, train_loss: 0.2783, step time: 1.5330\n",
      "372/388, train_loss: 0.1326, step time: 1.5321\n",
      "373/388, train_loss: 0.2441, step time: 1.5318\n",
      "374/388, train_loss: 0.1109, step time: 1.5357\n",
      "375/388, train_loss: 0.3202, step time: 1.5385\n",
      "376/388, train_loss: 0.1263, step time: 1.5373\n",
      "377/388, train_loss: 0.1628, step time: 1.5400\n",
      "378/388, train_loss: 0.2101, step time: 1.5340\n",
      "379/388, train_loss: 0.0970, step time: 1.5345\n",
      "380/388, train_loss: 0.3332, step time: 1.5390\n",
      "381/388, train_loss: 0.1272, step time: 1.5430\n",
      "382/388, train_loss: 0.0746, step time: 1.5367\n",
      "383/388, train_loss: 0.2769, step time: 1.5324\n",
      "384/388, train_loss: 0.2247, step time: 1.5331\n",
      "385/388, train_loss: 0.3991, step time: 1.5372\n",
      "386/388, train_loss: 0.1746, step time: 1.5378\n",
      "387/388, train_loss: 0.2351, step time: 1.5330\n",
      "388/388, train_loss: 0.1530, step time: 1.5380\n",
      "epoch 25 average loss: 0.2121\n",
      "current epoch: 25 current mean dice: 0.7385 tc: 0.7896 wt: 0.8813 et: 0.5445\n",
      "best mean dice: 0.7451 at epoch: 23\n",
      "time consuming of epoch 25 is: 702.7913\n",
      "----------\n",
      "epoch 26/100\n",
      "1/388, train_loss: 0.2933, step time: 1.5550\n",
      "2/388, train_loss: 0.4351, step time: 1.5348\n",
      "3/388, train_loss: 0.0854, step time: 1.5334\n",
      "4/388, train_loss: 0.1197, step time: 1.5368\n",
      "5/388, train_loss: 0.0787, step time: 1.5380\n",
      "6/388, train_loss: 0.2099, step time: 1.5350\n",
      "7/388, train_loss: 0.0669, step time: 1.5331\n",
      "8/388, train_loss: 0.2664, step time: 1.5320\n",
      "9/388, train_loss: 0.5402, step time: 1.5328\n",
      "10/388, train_loss: 0.1776, step time: 1.5364\n",
      "11/388, train_loss: 0.1025, step time: 1.5356\n",
      "12/388, train_loss: 0.5136, step time: 1.5335\n",
      "13/388, train_loss: 0.2089, step time: 1.5432\n",
      "14/388, train_loss: 0.3304, step time: 1.5317\n",
      "15/388, train_loss: 0.1666, step time: 1.5322\n",
      "16/388, train_loss: 0.1355, step time: 1.5323\n",
      "17/388, train_loss: 0.3094, step time: 1.5327\n",
      "18/388, train_loss: 0.3950, step time: 1.5438\n",
      "19/388, train_loss: 0.1454, step time: 1.5329\n",
      "20/388, train_loss: 0.2433, step time: 1.5355\n",
      "21/388, train_loss: 0.2836, step time: 1.5583\n",
      "22/388, train_loss: 0.3126, step time: 1.5326\n",
      "23/388, train_loss: 0.3647, step time: 1.5301\n",
      "24/388, train_loss: 0.1396, step time: 1.5312\n",
      "25/388, train_loss: 0.2006, step time: 1.5374\n",
      "26/388, train_loss: 0.6000, step time: 1.5368\n",
      "27/388, train_loss: 0.1075, step time: 1.5369\n",
      "28/388, train_loss: 0.0774, step time: 1.5341\n",
      "29/388, train_loss: 0.2894, step time: 1.5322\n",
      "30/388, train_loss: 0.1146, step time: 1.5311\n",
      "31/388, train_loss: 0.1171, step time: 1.5324\n",
      "32/388, train_loss: 0.1372, step time: 1.5363\n",
      "33/388, train_loss: 0.1075, step time: 1.5361\n",
      "34/388, train_loss: 0.3462, step time: 1.5335\n",
      "35/388, train_loss: 0.1102, step time: 1.5345\n",
      "36/388, train_loss: 0.3562, step time: 1.5315\n",
      "37/388, train_loss: 0.2009, step time: 1.5316\n",
      "38/388, train_loss: 0.5332, step time: 1.5348\n",
      "39/388, train_loss: 0.2142, step time: 1.5354\n",
      "40/388, train_loss: 0.0866, step time: 1.5384\n",
      "41/388, train_loss: 0.1248, step time: 1.5346\n",
      "42/388, train_loss: 0.1524, step time: 1.5296\n",
      "43/388, train_loss: 0.2397, step time: 1.5335\n",
      "44/388, train_loss: 0.2536, step time: 1.5347\n",
      "45/388, train_loss: 0.1128, step time: 1.5374\n",
      "46/388, train_loss: 0.1838, step time: 1.5386\n",
      "47/388, train_loss: 0.1697, step time: 1.5335\n",
      "48/388, train_loss: 0.3159, step time: 1.5433\n",
      "49/388, train_loss: 0.1928, step time: 1.5315\n",
      "50/388, train_loss: 0.2240, step time: 1.5631\n",
      "51/388, train_loss: 0.1181, step time: 1.5378\n",
      "52/388, train_loss: 0.3284, step time: 1.5339\n",
      "53/388, train_loss: 0.3165, step time: 1.5355\n",
      "54/388, train_loss: 0.0900, step time: 1.5354\n",
      "55/388, train_loss: 0.1497, step time: 1.5353\n",
      "56/388, train_loss: 0.1199, step time: 1.5344\n",
      "57/388, train_loss: 0.1943, step time: 1.5340\n",
      "58/388, train_loss: 0.4048, step time: 1.5324\n",
      "59/388, train_loss: 0.4886, step time: 1.5300\n",
      "60/388, train_loss: 0.1325, step time: 1.5370\n",
      "61/388, train_loss: 0.2158, step time: 1.5315\n",
      "62/388, train_loss: 0.6652, step time: 1.5364\n",
      "63/388, train_loss: 0.1821, step time: 1.5350\n",
      "64/388, train_loss: 0.4080, step time: 1.5334\n",
      "65/388, train_loss: 0.4340, step time: 1.5339\n",
      "66/388, train_loss: 0.3013, step time: 1.5326\n",
      "67/388, train_loss: 0.2196, step time: 1.5322\n",
      "68/388, train_loss: 0.2547, step time: 1.5351\n",
      "69/388, train_loss: 0.2443, step time: 1.5350\n",
      "70/388, train_loss: 0.3041, step time: 1.5413\n",
      "71/388, train_loss: 0.2498, step time: 1.5348\n",
      "72/388, train_loss: 0.1238, step time: 1.5317\n",
      "73/388, train_loss: 0.3756, step time: 1.5350\n",
      "74/388, train_loss: 0.3299, step time: 1.5309\n",
      "75/388, train_loss: 0.1989, step time: 1.5370\n",
      "76/388, train_loss: 0.0699, step time: 1.5360\n",
      "77/388, train_loss: 0.4328, step time: 1.5385\n",
      "78/388, train_loss: 0.3125, step time: 1.5374\n",
      "79/388, train_loss: 0.3932, step time: 1.5325\n",
      "80/388, train_loss: 0.1006, step time: 1.5352\n",
      "81/388, train_loss: 0.3517, step time: 1.5350\n",
      "82/388, train_loss: 0.1964, step time: 1.5362\n",
      "83/388, train_loss: 0.2276, step time: 1.5359\n",
      "84/388, train_loss: 0.4505, step time: 1.5362\n",
      "85/388, train_loss: 0.2948, step time: 1.5338\n",
      "86/388, train_loss: 0.1832, step time: 1.5315\n",
      "87/388, train_loss: 0.1408, step time: 1.5328\n",
      "88/388, train_loss: 0.0749, step time: 1.5305\n",
      "89/388, train_loss: 0.1971, step time: 1.5338\n",
      "90/388, train_loss: 0.0345, step time: 1.5380\n",
      "91/388, train_loss: 0.4944, step time: 1.5363\n",
      "92/388, train_loss: 0.1558, step time: 1.5367\n",
      "93/388, train_loss: 0.2946, step time: 1.5348\n",
      "94/388, train_loss: 0.0519, step time: 1.5329\n",
      "95/388, train_loss: 0.2159, step time: 1.5323\n",
      "96/388, train_loss: 0.4596, step time: 1.5364\n",
      "97/388, train_loss: 0.1946, step time: 1.5324\n",
      "98/388, train_loss: 0.0906, step time: 1.5357\n",
      "99/388, train_loss: 0.1819, step time: 1.5371\n",
      "100/388, train_loss: 0.0710, step time: 1.5312\n",
      "101/388, train_loss: 0.1669, step time: 1.5326\n",
      "102/388, train_loss: 0.2888, step time: 1.5373\n",
      "103/388, train_loss: 0.2711, step time: 1.5331\n",
      "104/388, train_loss: 0.5613, step time: 1.5371\n",
      "105/388, train_loss: 0.5273, step time: 1.5311\n",
      "106/388, train_loss: 0.2570, step time: 1.5338\n",
      "107/388, train_loss: 0.2264, step time: 1.5332\n",
      "108/388, train_loss: 0.3279, step time: 1.5332\n",
      "109/388, train_loss: 0.1361, step time: 1.5359\n",
      "110/388, train_loss: 0.1259, step time: 1.5380\n",
      "111/388, train_loss: 0.1504, step time: 1.5476\n",
      "112/388, train_loss: 0.2959, step time: 1.5434\n",
      "113/388, train_loss: 0.1880, step time: 1.5293\n",
      "114/388, train_loss: 0.1474, step time: 1.5328\n",
      "115/388, train_loss: 0.1521, step time: 1.5489\n",
      "116/388, train_loss: 0.1863, step time: 1.5316\n",
      "117/388, train_loss: 0.1322, step time: 1.5343\n",
      "118/388, train_loss: 0.2346, step time: 1.5333\n",
      "119/388, train_loss: 0.1280, step time: 1.5357\n",
      "120/388, train_loss: 0.2953, step time: 1.5343\n",
      "121/388, train_loss: 0.2568, step time: 1.5356\n",
      "122/388, train_loss: 0.0970, step time: 1.5343\n",
      "123/388, train_loss: 0.2281, step time: 1.5372\n",
      "124/388, train_loss: 0.1947, step time: 1.5399\n",
      "125/388, train_loss: 0.2168, step time: 1.5348\n",
      "126/388, train_loss: 0.0939, step time: 1.5343\n",
      "127/388, train_loss: 0.1606, step time: 1.5307\n",
      "128/388, train_loss: 0.1806, step time: 1.5299\n",
      "129/388, train_loss: 0.6250, step time: 1.5374\n",
      "130/388, train_loss: 0.1017, step time: 1.5403\n",
      "131/388, train_loss: 0.1211, step time: 1.5373\n",
      "132/388, train_loss: 0.0738, step time: 1.5317\n",
      "133/388, train_loss: 0.1761, step time: 1.5340\n",
      "134/388, train_loss: 0.1719, step time: 1.5314\n",
      "135/388, train_loss: 0.1367, step time: 1.5335\n",
      "136/388, train_loss: 0.2141, step time: 1.5362\n",
      "137/388, train_loss: 0.2131, step time: 1.5368\n",
      "138/388, train_loss: 0.0864, step time: 1.5434\n",
      "139/388, train_loss: 0.2148, step time: 1.5382\n",
      "140/388, train_loss: 0.3175, step time: 1.5315\n",
      "141/388, train_loss: 0.6151, step time: 1.5336\n",
      "142/388, train_loss: 0.1072, step time: 1.5347\n",
      "143/388, train_loss: 0.0989, step time: 1.5347\n",
      "144/388, train_loss: 0.3308, step time: 1.5347\n",
      "145/388, train_loss: 0.2063, step time: 1.5354\n",
      "146/388, train_loss: 0.0770, step time: 1.5409\n",
      "147/388, train_loss: 0.1651, step time: 1.5386\n",
      "148/388, train_loss: 0.1110, step time: 1.5348\n",
      "149/388, train_loss: 0.1173, step time: 1.5327\n",
      "150/388, train_loss: 0.4207, step time: 1.5348\n",
      "151/388, train_loss: 0.3351, step time: 1.5322\n",
      "152/388, train_loss: 0.0949, step time: 1.5376\n",
      "153/388, train_loss: 0.1200, step time: 1.5344\n",
      "154/388, train_loss: 0.0967, step time: 1.5354\n",
      "155/388, train_loss: 0.4423, step time: 1.5564\n",
      "156/388, train_loss: 0.1066, step time: 1.5368\n",
      "157/388, train_loss: 0.4004, step time: 1.5355\n",
      "158/388, train_loss: 0.3329, step time: 1.5357\n",
      "159/388, train_loss: 0.2525, step time: 1.5319\n",
      "160/388, train_loss: 0.2694, step time: 1.5366\n",
      "161/388, train_loss: 0.2687, step time: 1.5291\n",
      "162/388, train_loss: 0.1962, step time: 1.5347\n",
      "163/388, train_loss: 0.1584, step time: 1.5347\n",
      "164/388, train_loss: 0.1138, step time: 1.5361\n",
      "165/388, train_loss: 0.2251, step time: 1.5336\n",
      "166/388, train_loss: 0.0981, step time: 1.5352\n",
      "167/388, train_loss: 0.1874, step time: 1.5290\n",
      "168/388, train_loss: 0.1018, step time: 1.5350\n",
      "169/388, train_loss: 0.3808, step time: 1.5340\n",
      "170/388, train_loss: 0.1435, step time: 1.5309\n",
      "171/388, train_loss: 0.1683, step time: 1.5401\n",
      "172/388, train_loss: 0.2562, step time: 1.5361\n",
      "173/388, train_loss: 0.3584, step time: 1.5357\n",
      "174/388, train_loss: 0.5408, step time: 1.5367\n",
      "175/388, train_loss: 0.1217, step time: 1.5333\n",
      "176/388, train_loss: 0.2846, step time: 1.5310\n",
      "177/388, train_loss: 0.2934, step time: 1.5339\n",
      "178/388, train_loss: 0.1017, step time: 1.5375\n",
      "179/388, train_loss: 0.1444, step time: 1.5358\n",
      "180/388, train_loss: 0.1006, step time: 1.5355\n",
      "181/388, train_loss: 0.0936, step time: 1.5329\n",
      "182/388, train_loss: 0.4083, step time: 1.5314\n",
      "183/388, train_loss: 0.0558, step time: 1.5320\n",
      "184/388, train_loss: 0.3464, step time: 1.5389\n",
      "185/388, train_loss: 0.3259, step time: 1.5344\n",
      "186/388, train_loss: 0.1153, step time: 1.5358\n",
      "187/388, train_loss: 0.0939, step time: 1.5348\n",
      "188/388, train_loss: 0.1135, step time: 1.5348\n",
      "189/388, train_loss: 0.1190, step time: 1.5316\n",
      "190/388, train_loss: 0.2992, step time: 1.5469\n",
      "191/388, train_loss: 0.0579, step time: 1.5360\n",
      "192/388, train_loss: 0.5082, step time: 1.5363\n",
      "193/388, train_loss: 0.1076, step time: 1.5338\n",
      "194/388, train_loss: 0.4152, step time: 1.5355\n",
      "195/388, train_loss: 0.2955, step time: 1.5321\n",
      "196/388, train_loss: 0.1848, step time: 1.5308\n",
      "197/388, train_loss: 0.0874, step time: 1.5345\n",
      "198/388, train_loss: 0.0837, step time: 1.5377\n",
      "199/388, train_loss: 0.1867, step time: 1.5345\n",
      "200/388, train_loss: 0.0892, step time: 1.5363\n",
      "201/388, train_loss: 0.2238, step time: 1.5345\n",
      "202/388, train_loss: 0.1501, step time: 1.5338\n",
      "203/388, train_loss: 0.2889, step time: 1.5371\n",
      "204/388, train_loss: 0.1901, step time: 1.5356\n",
      "205/388, train_loss: 0.1629, step time: 1.5371\n",
      "206/388, train_loss: 0.4342, step time: 1.5385\n",
      "207/388, train_loss: 0.1402, step time: 1.5327\n",
      "208/388, train_loss: 0.0537, step time: 1.5319\n",
      "209/388, train_loss: 0.4336, step time: 1.5303\n",
      "210/388, train_loss: 0.1247, step time: 1.5323\n",
      "211/388, train_loss: 0.1591, step time: 1.5337\n",
      "212/388, train_loss: 0.2520, step time: 1.5402\n",
      "213/388, train_loss: 0.2373, step time: 1.5364\n",
      "214/388, train_loss: 0.1807, step time: 1.5389\n",
      "215/388, train_loss: 0.1131, step time: 1.5350\n",
      "216/388, train_loss: 0.0745, step time: 1.5330\n",
      "217/388, train_loss: 0.1600, step time: 1.5352\n",
      "218/388, train_loss: 0.2687, step time: 1.5338\n",
      "219/388, train_loss: 0.1613, step time: 1.5365\n",
      "220/388, train_loss: 0.2998, step time: 1.5392\n",
      "221/388, train_loss: 0.1963, step time: 1.5328\n",
      "222/388, train_loss: 0.1834, step time: 1.5342\n",
      "223/388, train_loss: 0.0702, step time: 1.5319\n",
      "224/388, train_loss: 0.3816, step time: 1.5308\n",
      "225/388, train_loss: 0.1306, step time: 1.5313\n",
      "226/388, train_loss: 0.1667, step time: 1.5366\n",
      "227/388, train_loss: 0.1318, step time: 1.5320\n",
      "228/388, train_loss: 0.2218, step time: 1.5347\n",
      "229/388, train_loss: 0.2075, step time: 1.5315\n",
      "230/388, train_loss: 0.1935, step time: 1.5331\n",
      "231/388, train_loss: 0.0976, step time: 1.5347\n",
      "232/388, train_loss: 0.1259, step time: 1.5332\n",
      "233/388, train_loss: 0.1299, step time: 1.5337\n",
      "234/388, train_loss: 0.1180, step time: 1.5404\n",
      "235/388, train_loss: 0.1119, step time: 1.5604\n",
      "236/388, train_loss: 0.3673, step time: 1.5294\n",
      "237/388, train_loss: 0.0876, step time: 1.5306\n",
      "238/388, train_loss: 0.1099, step time: 1.5356\n",
      "239/388, train_loss: 0.1135, step time: 1.5300\n",
      "240/388, train_loss: 0.0827, step time: 1.5336\n",
      "241/388, train_loss: 0.1408, step time: 1.5320\n",
      "242/388, train_loss: 0.1793, step time: 1.5389\n",
      "243/388, train_loss: 0.0965, step time: 1.5373\n",
      "244/388, train_loss: 0.0550, step time: 1.5353\n",
      "245/388, train_loss: 0.1483, step time: 1.5438\n",
      "246/388, train_loss: 0.0622, step time: 1.5354\n",
      "247/388, train_loss: 0.1379, step time: 1.5334\n",
      "248/388, train_loss: 0.2249, step time: 1.5367\n",
      "249/388, train_loss: 0.1537, step time: 1.5339\n",
      "250/388, train_loss: 0.1347, step time: 1.5366\n",
      "251/388, train_loss: 0.0926, step time: 1.5318\n",
      "252/388, train_loss: 0.1179, step time: 1.5332\n",
      "253/388, train_loss: 0.1714, step time: 1.5294\n",
      "254/388, train_loss: 0.1649, step time: 1.5324\n",
      "255/388, train_loss: 0.2365, step time: 1.5338\n",
      "256/388, train_loss: 0.2440, step time: 1.5419\n",
      "257/388, train_loss: 0.0905, step time: 1.5353\n",
      "258/388, train_loss: 0.0779, step time: 1.5354\n",
      "259/388, train_loss: 0.0968, step time: 1.5341\n",
      "260/388, train_loss: 0.2529, step time: 1.5314\n",
      "261/388, train_loss: 0.2552, step time: 1.5338\n",
      "262/388, train_loss: 0.1684, step time: 1.5355\n",
      "263/388, train_loss: 0.2316, step time: 1.5377\n",
      "264/388, train_loss: 0.1147, step time: 1.5370\n",
      "265/388, train_loss: 0.0781, step time: 1.5329\n",
      "266/388, train_loss: 0.1029, step time: 1.5327\n",
      "267/388, train_loss: 0.2060, step time: 1.5348\n",
      "268/388, train_loss: 0.2008, step time: 1.5380\n",
      "269/388, train_loss: 0.0956, step time: 1.5370\n",
      "270/388, train_loss: 0.1822, step time: 1.5321\n",
      "271/388, train_loss: 0.1571, step time: 1.5339\n",
      "272/388, train_loss: 0.1097, step time: 1.5335\n",
      "273/388, train_loss: 0.1248, step time: 1.5322\n",
      "274/388, train_loss: 0.2053, step time: 1.5360\n",
      "275/388, train_loss: 0.1345, step time: 1.5358\n",
      "276/388, train_loss: 0.1147, step time: 1.5377\n",
      "277/388, train_loss: 0.1681, step time: 1.5293\n",
      "278/388, train_loss: 0.1426, step time: 1.5296\n",
      "279/388, train_loss: 0.2294, step time: 1.5302\n",
      "280/388, train_loss: 0.1687, step time: 1.5296\n",
      "281/388, train_loss: 0.0538, step time: 1.5309\n",
      "282/388, train_loss: 0.1033, step time: 1.5367\n",
      "283/388, train_loss: 0.1941, step time: 1.5351\n",
      "284/388, train_loss: 0.1071, step time: 1.5354\n",
      "285/388, train_loss: 0.2259, step time: 1.5343\n",
      "286/388, train_loss: 0.2541, step time: 1.5347\n",
      "287/388, train_loss: 0.1276, step time: 1.5338\n",
      "288/388, train_loss: 0.0826, step time: 1.5306\n",
      "289/388, train_loss: 0.2202, step time: 1.5327\n",
      "290/388, train_loss: 0.1024, step time: 1.5342\n",
      "291/388, train_loss: 0.4786, step time: 1.5337\n",
      "292/388, train_loss: 0.2864, step time: 1.5307\n",
      "293/388, train_loss: 0.3545, step time: 1.5315\n",
      "294/388, train_loss: 0.0833, step time: 1.5354\n",
      "295/388, train_loss: 0.1984, step time: 1.5387\n",
      "296/388, train_loss: 0.1647, step time: 1.5391\n",
      "297/388, train_loss: 0.1843, step time: 1.5356\n",
      "298/388, train_loss: 0.1010, step time: 1.5340\n",
      "299/388, train_loss: 0.1478, step time: 1.5344\n",
      "300/388, train_loss: 0.2302, step time: 1.5340\n",
      "301/388, train_loss: 0.2536, step time: 1.5344\n",
      "302/388, train_loss: 0.1119, step time: 1.5336\n",
      "303/388, train_loss: 0.0997, step time: 1.5345\n",
      "304/388, train_loss: 0.5859, step time: 1.5343\n",
      "305/388, train_loss: 0.3075, step time: 1.5330\n",
      "306/388, train_loss: 0.1507, step time: 1.5323\n",
      "307/388, train_loss: 0.1025, step time: 1.5317\n",
      "308/388, train_loss: 0.1713, step time: 1.5347\n",
      "309/388, train_loss: 0.1174, step time: 1.5349\n",
      "310/388, train_loss: 0.1497, step time: 1.5339\n",
      "311/388, train_loss: 0.2072, step time: 1.5334\n",
      "312/388, train_loss: 0.5077, step time: 1.5340\n",
      "313/388, train_loss: 0.0688, step time: 1.5409\n",
      "314/388, train_loss: 0.3349, step time: 1.5322\n",
      "315/388, train_loss: 0.1673, step time: 1.5355\n",
      "316/388, train_loss: 0.2486, step time: 1.5374\n",
      "317/388, train_loss: 0.0824, step time: 1.5376\n",
      "318/388, train_loss: 0.1038, step time: 1.5336\n",
      "319/388, train_loss: 0.0731, step time: 1.5342\n",
      "320/388, train_loss: 0.2884, step time: 1.5349\n",
      "321/388, train_loss: 0.2931, step time: 1.5362\n",
      "322/388, train_loss: 0.2939, step time: 1.5396\n",
      "323/388, train_loss: 0.0353, step time: 1.5321\n",
      "324/388, train_loss: 0.1167, step time: 1.5337\n",
      "325/388, train_loss: 0.1609, step time: 1.5335\n",
      "326/388, train_loss: 0.1077, step time: 1.5314\n",
      "327/388, train_loss: 0.1636, step time: 1.5311\n",
      "328/388, train_loss: 0.1661, step time: 1.5351\n",
      "329/388, train_loss: 0.3286, step time: 1.5346\n",
      "330/388, train_loss: 0.1920, step time: 1.5346\n",
      "331/388, train_loss: 0.2679, step time: 1.5329\n",
      "332/388, train_loss: 0.0538, step time: 1.5332\n",
      "333/388, train_loss: 0.4169, step time: 1.5296\n",
      "334/388, train_loss: 0.1259, step time: 1.5313\n",
      "335/388, train_loss: 0.1412, step time: 1.5322\n",
      "336/388, train_loss: 0.3269, step time: 1.5357\n",
      "337/388, train_loss: 0.2751, step time: 1.5350\n",
      "338/388, train_loss: 0.1388, step time: 1.5375\n",
      "339/388, train_loss: 0.6360, step time: 1.5349\n",
      "340/388, train_loss: 0.2014, step time: 1.5324\n",
      "341/388, train_loss: 0.2812, step time: 1.5353\n",
      "342/388, train_loss: 0.0704, step time: 1.5343\n",
      "343/388, train_loss: 0.3894, step time: 1.5298\n",
      "344/388, train_loss: 0.1975, step time: 1.5329\n",
      "345/388, train_loss: 0.1505, step time: 1.5324\n",
      "346/388, train_loss: 0.2947, step time: 1.5331\n",
      "347/388, train_loss: 0.1298, step time: 1.5370\n",
      "348/388, train_loss: 0.1264, step time: 1.5352\n",
      "349/388, train_loss: 0.1641, step time: 1.5351\n",
      "350/388, train_loss: 0.1217, step time: 1.5384\n",
      "351/388, train_loss: 0.2711, step time: 1.5330\n",
      "352/388, train_loss: 0.2539, step time: 1.5336\n",
      "353/388, train_loss: 0.2832, step time: 1.5311\n",
      "354/388, train_loss: 0.6162, step time: 1.5322\n",
      "355/388, train_loss: 0.2080, step time: 1.5387\n",
      "356/388, train_loss: 0.2427, step time: 1.5380\n",
      "357/388, train_loss: 0.1937, step time: 1.5352\n",
      "358/388, train_loss: 0.0866, step time: 1.5309\n",
      "359/388, train_loss: 0.2652, step time: 1.5312\n",
      "360/388, train_loss: 0.1769, step time: 1.5299\n",
      "361/388, train_loss: 0.2235, step time: 1.5445\n",
      "362/388, train_loss: 0.1566, step time: 1.5364\n",
      "363/388, train_loss: 0.5500, step time: 1.5348\n",
      "364/388, train_loss: 0.1006, step time: 1.5309\n",
      "365/388, train_loss: 0.3512, step time: 1.5320\n",
      "366/388, train_loss: 0.1833, step time: 1.5323\n",
      "367/388, train_loss: 0.2318, step time: 1.5365\n",
      "368/388, train_loss: 0.0928, step time: 1.5374\n",
      "369/388, train_loss: 0.1404, step time: 1.5333\n",
      "370/388, train_loss: 0.1075, step time: 1.5311\n",
      "371/388, train_loss: 0.2093, step time: 1.5310\n",
      "372/388, train_loss: 0.0344, step time: 1.5327\n",
      "373/388, train_loss: 0.5067, step time: 1.5338\n",
      "374/388, train_loss: 0.1778, step time: 1.5318\n",
      "375/388, train_loss: 0.0741, step time: 1.5361\n",
      "376/388, train_loss: 0.2126, step time: 1.5375\n",
      "377/388, train_loss: 0.1562, step time: 1.5333\n",
      "378/388, train_loss: 0.0881, step time: 1.5335\n",
      "379/388, train_loss: 0.0818, step time: 1.5340\n",
      "380/388, train_loss: 0.4408, step time: 1.5324\n",
      "381/388, train_loss: 0.2812, step time: 1.5329\n",
      "382/388, train_loss: 0.2414, step time: 1.5368\n",
      "383/388, train_loss: 0.4035, step time: 1.5344\n",
      "384/388, train_loss: 0.2179, step time: 1.5367\n",
      "385/388, train_loss: 0.1353, step time: 1.5352\n",
      "386/388, train_loss: 0.2755, step time: 1.5314\n",
      "387/388, train_loss: 0.2811, step time: 1.5327\n",
      "388/388, train_loss: 0.3615, step time: 1.5320\n",
      "epoch 26 average loss: 0.2117\n",
      "saved new best metric model\n",
      "current epoch: 26 current mean dice: 0.7545 tc: 0.8010 wt: 0.8989 et: 0.5635\n",
      "best mean dice: 0.7545 at epoch: 26\n",
      "time consuming of epoch 26 is: 703.2038\n",
      "----------\n",
      "epoch 27/100\n",
      "1/388, train_loss: 0.1339, step time: 1.5471\n",
      "2/388, train_loss: 0.0885, step time: 1.5351\n",
      "3/388, train_loss: 0.1118, step time: 1.5309\n",
      "4/388, train_loss: 0.1494, step time: 1.5320\n",
      "5/388, train_loss: 0.6140, step time: 1.5431\n",
      "6/388, train_loss: 0.2461, step time: 1.5333\n",
      "7/388, train_loss: 0.0564, step time: 1.5356\n",
      "8/388, train_loss: 0.4346, step time: 1.5368\n",
      "9/388, train_loss: 0.0981, step time: 1.5358\n",
      "10/388, train_loss: 0.1146, step time: 1.5315\n",
      "11/388, train_loss: 0.1429, step time: 1.5333\n",
      "12/388, train_loss: 0.2445, step time: 1.5362\n",
      "13/388, train_loss: 0.1467, step time: 1.5407\n",
      "14/388, train_loss: 0.3029, step time: 1.5343\n",
      "15/388, train_loss: 0.1975, step time: 1.5300\n",
      "16/388, train_loss: 0.4465, step time: 1.5301\n",
      "17/388, train_loss: 0.1190, step time: 1.5329\n",
      "18/388, train_loss: 0.2205, step time: 1.5383\n",
      "19/388, train_loss: 0.2540, step time: 1.5323\n",
      "20/388, train_loss: 0.3363, step time: 1.5346\n",
      "21/388, train_loss: 0.1670, step time: 1.5301\n",
      "22/388, train_loss: 0.2163, step time: 1.5318\n",
      "23/388, train_loss: 0.5019, step time: 1.5382\n",
      "24/388, train_loss: 0.0986, step time: 1.5365\n",
      "25/388, train_loss: 0.0887, step time: 1.5323\n",
      "26/388, train_loss: 0.1049, step time: 1.5346\n",
      "27/388, train_loss: 0.2930, step time: 1.5342\n",
      "28/388, train_loss: 0.2282, step time: 1.5366\n",
      "29/388, train_loss: 0.2846, step time: 1.5342\n",
      "30/388, train_loss: 0.1828, step time: 1.5341\n",
      "31/388, train_loss: 0.2922, step time: 1.5357\n",
      "32/388, train_loss: 0.3369, step time: 1.5325\n",
      "33/388, train_loss: 0.0918, step time: 1.5336\n",
      "34/388, train_loss: 0.1988, step time: 1.5346\n",
      "35/388, train_loss: 0.2470, step time: 1.5378\n",
      "36/388, train_loss: 0.1903, step time: 1.5334\n",
      "37/388, train_loss: 0.1438, step time: 1.5308\n",
      "38/388, train_loss: 0.0760, step time: 1.5316\n",
      "39/388, train_loss: 0.2834, step time: 1.5286\n",
      "40/388, train_loss: 0.2274, step time: 1.5343\n",
      "41/388, train_loss: 0.1827, step time: 1.5611\n",
      "42/388, train_loss: 0.4051, step time: 1.5317\n",
      "43/388, train_loss: 0.4695, step time: 1.5312\n",
      "44/388, train_loss: 0.5172, step time: 1.5389\n",
      "45/388, train_loss: 0.1303, step time: 1.5328\n",
      "46/388, train_loss: 0.2869, step time: 1.5326\n",
      "47/388, train_loss: 0.0425, step time: 1.5285\n",
      "48/388, train_loss: 0.3865, step time: 1.5344\n",
      "49/388, train_loss: 0.0552, step time: 1.5353\n",
      "50/388, train_loss: 0.1609, step time: 1.5336\n",
      "51/388, train_loss: 0.1999, step time: 1.5352\n",
      "52/388, train_loss: 0.1080, step time: 1.5351\n",
      "53/388, train_loss: 0.1582, step time: 1.5359\n",
      "54/388, train_loss: 0.1334, step time: 1.5362\n",
      "55/388, train_loss: 0.0971, step time: 1.5347\n",
      "56/388, train_loss: 0.3935, step time: 1.5358\n",
      "57/388, train_loss: 0.1714, step time: 1.5363\n",
      "58/388, train_loss: 0.1554, step time: 1.5300\n",
      "59/388, train_loss: 0.1055, step time: 1.5450\n",
      "60/388, train_loss: 0.2983, step time: 1.5388\n",
      "61/388, train_loss: 0.1257, step time: 1.5454\n",
      "62/388, train_loss: 0.0942, step time: 1.5311\n",
      "63/388, train_loss: 0.2496, step time: 1.5435\n",
      "64/388, train_loss: 0.0529, step time: 1.5348\n",
      "65/388, train_loss: 0.1647, step time: 1.5389\n",
      "66/388, train_loss: 0.0856, step time: 1.5366\n",
      "67/388, train_loss: 0.2735, step time: 1.5371\n",
      "68/388, train_loss: 0.2076, step time: 1.5323\n",
      "69/388, train_loss: 0.3159, step time: 1.5318\n",
      "70/388, train_loss: 0.0857, step time: 1.5327\n",
      "71/388, train_loss: 0.0706, step time: 1.5342\n",
      "72/388, train_loss: 0.1230, step time: 1.5445\n",
      "73/388, train_loss: 0.0844, step time: 1.5344\n",
      "74/388, train_loss: 0.2071, step time: 1.5331\n",
      "75/388, train_loss: 0.2200, step time: 1.5339\n",
      "76/388, train_loss: 0.1606, step time: 1.5405\n",
      "77/388, train_loss: 0.2167, step time: 1.5438\n",
      "78/388, train_loss: 0.1541, step time: 1.5315\n",
      "79/388, train_loss: 0.2502, step time: 1.5327\n",
      "80/388, train_loss: 0.2155, step time: 1.5301\n",
      "81/388, train_loss: 0.2379, step time: 1.5337\n",
      "82/388, train_loss: 0.2497, step time: 1.5385\n",
      "83/388, train_loss: 0.1186, step time: 1.5387\n",
      "84/388, train_loss: 0.2880, step time: 1.5341\n",
      "85/388, train_loss: 0.3947, step time: 1.5306\n",
      "86/388, train_loss: 0.1852, step time: 1.5339\n",
      "87/388, train_loss: 0.2400, step time: 1.5289\n",
      "88/388, train_loss: 0.0908, step time: 1.5327\n",
      "89/388, train_loss: 0.2720, step time: 1.5398\n",
      "90/388, train_loss: 0.1170, step time: 1.5382\n",
      "91/388, train_loss: 0.1730, step time: 1.5340\n",
      "92/388, train_loss: 0.1045, step time: 1.5335\n",
      "93/388, train_loss: 0.0747, step time: 1.5306\n",
      "94/388, train_loss: 0.4408, step time: 1.5361\n",
      "95/388, train_loss: 0.2040, step time: 1.5346\n",
      "96/388, train_loss: 0.1114, step time: 1.5345\n",
      "97/388, train_loss: 0.3146, step time: 1.5338\n",
      "98/388, train_loss: 0.5058, step time: 1.5326\n",
      "99/388, train_loss: 0.2205, step time: 1.5334\n",
      "100/388, train_loss: 0.2939, step time: 1.5336\n",
      "101/388, train_loss: 0.2766, step time: 1.5399\n",
      "102/388, train_loss: 0.5237, step time: 1.5338\n",
      "103/388, train_loss: 0.0608, step time: 1.5357\n",
      "104/388, train_loss: 0.1663, step time: 1.5576\n",
      "105/388, train_loss: 0.3349, step time: 1.5589\n",
      "106/388, train_loss: 0.3891, step time: 1.5358\n",
      "107/388, train_loss: 0.1435, step time: 1.5362\n",
      "108/388, train_loss: 0.1053, step time: 1.5331\n",
      "109/388, train_loss: 0.1230, step time: 1.5317\n",
      "110/388, train_loss: 0.1555, step time: 1.5311\n",
      "111/388, train_loss: 0.2091, step time: 1.5335\n",
      "112/388, train_loss: 0.0563, step time: 1.5376\n",
      "113/388, train_loss: 0.3327, step time: 1.5358\n",
      "114/388, train_loss: 0.1304, step time: 1.5369\n",
      "115/388, train_loss: 0.2301, step time: 1.5314\n",
      "116/388, train_loss: 0.0401, step time: 1.5329\n",
      "117/388, train_loss: 0.1137, step time: 1.5325\n",
      "118/388, train_loss: 0.3569, step time: 1.5373\n",
      "119/388, train_loss: 0.3336, step time: 1.5381\n",
      "120/388, train_loss: 0.0753, step time: 1.5376\n",
      "121/388, train_loss: 0.1190, step time: 1.5317\n",
      "122/388, train_loss: 0.2426, step time: 1.5380\n",
      "123/388, train_loss: 0.1246, step time: 1.5316\n",
      "124/388, train_loss: 0.0478, step time: 1.5329\n",
      "125/388, train_loss: 0.2546, step time: 1.5343\n",
      "126/388, train_loss: 0.0653, step time: 1.5338\n",
      "127/388, train_loss: 0.1266, step time: 1.5325\n",
      "128/388, train_loss: 0.0710, step time: 1.5461\n",
      "129/388, train_loss: 0.2915, step time: 1.5410\n",
      "130/388, train_loss: 0.5978, step time: 1.5454\n",
      "131/388, train_loss: 0.2109, step time: 1.5336\n",
      "132/388, train_loss: 0.1328, step time: 1.5320\n",
      "133/388, train_loss: 0.2241, step time: 1.5346\n",
      "134/388, train_loss: 0.2808, step time: 1.5329\n",
      "135/388, train_loss: 0.4381, step time: 1.5370\n",
      "136/388, train_loss: 0.2665, step time: 1.5352\n",
      "137/388, train_loss: 0.0802, step time: 1.5390\n",
      "138/388, train_loss: 0.1063, step time: 1.5346\n",
      "139/388, train_loss: 0.1939, step time: 1.5363\n",
      "140/388, train_loss: 0.1704, step time: 1.5344\n",
      "141/388, train_loss: 0.1580, step time: 1.5325\n",
      "142/388, train_loss: 0.2683, step time: 1.5336\n",
      "143/388, train_loss: 0.4316, step time: 1.5364\n",
      "144/388, train_loss: 0.1927, step time: 1.5372\n",
      "145/388, train_loss: 0.2882, step time: 1.5339\n",
      "146/388, train_loss: 0.1916, step time: 1.5320\n",
      "147/388, train_loss: 0.1491, step time: 1.5338\n",
      "148/388, train_loss: 0.1887, step time: 1.5523\n",
      "149/388, train_loss: 0.1214, step time: 1.5331\n",
      "150/388, train_loss: 0.5656, step time: 1.5339\n",
      "151/388, train_loss: 0.2673, step time: 1.5354\n",
      "152/388, train_loss: 0.4269, step time: 1.5348\n",
      "153/388, train_loss: 0.0889, step time: 1.5310\n",
      "154/388, train_loss: 0.2286, step time: 1.5317\n",
      "155/388, train_loss: 0.3589, step time: 1.5347\n",
      "156/388, train_loss: 0.2344, step time: 1.5368\n",
      "157/388, train_loss: 0.1641, step time: 1.5360\n",
      "158/388, train_loss: 0.1231, step time: 1.5328\n",
      "159/388, train_loss: 0.1371, step time: 1.5311\n",
      "160/388, train_loss: 0.1216, step time: 1.5306\n",
      "161/388, train_loss: 0.3108, step time: 1.5320\n",
      "162/388, train_loss: 0.6336, step time: 1.5345\n",
      "163/388, train_loss: 0.3160, step time: 1.5451\n",
      "164/388, train_loss: 0.1165, step time: 1.5341\n",
      "165/388, train_loss: 0.2458, step time: 1.5467\n",
      "166/388, train_loss: 0.1071, step time: 1.5347\n",
      "167/388, train_loss: 0.2040, step time: 1.5330\n",
      "168/388, train_loss: 0.1130, step time: 1.5382\n",
      "169/388, train_loss: 0.2486, step time: 1.5382\n",
      "170/388, train_loss: 0.1516, step time: 1.5356\n",
      "171/388, train_loss: 0.1009, step time: 1.5312\n",
      "172/388, train_loss: 0.1245, step time: 1.5354\n",
      "173/388, train_loss: 0.4960, step time: 1.5326\n",
      "174/388, train_loss: 0.2496, step time: 1.5348\n",
      "175/388, train_loss: 0.1721, step time: 1.5377\n",
      "176/388, train_loss: 0.1020, step time: 1.5343\n",
      "177/388, train_loss: 0.1425, step time: 1.5339\n",
      "178/388, train_loss: 0.1097, step time: 1.5334\n",
      "179/388, train_loss: 0.2188, step time: 1.5305\n",
      "180/388, train_loss: 0.4546, step time: 1.5322\n",
      "181/388, train_loss: 0.2666, step time: 1.5296\n",
      "182/388, train_loss: 0.2670, step time: 1.5342\n",
      "183/388, train_loss: 0.1161, step time: 1.5332\n",
      "184/388, train_loss: 0.1144, step time: 1.5370\n",
      "185/388, train_loss: 0.1626, step time: 1.5349\n",
      "186/388, train_loss: 0.1819, step time: 1.5304\n",
      "187/388, train_loss: 0.1058, step time: 1.5350\n",
      "188/388, train_loss: 0.1163, step time: 1.5330\n",
      "189/388, train_loss: 0.7202, step time: 1.5382\n",
      "190/388, train_loss: 0.1906, step time: 1.5385\n",
      "191/388, train_loss: 0.2749, step time: 1.5313\n",
      "192/388, train_loss: 0.1171, step time: 1.5322\n",
      "193/388, train_loss: 0.0832, step time: 1.5318\n",
      "194/388, train_loss: 0.1067, step time: 1.5334\n",
      "195/388, train_loss: 0.4307, step time: 1.5348\n",
      "196/388, train_loss: 0.2815, step time: 1.5412\n",
      "197/388, train_loss: 0.1871, step time: 1.5369\n",
      "198/388, train_loss: 0.1491, step time: 1.5352\n",
      "199/388, train_loss: 0.1953, step time: 1.5327\n",
      "200/388, train_loss: 0.0691, step time: 1.5327\n",
      "201/388, train_loss: 0.4101, step time: 1.5343\n",
      "202/388, train_loss: 0.4218, step time: 1.5393\n",
      "203/388, train_loss: 0.0495, step time: 1.5411\n",
      "204/388, train_loss: 0.4142, step time: 1.5337\n",
      "205/388, train_loss: 0.1877, step time: 1.5281\n",
      "206/388, train_loss: 0.1594, step time: 1.5333\n",
      "207/388, train_loss: 0.4026, step time: 1.5283\n",
      "208/388, train_loss: 0.6498, step time: 1.5376\n",
      "209/388, train_loss: 0.3689, step time: 1.5348\n",
      "210/388, train_loss: 0.3872, step time: 1.5354\n",
      "211/388, train_loss: 0.2488, step time: 1.5338\n",
      "212/388, train_loss: 0.1221, step time: 1.5388\n",
      "213/388, train_loss: 0.2454, step time: 1.5326\n",
      "214/388, train_loss: 0.1412, step time: 1.5382\n",
      "215/388, train_loss: 0.3317, step time: 1.5376\n",
      "216/388, train_loss: 0.1500, step time: 1.5300\n",
      "217/388, train_loss: 0.3814, step time: 1.5336\n",
      "218/388, train_loss: 0.2924, step time: 1.5377\n",
      "219/388, train_loss: 0.1179, step time: 1.5344\n",
      "220/388, train_loss: 0.4808, step time: 1.5298\n",
      "221/388, train_loss: 0.0924, step time: 1.5334\n",
      "222/388, train_loss: 0.1674, step time: 1.5322\n",
      "223/388, train_loss: 0.1516, step time: 1.5578\n",
      "224/388, train_loss: 0.1695, step time: 1.5371\n",
      "225/388, train_loss: 0.1341, step time: 1.5326\n",
      "226/388, train_loss: 0.1061, step time: 1.5389\n",
      "227/388, train_loss: 0.5596, step time: 1.5349\n",
      "228/388, train_loss: 0.1942, step time: 1.5354\n",
      "229/388, train_loss: 0.1473, step time: 1.5313\n",
      "230/388, train_loss: 0.1031, step time: 1.5345\n",
      "231/388, train_loss: 0.5063, step time: 1.5337\n",
      "232/388, train_loss: 0.1181, step time: 1.5369\n",
      "233/388, train_loss: 0.2377, step time: 1.5363\n",
      "234/388, train_loss: 0.1404, step time: 1.5364\n",
      "235/388, train_loss: 0.1057, step time: 1.5341\n",
      "236/388, train_loss: 0.0782, step time: 1.5337\n",
      "237/388, train_loss: 0.0991, step time: 1.5354\n",
      "238/388, train_loss: 0.3043, step time: 1.5368\n",
      "239/388, train_loss: 0.1470, step time: 1.5349\n",
      "240/388, train_loss: 0.1895, step time: 1.5320\n",
      "241/388, train_loss: 0.3122, step time: 1.5320\n",
      "242/388, train_loss: 0.0930, step time: 1.5288\n",
      "243/388, train_loss: 0.1144, step time: 1.5401\n",
      "244/388, train_loss: 0.2681, step time: 1.5306\n",
      "245/388, train_loss: 0.2246, step time: 1.5341\n",
      "246/388, train_loss: 0.1176, step time: 1.5325\n",
      "247/388, train_loss: 0.4129, step time: 1.5340\n",
      "248/388, train_loss: 0.1232, step time: 1.5354\n",
      "249/388, train_loss: 0.2772, step time: 1.5352\n",
      "250/388, train_loss: 0.0819, step time: 1.5378\n",
      "251/388, train_loss: 0.2239, step time: 1.5367\n",
      "252/388, train_loss: 0.2127, step time: 1.5339\n",
      "253/388, train_loss: 0.0636, step time: 1.5371\n",
      "254/388, train_loss: 0.1055, step time: 1.5343\n",
      "255/388, train_loss: 0.4581, step time: 1.5313\n",
      "256/388, train_loss: 0.1978, step time: 1.5367\n",
      "257/388, train_loss: 0.5542, step time: 1.5380\n",
      "258/388, train_loss: 0.2216, step time: 1.5347\n",
      "259/388, train_loss: 0.3079, step time: 1.5317\n",
      "260/388, train_loss: 0.0900, step time: 1.5312\n",
      "261/388, train_loss: 0.1387, step time: 1.5288\n",
      "262/388, train_loss: 0.1302, step time: 1.5350\n",
      "263/388, train_loss: 0.5600, step time: 1.5369\n",
      "264/388, train_loss: 0.1379, step time: 1.5358\n",
      "265/388, train_loss: 0.2017, step time: 1.5346\n",
      "266/388, train_loss: 0.1260, step time: 1.5341\n",
      "267/388, train_loss: 0.1406, step time: 1.5294\n",
      "268/388, train_loss: 0.1633, step time: 1.5374\n",
      "269/388, train_loss: 0.2490, step time: 1.5355\n",
      "270/388, train_loss: 0.1443, step time: 1.5431\n",
      "271/388, train_loss: 0.1653, step time: 1.5325\n",
      "272/388, train_loss: 0.2182, step time: 1.5304\n",
      "273/388, train_loss: 0.2614, step time: 1.5336\n",
      "274/388, train_loss: 0.2240, step time: 1.5398\n",
      "275/388, train_loss: 0.2807, step time: 1.5354\n",
      "276/388, train_loss: 0.1539, step time: 1.5376\n",
      "277/388, train_loss: 0.1430, step time: 1.5363\n",
      "278/388, train_loss: 0.1805, step time: 1.5349\n",
      "279/388, train_loss: 0.2057, step time: 1.5357\n",
      "280/388, train_loss: 0.1584, step time: 1.5358\n",
      "281/388, train_loss: 0.1859, step time: 1.5365\n",
      "282/388, train_loss: 0.2668, step time: 1.5403\n",
      "283/388, train_loss: 0.1371, step time: 1.5339\n",
      "284/388, train_loss: 0.1648, step time: 1.5319\n",
      "285/388, train_loss: 0.2114, step time: 1.5330\n",
      "286/388, train_loss: 0.3634, step time: 1.5352\n",
      "287/388, train_loss: 0.2016, step time: 1.5375\n",
      "288/388, train_loss: 0.2713, step time: 1.5358\n",
      "289/388, train_loss: 0.1008, step time: 1.5317\n",
      "290/388, train_loss: 0.1251, step time: 1.5320\n",
      "291/388, train_loss: 0.1211, step time: 1.5320\n",
      "292/388, train_loss: 0.1926, step time: 1.5290\n",
      "293/388, train_loss: 0.1028, step time: 1.5371\n",
      "294/388, train_loss: 0.2389, step time: 1.5388\n",
      "295/388, train_loss: 0.1536, step time: 1.5365\n",
      "296/388, train_loss: 0.2809, step time: 1.5643\n",
      "297/388, train_loss: 0.0746, step time: 1.5339\n",
      "298/388, train_loss: 0.1145, step time: 1.5349\n",
      "299/388, train_loss: 0.0821, step time: 1.5315\n",
      "300/388, train_loss: 0.4872, step time: 1.5330\n",
      "301/388, train_loss: 0.1960, step time: 1.5353\n",
      "302/388, train_loss: 0.1440, step time: 1.5349\n",
      "303/388, train_loss: 0.1848, step time: 1.5360\n",
      "304/388, train_loss: 0.5329, step time: 1.5401\n",
      "305/388, train_loss: 0.3169, step time: 1.5303\n",
      "306/388, train_loss: 0.2812, step time: 1.5325\n",
      "307/388, train_loss: 0.2469, step time: 1.5329\n",
      "308/388, train_loss: 0.1422, step time: 1.5374\n",
      "309/388, train_loss: 0.4110, step time: 1.5516\n",
      "310/388, train_loss: 0.1586, step time: 1.5344\n",
      "311/388, train_loss: 0.1115, step time: 1.5345\n",
      "312/388, train_loss: 0.1232, step time: 1.5386\n",
      "313/388, train_loss: 0.1670, step time: 1.5355\n",
      "314/388, train_loss: 0.2080, step time: 1.5385\n",
      "315/388, train_loss: 0.2006, step time: 1.5420\n",
      "316/388, train_loss: 0.1466, step time: 1.5309\n",
      "317/388, train_loss: 0.1888, step time: 1.5332\n",
      "318/388, train_loss: 0.1854, step time: 1.5368\n",
      "319/388, train_loss: 0.0763, step time: 1.5344\n",
      "320/388, train_loss: 0.1970, step time: 1.5308\n",
      "321/388, train_loss: 0.3401, step time: 1.5321\n",
      "322/388, train_loss: 0.1337, step time: 1.5319\n",
      "323/388, train_loss: 0.3347, step time: 1.5348\n",
      "324/388, train_loss: 0.1343, step time: 1.5350\n",
      "325/388, train_loss: 0.1301, step time: 1.5354\n",
      "326/388, train_loss: 0.3490, step time: 1.5343\n",
      "327/388, train_loss: 0.0722, step time: 1.5358\n",
      "328/388, train_loss: 0.3405, step time: 1.5337\n",
      "329/388, train_loss: 0.0677, step time: 1.5325\n",
      "330/388, train_loss: 0.1085, step time: 1.5335\n",
      "331/388, train_loss: 0.2294, step time: 1.5367\n",
      "332/388, train_loss: 0.3218, step time: 1.5356\n",
      "333/388, train_loss: 0.1020, step time: 1.5308\n",
      "334/388, train_loss: 0.3476, step time: 1.5321\n",
      "335/388, train_loss: 0.1926, step time: 1.5351\n",
      "336/388, train_loss: 0.1673, step time: 1.5370\n",
      "337/388, train_loss: 0.5602, step time: 1.5348\n",
      "338/388, train_loss: 0.1651, step time: 1.5418\n",
      "339/388, train_loss: 0.1074, step time: 1.5316\n",
      "340/388, train_loss: 0.0687, step time: 1.5356\n",
      "341/388, train_loss: 0.3002, step time: 1.5320\n",
      "342/388, train_loss: 0.2072, step time: 1.5344\n",
      "343/388, train_loss: 0.1090, step time: 1.5334\n",
      "344/388, train_loss: 0.1805, step time: 1.5417\n",
      "345/388, train_loss: 0.2264, step time: 1.5366\n",
      "346/388, train_loss: 0.1427, step time: 1.5322\n",
      "347/388, train_loss: 0.1806, step time: 1.5349\n",
      "348/388, train_loss: 0.0826, step time: 1.5378\n",
      "349/388, train_loss: 0.1217, step time: 1.5371\n",
      "350/388, train_loss: 0.0301, step time: 1.5307\n",
      "351/388, train_loss: 0.1902, step time: 1.5323\n",
      "352/388, train_loss: 0.0582, step time: 1.5327\n",
      "353/388, train_loss: 0.1960, step time: 1.5376\n",
      "354/388, train_loss: 0.1629, step time: 1.5351\n",
      "355/388, train_loss: 0.2978, step time: 1.5353\n",
      "356/388, train_loss: 0.1465, step time: 1.5341\n",
      "357/388, train_loss: 0.2069, step time: 1.5337\n",
      "358/388, train_loss: 0.1238, step time: 1.5325\n",
      "359/388, train_loss: 0.0825, step time: 1.5333\n",
      "360/388, train_loss: 0.4489, step time: 1.5353\n",
      "361/388, train_loss: 0.2091, step time: 1.5345\n",
      "362/388, train_loss: 0.2053, step time: 1.5343\n",
      "363/388, train_loss: 0.1119, step time: 1.5337\n",
      "364/388, train_loss: 0.3111, step time: 1.5346\n",
      "365/388, train_loss: 0.1445, step time: 1.5273\n",
      "366/388, train_loss: 0.4369, step time: 1.5303\n",
      "367/388, train_loss: 0.1128, step time: 1.5426\n",
      "368/388, train_loss: 0.2074, step time: 1.5324\n",
      "369/388, train_loss: 0.1751, step time: 1.5356\n",
      "370/388, train_loss: 0.5444, step time: 1.5329\n",
      "371/388, train_loss: 0.0831, step time: 1.5323\n",
      "372/388, train_loss: 0.3030, step time: 1.5336\n",
      "373/388, train_loss: 0.1181, step time: 1.5295\n",
      "374/388, train_loss: 0.2834, step time: 1.5317\n",
      "375/388, train_loss: 0.2766, step time: 1.5281\n",
      "376/388, train_loss: 0.1913, step time: 1.5346\n",
      "377/388, train_loss: 0.2313, step time: 1.5352\n",
      "378/388, train_loss: 0.2421, step time: 1.5367\n",
      "379/388, train_loss: 0.1826, step time: 1.5356\n",
      "380/388, train_loss: 0.1221, step time: 1.5305\n",
      "381/388, train_loss: 0.2267, step time: 1.5324\n",
      "382/388, train_loss: 0.0683, step time: 1.5275\n",
      "383/388, train_loss: 0.0756, step time: 1.5286\n",
      "384/388, train_loss: 0.4288, step time: 1.5320\n",
      "385/388, train_loss: 0.1376, step time: 1.5356\n",
      "386/388, train_loss: 0.1806, step time: 1.5562\n",
      "387/388, train_loss: 0.3475, step time: 1.5285\n",
      "388/388, train_loss: 0.4166, step time: 1.5325\n",
      "epoch 27 average loss: 0.2145\n",
      "current epoch: 27 current mean dice: 0.7133 tc: 0.7510 wt: 0.8881 et: 0.5006\n",
      "best mean dice: 0.7545 at epoch: 26\n",
      "time consuming of epoch 27 is: 702.0564\n",
      "----------\n",
      "epoch 28/100\n",
      "1/388, train_loss: 0.1089, step time: 1.5439\n",
      "2/388, train_loss: 0.0644, step time: 1.5350\n",
      "3/388, train_loss: 0.1228, step time: 1.5373\n",
      "4/388, train_loss: 0.3034, step time: 1.5373\n",
      "5/388, train_loss: 0.3233, step time: 1.5301\n",
      "6/388, train_loss: 0.2434, step time: 1.5347\n",
      "7/388, train_loss: 0.1821, step time: 1.5402\n",
      "8/388, train_loss: 0.1881, step time: 1.5381\n",
      "9/388, train_loss: 0.1886, step time: 1.5354\n",
      "10/388, train_loss: 0.2688, step time: 1.5371\n",
      "11/388, train_loss: 0.1467, step time: 1.5337\n",
      "12/388, train_loss: 0.1805, step time: 1.5381\n",
      "13/388, train_loss: 0.1488, step time: 1.5340\n",
      "14/388, train_loss: 0.2364, step time: 1.5352\n",
      "15/388, train_loss: 0.1142, step time: 1.5360\n",
      "16/388, train_loss: 0.1740, step time: 1.5356\n",
      "17/388, train_loss: 0.1800, step time: 1.5398\n",
      "18/388, train_loss: 0.2981, step time: 1.5320\n",
      "19/388, train_loss: 0.3157, step time: 1.5317\n",
      "20/388, train_loss: 0.4701, step time: 1.5341\n",
      "21/388, train_loss: 0.2930, step time: 1.5397\n",
      "22/388, train_loss: 0.1094, step time: 1.5343\n",
      "23/388, train_loss: 0.1356, step time: 1.5347\n",
      "24/388, train_loss: 0.1934, step time: 1.5336\n",
      "25/388, train_loss: 0.5672, step time: 1.5307\n",
      "26/388, train_loss: 0.2154, step time: 1.5327\n",
      "27/388, train_loss: 0.2485, step time: 1.5360\n",
      "28/388, train_loss: 0.4079, step time: 1.5361\n",
      "29/388, train_loss: 0.2374, step time: 1.5349\n",
      "30/388, train_loss: 0.1449, step time: 1.5318\n",
      "31/388, train_loss: 0.1397, step time: 1.5332\n",
      "32/388, train_loss: 0.2486, step time: 1.5355\n",
      "33/388, train_loss: 0.5558, step time: 1.5325\n",
      "34/388, train_loss: 0.2700, step time: 1.5348\n",
      "35/388, train_loss: 0.1035, step time: 1.5349\n",
      "36/388, train_loss: 0.1243, step time: 1.5354\n",
      "37/388, train_loss: 0.2251, step time: 1.5321\n",
      "38/388, train_loss: 0.0455, step time: 1.5319\n",
      "39/388, train_loss: 0.1044, step time: 1.5382\n",
      "40/388, train_loss: 0.2102, step time: 1.5383\n",
      "41/388, train_loss: 0.2006, step time: 1.5360\n",
      "42/388, train_loss: 0.1312, step time: 1.5326\n",
      "43/388, train_loss: 0.3239, step time: 1.5343\n",
      "44/388, train_loss: 0.1393, step time: 1.5355\n",
      "45/388, train_loss: 0.3409, step time: 1.5499\n",
      "46/388, train_loss: 0.2156, step time: 1.5361\n",
      "47/388, train_loss: 0.1650, step time: 1.5372\n",
      "48/388, train_loss: 0.1287, step time: 1.5326\n",
      "49/388, train_loss: 0.4814, step time: 1.5329\n",
      "50/388, train_loss: 0.3821, step time: 1.5376\n",
      "51/388, train_loss: 0.2476, step time: 1.5397\n",
      "52/388, train_loss: 0.1902, step time: 1.5420\n",
      "53/388, train_loss: 0.0968, step time: 1.5301\n",
      "54/388, train_loss: 0.1373, step time: 1.5356\n",
      "55/388, train_loss: 0.5064, step time: 1.5374\n",
      "56/388, train_loss: 0.3010, step time: 1.5352\n",
      "57/388, train_loss: 0.3665, step time: 1.5383\n",
      "58/388, train_loss: 0.1433, step time: 1.5377\n",
      "59/388, train_loss: 0.1871, step time: 1.5304\n",
      "60/388, train_loss: 0.2388, step time: 1.5327\n",
      "61/388, train_loss: 0.3914, step time: 1.5316\n",
      "62/388, train_loss: 0.0913, step time: 1.5322\n",
      "63/388, train_loss: 0.1448, step time: 1.5351\n",
      "64/388, train_loss: 0.1492, step time: 1.5331\n",
      "65/388, train_loss: 0.0817, step time: 1.5360\n",
      "66/388, train_loss: 0.2130, step time: 1.5384\n",
      "67/388, train_loss: 0.2071, step time: 1.5305\n",
      "68/388, train_loss: 0.1110, step time: 1.5354\n",
      "69/388, train_loss: 0.2347, step time: 1.5384\n",
      "70/388, train_loss: 0.1800, step time: 1.5347\n",
      "71/388, train_loss: 0.5722, step time: 1.5345\n",
      "72/388, train_loss: 0.1048, step time: 1.5322\n",
      "73/388, train_loss: 0.2605, step time: 1.5302\n",
      "74/388, train_loss: 0.1134, step time: 1.5304\n",
      "75/388, train_loss: 0.1962, step time: 1.5308\n",
      "76/388, train_loss: 0.2731, step time: 1.5392\n",
      "77/388, train_loss: 0.1638, step time: 1.5603\n",
      "78/388, train_loss: 0.3001, step time: 1.5311\n",
      "79/388, train_loss: 0.1312, step time: 1.5421\n",
      "80/388, train_loss: 0.3189, step time: 1.5388\n",
      "81/388, train_loss: 0.2560, step time: 1.5347\n",
      "82/388, train_loss: 0.0770, step time: 1.5432\n",
      "83/388, train_loss: 0.0873, step time: 1.5290\n",
      "84/388, train_loss: 0.1124, step time: 1.5332\n",
      "85/388, train_loss: 0.2857, step time: 1.5332\n",
      "86/388, train_loss: 0.0877, step time: 1.5408\n",
      "87/388, train_loss: 0.1429, step time: 1.5402\n",
      "88/388, train_loss: 0.1458, step time: 1.5336\n",
      "89/388, train_loss: 0.3597, step time: 1.5322\n",
      "90/388, train_loss: 0.1559, step time: 1.5344\n",
      "91/388, train_loss: 0.4896, step time: 1.5328\n",
      "92/388, train_loss: 0.0758, step time: 1.5379\n",
      "93/388, train_loss: 0.2250, step time: 1.5321\n",
      "94/388, train_loss: 0.4473, step time: 1.5343\n",
      "95/388, train_loss: 0.1833, step time: 1.5321\n",
      "96/388, train_loss: 0.1006, step time: 1.5298\n",
      "97/388, train_loss: 0.0777, step time: 1.5326\n",
      "98/388, train_loss: 0.1054, step time: 1.5333\n",
      "99/388, train_loss: 0.2531, step time: 1.5376\n",
      "100/388, train_loss: 0.2734, step time: 1.5351\n",
      "101/388, train_loss: 0.2167, step time: 1.5382\n",
      "102/388, train_loss: 0.0832, step time: 1.5331\n",
      "103/388, train_loss: 0.3123, step time: 1.5313\n",
      "104/388, train_loss: 0.1388, step time: 1.5329\n",
      "105/388, train_loss: 0.2198, step time: 1.5347\n",
      "106/388, train_loss: 0.0756, step time: 1.5362\n",
      "107/388, train_loss: 0.0888, step time: 1.5381\n",
      "108/388, train_loss: 0.1623, step time: 1.5321\n",
      "109/388, train_loss: 0.1976, step time: 1.5309\n",
      "110/388, train_loss: 0.1743, step time: 1.5320\n",
      "111/388, train_loss: 0.1142, step time: 1.5439\n",
      "112/388, train_loss: 0.1902, step time: 1.5341\n",
      "113/388, train_loss: 0.2157, step time: 1.5354\n",
      "114/388, train_loss: 0.1914, step time: 1.5356\n",
      "115/388, train_loss: 0.4150, step time: 1.5331\n",
      "116/388, train_loss: 0.2857, step time: 1.5334\n",
      "117/388, train_loss: 0.0918, step time: 1.5338\n",
      "118/388, train_loss: 0.2891, step time: 1.5396\n",
      "119/388, train_loss: 0.3666, step time: 1.5413\n",
      "120/388, train_loss: 0.1650, step time: 1.5370\n",
      "121/388, train_loss: 0.1247, step time: 1.5318\n",
      "122/388, train_loss: 0.1951, step time: 1.5365\n",
      "123/388, train_loss: 0.1796, step time: 1.5489\n",
      "124/388, train_loss: 0.2058, step time: 1.5414\n",
      "125/388, train_loss: 0.2950, step time: 1.5369\n",
      "126/388, train_loss: 0.1968, step time: 1.5315\n",
      "127/388, train_loss: 0.1361, step time: 1.5330\n",
      "128/388, train_loss: 0.0713, step time: 1.5433\n",
      "129/388, train_loss: 0.1400, step time: 1.5368\n",
      "130/388, train_loss: 0.0793, step time: 1.5397\n",
      "131/388, train_loss: 0.5059, step time: 1.5332\n",
      "132/388, train_loss: 0.0889, step time: 1.5437\n",
      "133/388, train_loss: 0.1794, step time: 1.5336\n",
      "134/388, train_loss: 0.1865, step time: 1.5307\n",
      "135/388, train_loss: 0.2120, step time: 1.5390\n",
      "136/388, train_loss: 0.3233, step time: 1.5363\n",
      "137/388, train_loss: 0.2306, step time: 1.5414\n",
      "138/388, train_loss: 0.3550, step time: 1.5394\n",
      "139/388, train_loss: 0.0993, step time: 1.5412\n",
      "140/388, train_loss: 0.2463, step time: 1.5351\n",
      "141/388, train_loss: 0.2468, step time: 1.5376\n",
      "142/388, train_loss: 0.1121, step time: 1.5340\n",
      "143/388, train_loss: 0.1350, step time: 1.5285\n",
      "144/388, train_loss: 0.1349, step time: 1.5318\n",
      "145/388, train_loss: 0.3315, step time: 1.5353\n",
      "146/388, train_loss: 0.2520, step time: 1.5367\n",
      "147/388, train_loss: 0.1289, step time: 1.5354\n",
      "148/388, train_loss: 0.1848, step time: 1.5399\n",
      "149/388, train_loss: 0.4330, step time: 1.5362\n",
      "150/388, train_loss: 0.1418, step time: 1.5335\n",
      "151/388, train_loss: 0.1996, step time: 1.5339\n",
      "152/388, train_loss: 0.2590, step time: 1.5489\n",
      "153/388, train_loss: 0.3280, step time: 1.5375\n",
      "154/388, train_loss: 0.1454, step time: 1.5325\n",
      "155/388, train_loss: 0.1187, step time: 1.5333\n",
      "156/388, train_loss: 0.3944, step time: 1.5364\n",
      "157/388, train_loss: 0.1366, step time: 1.5360\n",
      "158/388, train_loss: 0.5906, step time: 1.5362\n",
      "159/388, train_loss: 0.2303, step time: 1.5317\n",
      "160/388, train_loss: 0.1014, step time: 1.5398\n",
      "161/388, train_loss: 0.1297, step time: 1.5448\n",
      "162/388, train_loss: 0.1186, step time: 1.5392\n",
      "163/388, train_loss: 0.1301, step time: 1.5332\n",
      "164/388, train_loss: 0.6486, step time: 1.5338\n",
      "165/388, train_loss: 0.6159, step time: 1.5371\n",
      "166/388, train_loss: 0.1501, step time: 1.5332\n",
      "167/388, train_loss: 0.2516, step time: 1.5431\n",
      "168/388, train_loss: 0.0415, step time: 1.5326\n",
      "169/388, train_loss: 0.0950, step time: 1.5446\n",
      "170/388, train_loss: 0.0590, step time: 1.5375\n",
      "171/388, train_loss: 0.6004, step time: 1.5403\n",
      "172/388, train_loss: 0.2097, step time: 1.5373\n",
      "173/388, train_loss: 0.4289, step time: 1.5353\n",
      "174/388, train_loss: 0.0950, step time: 1.5388\n",
      "175/388, train_loss: 0.1000, step time: 1.5373\n",
      "176/388, train_loss: 0.1864, step time: 1.5393\n",
      "177/388, train_loss: 0.1428, step time: 1.5351\n",
      "178/388, train_loss: 0.2403, step time: 1.5339\n",
      "179/388, train_loss: 0.2147, step time: 1.5315\n",
      "180/388, train_loss: 0.2311, step time: 1.5320\n",
      "181/388, train_loss: 0.3486, step time: 1.5327\n",
      "182/388, train_loss: 0.2432, step time: 1.5349\n",
      "183/388, train_loss: 0.0998, step time: 1.5332\n",
      "184/388, train_loss: 0.2130, step time: 1.5478\n",
      "185/388, train_loss: 0.2485, step time: 1.5351\n",
      "186/388, train_loss: 0.3049, step time: 1.5324\n",
      "187/388, train_loss: 0.2612, step time: 1.5345\n",
      "188/388, train_loss: 0.3665, step time: 1.5457\n",
      "189/388, train_loss: 0.0658, step time: 1.5400\n",
      "190/388, train_loss: 0.3758, step time: 1.5334\n",
      "191/388, train_loss: 0.1073, step time: 1.5329\n",
      "192/388, train_loss: 0.3179, step time: 1.5336\n",
      "193/388, train_loss: 0.2016, step time: 1.5407\n",
      "194/388, train_loss: 0.1282, step time: 1.5382\n",
      "195/388, train_loss: 0.2134, step time: 1.5363\n",
      "196/388, train_loss: 0.2685, step time: 1.5328\n",
      "197/388, train_loss: 0.2606, step time: 1.5323\n",
      "198/388, train_loss: 0.2419, step time: 1.5642\n",
      "199/388, train_loss: 0.3695, step time: 1.5334\n",
      "200/388, train_loss: 0.1696, step time: 1.5352\n",
      "201/388, train_loss: 0.2058, step time: 1.5349\n",
      "202/388, train_loss: 0.3237, step time: 1.5384\n",
      "203/388, train_loss: 0.0766, step time: 1.5337\n",
      "204/388, train_loss: 0.2649, step time: 1.5343\n",
      "205/388, train_loss: 0.2804, step time: 1.5333\n",
      "206/388, train_loss: 0.1567, step time: 1.5438\n",
      "207/388, train_loss: 0.1360, step time: 1.5342\n",
      "208/388, train_loss: 0.1447, step time: 1.5335\n",
      "209/388, train_loss: 0.2628, step time: 1.5335\n",
      "210/388, train_loss: 0.0987, step time: 1.5331\n",
      "211/388, train_loss: 0.1357, step time: 1.5373\n",
      "212/388, train_loss: 0.1867, step time: 1.5404\n",
      "213/388, train_loss: 0.1256, step time: 1.5358\n",
      "214/388, train_loss: 0.0804, step time: 1.5309\n",
      "215/388, train_loss: 0.1892, step time: 1.5371\n",
      "216/388, train_loss: 0.1072, step time: 1.5398\n",
      "217/388, train_loss: 0.1495, step time: 1.5359\n",
      "218/388, train_loss: 0.1146, step time: 1.5352\n",
      "219/388, train_loss: 0.1402, step time: 1.5327\n",
      "220/388, train_loss: 0.1554, step time: 1.5384\n",
      "221/388, train_loss: 0.3213, step time: 1.5354\n",
      "222/388, train_loss: 0.0656, step time: 1.5341\n",
      "223/388, train_loss: 0.1884, step time: 1.5343\n",
      "224/388, train_loss: 0.4153, step time: 1.5377\n",
      "225/388, train_loss: 0.2233, step time: 1.5317\n",
      "226/388, train_loss: 0.2153, step time: 1.5353\n",
      "227/388, train_loss: 0.0991, step time: 1.5379\n",
      "228/388, train_loss: 0.1094, step time: 1.5668\n",
      "229/388, train_loss: 0.3059, step time: 1.5415\n",
      "230/388, train_loss: 0.3119, step time: 1.5331\n",
      "231/388, train_loss: 0.2338, step time: 1.5326\n",
      "232/388, train_loss: 0.5723, step time: 1.5353\n",
      "233/388, train_loss: 0.3845, step time: 1.5327\n",
      "234/388, train_loss: 0.2302, step time: 1.5369\n",
      "235/388, train_loss: 0.1805, step time: 1.5342\n",
      "236/388, train_loss: 0.0709, step time: 1.5311\n",
      "237/388, train_loss: 0.1036, step time: 1.5349\n",
      "238/388, train_loss: 0.1546, step time: 1.5343\n",
      "239/388, train_loss: 0.1040, step time: 1.5390\n",
      "240/388, train_loss: 0.1632, step time: 1.5343\n",
      "241/388, train_loss: 0.0689, step time: 1.5309\n",
      "242/388, train_loss: 0.1420, step time: 1.5357\n",
      "243/388, train_loss: 0.1555, step time: 1.5373\n",
      "244/388, train_loss: 0.1171, step time: 1.5382\n",
      "245/388, train_loss: 0.1561, step time: 1.5345\n",
      "246/388, train_loss: 0.1957, step time: 1.5350\n",
      "247/388, train_loss: 0.3332, step time: 1.5353\n",
      "248/388, train_loss: 0.0755, step time: 1.5365\n",
      "249/388, train_loss: 0.2241, step time: 1.5350\n",
      "250/388, train_loss: 0.2592, step time: 1.5387\n",
      "251/388, train_loss: 0.0404, step time: 1.5345\n",
      "252/388, train_loss: 0.2019, step time: 1.5331\n",
      "253/388, train_loss: 0.1650, step time: 1.5332\n",
      "254/388, train_loss: 0.0941, step time: 1.5359\n",
      "255/388, train_loss: 0.2492, step time: 1.5333\n",
      "256/388, train_loss: 0.1179, step time: 1.5330\n",
      "257/388, train_loss: 0.1891, step time: 1.5349\n",
      "258/388, train_loss: 0.4877, step time: 1.5299\n",
      "259/388, train_loss: 0.1319, step time: 1.5322\n",
      "260/388, train_loss: 0.0889, step time: 1.5308\n",
      "261/388, train_loss: 0.2086, step time: 1.5304\n",
      "262/388, train_loss: 0.2293, step time: 1.5336\n",
      "263/388, train_loss: 0.1850, step time: 1.5370\n",
      "264/388, train_loss: 0.1754, step time: 1.5359\n",
      "265/388, train_loss: 0.3100, step time: 1.5342\n",
      "266/388, train_loss: 0.4518, step time: 1.5308\n",
      "267/388, train_loss: 0.2172, step time: 1.5289\n",
      "268/388, train_loss: 0.3971, step time: 1.5292\n",
      "269/388, train_loss: 0.1019, step time: 1.5306\n",
      "270/388, train_loss: 0.3622, step time: 1.5302\n",
      "271/388, train_loss: 0.2808, step time: 1.5452\n",
      "272/388, train_loss: 0.1034, step time: 1.5379\n",
      "273/388, train_loss: 0.1707, step time: 1.5402\n",
      "274/388, train_loss: 0.1394, step time: 1.5323\n",
      "275/388, train_loss: 0.2760, step time: 1.5341\n",
      "276/388, train_loss: 0.2357, step time: 1.5322\n",
      "277/388, train_loss: 0.0962, step time: 1.5450\n",
      "278/388, train_loss: 0.1640, step time: 1.5366\n",
      "279/388, train_loss: 0.1283, step time: 1.5303\n",
      "280/388, train_loss: 0.1845, step time: 1.5367\n",
      "281/388, train_loss: 0.0615, step time: 1.5356\n",
      "282/388, train_loss: 0.3066, step time: 1.5331\n",
      "283/388, train_loss: 0.3248, step time: 1.5307\n",
      "284/388, train_loss: 0.5015, step time: 1.5393\n",
      "285/388, train_loss: 0.4420, step time: 1.5353\n",
      "286/388, train_loss: 0.7026, step time: 1.5361\n",
      "287/388, train_loss: 0.1020, step time: 1.5349\n",
      "288/388, train_loss: 0.0920, step time: 1.5311\n",
      "289/388, train_loss: 0.3692, step time: 1.5349\n",
      "290/388, train_loss: 0.1878, step time: 1.5384\n",
      "291/388, train_loss: 0.0980, step time: 1.5373\n",
      "292/388, train_loss: 0.1718, step time: 1.5354\n",
      "293/388, train_loss: 0.2042, step time: 1.5340\n",
      "294/388, train_loss: 0.3016, step time: 1.5360\n",
      "295/388, train_loss: 0.1619, step time: 1.5401\n",
      "296/388, train_loss: 0.1237, step time: 1.5352\n",
      "297/388, train_loss: 0.1372, step time: 1.5298\n",
      "298/388, train_loss: 0.1212, step time: 1.5332\n",
      "299/388, train_loss: 0.2867, step time: 1.5310\n",
      "300/388, train_loss: 0.0847, step time: 1.5313\n",
      "301/388, train_loss: 0.0865, step time: 1.5348\n",
      "302/388, train_loss: 0.5777, step time: 1.5370\n",
      "303/388, train_loss: 0.6465, step time: 1.5313\n",
      "304/388, train_loss: 0.2041, step time: 1.5312\n",
      "305/388, train_loss: 0.2515, step time: 1.5318\n",
      "306/388, train_loss: 0.1318, step time: 1.5361\n",
      "307/388, train_loss: 0.2402, step time: 1.5361\n",
      "308/388, train_loss: 0.1302, step time: 1.5416\n",
      "309/388, train_loss: 0.2728, step time: 1.5333\n",
      "310/388, train_loss: 0.0798, step time: 1.5325\n",
      "311/388, train_loss: 0.1998, step time: 1.5331\n",
      "312/388, train_loss: 0.3206, step time: 1.5346\n",
      "313/388, train_loss: 0.1675, step time: 1.5440\n",
      "314/388, train_loss: 0.5346, step time: 1.5366\n",
      "315/388, train_loss: 0.1404, step time: 1.5347\n",
      "316/388, train_loss: 0.1070, step time: 1.5349\n",
      "317/388, train_loss: 0.0873, step time: 1.5317\n",
      "318/388, train_loss: 0.1433, step time: 1.5341\n",
      "319/388, train_loss: 0.1775, step time: 1.5300\n",
      "320/388, train_loss: 0.1285, step time: 1.5321\n",
      "321/388, train_loss: 0.2346, step time: 1.5339\n",
      "322/388, train_loss: 0.1007, step time: 1.5371\n",
      "323/388, train_loss: 0.2496, step time: 1.5382\n",
      "324/388, train_loss: 0.1514, step time: 1.5309\n",
      "325/388, train_loss: 0.2299, step time: 1.5341\n",
      "326/388, train_loss: 0.1658, step time: 1.5299\n",
      "327/388, train_loss: 0.0508, step time: 1.5340\n",
      "328/388, train_loss: 0.0653, step time: 1.5436\n",
      "329/388, train_loss: 0.1540, step time: 1.5390\n",
      "330/388, train_loss: 0.1579, step time: 1.5343\n",
      "331/388, train_loss: 0.1091, step time: 1.5339\n",
      "332/388, train_loss: 0.2361, step time: 1.5342\n",
      "333/388, train_loss: 0.0533, step time: 1.5330\n",
      "334/388, train_loss: 0.1476, step time: 1.5338\n",
      "335/388, train_loss: 0.0635, step time: 1.5370\n",
      "336/388, train_loss: 0.2597, step time: 1.5355\n",
      "337/388, train_loss: 0.3102, step time: 1.5370\n",
      "338/388, train_loss: 0.1533, step time: 1.5352\n",
      "339/388, train_loss: 0.1182, step time: 1.5361\n",
      "340/388, train_loss: 0.0921, step time: 1.5358\n",
      "341/388, train_loss: 0.2209, step time: 1.5345\n",
      "342/388, train_loss: 0.3577, step time: 1.5351\n",
      "343/388, train_loss: 0.1869, step time: 1.5342\n",
      "344/388, train_loss: 0.1104, step time: 1.5356\n",
      "345/388, train_loss: 0.3150, step time: 1.5331\n",
      "346/388, train_loss: 0.0698, step time: 1.5350\n",
      "347/388, train_loss: 0.1110, step time: 1.5356\n",
      "348/388, train_loss: 0.1907, step time: 1.5377\n",
      "349/388, train_loss: 0.1436, step time: 1.5315\n",
      "350/388, train_loss: 0.2998, step time: 1.5332\n",
      "351/388, train_loss: 0.3337, step time: 1.5327\n",
      "352/388, train_loss: 0.1492, step time: 1.5462\n",
      "353/388, train_loss: 0.1052, step time: 1.5365\n",
      "354/388, train_loss: 0.2493, step time: 1.5364\n",
      "355/388, train_loss: 0.3856, step time: 1.5351\n",
      "356/388, train_loss: 0.0490, step time: 1.5330\n",
      "357/388, train_loss: 0.2605, step time: 1.5328\n",
      "358/388, train_loss: 0.0690, step time: 1.5344\n",
      "359/388, train_loss: 0.0640, step time: 1.5385\n",
      "360/388, train_loss: 0.2435, step time: 1.5370\n",
      "361/388, train_loss: 0.0983, step time: 1.5356\n",
      "362/388, train_loss: 0.2247, step time: 1.5345\n",
      "363/388, train_loss: 0.3578, step time: 1.5318\n",
      "364/388, train_loss: 0.3199, step time: 1.5365\n",
      "365/388, train_loss: 0.2287, step time: 1.5351\n",
      "366/388, train_loss: 0.1317, step time: 1.5356\n",
      "367/388, train_loss: 0.1629, step time: 1.5352\n",
      "368/388, train_loss: 0.1437, step time: 1.5335\n",
      "369/388, train_loss: 0.1272, step time: 1.5308\n",
      "370/388, train_loss: 0.2873, step time: 1.5331\n",
      "371/388, train_loss: 0.2581, step time: 1.5314\n",
      "372/388, train_loss: 0.5350, step time: 1.5322\n",
      "373/388, train_loss: 0.1385, step time: 1.5441\n",
      "374/388, train_loss: 0.1334, step time: 1.5359\n",
      "375/388, train_loss: 0.1117, step time: 1.5333\n",
      "376/388, train_loss: 0.1537, step time: 1.5325\n",
      "377/388, train_loss: 0.1889, step time: 1.5334\n",
      "378/388, train_loss: 0.1107, step time: 1.5341\n",
      "379/388, train_loss: 0.2158, step time: 1.5397\n",
      "380/388, train_loss: 0.0881, step time: 1.5362\n",
      "381/388, train_loss: 0.1281, step time: 1.5322\n",
      "382/388, train_loss: 0.1908, step time: 1.5273\n",
      "383/388, train_loss: 0.0999, step time: 1.5306\n",
      "384/388, train_loss: 0.2479, step time: 1.5357\n",
      "385/388, train_loss: 0.3417, step time: 1.5359\n",
      "386/388, train_loss: 0.1102, step time: 1.5362\n",
      "387/388, train_loss: 0.0734, step time: 1.5347\n",
      "388/388, train_loss: 0.5051, step time: 1.5359\n",
      "epoch 28 average loss: 0.2111\n",
      "current epoch: 28 current mean dice: 0.7315 tc: 0.7683 wt: 0.8959 et: 0.5302\n",
      "best mean dice: 0.7545 at epoch: 26\n",
      "time consuming of epoch 28 is: 704.1767\n",
      "----------\n",
      "epoch 29/100\n",
      "1/388, train_loss: 0.4083, step time: 1.5542\n",
      "2/388, train_loss: 0.1254, step time: 1.5340\n",
      "3/388, train_loss: 0.0918, step time: 1.5384\n",
      "4/388, train_loss: 0.1072, step time: 1.5400\n",
      "5/388, train_loss: 0.1824, step time: 1.5343\n",
      "6/388, train_loss: 0.0754, step time: 1.5300\n",
      "7/388, train_loss: 0.5300, step time: 1.5361\n",
      "8/388, train_loss: 0.1010, step time: 1.5369\n",
      "9/388, train_loss: 0.1878, step time: 1.5314\n",
      "10/388, train_loss: 0.1564, step time: 1.5339\n",
      "11/388, train_loss: 0.0433, step time: 1.5325\n",
      "12/388, train_loss: 0.0668, step time: 1.5336\n",
      "13/388, train_loss: 0.1008, step time: 1.5330\n",
      "14/388, train_loss: 0.3099, step time: 1.5383\n",
      "15/388, train_loss: 0.1848, step time: 1.5397\n",
      "16/388, train_loss: 0.5261, step time: 1.5376\n",
      "17/388, train_loss: 0.2076, step time: 1.5348\n",
      "18/388, train_loss: 0.1899, step time: 1.5376\n",
      "19/388, train_loss: 0.2117, step time: 1.5363\n",
      "20/388, train_loss: 0.1497, step time: 1.5395\n",
      "21/388, train_loss: 0.1546, step time: 1.5332\n",
      "22/388, train_loss: 0.3583, step time: 1.5426\n",
      "23/388, train_loss: 0.2125, step time: 1.5323\n",
      "24/388, train_loss: 0.3091, step time: 1.5408\n",
      "25/388, train_loss: 0.6014, step time: 1.5385\n",
      "26/388, train_loss: 0.2890, step time: 1.5327\n",
      "27/388, train_loss: 0.3500, step time: 1.5346\n",
      "28/388, train_loss: 0.1226, step time: 1.5339\n",
      "29/388, train_loss: 0.1231, step time: 1.5349\n",
      "30/388, train_loss: 0.2499, step time: 1.5359\n",
      "31/388, train_loss: 0.1517, step time: 1.5361\n",
      "32/388, train_loss: 0.1224, step time: 1.5342\n",
      "33/388, train_loss: 0.1575, step time: 1.5330\n",
      "34/388, train_loss: 0.1512, step time: 1.5328\n",
      "35/388, train_loss: 0.1713, step time: 1.5391\n",
      "36/388, train_loss: 0.2044, step time: 1.5363\n",
      "37/388, train_loss: 0.1562, step time: 1.5321\n",
      "38/388, train_loss: 0.2101, step time: 1.5300\n",
      "39/388, train_loss: 0.0676, step time: 1.5312\n",
      "40/388, train_loss: 0.2653, step time: 1.5323\n",
      "41/388, train_loss: 0.2511, step time: 1.5370\n",
      "42/388, train_loss: 0.1041, step time: 1.5351\n",
      "43/388, train_loss: 0.3208, step time: 1.5361\n",
      "44/388, train_loss: 0.0487, step time: 1.5337\n",
      "45/388, train_loss: 0.5751, step time: 1.5320\n",
      "46/388, train_loss: 0.3364, step time: 1.5333\n",
      "47/388, train_loss: 0.1339, step time: 1.5382\n",
      "48/388, train_loss: 0.1553, step time: 1.5371\n",
      "49/388, train_loss: 0.1666, step time: 1.5331\n",
      "50/388, train_loss: 0.1547, step time: 1.5348\n",
      "51/388, train_loss: 0.2060, step time: 1.5337\n",
      "52/388, train_loss: 0.5520, step time: 1.5331\n",
      "53/388, train_loss: 0.4725, step time: 1.5320\n",
      "54/388, train_loss: 0.1248, step time: 1.5313\n",
      "55/388, train_loss: 0.1695, step time: 1.5334\n",
      "56/388, train_loss: 0.1364, step time: 1.5349\n",
      "57/388, train_loss: 0.2020, step time: 1.5365\n",
      "58/388, train_loss: 0.1534, step time: 1.5333\n",
      "59/388, train_loss: 0.1933, step time: 1.5336\n",
      "60/388, train_loss: 0.0757, step time: 1.5338\n",
      "61/388, train_loss: 0.3751, step time: 1.5351\n",
      "62/388, train_loss: 0.2859, step time: 1.5352\n",
      "63/388, train_loss: 0.3025, step time: 1.5364\n",
      "64/388, train_loss: 0.3672, step time: 1.5349\n",
      "65/388, train_loss: 0.1914, step time: 1.5318\n",
      "66/388, train_loss: 0.2375, step time: 1.5323\n",
      "67/388, train_loss: 0.0557, step time: 1.5337\n",
      "68/388, train_loss: 0.1088, step time: 1.5358\n",
      "69/388, train_loss: 0.4177, step time: 1.5386\n",
      "70/388, train_loss: 0.6534, step time: 1.5319\n",
      "71/388, train_loss: 0.1304, step time: 1.5293\n",
      "72/388, train_loss: 0.2159, step time: 1.5296\n",
      "73/388, train_loss: 0.4046, step time: 1.5335\n",
      "74/388, train_loss: 0.1831, step time: 1.5313\n",
      "75/388, train_loss: 0.1499, step time: 1.5316\n",
      "76/388, train_loss: 0.0819, step time: 1.5379\n",
      "77/388, train_loss: 0.2223, step time: 1.5342\n",
      "78/388, train_loss: 0.1234, step time: 1.5346\n",
      "79/388, train_loss: 0.2299, step time: 1.5364\n",
      "80/388, train_loss: 0.0651, step time: 1.5314\n",
      "81/388, train_loss: 0.2777, step time: 1.5310\n",
      "82/388, train_loss: 0.1494, step time: 1.5312\n",
      "83/388, train_loss: 0.2121, step time: 1.5311\n",
      "84/388, train_loss: 0.0665, step time: 1.5372\n",
      "85/388, train_loss: 0.0926, step time: 1.5327\n",
      "86/388, train_loss: 0.3865, step time: 1.5292\n",
      "87/388, train_loss: 0.2906, step time: 1.5323\n",
      "88/388, train_loss: 0.0934, step time: 1.5307\n",
      "89/388, train_loss: 0.2344, step time: 1.5331\n",
      "90/388, train_loss: 0.2973, step time: 1.5350\n",
      "91/388, train_loss: 0.1046, step time: 1.5474\n",
      "92/388, train_loss: 0.5083, step time: 1.5318\n",
      "93/388, train_loss: 0.1129, step time: 1.5316\n",
      "94/388, train_loss: 0.1836, step time: 1.5373\n",
      "95/388, train_loss: 0.1170, step time: 1.5355\n",
      "96/388, train_loss: 0.2568, step time: 1.5360\n",
      "97/388, train_loss: 0.0986, step time: 1.5346\n",
      "98/388, train_loss: 0.0983, step time: 1.5332\n",
      "99/388, train_loss: 0.1420, step time: 1.5333\n",
      "100/388, train_loss: 0.0768, step time: 1.5319\n",
      "101/388, train_loss: 0.1394, step time: 1.5321\n",
      "102/388, train_loss: 0.5278, step time: 1.5335\n",
      "103/388, train_loss: 0.1067, step time: 1.5326\n",
      "104/388, train_loss: 0.0931, step time: 1.5352\n",
      "105/388, train_loss: 0.1757, step time: 1.5318\n",
      "106/388, train_loss: 0.2045, step time: 1.5335\n",
      "107/388, train_loss: 0.2404, step time: 1.5310\n",
      "108/388, train_loss: 0.2055, step time: 1.5309\n",
      "109/388, train_loss: 0.1918, step time: 1.5356\n",
      "110/388, train_loss: 0.1976, step time: 1.5336\n",
      "111/388, train_loss: 0.0843, step time: 1.5331\n",
      "112/388, train_loss: 0.2344, step time: 1.5359\n",
      "113/388, train_loss: 0.3426, step time: 1.5303\n",
      "114/388, train_loss: 0.0601, step time: 1.5590\n",
      "115/388, train_loss: 0.4184, step time: 1.5330\n",
      "116/388, train_loss: 0.3247, step time: 1.5309\n",
      "117/388, train_loss: 0.0831, step time: 1.5298\n",
      "118/388, train_loss: 0.2560, step time: 1.5269\n",
      "119/388, train_loss: 0.4850, step time: 1.5381\n",
      "120/388, train_loss: 0.0946, step time: 1.5350\n",
      "121/388, train_loss: 0.2903, step time: 1.5317\n",
      "122/388, train_loss: 0.4559, step time: 1.5293\n",
      "123/388, train_loss: 0.1817, step time: 1.5308\n",
      "124/388, train_loss: 0.1031, step time: 1.5349\n",
      "125/388, train_loss: 0.2619, step time: 1.5349\n",
      "126/388, train_loss: 0.2270, step time: 1.5367\n",
      "127/388, train_loss: 0.2384, step time: 1.5353\n",
      "128/388, train_loss: 0.2074, step time: 1.5322\n",
      "129/388, train_loss: 0.1771, step time: 1.5308\n",
      "130/388, train_loss: 0.2162, step time: 1.5304\n",
      "131/388, train_loss: 0.3301, step time: 1.5313\n",
      "132/388, train_loss: 0.3572, step time: 1.5320\n",
      "133/388, train_loss: 0.1195, step time: 1.5348\n",
      "134/388, train_loss: 0.3229, step time: 1.5374\n",
      "135/388, train_loss: 0.0710, step time: 1.5339\n",
      "136/388, train_loss: 0.1690, step time: 1.5288\n",
      "137/388, train_loss: 0.1383, step time: 1.5344\n",
      "138/388, train_loss: 0.2697, step time: 1.5352\n",
      "139/388, train_loss: 0.2970, step time: 1.5360\n",
      "140/388, train_loss: 0.0837, step time: 1.5347\n",
      "141/388, train_loss: 0.3002, step time: 1.5337\n",
      "142/388, train_loss: 0.1586, step time: 1.5322\n",
      "143/388, train_loss: 0.1263, step time: 1.5340\n",
      "144/388, train_loss: 0.2515, step time: 1.5332\n",
      "145/388, train_loss: 0.6175, step time: 1.5314\n",
      "146/388, train_loss: 0.0551, step time: 1.5379\n",
      "147/388, train_loss: 0.2851, step time: 1.5356\n",
      "148/388, train_loss: 0.1123, step time: 1.5301\n",
      "149/388, train_loss: 0.0848, step time: 1.5325\n",
      "150/388, train_loss: 0.1429, step time: 1.5304\n",
      "151/388, train_loss: 0.2414, step time: 1.5321\n",
      "152/388, train_loss: 0.1324, step time: 1.5368\n",
      "153/388, train_loss: 0.1684, step time: 1.5360\n",
      "154/388, train_loss: 0.3841, step time: 1.5364\n",
      "155/388, train_loss: 0.1433, step time: 1.5297\n",
      "156/388, train_loss: 0.1644, step time: 1.5315\n",
      "157/388, train_loss: 0.1083, step time: 1.5308\n",
      "158/388, train_loss: 0.1473, step time: 1.5331\n",
      "159/388, train_loss: 0.2352, step time: 1.5414\n",
      "160/388, train_loss: 0.0967, step time: 1.5358\n",
      "161/388, train_loss: 0.1109, step time: 1.5295\n",
      "162/388, train_loss: 0.1720, step time: 1.5312\n",
      "163/388, train_loss: 0.2273, step time: 1.5304\n",
      "164/388, train_loss: 0.6381, step time: 1.5291\n",
      "165/388, train_loss: 0.1347, step time: 1.5283\n",
      "166/388, train_loss: 0.1296, step time: 1.5399\n",
      "167/388, train_loss: 0.3101, step time: 1.5325\n",
      "168/388, train_loss: 0.0989, step time: 1.5318\n",
      "169/388, train_loss: 0.2472, step time: 1.5301\n",
      "170/388, train_loss: 0.2312, step time: 1.5335\n",
      "171/388, train_loss: 0.1982, step time: 1.5343\n",
      "172/388, train_loss: 0.3330, step time: 1.5386\n",
      "173/388, train_loss: 0.0855, step time: 1.5346\n",
      "174/388, train_loss: 0.0867, step time: 1.5352\n",
      "175/388, train_loss: 0.1019, step time: 1.5298\n",
      "176/388, train_loss: 0.0614, step time: 1.5318\n",
      "177/388, train_loss: 0.2453, step time: 1.5311\n",
      "178/388, train_loss: 0.0669, step time: 1.5359\n",
      "179/388, train_loss: 0.2281, step time: 1.5369\n",
      "180/388, train_loss: 0.2893, step time: 1.5350\n",
      "181/388, train_loss: 0.1233, step time: 1.5360\n",
      "182/388, train_loss: 0.1448, step time: 1.5328\n",
      "183/388, train_loss: 0.2753, step time: 1.5345\n",
      "184/388, train_loss: 0.0957, step time: 1.5355\n",
      "185/388, train_loss: 0.0974, step time: 1.5313\n",
      "186/388, train_loss: 0.0791, step time: 1.5364\n",
      "187/388, train_loss: 0.3086, step time: 1.5330\n",
      "188/388, train_loss: 0.1905, step time: 1.5422\n",
      "189/388, train_loss: 0.1564, step time: 1.5314\n",
      "190/388, train_loss: 0.1657, step time: 1.5307\n",
      "191/388, train_loss: 0.1111, step time: 1.5300\n",
      "192/388, train_loss: 0.2967, step time: 1.5322\n",
      "193/388, train_loss: 0.2331, step time: 1.5376\n",
      "194/388, train_loss: 0.4599, step time: 1.5393\n",
      "195/388, train_loss: 0.4300, step time: 1.5359\n",
      "196/388, train_loss: 0.0854, step time: 1.5331\n",
      "197/388, train_loss: 0.1630, step time: 1.5351\n",
      "198/388, train_loss: 0.1768, step time: 1.5330\n",
      "199/388, train_loss: 0.2388, step time: 1.5326\n",
      "200/388, train_loss: 0.1065, step time: 1.5313\n",
      "201/388, train_loss: 0.1167, step time: 1.5330\n",
      "202/388, train_loss: 0.1236, step time: 1.5368\n",
      "203/388, train_loss: 0.3150, step time: 1.5385\n",
      "204/388, train_loss: 0.1051, step time: 1.5331\n",
      "205/388, train_loss: 0.5379, step time: 1.5433\n",
      "206/388, train_loss: 0.0868, step time: 1.5315\n",
      "207/388, train_loss: 0.0672, step time: 1.5350\n",
      "208/388, train_loss: 0.2386, step time: 1.5384\n",
      "209/388, train_loss: 0.1437, step time: 1.5335\n",
      "210/388, train_loss: 0.1751, step time: 1.5314\n",
      "211/388, train_loss: 0.2777, step time: 1.5309\n",
      "212/388, train_loss: 0.1163, step time: 1.5302\n",
      "213/388, train_loss: 0.1116, step time: 1.5310\n",
      "214/388, train_loss: 0.1269, step time: 1.5365\n",
      "215/388, train_loss: 0.1015, step time: 1.5345\n",
      "216/388, train_loss: 0.1381, step time: 1.5300\n",
      "217/388, train_loss: 0.1322, step time: 1.5348\n",
      "218/388, train_loss: 0.1354, step time: 1.5305\n",
      "219/388, train_loss: 0.1767, step time: 1.5325\n",
      "220/388, train_loss: 0.0854, step time: 1.5338\n",
      "221/388, train_loss: 0.3258, step time: 1.5350\n",
      "222/388, train_loss: 0.1242, step time: 1.5344\n",
      "223/388, train_loss: 0.0369, step time: 1.5341\n",
      "224/388, train_loss: 0.1897, step time: 1.5306\n",
      "225/388, train_loss: 0.3719, step time: 1.5320\n",
      "226/388, train_loss: 0.2399, step time: 1.5319\n",
      "227/388, train_loss: 0.1287, step time: 1.5459\n",
      "228/388, train_loss: 0.2016, step time: 1.5313\n",
      "229/388, train_loss: 0.0465, step time: 1.5338\n",
      "230/388, train_loss: 0.1556, step time: 1.5339\n",
      "231/388, train_loss: 0.2417, step time: 1.5330\n",
      "232/388, train_loss: 0.0696, step time: 1.5377\n",
      "233/388, train_loss: 0.1768, step time: 1.5361\n",
      "234/388, train_loss: 0.2435, step time: 1.5337\n",
      "235/388, train_loss: 0.0697, step time: 1.5319\n",
      "236/388, train_loss: 0.0501, step time: 1.5319\n",
      "237/388, train_loss: 0.0816, step time: 1.5342\n",
      "238/388, train_loss: 0.2103, step time: 1.5311\n",
      "239/388, train_loss: 0.0949, step time: 1.5461\n",
      "240/388, train_loss: 0.1051, step time: 1.5344\n",
      "241/388, train_loss: 0.2262, step time: 1.5298\n",
      "242/388, train_loss: 0.1327, step time: 1.5284\n",
      "243/388, train_loss: 0.2545, step time: 1.5284\n",
      "244/388, train_loss: 0.2067, step time: 1.5353\n",
      "245/388, train_loss: 0.2481, step time: 1.5353\n",
      "246/388, train_loss: 0.1658, step time: 1.5337\n",
      "247/388, train_loss: 0.1557, step time: 1.5353\n",
      "248/388, train_loss: 0.1260, step time: 1.5347\n",
      "249/388, train_loss: 0.2289, step time: 1.5320\n",
      "250/388, train_loss: 0.0872, step time: 1.5302\n",
      "251/388, train_loss: 0.2592, step time: 1.5332\n",
      "252/388, train_loss: 0.1447, step time: 1.5347\n",
      "253/388, train_loss: 0.3857, step time: 1.5323\n",
      "254/388, train_loss: 0.1636, step time: 1.5359\n",
      "255/388, train_loss: 0.2225, step time: 1.5375\n",
      "256/388, train_loss: 0.1765, step time: 1.5327\n",
      "257/388, train_loss: 0.3613, step time: 1.5307\n",
      "258/388, train_loss: 0.2170, step time: 1.5315\n",
      "259/388, train_loss: 0.1149, step time: 1.5319\n",
      "260/388, train_loss: 0.1331, step time: 1.5356\n",
      "261/388, train_loss: 0.2174, step time: 1.5337\n",
      "262/388, train_loss: 0.3602, step time: 1.5308\n",
      "263/388, train_loss: 0.2167, step time: 1.5358\n",
      "264/388, train_loss: 0.1074, step time: 1.5318\n",
      "265/388, train_loss: 0.1385, step time: 1.5329\n",
      "266/388, train_loss: 0.0868, step time: 1.5364\n",
      "267/388, train_loss: 0.2131, step time: 1.5383\n",
      "268/388, train_loss: 0.2285, step time: 1.5316\n",
      "269/388, train_loss: 0.1228, step time: 1.5333\n",
      "270/388, train_loss: 0.2255, step time: 1.5300\n",
      "271/388, train_loss: 0.1269, step time: 1.5326\n",
      "272/388, train_loss: 0.3462, step time: 1.5293\n",
      "273/388, train_loss: 0.1904, step time: 1.5437\n",
      "274/388, train_loss: 0.1486, step time: 1.5363\n",
      "275/388, train_loss: 0.1068, step time: 1.5324\n",
      "276/388, train_loss: 0.4184, step time: 1.5303\n",
      "277/388, train_loss: 0.4118, step time: 1.5304\n",
      "278/388, train_loss: 0.3916, step time: 1.5304\n",
      "279/388, train_loss: 0.0914, step time: 1.5444\n",
      "280/388, train_loss: 0.2857, step time: 1.5362\n",
      "281/388, train_loss: 0.0587, step time: 1.5294\n",
      "282/388, train_loss: 0.1679, step time: 1.5341\n",
      "283/388, train_loss: 0.1212, step time: 1.5348\n",
      "284/388, train_loss: 0.2491, step time: 1.5334\n",
      "285/388, train_loss: 0.2730, step time: 1.5369\n",
      "286/388, train_loss: 0.1927, step time: 1.5437\n",
      "287/388, train_loss: 0.2562, step time: 1.5347\n",
      "288/388, train_loss: 0.3163, step time: 1.5327\n",
      "289/388, train_loss: 0.1372, step time: 1.5343\n",
      "290/388, train_loss: 0.1143, step time: 1.5358\n",
      "291/388, train_loss: 0.2547, step time: 1.5447\n",
      "292/388, train_loss: 0.2837, step time: 1.5326\n",
      "293/388, train_loss: 0.2708, step time: 1.5300\n",
      "294/388, train_loss: 0.2953, step time: 1.5339\n",
      "295/388, train_loss: 0.1490, step time: 1.5342\n",
      "296/388, train_loss: 0.1295, step time: 1.5475\n",
      "297/388, train_loss: 0.3494, step time: 1.5328\n",
      "298/388, train_loss: 0.3200, step time: 1.5325\n",
      "299/388, train_loss: 0.1472, step time: 1.5334\n",
      "300/388, train_loss: 0.2362, step time: 1.5318\n",
      "301/388, train_loss: 0.0993, step time: 1.5542\n",
      "302/388, train_loss: 0.1518, step time: 1.5349\n",
      "303/388, train_loss: 0.2685, step time: 1.5310\n",
      "304/388, train_loss: 0.1619, step time: 1.5311\n",
      "305/388, train_loss: 0.3638, step time: 1.5324\n",
      "306/388, train_loss: 0.1763, step time: 1.5371\n",
      "307/388, train_loss: 0.1947, step time: 1.5351\n",
      "308/388, train_loss: 0.3935, step time: 1.5317\n",
      "309/388, train_loss: 0.2211, step time: 1.5294\n",
      "310/388, train_loss: 0.2956, step time: 1.5308\n",
      "311/388, train_loss: 0.1970, step time: 1.5299\n",
      "312/388, train_loss: 0.0925, step time: 1.5326\n",
      "313/388, train_loss: 0.1458, step time: 1.5330\n",
      "314/388, train_loss: 0.0888, step time: 1.5328\n",
      "315/388, train_loss: 0.1917, step time: 1.5343\n",
      "316/388, train_loss: 0.2170, step time: 1.5325\n",
      "317/388, train_loss: 0.1295, step time: 1.5352\n",
      "318/388, train_loss: 0.2065, step time: 1.5320\n",
      "319/388, train_loss: 0.1199, step time: 1.5387\n",
      "320/388, train_loss: 0.3444, step time: 1.5341\n",
      "321/388, train_loss: 0.0602, step time: 1.5364\n",
      "322/388, train_loss: 0.1518, step time: 1.5336\n",
      "323/388, train_loss: 0.1328, step time: 1.5337\n",
      "324/388, train_loss: 0.1645, step time: 1.5321\n",
      "325/388, train_loss: 0.3038, step time: 1.5305\n",
      "326/388, train_loss: 0.5247, step time: 1.5374\n",
      "327/388, train_loss: 0.3129, step time: 1.5370\n",
      "328/388, train_loss: 0.2151, step time: 1.5342\n",
      "329/388, train_loss: 0.0930, step time: 1.5338\n",
      "330/388, train_loss: 0.1609, step time: 1.5309\n",
      "331/388, train_loss: 0.1366, step time: 1.5280\n",
      "332/388, train_loss: 0.1663, step time: 1.5325\n",
      "333/388, train_loss: 0.0917, step time: 1.5320\n",
      "334/388, train_loss: 0.4082, step time: 1.5347\n",
      "335/388, train_loss: 0.2832, step time: 1.5361\n",
      "336/388, train_loss: 0.1066, step time: 1.5339\n",
      "337/388, train_loss: 0.3634, step time: 1.5328\n",
      "338/388, train_loss: 0.6086, step time: 1.5328\n",
      "339/388, train_loss: 0.2459, step time: 1.5328\n",
      "340/388, train_loss: 0.1962, step time: 1.5371\n",
      "341/388, train_loss: 0.3098, step time: 1.5336\n",
      "342/388, train_loss: 0.0983, step time: 1.5293\n",
      "343/388, train_loss: 0.1586, step time: 1.5325\n",
      "344/388, train_loss: 0.2315, step time: 1.5388\n",
      "345/388, train_loss: 0.2139, step time: 1.5358\n",
      "346/388, train_loss: 0.2215, step time: 1.5330\n",
      "347/388, train_loss: 0.1051, step time: 1.5355\n",
      "348/388, train_loss: 0.1694, step time: 1.5301\n",
      "349/388, train_loss: 0.2493, step time: 1.5332\n",
      "350/388, train_loss: 0.2338, step time: 1.5427\n",
      "351/388, train_loss: 0.1013, step time: 1.5342\n",
      "352/388, train_loss: 0.2451, step time: 1.5339\n",
      "353/388, train_loss: 0.1203, step time: 1.5358\n",
      "354/388, train_loss: 0.1742, step time: 1.5325\n",
      "355/388, train_loss: 0.0578, step time: 1.5292\n",
      "356/388, train_loss: 0.2210, step time: 1.5334\n",
      "357/388, train_loss: 0.3982, step time: 1.5355\n",
      "358/388, train_loss: 0.1586, step time: 1.5341\n",
      "359/388, train_loss: 0.1460, step time: 1.5357\n",
      "360/388, train_loss: 0.1735, step time: 1.5338\n",
      "361/388, train_loss: 0.3485, step time: 1.5308\n",
      "362/388, train_loss: 0.0370, step time: 1.5311\n",
      "363/388, train_loss: 0.0894, step time: 1.5363\n",
      "364/388, train_loss: 0.1129, step time: 1.5344\n",
      "365/388, train_loss: 0.2149, step time: 1.5356\n",
      "366/388, train_loss: 0.3199, step time: 1.5327\n",
      "367/388, train_loss: 0.1536, step time: 1.5291\n",
      "368/388, train_loss: 0.0794, step time: 1.5321\n",
      "369/388, train_loss: 0.3037, step time: 1.5397\n",
      "370/388, train_loss: 0.2969, step time: 1.5307\n",
      "371/388, train_loss: 0.0540, step time: 1.5534\n",
      "372/388, train_loss: 0.1621, step time: 1.5368\n",
      "373/388, train_loss: 0.3167, step time: 1.5360\n",
      "374/388, train_loss: 0.3010, step time: 1.5342\n",
      "375/388, train_loss: 0.1742, step time: 1.5304\n",
      "376/388, train_loss: 0.3216, step time: 1.5324\n",
      "377/388, train_loss: 0.3629, step time: 1.5365\n",
      "378/388, train_loss: 0.3761, step time: 1.5309\n",
      "379/388, train_loss: 0.3128, step time: 1.5313\n",
      "380/388, train_loss: 0.4024, step time: 1.5334\n",
      "381/388, train_loss: 0.1241, step time: 1.5315\n",
      "382/388, train_loss: 0.1368, step time: 1.5344\n",
      "383/388, train_loss: 0.1318, step time: 1.5374\n",
      "384/388, train_loss: 0.3125, step time: 1.5356\n",
      "385/388, train_loss: 0.2089, step time: 1.5352\n",
      "386/388, train_loss: 0.1625, step time: 1.5328\n",
      "387/388, train_loss: 0.1579, step time: 1.5323\n",
      "388/388, train_loss: 0.1207, step time: 1.5325\n",
      "epoch 29 average loss: 0.2070\n",
      "current epoch: 29 current mean dice: 0.7335 tc: 0.7774 wt: 0.8773 et: 0.5456\n",
      "best mean dice: 0.7545 at epoch: 26\n",
      "time consuming of epoch 29 is: 704.4282\n",
      "----------\n",
      "epoch 30/100\n",
      "1/388, train_loss: 0.1430, step time: 1.5481\n",
      "2/388, train_loss: 0.2890, step time: 1.5388\n",
      "3/388, train_loss: 0.2248, step time: 1.5371\n",
      "4/388, train_loss: 0.4353, step time: 1.5362\n",
      "5/388, train_loss: 0.2101, step time: 1.5327\n",
      "6/388, train_loss: 0.2712, step time: 1.5342\n",
      "7/388, train_loss: 0.3763, step time: 1.5345\n",
      "8/388, train_loss: 0.1798, step time: 1.5338\n",
      "9/388, train_loss: 0.2437, step time: 1.5316\n",
      "10/388, train_loss: 0.2161, step time: 1.5340\n",
      "11/388, train_loss: 0.0573, step time: 1.5353\n",
      "12/388, train_loss: 0.1892, step time: 1.5341\n",
      "13/388, train_loss: 0.1965, step time: 1.5584\n",
      "14/388, train_loss: 0.1308, step time: 1.5329\n",
      "15/388, train_loss: 0.2518, step time: 1.5377\n",
      "16/388, train_loss: 0.2268, step time: 1.5356\n",
      "17/388, train_loss: 0.0929, step time: 1.5354\n",
      "18/388, train_loss: 0.1759, step time: 1.5345\n",
      "19/388, train_loss: 0.2114, step time: 1.5292\n",
      "20/388, train_loss: 0.2166, step time: 1.5324\n",
      "21/388, train_loss: 0.0994, step time: 1.5301\n",
      "22/388, train_loss: 0.5607, step time: 1.5335\n",
      "23/388, train_loss: 0.2875, step time: 1.5355\n",
      "24/388, train_loss: 0.1294, step time: 1.5422\n",
      "25/388, train_loss: 0.1392, step time: 1.5331\n",
      "26/388, train_loss: 0.3644, step time: 1.5369\n",
      "27/388, train_loss: 0.1465, step time: 1.5340\n",
      "28/388, train_loss: 0.2184, step time: 1.5332\n",
      "29/388, train_loss: 0.3039, step time: 1.5374\n",
      "30/388, train_loss: 0.1548, step time: 1.5323\n",
      "31/388, train_loss: 0.0732, step time: 1.5341\n",
      "32/388, train_loss: 0.2109, step time: 1.5303\n",
      "33/388, train_loss: 0.1437, step time: 1.5314\n",
      "34/388, train_loss: 0.0850, step time: 1.5345\n",
      "35/388, train_loss: 0.0920, step time: 1.5363\n",
      "36/388, train_loss: 0.2178, step time: 1.5335\n",
      "37/388, train_loss: 0.2762, step time: 1.5309\n",
      "38/388, train_loss: 0.0563, step time: 1.5415\n",
      "39/388, train_loss: 0.3072, step time: 1.5312\n",
      "40/388, train_loss: 0.5451, step time: 1.5345\n",
      "41/388, train_loss: 0.0872, step time: 1.5359\n",
      "42/388, train_loss: 0.4221, step time: 1.5321\n",
      "43/388, train_loss: 0.1711, step time: 1.5305\n",
      "44/388, train_loss: 0.3187, step time: 1.5323\n",
      "45/388, train_loss: 0.1379, step time: 1.5317\n",
      "46/388, train_loss: 0.1073, step time: 1.5402\n",
      "47/388, train_loss: 0.0837, step time: 1.5410\n",
      "48/388, train_loss: 0.1218, step time: 1.5327\n",
      "49/388, train_loss: 0.2410, step time: 1.5334\n",
      "50/388, train_loss: 0.0811, step time: 1.5322\n",
      "51/388, train_loss: 0.0951, step time: 1.5321\n",
      "52/388, train_loss: 0.1282, step time: 1.5348\n",
      "53/388, train_loss: 0.2159, step time: 1.5325\n",
      "54/388, train_loss: 0.3178, step time: 1.5326\n",
      "55/388, train_loss: 0.1465, step time: 1.5382\n",
      "56/388, train_loss: 0.0653, step time: 1.5301\n",
      "57/388, train_loss: 0.2393, step time: 1.5325\n",
      "58/388, train_loss: 0.2272, step time: 1.5338\n",
      "59/388, train_loss: 0.2533, step time: 1.5291\n",
      "60/388, train_loss: 0.1355, step time: 1.5366\n",
      "61/388, train_loss: 0.1557, step time: 1.5337\n",
      "62/388, train_loss: 0.2445, step time: 1.5359\n",
      "63/388, train_loss: 0.3124, step time: 1.5344\n",
      "64/388, train_loss: 0.3756, step time: 1.5317\n",
      "65/388, train_loss: 0.2836, step time: 1.5336\n",
      "66/388, train_loss: 0.1733, step time: 1.5327\n",
      "67/388, train_loss: 0.1854, step time: 1.5360\n",
      "68/388, train_loss: 0.2952, step time: 1.5335\n",
      "69/388, train_loss: 0.1374, step time: 1.5331\n",
      "70/388, train_loss: 0.2677, step time: 1.5324\n",
      "71/388, train_loss: 0.0571, step time: 1.5337\n",
      "72/388, train_loss: 0.1615, step time: 1.5312\n",
      "73/388, train_loss: 0.0708, step time: 1.5308\n",
      "74/388, train_loss: 0.0995, step time: 1.5293\n",
      "75/388, train_loss: 0.0928, step time: 1.5335\n",
      "76/388, train_loss: 0.2128, step time: 1.5368\n",
      "77/388, train_loss: 0.1435, step time: 1.5367\n",
      "78/388, train_loss: 0.5375, step time: 1.5325\n",
      "79/388, train_loss: 0.0967, step time: 1.5308\n",
      "80/388, train_loss: 0.2895, step time: 1.5317\n",
      "81/388, train_loss: 0.0533, step time: 1.5320\n",
      "82/388, train_loss: 0.0921, step time: 1.5393\n",
      "83/388, train_loss: 0.1660, step time: 1.5345\n",
      "84/388, train_loss: 0.1608, step time: 1.5352\n",
      "85/388, train_loss: 0.2514, step time: 1.5301\n",
      "86/388, train_loss: 0.2153, step time: 1.5290\n",
      "87/388, train_loss: 0.0905, step time: 1.5293\n",
      "88/388, train_loss: 0.1757, step time: 1.5338\n",
      "89/388, train_loss: 0.1843, step time: 1.5321\n",
      "90/388, train_loss: 0.1983, step time: 1.5325\n",
      "91/388, train_loss: 0.0391, step time: 1.5379\n",
      "92/388, train_loss: 0.0674, step time: 1.5342\n",
      "93/388, train_loss: 0.1433, step time: 1.5328\n",
      "94/388, train_loss: 0.1499, step time: 1.5298\n",
      "95/388, train_loss: 0.1884, step time: 1.5315\n",
      "96/388, train_loss: 0.2831, step time: 1.5324\n",
      "97/388, train_loss: 0.4101, step time: 1.5300\n",
      "98/388, train_loss: 0.1861, step time: 1.5345\n",
      "99/388, train_loss: 0.0831, step time: 1.5401\n",
      "100/388, train_loss: 0.1520, step time: 1.5365\n",
      "101/388, train_loss: 0.2519, step time: 1.5295\n",
      "102/388, train_loss: 0.2551, step time: 1.5314\n",
      "103/388, train_loss: 0.1659, step time: 1.5338\n",
      "104/388, train_loss: 0.1489, step time: 1.5359\n",
      "105/388, train_loss: 0.1716, step time: 1.5370\n",
      "106/388, train_loss: 0.1084, step time: 1.5344\n",
      "107/388, train_loss: 0.0668, step time: 1.5315\n",
      "108/388, train_loss: 0.2873, step time: 1.5304\n",
      "109/388, train_loss: 0.1743, step time: 1.5289\n",
      "110/388, train_loss: 0.2138, step time: 1.5313\n",
      "111/388, train_loss: 0.1162, step time: 1.5315\n",
      "112/388, train_loss: 0.2609, step time: 1.5347\n",
      "113/388, train_loss: 0.0736, step time: 1.5350\n",
      "114/388, train_loss: 0.3049, step time: 1.5377\n",
      "115/388, train_loss: 0.1152, step time: 1.5325\n",
      "116/388, train_loss: 0.1432, step time: 1.5290\n",
      "117/388, train_loss: 0.1365, step time: 1.5305\n",
      "118/388, train_loss: 0.5420, step time: 1.5356\n",
      "119/388, train_loss: 0.3300, step time: 1.5342\n",
      "120/388, train_loss: 0.1664, step time: 1.5383\n",
      "121/388, train_loss: 0.1193, step time: 1.5311\n",
      "122/388, train_loss: 0.1623, step time: 1.5306\n",
      "123/388, train_loss: 0.3576, step time: 1.5319\n",
      "124/388, train_loss: 0.2449, step time: 1.5356\n",
      "125/388, train_loss: 0.1073, step time: 1.5343\n",
      "126/388, train_loss: 0.3656, step time: 1.5388\n",
      "127/388, train_loss: 0.1118, step time: 1.5310\n",
      "128/388, train_loss: 0.1992, step time: 1.5304\n",
      "129/388, train_loss: 0.1263, step time: 1.5300\n",
      "130/388, train_loss: 0.1095, step time: 1.5354\n",
      "131/388, train_loss: 0.5192, step time: 1.5328\n",
      "132/388, train_loss: 0.0890, step time: 1.5316\n",
      "133/388, train_loss: 0.1755, step time: 1.5281\n",
      "134/388, train_loss: 0.2758, step time: 1.5330\n",
      "135/388, train_loss: 0.0357, step time: 1.5312\n",
      "136/388, train_loss: 0.3408, step time: 1.5344\n",
      "137/388, train_loss: 0.5388, step time: 1.5348\n",
      "138/388, train_loss: 0.1849, step time: 1.5326\n",
      "139/388, train_loss: 0.0987, step time: 1.5273\n",
      "140/388, train_loss: 0.3297, step time: 1.5354\n",
      "141/388, train_loss: 0.0939, step time: 1.5362\n",
      "142/388, train_loss: 0.2120, step time: 1.5348\n",
      "143/388, train_loss: 0.1701, step time: 1.5317\n",
      "144/388, train_loss: 0.1933, step time: 1.5331\n",
      "145/388, train_loss: 0.2638, step time: 1.5302\n",
      "146/388, train_loss: 0.1048, step time: 1.5323\n",
      "147/388, train_loss: 0.2545, step time: 1.5345\n",
      "148/388, train_loss: 0.1075, step time: 1.5286\n",
      "149/388, train_loss: 0.2980, step time: 1.5316\n",
      "150/388, train_loss: 0.1496, step time: 1.5308\n",
      "151/388, train_loss: 0.3331, step time: 1.5306\n",
      "152/388, train_loss: 0.1881, step time: 1.5329\n",
      "153/388, train_loss: 0.1775, step time: 1.5322\n",
      "154/388, train_loss: 0.1386, step time: 1.5348\n",
      "155/388, train_loss: 0.1032, step time: 1.5330\n",
      "156/388, train_loss: 0.0796, step time: 1.5312\n",
      "157/388, train_loss: 0.0790, step time: 1.5329\n",
      "158/388, train_loss: 0.1554, step time: 1.5334\n",
      "159/388, train_loss: 0.1524, step time: 1.5327\n",
      "160/388, train_loss: 0.1537, step time: 1.5349\n",
      "161/388, train_loss: 0.0959, step time: 1.5333\n",
      "162/388, train_loss: 0.4586, step time: 1.5293\n",
      "163/388, train_loss: 0.3289, step time: 1.5360\n",
      "164/388, train_loss: 0.1884, step time: 1.5369\n",
      "165/388, train_loss: 0.1341, step time: 1.5329\n",
      "166/388, train_loss: 0.2514, step time: 1.5325\n",
      "167/388, train_loss: 0.0934, step time: 1.5324\n",
      "168/388, train_loss: 0.1664, step time: 1.5358\n",
      "169/388, train_loss: 0.1283, step time: 1.5318\n",
      "170/388, train_loss: 0.3478, step time: 1.5368\n",
      "171/388, train_loss: 0.1241, step time: 1.5380\n",
      "172/388, train_loss: 0.1233, step time: 1.5386\n",
      "173/388, train_loss: 0.2183, step time: 1.5372\n",
      "174/388, train_loss: 0.2089, step time: 1.5334\n",
      "175/388, train_loss: 0.1441, step time: 1.5334\n",
      "176/388, train_loss: 0.3145, step time: 1.5333\n",
      "177/388, train_loss: 0.1220, step time: 1.5361\n",
      "178/388, train_loss: 0.2366, step time: 1.5391\n",
      "179/388, train_loss: 0.1193, step time: 1.5328\n",
      "180/388, train_loss: 0.3361, step time: 1.5346\n",
      "181/388, train_loss: 0.2720, step time: 1.5284\n",
      "182/388, train_loss: 0.1276, step time: 1.5568\n",
      "183/388, train_loss: 0.0660, step time: 1.5349\n",
      "184/388, train_loss: 0.2023, step time: 1.5358\n",
      "185/388, train_loss: 0.1180, step time: 1.5365\n",
      "186/388, train_loss: 0.2983, step time: 1.5358\n",
      "187/388, train_loss: 0.1815, step time: 1.5335\n",
      "188/388, train_loss: 0.1443, step time: 1.5380\n",
      "189/388, train_loss: 0.2837, step time: 1.5353\n",
      "190/388, train_loss: 0.2259, step time: 1.5336\n",
      "191/388, train_loss: 0.0958, step time: 1.5308\n",
      "192/388, train_loss: 0.2446, step time: 1.5320\n",
      "193/388, train_loss: 0.5425, step time: 1.5330\n",
      "194/388, train_loss: 0.0768, step time: 1.5342\n",
      "195/388, train_loss: 0.1416, step time: 1.5358\n",
      "196/388, train_loss: 0.2384, step time: 1.5383\n",
      "197/388, train_loss: 0.6397, step time: 1.5294\n",
      "198/388, train_loss: 0.2206, step time: 1.5321\n",
      "199/388, train_loss: 0.0951, step time: 1.5298\n",
      "200/388, train_loss: 0.3744, step time: 1.5342\n",
      "201/388, train_loss: 0.0932, step time: 1.5381\n",
      "202/388, train_loss: 0.3122, step time: 1.5366\n",
      "203/388, train_loss: 0.1226, step time: 1.5320\n",
      "204/388, train_loss: 0.0956, step time: 1.5337\n",
      "205/388, train_loss: 0.3117, step time: 1.5401\n",
      "206/388, train_loss: 0.1175, step time: 1.5337\n",
      "207/388, train_loss: 0.3250, step time: 1.5322\n",
      "208/388, train_loss: 0.3197, step time: 1.5293\n",
      "209/388, train_loss: 0.5861, step time: 1.5311\n",
      "210/388, train_loss: 0.1945, step time: 1.5372\n",
      "211/388, train_loss: 0.1069, step time: 1.5354\n",
      "212/388, train_loss: 0.4672, step time: 1.5365\n",
      "213/388, train_loss: 0.1180, step time: 1.5325\n",
      "214/388, train_loss: 0.0912, step time: 1.5359\n",
      "215/388, train_loss: 0.2282, step time: 1.5327\n",
      "216/388, train_loss: 0.3099, step time: 1.5298\n",
      "217/388, train_loss: 0.1146, step time: 1.5487\n",
      "218/388, train_loss: 0.1170, step time: 1.5354\n",
      "219/388, train_loss: 0.2904, step time: 1.5349\n",
      "220/388, train_loss: 0.1872, step time: 1.5335\n",
      "221/388, train_loss: 0.1226, step time: 1.5332\n",
      "222/388, train_loss: 0.1790, step time: 1.5404\n",
      "223/388, train_loss: 0.1954, step time: 1.5338\n",
      "224/388, train_loss: 0.1962, step time: 1.5358\n",
      "225/388, train_loss: 0.0719, step time: 1.5324\n",
      "226/388, train_loss: 0.5509, step time: 1.5317\n",
      "227/388, train_loss: 0.1231, step time: 1.5325\n",
      "228/388, train_loss: 0.2307, step time: 1.5322\n",
      "229/388, train_loss: 0.1461, step time: 1.5372\n",
      "230/388, train_loss: 0.0827, step time: 1.5520\n",
      "231/388, train_loss: 0.2013, step time: 1.5361\n",
      "232/388, train_loss: 0.1530, step time: 1.5361\n",
      "233/388, train_loss: 0.1572, step time: 1.5332\n",
      "234/388, train_loss: 0.0767, step time: 1.5401\n",
      "235/388, train_loss: 0.0691, step time: 1.5314\n",
      "236/388, train_loss: 0.0935, step time: 1.5346\n",
      "237/388, train_loss: 0.1810, step time: 1.5368\n",
      "238/388, train_loss: 0.1190, step time: 1.5378\n",
      "239/388, train_loss: 0.2727, step time: 1.5334\n",
      "240/388, train_loss: 0.1340, step time: 1.5308\n",
      "241/388, train_loss: 0.2170, step time: 1.5303\n",
      "242/388, train_loss: 0.4404, step time: 1.5361\n",
      "243/388, train_loss: 0.2695, step time: 1.5352\n",
      "244/388, train_loss: 0.1779, step time: 1.5380\n",
      "245/388, train_loss: 0.2617, step time: 1.5347\n",
      "246/388, train_loss: 0.1841, step time: 1.5330\n",
      "247/388, train_loss: 0.0788, step time: 1.5320\n",
      "248/388, train_loss: 0.1586, step time: 1.5357\n",
      "249/388, train_loss: 0.3119, step time: 1.5365\n",
      "250/388, train_loss: 0.1841, step time: 1.5365\n",
      "251/388, train_loss: 0.1132, step time: 1.5564\n",
      "252/388, train_loss: 0.2504, step time: 1.5380\n",
      "253/388, train_loss: 0.0970, step time: 1.5318\n",
      "254/388, train_loss: 0.2957, step time: 1.5340\n",
      "255/388, train_loss: 0.1824, step time: 1.5373\n",
      "256/388, train_loss: 0.1112, step time: 1.5335\n",
      "257/388, train_loss: 0.2243, step time: 1.5309\n",
      "258/388, train_loss: 0.1292, step time: 1.5347\n",
      "259/388, train_loss: 0.3771, step time: 1.5335\n",
      "260/388, train_loss: 0.3974, step time: 1.5441\n",
      "261/388, train_loss: 0.2709, step time: 1.5448\n",
      "262/388, train_loss: 0.1912, step time: 1.5346\n",
      "263/388, train_loss: 0.2089, step time: 1.5349\n",
      "264/388, train_loss: 0.2092, step time: 1.5321\n",
      "265/388, train_loss: 0.1690, step time: 1.5300\n",
      "266/388, train_loss: 0.1039, step time: 1.5324\n",
      "267/388, train_loss: 0.4893, step time: 1.5344\n",
      "268/388, train_loss: 0.1060, step time: 1.5373\n",
      "269/388, train_loss: 0.0962, step time: 1.5345\n",
      "270/388, train_loss: 0.0890, step time: 1.5348\n",
      "271/388, train_loss: 0.5104, step time: 1.5351\n",
      "272/388, train_loss: 0.3431, step time: 1.5339\n",
      "273/388, train_loss: 0.3203, step time: 1.5345\n",
      "274/388, train_loss: 0.1317, step time: 1.5356\n",
      "275/388, train_loss: 0.1595, step time: 1.5362\n",
      "276/388, train_loss: 0.1908, step time: 1.5315\n",
      "277/388, train_loss: 0.2029, step time: 1.5320\n",
      "278/388, train_loss: 0.1305, step time: 1.5321\n",
      "279/388, train_loss: 0.1258, step time: 1.5329\n",
      "280/388, train_loss: 0.2152, step time: 1.5333\n",
      "281/388, train_loss: 0.0782, step time: 1.5365\n",
      "282/388, train_loss: 0.1385, step time: 1.5356\n",
      "283/388, train_loss: 0.1429, step time: 1.5367\n",
      "284/388, train_loss: 0.4109, step time: 1.5378\n",
      "285/388, train_loss: 0.1367, step time: 1.5324\n",
      "286/388, train_loss: 0.1097, step time: 1.5295\n",
      "287/388, train_loss: 0.1378, step time: 1.5333\n",
      "288/388, train_loss: 0.2709, step time: 1.5346\n",
      "289/388, train_loss: 0.1046, step time: 1.5413\n",
      "290/388, train_loss: 0.2719, step time: 1.5328\n",
      "291/388, train_loss: 0.5361, step time: 1.5310\n",
      "292/388, train_loss: 0.1066, step time: 1.5316\n",
      "293/388, train_loss: 0.0655, step time: 1.5360\n",
      "294/388, train_loss: 0.3085, step time: 1.5356\n",
      "295/388, train_loss: 0.2428, step time: 1.5613\n",
      "296/388, train_loss: 0.1729, step time: 1.5356\n",
      "297/388, train_loss: 0.1521, step time: 1.5345\n",
      "298/388, train_loss: 0.5466, step time: 1.5301\n",
      "299/388, train_loss: 0.2407, step time: 1.5292\n",
      "300/388, train_loss: 0.1620, step time: 1.5393\n",
      "301/388, train_loss: 0.4406, step time: 1.5357\n",
      "302/388, train_loss: 0.1195, step time: 1.5373\n",
      "303/388, train_loss: 0.1134, step time: 1.5314\n",
      "304/388, train_loss: 0.1413, step time: 1.5287\n",
      "305/388, train_loss: 0.2860, step time: 1.5296\n",
      "306/388, train_loss: 0.5439, step time: 1.5296\n",
      "307/388, train_loss: 0.2306, step time: 1.5354\n",
      "308/388, train_loss: 0.2121, step time: 1.5367\n",
      "309/388, train_loss: 0.3071, step time: 1.5339\n",
      "310/388, train_loss: 0.0973, step time: 1.5359\n",
      "311/388, train_loss: 0.0958, step time: 1.5381\n",
      "312/388, train_loss: 0.2057, step time: 1.5333\n",
      "313/388, train_loss: 0.0817, step time: 1.5280\n",
      "314/388, train_loss: 0.5912, step time: 1.5355\n",
      "315/388, train_loss: 0.2406, step time: 1.5350\n",
      "316/388, train_loss: 0.1973, step time: 1.5365\n",
      "317/388, train_loss: 0.2070, step time: 1.5354\n",
      "318/388, train_loss: 0.1544, step time: 1.5329\n",
      "319/388, train_loss: 0.0950, step time: 1.5310\n",
      "320/388, train_loss: 0.1090, step time: 1.5368\n",
      "321/388, train_loss: 0.2079, step time: 1.5364\n",
      "322/388, train_loss: 0.0759, step time: 1.5394\n",
      "323/388, train_loss: 0.2675, step time: 1.5330\n",
      "324/388, train_loss: 0.0780, step time: 1.5333\n",
      "325/388, train_loss: 0.0707, step time: 1.5353\n",
      "326/388, train_loss: 0.1506, step time: 1.5318\n",
      "327/388, train_loss: 0.2764, step time: 1.5360\n",
      "328/388, train_loss: 0.2228, step time: 1.5388\n",
      "329/388, train_loss: 0.1524, step time: 1.5358\n",
      "330/388, train_loss: 0.2181, step time: 1.5324\n",
      "331/388, train_loss: 0.1348, step time: 1.5337\n",
      "332/388, train_loss: 0.1574, step time: 1.5368\n",
      "333/388, train_loss: 0.2689, step time: 1.5371\n",
      "334/388, train_loss: 0.0999, step time: 1.5336\n",
      "335/388, train_loss: 0.1161, step time: 1.5275\n",
      "336/388, train_loss: 0.3905, step time: 1.5346\n",
      "337/388, train_loss: 0.1159, step time: 1.5342\n",
      "338/388, train_loss: 0.3687, step time: 1.5357\n",
      "339/388, train_loss: 0.2719, step time: 1.5358\n",
      "340/388, train_loss: 0.2272, step time: 1.5354\n",
      "341/388, train_loss: 0.2374, step time: 1.5308\n",
      "342/388, train_loss: 0.1600, step time: 1.5340\n",
      "343/388, train_loss: 0.0715, step time: 1.5376\n",
      "344/388, train_loss: 0.2382, step time: 1.5353\n",
      "345/388, train_loss: 0.1392, step time: 1.5402\n",
      "346/388, train_loss: 0.1118, step time: 1.5355\n",
      "347/388, train_loss: 0.0518, step time: 1.5329\n",
      "348/388, train_loss: 0.0879, step time: 1.5343\n",
      "349/388, train_loss: 0.1831, step time: 1.5370\n",
      "350/388, train_loss: 0.2040, step time: 1.5324\n",
      "351/388, train_loss: 0.2036, step time: 1.5316\n",
      "352/388, train_loss: 0.2222, step time: 1.5333\n",
      "353/388, train_loss: 0.1428, step time: 1.5295\n",
      "354/388, train_loss: 0.0306, step time: 1.5296\n",
      "355/388, train_loss: 0.2901, step time: 1.5337\n",
      "356/388, train_loss: 0.2598, step time: 1.5348\n",
      "357/388, train_loss: 0.2904, step time: 1.5358\n",
      "358/388, train_loss: 0.0918, step time: 1.5363\n",
      "359/388, train_loss: 0.1113, step time: 1.5328\n",
      "360/388, train_loss: 0.0703, step time: 1.5314\n",
      "361/388, train_loss: 0.1626, step time: 1.5293\n",
      "362/388, train_loss: 0.3258, step time: 1.5479\n",
      "363/388, train_loss: 0.2733, step time: 1.5362\n",
      "364/388, train_loss: 0.2571, step time: 1.5352\n",
      "365/388, train_loss: 0.1268, step time: 1.5285\n",
      "366/388, train_loss: 0.3558, step time: 1.5302\n",
      "367/388, train_loss: 0.1426, step time: 1.5291\n",
      "368/388, train_loss: 0.4911, step time: 1.5332\n",
      "369/388, train_loss: 0.3481, step time: 1.5341\n",
      "370/388, train_loss: 0.4685, step time: 1.5496\n",
      "371/388, train_loss: 0.0652, step time: 1.5303\n",
      "372/388, train_loss: 0.1662, step time: 1.5319\n",
      "373/388, train_loss: 0.2771, step time: 1.5361\n",
      "374/388, train_loss: 0.2623, step time: 1.5316\n",
      "375/388, train_loss: 0.1375, step time: 1.5301\n",
      "376/388, train_loss: 0.3088, step time: 1.5372\n",
      "377/388, train_loss: 0.3184, step time: 1.5345\n",
      "378/388, train_loss: 0.0351, step time: 1.5337\n",
      "379/388, train_loss: 0.2238, step time: 1.5295\n",
      "380/388, train_loss: 0.0652, step time: 1.5356\n",
      "381/388, train_loss: 0.4945, step time: 1.5313\n",
      "382/388, train_loss: 0.0608, step time: 1.5440\n",
      "383/388, train_loss: 0.1225, step time: 1.5319\n",
      "384/388, train_loss: 0.1838, step time: 1.5334\n",
      "385/388, train_loss: 0.1057, step time: 1.5340\n",
      "386/388, train_loss: 0.5759, step time: 1.5345\n",
      "387/388, train_loss: 0.3349, step time: 1.5372\n",
      "388/388, train_loss: 0.2044, step time: 1.5319\n",
      "epoch 30 average loss: 0.2058\n",
      "current epoch: 30 current mean dice: 0.7497 tc: 0.7995 wt: 0.8926 et: 0.5569\n",
      "best mean dice: 0.7545 at epoch: 26\n",
      "time consuming of epoch 30 is: 702.9864\n",
      "----------\n",
      "epoch 31/100\n",
      "1/388, train_loss: 0.2147, step time: 1.5510\n",
      "2/388, train_loss: 0.0987, step time: 1.5378\n",
      "3/388, train_loss: 0.1077, step time: 1.5350\n",
      "4/388, train_loss: 0.2047, step time: 1.5398\n",
      "5/388, train_loss: 0.1495, step time: 1.5381\n",
      "6/388, train_loss: 0.2501, step time: 1.5338\n",
      "7/388, train_loss: 0.2612, step time: 1.5336\n",
      "8/388, train_loss: 0.1605, step time: 1.5350\n",
      "9/388, train_loss: 0.1171, step time: 1.5354\n",
      "10/388, train_loss: 0.0952, step time: 1.5362\n",
      "11/388, train_loss: 0.2633, step time: 1.5350\n",
      "12/388, train_loss: 0.0961, step time: 1.5406\n",
      "13/388, train_loss: 0.1330, step time: 1.5403\n",
      "14/388, train_loss: 0.2015, step time: 1.5363\n",
      "15/388, train_loss: 0.1861, step time: 1.5348\n",
      "16/388, train_loss: 0.3112, step time: 1.5363\n",
      "17/388, train_loss: 0.1005, step time: 1.5345\n",
      "18/388, train_loss: 0.4546, step time: 1.5342\n",
      "19/388, train_loss: 0.2917, step time: 1.5357\n",
      "20/388, train_loss: 0.2037, step time: 1.5448\n",
      "21/388, train_loss: 0.4768, step time: 1.5309\n",
      "22/388, train_loss: 0.1623, step time: 1.5356\n",
      "23/388, train_loss: 0.2026, step time: 1.5360\n",
      "24/388, train_loss: 0.3234, step time: 1.5378\n",
      "25/388, train_loss: 0.1392, step time: 1.5375\n",
      "26/388, train_loss: 0.2335, step time: 1.5380\n",
      "27/388, train_loss: 0.2640, step time: 1.5343\n",
      "28/388, train_loss: 0.2818, step time: 1.5421\n",
      "29/388, train_loss: 0.1315, step time: 1.5365\n",
      "30/388, train_loss: 0.2858, step time: 1.5371\n",
      "31/388, train_loss: 0.0557, step time: 1.5327\n",
      "32/388, train_loss: 0.0930, step time: 1.5325\n",
      "33/388, train_loss: 0.3294, step time: 1.5313\n",
      "34/388, train_loss: 0.1756, step time: 1.5412\n",
      "35/388, train_loss: 0.2673, step time: 1.5367\n",
      "36/388, train_loss: 0.2211, step time: 1.5463\n",
      "37/388, train_loss: 0.1350, step time: 1.5345\n",
      "38/388, train_loss: 0.0929, step time: 1.5312\n",
      "39/388, train_loss: 0.1173, step time: 1.5413\n",
      "40/388, train_loss: 0.1685, step time: 1.5534\n",
      "41/388, train_loss: 0.0984, step time: 1.5309\n",
      "42/388, train_loss: 0.4765, step time: 1.5353\n",
      "43/388, train_loss: 0.1323, step time: 1.5620\n",
      "44/388, train_loss: 0.3349, step time: 1.5395\n",
      "45/388, train_loss: 0.2959, step time: 1.5334\n",
      "46/388, train_loss: 0.0929, step time: 1.5327\n",
      "47/388, train_loss: 0.1870, step time: 1.5328\n",
      "48/388, train_loss: 0.5005, step time: 1.5462\n",
      "49/388, train_loss: 0.5172, step time: 1.5304\n",
      "50/388, train_loss: 0.1331, step time: 1.5450\n",
      "51/388, train_loss: 0.3132, step time: 1.5352\n",
      "52/388, train_loss: 0.4263, step time: 1.5433\n",
      "53/388, train_loss: 0.2317, step time: 1.5325\n",
      "54/388, train_loss: 0.3602, step time: 1.5326\n",
      "55/388, train_loss: 0.1424, step time: 1.5356\n",
      "56/388, train_loss: 0.2066, step time: 1.5508\n",
      "57/388, train_loss: 0.1517, step time: 1.5330\n",
      "58/388, train_loss: 0.2019, step time: 1.5285\n",
      "59/388, train_loss: 0.2922, step time: 1.5482\n",
      "60/388, train_loss: 0.1125, step time: 1.5449\n",
      "61/388, train_loss: 0.2072, step time: 1.5306\n",
      "62/388, train_loss: 0.2235, step time: 1.5345\n",
      "63/388, train_loss: 0.1304, step time: 1.5367\n",
      "64/388, train_loss: 0.0972, step time: 1.5492\n",
      "65/388, train_loss: 0.1479, step time: 1.5310\n",
      "66/388, train_loss: 0.1011, step time: 1.5352\n",
      "67/388, train_loss: 0.1555, step time: 1.5331\n",
      "68/388, train_loss: 0.0721, step time: 1.5473\n",
      "69/388, train_loss: 0.1903, step time: 1.5370\n",
      "70/388, train_loss: 0.3167, step time: 1.5346\n",
      "71/388, train_loss: 0.2702, step time: 1.5356\n",
      "72/388, train_loss: 0.3665, step time: 1.5403\n",
      "73/388, train_loss: 0.1898, step time: 1.5348\n",
      "74/388, train_loss: 0.3380, step time: 1.5380\n",
      "75/388, train_loss: 0.0964, step time: 1.5377\n",
      "76/388, train_loss: 0.1971, step time: 1.5472\n",
      "77/388, train_loss: 0.5402, step time: 1.5371\n",
      "78/388, train_loss: 0.2069, step time: 1.5441\n",
      "79/388, train_loss: 0.1000, step time: 1.5358\n",
      "80/388, train_loss: 0.2431, step time: 1.5427\n",
      "81/388, train_loss: 0.3008, step time: 1.5402\n",
      "82/388, train_loss: 0.2446, step time: 1.5346\n",
      "83/388, train_loss: 0.3064, step time: 1.5348\n",
      "84/388, train_loss: 0.2339, step time: 1.5433\n",
      "85/388, train_loss: 0.2738, step time: 1.5351\n",
      "86/388, train_loss: 0.2836, step time: 1.5378\n",
      "87/388, train_loss: 0.0530, step time: 1.5359\n",
      "88/388, train_loss: 0.2849, step time: 1.5395\n",
      "89/388, train_loss: 0.3235, step time: 1.5343\n",
      "90/388, train_loss: 0.0955, step time: 1.5372\n",
      "91/388, train_loss: 0.4177, step time: 1.5432\n",
      "92/388, train_loss: 0.2636, step time: 1.5481\n",
      "93/388, train_loss: 0.0701, step time: 1.5375\n",
      "94/388, train_loss: 0.1439, step time: 1.5375\n",
      "95/388, train_loss: 0.0728, step time: 1.5369\n",
      "96/388, train_loss: 0.2635, step time: 1.5423\n",
      "97/388, train_loss: 0.1871, step time: 1.5312\n",
      "98/388, train_loss: 0.2834, step time: 1.5361\n",
      "99/388, train_loss: 0.3005, step time: 1.5409\n",
      "100/388, train_loss: 0.1185, step time: 1.5516\n",
      "101/388, train_loss: 0.3902, step time: 1.5445\n",
      "102/388, train_loss: 0.4371, step time: 1.5381\n",
      "103/388, train_loss: 0.2012, step time: 1.5364\n",
      "104/388, train_loss: 0.2265, step time: 1.5392\n",
      "105/388, train_loss: 0.2245, step time: 1.5318\n",
      "106/388, train_loss: 0.2111, step time: 1.5377\n",
      "107/388, train_loss: 0.2046, step time: 1.5374\n",
      "108/388, train_loss: 0.2371, step time: 1.5466\n",
      "109/388, train_loss: 0.0897, step time: 1.5340\n",
      "110/388, train_loss: 0.1289, step time: 1.5351\n",
      "111/388, train_loss: 0.2837, step time: 1.5386\n",
      "112/388, train_loss: 0.1013, step time: 1.5432\n",
      "113/388, train_loss: 0.1088, step time: 1.5289\n",
      "114/388, train_loss: 0.0289, step time: 1.5359\n",
      "115/388, train_loss: 0.0551, step time: 1.5326\n",
      "116/388, train_loss: 0.7283, step time: 1.5464\n",
      "117/388, train_loss: 0.2719, step time: 1.5329\n",
      "118/388, train_loss: 0.1824, step time: 1.5322\n",
      "119/388, train_loss: 0.5173, step time: 1.5333\n",
      "120/388, train_loss: 0.3093, step time: 1.5469\n",
      "121/388, train_loss: 0.2176, step time: 1.5354\n",
      "122/388, train_loss: 0.1033, step time: 1.5350\n",
      "123/388, train_loss: 0.1381, step time: 1.5294\n",
      "124/388, train_loss: 0.3764, step time: 1.5402\n",
      "125/388, train_loss: 0.1305, step time: 1.5326\n",
      "126/388, train_loss: 0.1312, step time: 1.5367\n",
      "127/388, train_loss: 0.1475, step time: 1.5350\n",
      "128/388, train_loss: 0.0979, step time: 1.5489\n",
      "129/388, train_loss: 0.1731, step time: 1.5322\n",
      "130/388, train_loss: 0.0714, step time: 1.5351\n",
      "131/388, train_loss: 0.1604, step time: 1.5376\n",
      "132/388, train_loss: 0.2220, step time: 1.5577\n",
      "133/388, train_loss: 0.1072, step time: 1.5308\n",
      "134/388, train_loss: 0.3163, step time: 1.5387\n",
      "135/388, train_loss: 0.1349, step time: 1.5426\n",
      "136/388, train_loss: 0.1638, step time: 1.5450\n",
      "137/388, train_loss: 0.2298, step time: 1.5322\n",
      "138/388, train_loss: 0.1522, step time: 1.5337\n",
      "139/388, train_loss: 0.2157, step time: 1.5325\n",
      "140/388, train_loss: 0.2261, step time: 1.5499\n",
      "141/388, train_loss: 0.3027, step time: 1.5339\n",
      "142/388, train_loss: 0.0631, step time: 1.5347\n",
      "143/388, train_loss: 0.3289, step time: 1.5311\n",
      "144/388, train_loss: 0.0704, step time: 1.5585\n",
      "145/388, train_loss: 0.1525, step time: 1.5365\n",
      "146/388, train_loss: 0.0968, step time: 1.5428\n",
      "147/388, train_loss: 0.3269, step time: 1.5317\n",
      "148/388, train_loss: 0.2116, step time: 1.5521\n",
      "149/388, train_loss: 0.1362, step time: 1.5355\n",
      "150/388, train_loss: 0.1113, step time: 1.5327\n",
      "151/388, train_loss: 0.1974, step time: 1.5317\n",
      "152/388, train_loss: 0.1046, step time: 1.5485\n",
      "153/388, train_loss: 0.1048, step time: 1.5350\n",
      "154/388, train_loss: 0.1035, step time: 1.5361\n",
      "155/388, train_loss: 0.2221, step time: 1.5323\n",
      "156/388, train_loss: 0.0335, step time: 1.5439\n",
      "157/388, train_loss: 0.3636, step time: 1.5412\n",
      "158/388, train_loss: 0.1224, step time: 1.5363\n",
      "159/388, train_loss: 0.3720, step time: 1.5388\n",
      "160/388, train_loss: 0.2455, step time: 1.5385\n",
      "161/388, train_loss: 0.1635, step time: 1.5311\n",
      "162/388, train_loss: 0.3869, step time: 1.5373\n",
      "163/388, train_loss: 0.1316, step time: 1.5345\n",
      "164/388, train_loss: 0.1085, step time: 1.5381\n",
      "165/388, train_loss: 0.1940, step time: 1.5336\n",
      "166/388, train_loss: 0.1134, step time: 1.5351\n",
      "167/388, train_loss: 0.0902, step time: 1.5373\n",
      "168/388, train_loss: 0.4800, step time: 1.5346\n",
      "169/388, train_loss: 0.6014, step time: 1.5354\n",
      "170/388, train_loss: 0.1862, step time: 1.5369\n",
      "171/388, train_loss: 0.2386, step time: 1.5304\n",
      "172/388, train_loss: 0.2135, step time: 1.5340\n",
      "173/388, train_loss: 0.2784, step time: 1.5338\n",
      "174/388, train_loss: 0.1986, step time: 1.5389\n",
      "175/388, train_loss: 0.1449, step time: 1.5358\n",
      "176/388, train_loss: 0.1189, step time: 1.5342\n",
      "177/388, train_loss: 0.2024, step time: 1.5351\n",
      "178/388, train_loss: 0.1684, step time: 1.5318\n",
      "179/388, train_loss: 0.1117, step time: 1.5296\n",
      "180/388, train_loss: 0.3264, step time: 1.5330\n",
      "181/388, train_loss: 0.1403, step time: 1.5357\n",
      "182/388, train_loss: 0.2775, step time: 1.5352\n",
      "183/388, train_loss: 0.1558, step time: 1.5360\n",
      "184/388, train_loss: 0.1978, step time: 1.5355\n",
      "185/388, train_loss: 0.1141, step time: 1.5316\n",
      "186/388, train_loss: 0.2678, step time: 1.5463\n",
      "187/388, train_loss: 0.1362, step time: 1.5371\n",
      "188/388, train_loss: 0.1243, step time: 1.5339\n",
      "189/388, train_loss: 0.1035, step time: 1.5304\n",
      "190/388, train_loss: 0.1306, step time: 1.5338\n",
      "191/388, train_loss: 0.4937, step time: 1.5323\n",
      "192/388, train_loss: 0.1444, step time: 1.5317\n",
      "193/388, train_loss: 0.3500, step time: 1.5309\n",
      "194/388, train_loss: 0.3992, step time: 1.5432\n",
      "195/388, train_loss: 0.1216, step time: 1.5367\n",
      "196/388, train_loss: 0.1453, step time: 1.5458\n",
      "197/388, train_loss: 0.1817, step time: 1.5420\n",
      "198/388, train_loss: 0.0963, step time: 1.5404\n",
      "199/388, train_loss: 0.0660, step time: 1.5313\n",
      "200/388, train_loss: 0.3203, step time: 1.5321\n",
      "201/388, train_loss: 0.1324, step time: 1.5302\n",
      "202/388, train_loss: 0.1650, step time: 1.5421\n",
      "203/388, train_loss: 0.0511, step time: 1.5371\n",
      "204/388, train_loss: 0.3662, step time: 1.5322\n",
      "205/388, train_loss: 0.5698, step time: 1.5306\n",
      "206/388, train_loss: 0.3120, step time: 1.5331\n",
      "207/388, train_loss: 0.1280, step time: 1.5344\n",
      "208/388, train_loss: 0.0796, step time: 1.5536\n",
      "209/388, train_loss: 0.1181, step time: 1.5410\n",
      "210/388, train_loss: 0.2194, step time: 1.5460\n",
      "211/388, train_loss: 0.1630, step time: 1.5291\n",
      "212/388, train_loss: 0.5170, step time: 1.5325\n",
      "213/388, train_loss: 0.0505, step time: 1.5323\n",
      "214/388, train_loss: 0.3203, step time: 1.5360\n",
      "215/388, train_loss: 0.1422, step time: 1.5368\n",
      "216/388, train_loss: 0.1349, step time: 1.5322\n",
      "217/388, train_loss: 0.0901, step time: 1.5302\n",
      "218/388, train_loss: 0.1973, step time: 1.5350\n",
      "219/388, train_loss: 0.0815, step time: 1.5389\n",
      "220/388, train_loss: 0.1828, step time: 1.5377\n",
      "221/388, train_loss: 0.1565, step time: 1.5347\n",
      "222/388, train_loss: 0.2579, step time: 1.5335\n",
      "223/388, train_loss: 0.2003, step time: 1.5364\n",
      "224/388, train_loss: 0.0704, step time: 1.5541\n",
      "225/388, train_loss: 0.6621, step time: 1.5364\n",
      "226/388, train_loss: 0.1122, step time: 1.5352\n",
      "227/388, train_loss: 0.1216, step time: 1.5355\n",
      "228/388, train_loss: 0.1213, step time: 1.5312\n",
      "229/388, train_loss: 0.1425, step time: 1.5301\n",
      "230/388, train_loss: 0.0888, step time: 1.5370\n",
      "231/388, train_loss: 0.0999, step time: 1.5347\n",
      "232/388, train_loss: 0.1162, step time: 1.5468\n",
      "233/388, train_loss: 0.1355, step time: 1.5320\n",
      "234/388, train_loss: 0.1380, step time: 1.5330\n",
      "235/388, train_loss: 0.0897, step time: 1.5287\n",
      "236/388, train_loss: 0.1157, step time: 1.5322\n",
      "237/388, train_loss: 0.0988, step time: 1.5377\n",
      "238/388, train_loss: 0.2017, step time: 1.5363\n",
      "239/388, train_loss: 0.0656, step time: 1.5373\n",
      "240/388, train_loss: 0.2399, step time: 1.5303\n",
      "241/388, train_loss: 0.3913, step time: 1.5367\n",
      "242/388, train_loss: 0.5885, step time: 1.5308\n",
      "243/388, train_loss: 0.4336, step time: 1.5725\n",
      "244/388, train_loss: 0.2303, step time: 1.5343\n",
      "245/388, train_loss: 0.0598, step time: 1.5296\n",
      "246/388, train_loss: 0.4863, step time: 1.5308\n",
      "247/388, train_loss: 0.1666, step time: 1.5400\n",
      "248/388, train_loss: 0.0939, step time: 1.5367\n",
      "249/388, train_loss: 0.4496, step time: 1.5330\n",
      "250/388, train_loss: 0.1795, step time: 1.5316\n",
      "251/388, train_loss: 0.1086, step time: 1.5304\n",
      "252/388, train_loss: 0.2096, step time: 1.5326\n",
      "253/388, train_loss: 0.1219, step time: 1.5313\n",
      "254/388, train_loss: 0.1645, step time: 1.5364\n",
      "255/388, train_loss: 0.2162, step time: 1.5385\n",
      "256/388, train_loss: 0.3857, step time: 1.5366\n",
      "257/388, train_loss: 0.0918, step time: 1.5364\n",
      "258/388, train_loss: 0.1031, step time: 1.5326\n",
      "259/388, train_loss: 0.1304, step time: 1.5304\n",
      "260/388, train_loss: 0.0507, step time: 1.5337\n",
      "261/388, train_loss: 0.0859, step time: 1.5336\n",
      "262/388, train_loss: 0.1953, step time: 1.5405\n",
      "263/388, train_loss: 0.1223, step time: 1.5356\n",
      "264/388, train_loss: 0.2468, step time: 1.5312\n",
      "265/388, train_loss: 0.1015, step time: 1.5420\n",
      "266/388, train_loss: 0.1896, step time: 1.5335\n",
      "267/388, train_loss: 0.2575, step time: 1.5345\n",
      "268/388, train_loss: 0.0898, step time: 1.5334\n",
      "269/388, train_loss: 0.1136, step time: 1.5360\n",
      "270/388, train_loss: 0.1598, step time: 1.5313\n",
      "271/388, train_loss: 0.2750, step time: 1.5302\n",
      "272/388, train_loss: 0.1435, step time: 1.5300\n",
      "273/388, train_loss: 0.1405, step time: 1.5302\n",
      "274/388, train_loss: 0.1701, step time: 1.5333\n",
      "275/388, train_loss: 0.1360, step time: 1.5350\n",
      "276/388, train_loss: 0.1881, step time: 1.5331\n",
      "277/388, train_loss: 0.2125, step time: 1.5326\n",
      "278/388, train_loss: 0.0706, step time: 1.5346\n",
      "279/388, train_loss: 0.1839, step time: 1.5294\n",
      "280/388, train_loss: 0.1404, step time: 1.5307\n",
      "281/388, train_loss: 0.2099, step time: 1.5328\n",
      "282/388, train_loss: 0.0718, step time: 1.5372\n",
      "283/388, train_loss: 0.2840, step time: 1.5364\n",
      "284/388, train_loss: 0.2571, step time: 1.5362\n",
      "285/388, train_loss: 0.1003, step time: 1.5320\n",
      "286/388, train_loss: 0.2297, step time: 1.5339\n",
      "287/388, train_loss: 0.2145, step time: 1.5343\n",
      "288/388, train_loss: 0.2829, step time: 1.5329\n",
      "289/388, train_loss: 0.1479, step time: 1.5385\n",
      "290/388, train_loss: 0.1684, step time: 1.5356\n",
      "291/388, train_loss: 0.0949, step time: 1.5342\n",
      "292/388, train_loss: 0.1122, step time: 1.5340\n",
      "293/388, train_loss: 0.2325, step time: 1.5314\n",
      "294/388, train_loss: 0.1641, step time: 1.5325\n",
      "295/388, train_loss: 0.0693, step time: 1.5322\n",
      "296/388, train_loss: 0.1960, step time: 1.5325\n",
      "297/388, train_loss: 0.5210, step time: 1.5345\n",
      "298/388, train_loss: 0.0597, step time: 1.5352\n",
      "299/388, train_loss: 0.1503, step time: 1.5331\n",
      "300/388, train_loss: 0.1322, step time: 1.5314\n",
      "301/388, train_loss: 0.2519, step time: 1.5310\n",
      "302/388, train_loss: 0.0614, step time: 1.5330\n",
      "303/388, train_loss: 0.3171, step time: 1.5318\n",
      "304/388, train_loss: 0.1647, step time: 1.5320\n",
      "305/388, train_loss: 0.2616, step time: 1.5325\n",
      "306/388, train_loss: 0.3052, step time: 1.5354\n",
      "307/388, train_loss: 0.1802, step time: 1.5372\n",
      "308/388, train_loss: 0.0606, step time: 1.5589\n",
      "309/388, train_loss: 0.1916, step time: 1.5698\n",
      "310/388, train_loss: 0.2302, step time: 1.5307\n",
      "311/388, train_loss: 0.1493, step time: 1.5347\n",
      "312/388, train_loss: 0.2152, step time: 1.5418\n",
      "313/388, train_loss: 0.0643, step time: 1.5349\n",
      "314/388, train_loss: 0.1031, step time: 1.5351\n",
      "315/388, train_loss: 0.1411, step time: 1.5313\n",
      "316/388, train_loss: 0.2236, step time: 1.5327\n",
      "317/388, train_loss: 0.2413, step time: 1.5368\n",
      "318/388, train_loss: 0.1286, step time: 1.5378\n",
      "319/388, train_loss: 0.1933, step time: 1.5370\n",
      "320/388, train_loss: 0.3924, step time: 1.5339\n",
      "321/388, train_loss: 0.0630, step time: 1.5292\n",
      "322/388, train_loss: 0.1991, step time: 1.5315\n",
      "323/388, train_loss: 0.2509, step time: 1.5329\n",
      "324/388, train_loss: 0.2363, step time: 1.5339\n",
      "325/388, train_loss: 0.1432, step time: 1.5344\n",
      "326/388, train_loss: 0.1632, step time: 1.5345\n",
      "327/388, train_loss: 0.2958, step time: 1.5331\n",
      "328/388, train_loss: 0.2010, step time: 1.5366\n",
      "329/388, train_loss: 0.0610, step time: 1.5338\n",
      "330/388, train_loss: 0.2293, step time: 1.5358\n",
      "331/388, train_loss: 0.4266, step time: 1.5352\n",
      "332/388, train_loss: 0.2051, step time: 1.5387\n",
      "333/388, train_loss: 0.1183, step time: 1.5371\n",
      "334/388, train_loss: 0.1679, step time: 1.5324\n",
      "335/388, train_loss: 0.1794, step time: 1.5304\n",
      "336/388, train_loss: 0.1549, step time: 1.5316\n",
      "337/388, train_loss: 0.0923, step time: 1.5310\n",
      "338/388, train_loss: 0.1038, step time: 1.5342\n",
      "339/388, train_loss: 0.0888, step time: 1.5353\n",
      "340/388, train_loss: 0.1091, step time: 1.5339\n",
      "341/388, train_loss: 0.1264, step time: 1.5381\n",
      "342/388, train_loss: 0.2232, step time: 1.5330\n",
      "343/388, train_loss: 0.1036, step time: 1.5310\n",
      "344/388, train_loss: 0.1840, step time: 1.5309\n",
      "345/388, train_loss: 0.1482, step time: 1.5319\n",
      "346/388, train_loss: 0.2905, step time: 1.5357\n",
      "347/388, train_loss: 0.2032, step time: 1.5335\n",
      "348/388, train_loss: 0.1946, step time: 1.5329\n",
      "349/388, train_loss: 0.1259, step time: 1.5345\n",
      "350/388, train_loss: 0.0812, step time: 1.5353\n",
      "351/388, train_loss: 0.0513, step time: 1.5344\n",
      "352/388, train_loss: 0.1150, step time: 1.5316\n",
      "353/388, train_loss: 0.1071, step time: 1.5329\n",
      "354/388, train_loss: 0.2034, step time: 1.5327\n",
      "355/388, train_loss: 0.2220, step time: 1.5366\n",
      "356/388, train_loss: 0.1194, step time: 1.5318\n",
      "357/388, train_loss: 0.2195, step time: 1.5490\n",
      "358/388, train_loss: 0.2969, step time: 1.5358\n",
      "359/388, train_loss: 0.1753, step time: 1.5349\n",
      "360/388, train_loss: 0.1371, step time: 1.5309\n",
      "361/388, train_loss: 0.1352, step time: 1.5365\n",
      "362/388, train_loss: 0.1486, step time: 1.5391\n",
      "363/388, train_loss: 0.2741, step time: 1.5333\n",
      "364/388, train_loss: 0.1288, step time: 1.5322\n",
      "365/388, train_loss: 0.0713, step time: 1.5318\n",
      "366/388, train_loss: 0.1051, step time: 1.5287\n",
      "367/388, train_loss: 0.3935, step time: 1.5328\n",
      "368/388, train_loss: 0.2069, step time: 1.5639\n",
      "369/388, train_loss: 0.2027, step time: 1.5327\n",
      "370/388, train_loss: 0.7048, step time: 1.5338\n",
      "371/388, train_loss: 0.1065, step time: 1.5317\n",
      "372/388, train_loss: 0.1698, step time: 1.5357\n",
      "373/388, train_loss: 0.2529, step time: 1.5349\n",
      "374/388, train_loss: 0.3742, step time: 1.5344\n",
      "375/388, train_loss: 0.2170, step time: 1.5308\n",
      "376/388, train_loss: 0.1741, step time: 1.5445\n",
      "377/388, train_loss: 0.3426, step time: 1.5353\n",
      "378/388, train_loss: 0.4361, step time: 1.5340\n",
      "379/388, train_loss: 0.5688, step time: 1.5365\n",
      "380/388, train_loss: 0.2139, step time: 1.5340\n",
      "381/388, train_loss: 0.1704, step time: 1.5342\n",
      "382/388, train_loss: 0.1086, step time: 1.5350\n",
      "383/388, train_loss: 0.1995, step time: 1.5330\n",
      "384/388, train_loss: 0.1390, step time: 1.5307\n",
      "385/388, train_loss: 0.2110, step time: 1.5333\n",
      "386/388, train_loss: 0.3761, step time: 1.5442\n",
      "387/388, train_loss: 0.2733, step time: 1.5314\n",
      "388/388, train_loss: 0.0834, step time: 1.5361\n",
      "epoch 31 average loss: 0.2032\n",
      "current epoch: 31 current mean dice: 0.7517 tc: 0.7990 wt: 0.8976 et: 0.5585\n",
      "best mean dice: 0.7545 at epoch: 26\n",
      "time consuming of epoch 31 is: 706.6480\n",
      "----------\n",
      "epoch 32/100\n",
      "1/388, train_loss: 0.0741, step time: 1.5700\n",
      "2/388, train_loss: 0.0978, step time: 1.5366\n",
      "3/388, train_loss: 0.2767, step time: 1.5338\n",
      "4/388, train_loss: 0.2326, step time: 1.5339\n",
      "5/388, train_loss: 0.1729, step time: 1.5311\n",
      "6/388, train_loss: 0.0908, step time: 1.5308\n",
      "7/388, train_loss: 0.2599, step time: 1.5377\n",
      "8/388, train_loss: 0.0915, step time: 1.5338\n",
      "9/388, train_loss: 0.1976, step time: 1.5321\n",
      "10/388, train_loss: 0.3528, step time: 1.5320\n",
      "11/388, train_loss: 0.1174, step time: 1.5301\n",
      "12/388, train_loss: 0.1585, step time: 1.5320\n",
      "13/388, train_loss: 0.1729, step time: 1.5295\n",
      "14/388, train_loss: 0.1433, step time: 1.5324\n",
      "15/388, train_loss: 0.1552, step time: 1.5351\n",
      "16/388, train_loss: 0.2224, step time: 1.5363\n",
      "17/388, train_loss: 0.2066, step time: 1.5325\n",
      "18/388, train_loss: 0.1656, step time: 1.5366\n",
      "19/388, train_loss: 0.1100, step time: 1.5376\n",
      "20/388, train_loss: 0.3185, step time: 1.5340\n",
      "21/388, train_loss: 0.0879, step time: 1.5338\n",
      "22/388, train_loss: 0.0703, step time: 1.5370\n",
      "23/388, train_loss: 0.0936, step time: 1.5369\n",
      "24/388, train_loss: 0.1507, step time: 1.5385\n",
      "25/388, train_loss: 0.0862, step time: 1.5303\n",
      "26/388, train_loss: 0.2180, step time: 1.5335\n",
      "27/388, train_loss: 0.3965, step time: 1.5293\n",
      "28/388, train_loss: 0.3107, step time: 1.5344\n",
      "29/388, train_loss: 0.1234, step time: 1.5359\n",
      "30/388, train_loss: 0.0925, step time: 1.5395\n",
      "31/388, train_loss: 0.3241, step time: 1.5326\n",
      "32/388, train_loss: 0.0664, step time: 1.5344\n",
      "33/388, train_loss: 0.2564, step time: 1.5601\n",
      "34/388, train_loss: 0.1137, step time: 1.5398\n",
      "35/388, train_loss: 0.2939, step time: 1.5321\n",
      "36/388, train_loss: 0.2212, step time: 1.5338\n",
      "37/388, train_loss: 0.1180, step time: 1.5352\n",
      "38/388, train_loss: 0.1374, step time: 1.5296\n",
      "39/388, train_loss: 0.0897, step time: 1.5326\n",
      "40/388, train_loss: 0.2022, step time: 1.5366\n",
      "41/388, train_loss: 0.1082, step time: 1.5372\n",
      "42/388, train_loss: 0.2196, step time: 1.5361\n",
      "43/388, train_loss: 0.2539, step time: 1.5312\n",
      "44/388, train_loss: 0.2175, step time: 1.5350\n",
      "45/388, train_loss: 0.0499, step time: 1.5355\n",
      "46/388, train_loss: 0.0855, step time: 1.5315\n",
      "47/388, train_loss: 0.2773, step time: 1.5327\n",
      "48/388, train_loss: 0.2513, step time: 1.5356\n",
      "49/388, train_loss: 0.2227, step time: 1.5343\n",
      "50/388, train_loss: 0.3548, step time: 1.5364\n",
      "51/388, train_loss: 0.0741, step time: 1.5340\n",
      "52/388, train_loss: 0.2591, step time: 1.5339\n",
      "53/388, train_loss: 0.1232, step time: 1.5306\n",
      "54/388, train_loss: 0.2127, step time: 1.5317\n",
      "55/388, train_loss: 0.1748, step time: 1.5330\n",
      "56/388, train_loss: 0.3864, step time: 1.5340\n",
      "57/388, train_loss: 0.1365, step time: 1.5349\n",
      "58/388, train_loss: 0.1000, step time: 1.5351\n",
      "59/388, train_loss: 0.0920, step time: 1.5316\n",
      "60/388, train_loss: 0.0485, step time: 1.5377\n",
      "61/388, train_loss: 0.1977, step time: 1.5292\n",
      "62/388, train_loss: 0.2416, step time: 1.5332\n",
      "63/388, train_loss: 0.3777, step time: 1.5332\n",
      "64/388, train_loss: 0.2014, step time: 1.5474\n",
      "65/388, train_loss: 0.2609, step time: 1.5295\n",
      "66/388, train_loss: 0.1337, step time: 1.5291\n",
      "67/388, train_loss: 0.1028, step time: 1.5310\n",
      "68/388, train_loss: 0.2907, step time: 1.5353\n",
      "69/388, train_loss: 0.0347, step time: 1.5383\n",
      "70/388, train_loss: 0.1322, step time: 1.5348\n",
      "71/388, train_loss: 0.4985, step time: 1.5355\n",
      "72/388, train_loss: 0.1115, step time: 1.5347\n",
      "73/388, train_loss: 0.1662, step time: 1.5346\n",
      "74/388, train_loss: 0.1849, step time: 1.5315\n",
      "75/388, train_loss: 0.1584, step time: 1.5341\n",
      "76/388, train_loss: 0.2601, step time: 1.5342\n",
      "77/388, train_loss: 0.4081, step time: 1.5363\n",
      "78/388, train_loss: 0.1438, step time: 1.5332\n",
      "79/388, train_loss: 0.2791, step time: 1.5388\n",
      "80/388, train_loss: 0.5637, step time: 1.5398\n",
      "81/388, train_loss: 0.1495, step time: 1.5325\n",
      "82/388, train_loss: 0.1873, step time: 1.5397\n",
      "83/388, train_loss: 0.2743, step time: 1.5362\n",
      "84/388, train_loss: 0.1114, step time: 1.5337\n",
      "85/388, train_loss: 0.4040, step time: 1.5271\n",
      "86/388, train_loss: 0.0526, step time: 1.5316\n",
      "87/388, train_loss: 0.2509, step time: 1.5352\n",
      "88/388, train_loss: 0.1541, step time: 1.5356\n",
      "89/388, train_loss: 0.0636, step time: 1.5348\n",
      "90/388, train_loss: 0.0968, step time: 1.5342\n",
      "91/388, train_loss: 0.2945, step time: 1.5324\n",
      "92/388, train_loss: 0.2445, step time: 1.5328\n",
      "93/388, train_loss: 0.2029, step time: 1.5310\n",
      "94/388, train_loss: 0.0990, step time: 1.5325\n",
      "95/388, train_loss: 0.2676, step time: 1.5337\n",
      "96/388, train_loss: 0.2052, step time: 1.5343\n",
      "97/388, train_loss: 0.0786, step time: 1.5366\n",
      "98/388, train_loss: 0.1465, step time: 1.5310\n",
      "99/388, train_loss: 0.1639, step time: 1.5331\n",
      "100/388, train_loss: 0.1842, step time: 1.5384\n",
      "101/388, train_loss: 0.1583, step time: 1.5408\n",
      "102/388, train_loss: 0.0299, step time: 1.5349\n",
      "103/388, train_loss: 0.5413, step time: 1.5324\n",
      "104/388, train_loss: 0.1366, step time: 1.5305\n",
      "105/388, train_loss: 0.2148, step time: 1.5372\n",
      "106/388, train_loss: 0.2180, step time: 1.5381\n",
      "107/388, train_loss: 0.1956, step time: 1.5317\n",
      "108/388, train_loss: 0.3380, step time: 1.5355\n",
      "109/388, train_loss: 0.2296, step time: 1.5309\n",
      "110/388, train_loss: 0.1128, step time: 1.5287\n",
      "111/388, train_loss: 0.3060, step time: 1.5315\n",
      "112/388, train_loss: 0.2353, step time: 1.5320\n",
      "113/388, train_loss: 0.1532, step time: 1.5379\n",
      "114/388, train_loss: 0.1685, step time: 1.5349\n",
      "115/388, train_loss: 0.2917, step time: 1.5346\n",
      "116/388, train_loss: 0.0565, step time: 1.5329\n",
      "117/388, train_loss: 0.2419, step time: 1.5301\n",
      "118/388, train_loss: 0.1220, step time: 1.5323\n",
      "119/388, train_loss: 0.0881, step time: 1.5325\n",
      "120/388, train_loss: 0.1359, step time: 1.5451\n",
      "121/388, train_loss: 0.0871, step time: 1.5438\n",
      "122/388, train_loss: 0.2164, step time: 1.5366\n",
      "123/388, train_loss: 0.0337, step time: 1.5303\n",
      "124/388, train_loss: 0.1953, step time: 1.5332\n",
      "125/388, train_loss: 0.2599, step time: 1.5314\n",
      "126/388, train_loss: 0.3597, step time: 1.5336\n",
      "127/388, train_loss: 0.1659, step time: 1.5342\n",
      "128/388, train_loss: 0.1011, step time: 1.5355\n",
      "129/388, train_loss: 0.3432, step time: 1.5319\n",
      "130/388, train_loss: 0.2399, step time: 1.5329\n",
      "131/388, train_loss: 0.2271, step time: 1.5307\n",
      "132/388, train_loss: 0.0548, step time: 1.5331\n",
      "133/388, train_loss: 0.0500, step time: 1.5321\n",
      "134/388, train_loss: 0.2032, step time: 1.5375\n",
      "135/388, train_loss: 0.0847, step time: 1.5332\n",
      "136/388, train_loss: 0.3380, step time: 1.5354\n",
      "137/388, train_loss: 0.4151, step time: 1.5316\n",
      "138/388, train_loss: 0.1425, step time: 1.5326\n",
      "139/388, train_loss: 0.1461, step time: 1.5368\n",
      "140/388, train_loss: 0.1182, step time: 1.5314\n",
      "141/388, train_loss: 0.0931, step time: 1.5333\n",
      "142/388, train_loss: 0.4703, step time: 1.5353\n",
      "143/388, train_loss: 0.1363, step time: 1.5319\n",
      "144/388, train_loss: 0.2441, step time: 1.5384\n",
      "145/388, train_loss: 0.1172, step time: 1.5344\n",
      "146/388, train_loss: 0.1891, step time: 1.5354\n",
      "147/388, train_loss: 0.1482, step time: 1.5353\n",
      "148/388, train_loss: 0.1274, step time: 1.5462\n",
      "149/388, train_loss: 0.0539, step time: 1.5342\n",
      "150/388, train_loss: 0.2149, step time: 1.5291\n",
      "151/388, train_loss: 0.1106, step time: 1.5313\n",
      "152/388, train_loss: 0.2052, step time: 1.5340\n",
      "153/388, train_loss: 0.0974, step time: 1.5329\n",
      "154/388, train_loss: 0.2765, step time: 1.5368\n",
      "155/388, train_loss: 0.1624, step time: 1.5420\n",
      "156/388, train_loss: 0.2231, step time: 1.5338\n",
      "157/388, train_loss: 0.2607, step time: 1.5304\n",
      "158/388, train_loss: 0.0963, step time: 1.5304\n",
      "159/388, train_loss: 0.2776, step time: 1.5326\n",
      "160/388, train_loss: 0.1373, step time: 1.5332\n",
      "161/388, train_loss: 0.2866, step time: 1.5332\n",
      "162/388, train_loss: 0.1195, step time: 1.5350\n",
      "163/388, train_loss: 0.3966, step time: 1.5328\n",
      "164/388, train_loss: 0.1699, step time: 1.5314\n",
      "165/388, train_loss: 0.1555, step time: 1.5311\n",
      "166/388, train_loss: 0.0869, step time: 1.5395\n",
      "167/388, train_loss: 0.0633, step time: 1.5351\n",
      "168/388, train_loss: 0.1607, step time: 1.5370\n",
      "169/388, train_loss: 0.3813, step time: 1.5586\n",
      "170/388, train_loss: 0.3067, step time: 1.5355\n",
      "171/388, train_loss: 0.1699, step time: 1.5357\n",
      "172/388, train_loss: 0.0953, step time: 1.5365\n",
      "173/388, train_loss: 0.1032, step time: 1.5339\n",
      "174/388, train_loss: 0.2471, step time: 1.5322\n",
      "175/388, train_loss: 0.1485, step time: 1.5300\n",
      "176/388, train_loss: 0.2340, step time: 1.5337\n",
      "177/388, train_loss: 0.2328, step time: 1.5353\n",
      "178/388, train_loss: 0.3219, step time: 1.5480\n",
      "179/388, train_loss: 0.3445, step time: 1.5338\n",
      "180/388, train_loss: 0.4223, step time: 1.5324\n",
      "181/388, train_loss: 0.1313, step time: 1.5318\n",
      "182/388, train_loss: 0.1484, step time: 1.5375\n",
      "183/388, train_loss: 0.0735, step time: 1.5519\n",
      "184/388, train_loss: 0.6006, step time: 1.5315\n",
      "185/388, train_loss: 0.2291, step time: 1.5330\n",
      "186/388, train_loss: 0.1919, step time: 1.5314\n",
      "187/388, train_loss: 0.0838, step time: 1.5358\n",
      "188/388, train_loss: 0.1762, step time: 1.5351\n",
      "189/388, train_loss: 0.1762, step time: 1.5397\n",
      "190/388, train_loss: 0.2432, step time: 1.5306\n",
      "191/388, train_loss: 0.2128, step time: 1.5317\n",
      "192/388, train_loss: 0.3303, step time: 1.5358\n",
      "193/388, train_loss: 0.2435, step time: 1.5345\n",
      "194/388, train_loss: 0.2750, step time: 1.5346\n",
      "195/388, train_loss: 0.2613, step time: 1.5369\n",
      "196/388, train_loss: 0.0896, step time: 1.5301\n",
      "197/388, train_loss: 0.0972, step time: 1.5313\n",
      "198/388, train_loss: 0.1475, step time: 1.5332\n",
      "199/388, train_loss: 0.0539, step time: 1.5348\n",
      "200/388, train_loss: 0.2662, step time: 1.5354\n",
      "201/388, train_loss: 0.1104, step time: 1.5369\n",
      "202/388, train_loss: 0.3784, step time: 1.5341\n",
      "203/388, train_loss: 0.1058, step time: 1.5308\n",
      "204/388, train_loss: 0.4450, step time: 1.5353\n",
      "205/388, train_loss: 0.3370, step time: 1.5314\n",
      "206/388, train_loss: 0.1641, step time: 1.5401\n",
      "207/388, train_loss: 0.1635, step time: 1.5392\n",
      "208/388, train_loss: 0.1098, step time: 1.5324\n",
      "209/388, train_loss: 0.0944, step time: 1.5330\n",
      "210/388, train_loss: 0.4138, step time: 1.5303\n",
      "211/388, train_loss: 0.3413, step time: 1.5345\n",
      "212/388, train_loss: 0.1697, step time: 1.5292\n",
      "213/388, train_loss: 0.2067, step time: 1.5355\n",
      "214/388, train_loss: 0.2191, step time: 1.5356\n",
      "215/388, train_loss: 0.2745, step time: 1.5324\n",
      "216/388, train_loss: 0.2238, step time: 1.5303\n",
      "217/388, train_loss: 0.0885, step time: 1.5319\n",
      "218/388, train_loss: 0.3352, step time: 1.5367\n",
      "219/388, train_loss: 0.3401, step time: 1.5338\n",
      "220/388, train_loss: 0.2921, step time: 1.5312\n",
      "221/388, train_loss: 0.1022, step time: 1.5320\n",
      "222/388, train_loss: 0.1500, step time: 1.5339\n",
      "223/388, train_loss: 0.0712, step time: 1.5368\n",
      "224/388, train_loss: 0.3015, step time: 1.5351\n",
      "225/388, train_loss: 0.2597, step time: 1.5339\n",
      "226/388, train_loss: 0.0767, step time: 1.5328\n",
      "227/388, train_loss: 0.1637, step time: 1.5306\n",
      "228/388, train_loss: 0.1323, step time: 1.5334\n",
      "229/388, train_loss: 0.3151, step time: 1.5347\n",
      "230/388, train_loss: 0.3084, step time: 1.5339\n",
      "231/388, train_loss: 0.1952, step time: 1.5328\n",
      "232/388, train_loss: 0.1872, step time: 1.5294\n",
      "233/388, train_loss: 0.2045, step time: 1.5275\n",
      "234/388, train_loss: 0.1049, step time: 1.5325\n",
      "235/388, train_loss: 0.1725, step time: 1.5377\n",
      "236/388, train_loss: 0.1835, step time: 1.5373\n",
      "237/388, train_loss: 0.2050, step time: 1.5372\n",
      "238/388, train_loss: 0.1252, step time: 1.5311\n",
      "239/388, train_loss: 0.1039, step time: 1.5318\n",
      "240/388, train_loss: 0.1505, step time: 1.5328\n",
      "241/388, train_loss: 0.1701, step time: 1.5316\n",
      "242/388, train_loss: 0.1506, step time: 1.5337\n",
      "243/388, train_loss: 0.2931, step time: 1.5428\n",
      "244/388, train_loss: 0.1736, step time: 1.5385\n",
      "245/388, train_loss: 0.1493, step time: 1.5319\n",
      "246/388, train_loss: 0.3034, step time: 1.5299\n",
      "247/388, train_loss: 0.2037, step time: 1.5351\n",
      "248/388, train_loss: 0.1614, step time: 1.5354\n",
      "249/388, train_loss: 0.0981, step time: 1.5334\n",
      "250/388, train_loss: 0.1072, step time: 1.5316\n",
      "251/388, train_loss: 0.2650, step time: 1.5337\n",
      "252/388, train_loss: 0.1412, step time: 1.5321\n",
      "253/388, train_loss: 0.3556, step time: 1.5342\n",
      "254/388, train_loss: 0.1646, step time: 1.5318\n",
      "255/388, train_loss: 0.2146, step time: 1.5324\n",
      "256/388, train_loss: 0.1858, step time: 1.5345\n",
      "257/388, train_loss: 0.5176, step time: 1.5292\n",
      "258/388, train_loss: 0.0968, step time: 1.5351\n",
      "259/388, train_loss: 0.1126, step time: 1.5370\n",
      "260/388, train_loss: 0.0848, step time: 1.5351\n",
      "261/388, train_loss: 0.0731, step time: 1.5340\n",
      "262/388, train_loss: 0.1682, step time: 1.5306\n",
      "263/388, train_loss: 0.2433, step time: 1.5304\n",
      "264/388, train_loss: 0.1765, step time: 1.5317\n",
      "265/388, train_loss: 0.3920, step time: 1.5317\n",
      "266/388, train_loss: 0.0634, step time: 1.5344\n",
      "267/388, train_loss: 0.1094, step time: 1.5340\n",
      "268/388, train_loss: 0.0840, step time: 1.5368\n",
      "269/388, train_loss: 0.1700, step time: 1.5328\n",
      "270/388, train_loss: 0.4285, step time: 1.5316\n",
      "271/388, train_loss: 0.1677, step time: 1.5313\n",
      "272/388, train_loss: 0.3890, step time: 1.5314\n",
      "273/388, train_loss: 0.3021, step time: 1.5331\n",
      "274/388, train_loss: 0.4665, step time: 1.5348\n",
      "275/388, train_loss: 0.1554, step time: 1.5313\n",
      "276/388, train_loss: 0.0817, step time: 1.5288\n",
      "277/388, train_loss: 0.2920, step time: 1.5318\n",
      "278/388, train_loss: 0.0679, step time: 1.5345\n",
      "279/388, train_loss: 0.1378, step time: 1.5359\n",
      "280/388, train_loss: 0.1267, step time: 1.5373\n",
      "281/388, train_loss: 0.5440, step time: 1.5351\n",
      "282/388, train_loss: 0.1898, step time: 1.5328\n",
      "283/388, train_loss: 0.3863, step time: 1.5319\n",
      "284/388, train_loss: 0.1063, step time: 1.5320\n",
      "285/388, train_loss: 0.0831, step time: 1.5342\n",
      "286/388, train_loss: 0.2948, step time: 1.5360\n",
      "287/388, train_loss: 0.0683, step time: 1.5380\n",
      "288/388, train_loss: 0.1715, step time: 1.5358\n",
      "289/388, train_loss: 0.1143, step time: 1.5345\n",
      "290/388, train_loss: 0.1504, step time: 1.5333\n",
      "291/388, train_loss: 0.2166, step time: 1.5353\n",
      "292/388, train_loss: 0.4195, step time: 1.5345\n",
      "293/388, train_loss: 0.2614, step time: 1.5321\n",
      "294/388, train_loss: 0.0951, step time: 1.5372\n",
      "295/388, train_loss: 0.4214, step time: 1.5316\n",
      "296/388, train_loss: 0.2165, step time: 1.5299\n",
      "297/388, train_loss: 0.0730, step time: 1.5319\n",
      "298/388, train_loss: 0.1768, step time: 1.5628\n",
      "299/388, train_loss: 0.2654, step time: 1.5313\n",
      "300/388, train_loss: 0.3248, step time: 1.5335\n",
      "301/388, train_loss: 0.1408, step time: 1.5377\n",
      "302/388, train_loss: 0.1030, step time: 1.5330\n",
      "303/388, train_loss: 0.2335, step time: 1.5335\n",
      "304/388, train_loss: 0.1572, step time: 1.5334\n",
      "305/388, train_loss: 0.1941, step time: 1.5320\n",
      "306/388, train_loss: 0.2337, step time: 1.5339\n",
      "307/388, train_loss: 0.0647, step time: 1.5325\n",
      "308/388, train_loss: 0.1109, step time: 1.5385\n",
      "309/388, train_loss: 0.4187, step time: 1.5364\n",
      "310/388, train_loss: 0.2496, step time: 1.5340\n",
      "311/388, train_loss: 0.2134, step time: 1.5334\n",
      "312/388, train_loss: 0.2245, step time: 1.5340\n",
      "313/388, train_loss: 0.0870, step time: 1.5304\n",
      "314/388, train_loss: 0.1893, step time: 1.5364\n",
      "315/388, train_loss: 0.2111, step time: 1.5357\n",
      "316/388, train_loss: 0.1401, step time: 1.5386\n",
      "317/388, train_loss: 0.1274, step time: 1.5344\n",
      "318/388, train_loss: 0.2034, step time: 1.5316\n",
      "319/388, train_loss: 0.1215, step time: 1.5312\n",
      "320/388, train_loss: 0.1240, step time: 1.5312\n",
      "321/388, train_loss: 0.4586, step time: 1.5312\n",
      "322/388, train_loss: 0.1323, step time: 1.5519\n",
      "323/388, train_loss: 0.2429, step time: 1.5338\n",
      "324/388, train_loss: 0.1180, step time: 1.5363\n",
      "325/388, train_loss: 0.1223, step time: 1.5329\n",
      "326/388, train_loss: 0.1981, step time: 1.5307\n",
      "327/388, train_loss: 0.1452, step time: 1.5338\n",
      "328/388, train_loss: 0.0857, step time: 1.5360\n",
      "329/388, train_loss: 0.2951, step time: 1.5341\n",
      "330/388, train_loss: 0.1086, step time: 1.5352\n",
      "331/388, train_loss: 0.3977, step time: 1.5386\n",
      "332/388, train_loss: 0.1498, step time: 1.5329\n",
      "333/388, train_loss: 0.1088, step time: 1.5327\n",
      "334/388, train_loss: 0.2179, step time: 1.5423\n",
      "335/388, train_loss: 0.4968, step time: 1.5352\n",
      "336/388, train_loss: 0.3247, step time: 1.5355\n",
      "337/388, train_loss: 0.1196, step time: 1.5317\n",
      "338/388, train_loss: 0.1076, step time: 1.5316\n",
      "339/388, train_loss: 0.0902, step time: 1.5351\n",
      "340/388, train_loss: 0.1959, step time: 1.5351\n",
      "341/388, train_loss: 0.1265, step time: 1.5382\n",
      "342/388, train_loss: 0.3221, step time: 1.5389\n",
      "343/388, train_loss: 0.1326, step time: 1.5388\n",
      "344/388, train_loss: 0.1746, step time: 1.5352\n",
      "345/388, train_loss: 0.1320, step time: 1.5376\n",
      "346/388, train_loss: 0.1418, step time: 1.5461\n",
      "347/388, train_loss: 0.1021, step time: 1.5318\n",
      "348/388, train_loss: 0.1066, step time: 1.5291\n",
      "349/388, train_loss: 0.1471, step time: 1.5319\n",
      "350/388, train_loss: 0.0991, step time: 1.5296\n",
      "351/388, train_loss: 0.1014, step time: 1.5576\n",
      "352/388, train_loss: 0.1998, step time: 1.5311\n",
      "353/388, train_loss: 0.1410, step time: 1.5334\n",
      "354/388, train_loss: 0.2033, step time: 1.5598\n",
      "355/388, train_loss: 0.4795, step time: 1.5329\n",
      "356/388, train_loss: 0.2115, step time: 1.5343\n",
      "357/388, train_loss: 0.1596, step time: 1.5319\n",
      "358/388, train_loss: 0.3675, step time: 1.5335\n",
      "359/388, train_loss: 0.1373, step time: 1.5348\n",
      "360/388, train_loss: 0.1502, step time: 1.5367\n",
      "361/388, train_loss: 0.1323, step time: 1.5324\n",
      "362/388, train_loss: 0.2908, step time: 1.5323\n",
      "363/388, train_loss: 0.1971, step time: 1.5333\n",
      "364/388, train_loss: 0.4881, step time: 1.5371\n",
      "365/388, train_loss: 0.0717, step time: 1.5373\n",
      "366/388, train_loss: 0.2223, step time: 1.5326\n",
      "367/388, train_loss: 0.6621, step time: 1.5329\n",
      "368/388, train_loss: 0.1944, step time: 1.5315\n",
      "369/388, train_loss: 0.1474, step time: 1.5312\n",
      "370/388, train_loss: 0.3881, step time: 1.5354\n",
      "371/388, train_loss: 0.1339, step time: 1.5353\n",
      "372/388, train_loss: 0.0791, step time: 1.5358\n",
      "373/388, train_loss: 0.2844, step time: 1.5356\n",
      "374/388, train_loss: 0.1306, step time: 1.5424\n",
      "375/388, train_loss: 0.0952, step time: 1.5315\n",
      "376/388, train_loss: 0.2303, step time: 1.5304\n",
      "377/388, train_loss: 0.1590, step time: 1.5322\n",
      "378/388, train_loss: 0.5108, step time: 1.5365\n",
      "379/388, train_loss: 0.1576, step time: 1.5361\n",
      "380/388, train_loss: 0.4183, step time: 1.5377\n",
      "381/388, train_loss: 0.0944, step time: 1.5290\n",
      "382/388, train_loss: 0.0867, step time: 1.5345\n",
      "383/388, train_loss: 0.1176, step time: 1.5319\n",
      "384/388, train_loss: 0.2047, step time: 1.5372\n",
      "385/388, train_loss: 0.5002, step time: 1.5396\n",
      "386/388, train_loss: 0.1529, step time: 1.5343\n",
      "387/388, train_loss: 0.2423, step time: 1.5339\n",
      "388/388, train_loss: 0.2175, step time: 1.5329\n",
      "epoch 32 average loss: 0.2006\n",
      "current epoch: 32 current mean dice: 0.7444 tc: 0.7857 wt: 0.8954 et: 0.5520\n",
      "best mean dice: 0.7545 at epoch: 26\n",
      "time consuming of epoch 32 is: 703.0582\n",
      "----------\n",
      "epoch 33/100\n",
      "1/388, train_loss: 0.0699, step time: 1.5442\n",
      "2/388, train_loss: 0.1648, step time: 1.5327\n",
      "3/388, train_loss: 0.1679, step time: 1.5369\n",
      "4/388, train_loss: 0.1103, step time: 1.5365\n",
      "5/388, train_loss: 0.2576, step time: 1.5602\n",
      "6/388, train_loss: 0.1028, step time: 1.5603\n",
      "7/388, train_loss: 0.1708, step time: 1.5340\n",
      "8/388, train_loss: 0.5225, step time: 1.5324\n",
      "9/388, train_loss: 0.2036, step time: 1.5316\n",
      "10/388, train_loss: 0.3084, step time: 1.5352\n",
      "11/388, train_loss: 0.1119, step time: 1.5361\n",
      "12/388, train_loss: 0.1989, step time: 1.5360\n",
      "13/388, train_loss: 0.0972, step time: 1.5356\n",
      "14/388, train_loss: 0.1299, step time: 1.5354\n",
      "15/388, train_loss: 0.1163, step time: 1.5326\n",
      "16/388, train_loss: 0.4980, step time: 1.5327\n",
      "17/388, train_loss: 0.1333, step time: 1.5385\n",
      "18/388, train_loss: 0.1558, step time: 1.5380\n",
      "19/388, train_loss: 0.3105, step time: 1.5350\n",
      "20/388, train_loss: 0.1824, step time: 1.5350\n",
      "21/388, train_loss: 0.1047, step time: 1.5311\n",
      "22/388, train_loss: 0.2240, step time: 1.5603\n",
      "23/388, train_loss: 0.2212, step time: 1.5316\n",
      "24/388, train_loss: 0.1351, step time: 1.5319\n",
      "25/388, train_loss: 0.1908, step time: 1.5311\n",
      "26/388, train_loss: 0.3302, step time: 1.5352\n",
      "27/388, train_loss: 0.3509, step time: 1.5304\n",
      "28/388, train_loss: 0.2272, step time: 1.5359\n",
      "29/388, train_loss: 0.3134, step time: 1.5326\n",
      "30/388, train_loss: 0.1031, step time: 1.5318\n",
      "31/388, train_loss: 0.0646, step time: 1.5330\n",
      "32/388, train_loss: 0.2101, step time: 1.5472\n",
      "33/388, train_loss: 0.0911, step time: 1.5329\n",
      "34/388, train_loss: 0.0844, step time: 1.5362\n",
      "35/388, train_loss: 0.2098, step time: 1.5428\n",
      "36/388, train_loss: 0.1863, step time: 1.5433\n",
      "37/388, train_loss: 0.2842, step time: 1.5324\n",
      "38/388, train_loss: 0.1402, step time: 1.5342\n",
      "39/388, train_loss: 0.2142, step time: 1.5370\n",
      "40/388, train_loss: 0.2484, step time: 1.5532\n",
      "41/388, train_loss: 0.4534, step time: 1.5308\n",
      "42/388, train_loss: 0.2179, step time: 1.5331\n",
      "43/388, train_loss: 0.3446, step time: 1.5359\n",
      "44/388, train_loss: 0.0913, step time: 1.5581\n",
      "45/388, train_loss: 0.1323, step time: 1.5331\n",
      "46/388, train_loss: 0.1329, step time: 1.5362\n",
      "47/388, train_loss: 0.5141, step time: 1.5493\n",
      "48/388, train_loss: 0.2734, step time: 1.5539\n",
      "49/388, train_loss: 0.1892, step time: 1.5340\n",
      "50/388, train_loss: 0.2213, step time: 1.5331\n",
      "51/388, train_loss: 0.4623, step time: 1.5346\n",
      "52/388, train_loss: 0.1420, step time: 1.5462\n",
      "53/388, train_loss: 0.1263, step time: 1.5287\n",
      "54/388, train_loss: 0.1208, step time: 1.5324\n",
      "55/388, train_loss: 0.1317, step time: 1.5372\n",
      "56/388, train_loss: 0.3376, step time: 1.5518\n",
      "57/388, train_loss: 0.3016, step time: 1.5323\n",
      "58/388, train_loss: 0.2271, step time: 1.5357\n",
      "59/388, train_loss: 0.1185, step time: 1.5348\n",
      "60/388, train_loss: 0.0573, step time: 1.5528\n",
      "61/388, train_loss: 0.1543, step time: 1.5500\n",
      "62/388, train_loss: 0.2370, step time: 1.5332\n",
      "63/388, train_loss: 0.2004, step time: 1.5317\n",
      "64/388, train_loss: 0.2300, step time: 1.5532\n",
      "65/388, train_loss: 0.0347, step time: 1.5370\n",
      "66/388, train_loss: 0.1383, step time: 1.5332\n",
      "67/388, train_loss: 0.3579, step time: 1.5312\n",
      "68/388, train_loss: 0.1916, step time: 1.5410\n",
      "69/388, train_loss: 0.3141, step time: 1.5357\n",
      "70/388, train_loss: 0.0663, step time: 1.5361\n",
      "71/388, train_loss: 0.1551, step time: 1.5329\n",
      "72/388, train_loss: 0.2999, step time: 1.5437\n",
      "73/388, train_loss: 0.1200, step time: 1.5316\n",
      "74/388, train_loss: 0.1670, step time: 1.5367\n",
      "75/388, train_loss: 0.5337, step time: 1.5358\n",
      "76/388, train_loss: 0.2887, step time: 1.5525\n",
      "77/388, train_loss: 0.1783, step time: 1.5347\n",
      "78/388, train_loss: 0.1315, step time: 1.5342\n",
      "79/388, train_loss: 0.2625, step time: 1.5348\n",
      "80/388, train_loss: 0.1577, step time: 1.5538\n",
      "81/388, train_loss: 0.1242, step time: 1.5378\n",
      "82/388, train_loss: 0.1137, step time: 1.5355\n",
      "83/388, train_loss: 0.0739, step time: 1.5461\n",
      "84/388, train_loss: 0.3352, step time: 1.5482\n",
      "85/388, train_loss: 0.4185, step time: 1.5337\n",
      "86/388, train_loss: 0.1759, step time: 1.5347\n",
      "87/388, train_loss: 0.1200, step time: 1.5367\n",
      "88/388, train_loss: 0.1346, step time: 1.5448\n",
      "89/388, train_loss: 0.1298, step time: 1.5313\n",
      "90/388, train_loss: 0.1481, step time: 1.5316\n",
      "91/388, train_loss: 0.1305, step time: 1.5350\n",
      "92/388, train_loss: 0.0994, step time: 1.5560\n",
      "93/388, train_loss: 0.2064, step time: 1.5474\n",
      "94/388, train_loss: 0.0860, step time: 1.5366\n",
      "95/388, train_loss: 0.0748, step time: 1.5375\n",
      "96/388, train_loss: 0.2556, step time: 1.5546\n",
      "97/388, train_loss: 0.1315, step time: 1.5352\n",
      "98/388, train_loss: 0.1509, step time: 1.5338\n",
      "99/388, train_loss: 0.3817, step time: 1.5351\n",
      "100/388, train_loss: 0.0811, step time: 1.5538\n",
      "101/388, train_loss: 0.1196, step time: 1.5701\n",
      "102/388, train_loss: 0.4390, step time: 1.5327\n",
      "103/388, train_loss: 0.0880, step time: 1.5363\n",
      "104/388, train_loss: 0.2088, step time: 1.5456\n",
      "105/388, train_loss: 0.1445, step time: 1.5309\n",
      "106/388, train_loss: 0.1632, step time: 1.5398\n",
      "107/388, train_loss: 0.1704, step time: 1.5374\n",
      "108/388, train_loss: 0.1044, step time: 1.5435\n",
      "109/388, train_loss: 0.2195, step time: 1.5319\n",
      "110/388, train_loss: 0.1341, step time: 1.5366\n",
      "111/388, train_loss: 0.1357, step time: 1.5358\n",
      "112/388, train_loss: 0.2424, step time: 1.5549\n",
      "113/388, train_loss: 0.2990, step time: 1.5311\n",
      "114/388, train_loss: 0.2730, step time: 1.5334\n",
      "115/388, train_loss: 0.2775, step time: 1.5400\n",
      "116/388, train_loss: 0.1474, step time: 1.5582\n",
      "117/388, train_loss: 0.0882, step time: 1.5331\n",
      "118/388, train_loss: 0.2069, step time: 1.5361\n",
      "119/388, train_loss: 0.3624, step time: 1.5348\n",
      "120/388, train_loss: 0.2054, step time: 1.5446\n",
      "121/388, train_loss: 0.2340, step time: 1.5332\n",
      "122/388, train_loss: 0.0729, step time: 1.5339\n",
      "123/388, train_loss: 0.0606, step time: 1.5374\n",
      "124/388, train_loss: 0.1507, step time: 1.5447\n",
      "125/388, train_loss: 0.1065, step time: 1.5329\n",
      "126/388, train_loss: 0.1688, step time: 1.5376\n",
      "127/388, train_loss: 0.1199, step time: 1.5386\n",
      "128/388, train_loss: 0.4987, step time: 1.5493\n",
      "129/388, train_loss: 0.2637, step time: 1.5314\n",
      "130/388, train_loss: 0.1865, step time: 1.5287\n",
      "131/388, train_loss: 0.2042, step time: 1.5313\n",
      "132/388, train_loss: 0.1803, step time: 1.5468\n",
      "133/388, train_loss: 0.3601, step time: 1.5322\n",
      "134/388, train_loss: 0.1693, step time: 1.5326\n",
      "135/388, train_loss: 0.6411, step time: 1.5322\n",
      "136/388, train_loss: 0.1045, step time: 1.5502\n",
      "137/388, train_loss: 0.0940, step time: 1.5372\n",
      "138/388, train_loss: 0.1599, step time: 1.5352\n",
      "139/388, train_loss: 0.0892, step time: 1.5333\n",
      "140/388, train_loss: 0.6659, step time: 1.5548\n",
      "141/388, train_loss: 0.2574, step time: 1.5364\n",
      "142/388, train_loss: 0.1091, step time: 1.5328\n",
      "143/388, train_loss: 0.0891, step time: 1.5345\n",
      "144/388, train_loss: 0.3080, step time: 1.5537\n",
      "145/388, train_loss: 0.1525, step time: 1.5308\n",
      "146/388, train_loss: 0.2133, step time: 1.5352\n",
      "147/388, train_loss: 0.1293, step time: 1.5345\n",
      "148/388, train_loss: 0.2523, step time: 1.5433\n",
      "149/388, train_loss: 0.3546, step time: 1.5396\n",
      "150/388, train_loss: 0.2000, step time: 1.5508\n",
      "151/388, train_loss: 0.2431, step time: 1.5323\n",
      "152/388, train_loss: 0.2839, step time: 1.5606\n",
      "153/388, train_loss: 0.3374, step time: 1.5344\n",
      "154/388, train_loss: 0.1872, step time: 1.5345\n",
      "155/388, train_loss: 0.5841, step time: 1.5321\n",
      "156/388, train_loss: 0.4510, step time: 1.5527\n",
      "157/388, train_loss: 0.1675, step time: 1.5366\n",
      "158/388, train_loss: 0.0943, step time: 1.5341\n",
      "159/388, train_loss: 0.1136, step time: 1.5344\n",
      "160/388, train_loss: 0.1122, step time: 1.5515\n",
      "161/388, train_loss: 0.1204, step time: 1.5328\n",
      "162/388, train_loss: 0.4258, step time: 1.5346\n",
      "163/388, train_loss: 0.1795, step time: 1.5332\n",
      "164/388, train_loss: 0.4454, step time: 1.5453\n",
      "165/388, train_loss: 0.0560, step time: 1.5371\n",
      "166/388, train_loss: 0.1999, step time: 1.5383\n",
      "167/388, train_loss: 0.0946, step time: 1.5367\n",
      "168/388, train_loss: 0.0962, step time: 1.5389\n",
      "169/388, train_loss: 0.2939, step time: 1.5390\n",
      "170/388, train_loss: 0.6423, step time: 1.5332\n",
      "171/388, train_loss: 0.6475, step time: 1.5334\n",
      "172/388, train_loss: 0.2767, step time: 1.5324\n",
      "173/388, train_loss: 0.1268, step time: 1.5313\n",
      "174/388, train_loss: 0.2526, step time: 1.5335\n",
      "175/388, train_loss: 0.3070, step time: 1.5361\n",
      "176/388, train_loss: 0.2324, step time: 1.5379\n",
      "177/388, train_loss: 0.1379, step time: 1.5344\n",
      "178/388, train_loss: 0.1472, step time: 1.5300\n",
      "179/388, train_loss: 0.3514, step time: 1.5330\n",
      "180/388, train_loss: 0.3126, step time: 1.5318\n",
      "181/388, train_loss: 0.7661, step time: 1.5292\n",
      "182/388, train_loss: 0.1297, step time: 1.5336\n",
      "183/388, train_loss: 0.0671, step time: 1.5349\n",
      "184/388, train_loss: 0.1764, step time: 1.5383\n",
      "185/388, train_loss: 0.2031, step time: 1.5332\n",
      "186/388, train_loss: 0.1646, step time: 1.5329\n",
      "187/388, train_loss: 0.1549, step time: 1.5311\n",
      "188/388, train_loss: 0.0522, step time: 1.5324\n",
      "189/388, train_loss: 0.2279, step time: 1.5328\n",
      "190/388, train_loss: 0.1856, step time: 1.5380\n",
      "191/388, train_loss: 0.3004, step time: 1.5377\n",
      "192/388, train_loss: 0.2173, step time: 1.5326\n",
      "193/388, train_loss: 0.1157, step time: 1.5307\n",
      "194/388, train_loss: 0.1219, step time: 1.5559\n",
      "195/388, train_loss: 0.2778, step time: 1.5322\n",
      "196/388, train_loss: 0.3523, step time: 1.5446\n",
      "197/388, train_loss: 0.2112, step time: 1.5576\n",
      "198/388, train_loss: 0.2671, step time: 1.5359\n",
      "199/388, train_loss: 0.0582, step time: 1.5359\n",
      "200/388, train_loss: 0.1460, step time: 1.5343\n",
      "201/388, train_loss: 0.0831, step time: 1.5327\n",
      "202/388, train_loss: 0.2017, step time: 1.5314\n",
      "203/388, train_loss: 0.1001, step time: 1.5333\n",
      "204/388, train_loss: 0.0612, step time: 1.5337\n",
      "205/388, train_loss: 0.1761, step time: 1.5373\n",
      "206/388, train_loss: 0.2228, step time: 1.5385\n",
      "207/388, train_loss: 0.2038, step time: 1.5362\n",
      "208/388, train_loss: 0.2170, step time: 1.5328\n",
      "209/388, train_loss: 0.2589, step time: 1.5318\n",
      "210/388, train_loss: 0.3329, step time: 1.5323\n",
      "211/388, train_loss: 0.1846, step time: 1.5346\n",
      "212/388, train_loss: 0.3884, step time: 1.5354\n",
      "213/388, train_loss: 0.0774, step time: 1.5374\n",
      "214/388, train_loss: 0.1182, step time: 1.5545\n",
      "215/388, train_loss: 0.2264, step time: 1.5332\n",
      "216/388, train_loss: 0.0955, step time: 1.5329\n",
      "217/388, train_loss: 0.1388, step time: 1.5353\n",
      "218/388, train_loss: 0.3254, step time: 1.5381\n",
      "219/388, train_loss: 0.1201, step time: 1.5305\n",
      "220/388, train_loss: 0.1929, step time: 1.5311\n",
      "221/388, train_loss: 0.2162, step time: 1.5305\n",
      "222/388, train_loss: 0.1674, step time: 1.5305\n",
      "223/388, train_loss: 0.3843, step time: 1.5332\n",
      "224/388, train_loss: 0.3021, step time: 1.5389\n",
      "225/388, train_loss: 0.1380, step time: 1.5356\n",
      "226/388, train_loss: 0.1907, step time: 1.5312\n",
      "227/388, train_loss: 0.1256, step time: 1.5279\n",
      "228/388, train_loss: 0.1058, step time: 1.5323\n",
      "229/388, train_loss: 0.4575, step time: 1.5317\n",
      "230/388, train_loss: 0.1713, step time: 1.5364\n",
      "231/388, train_loss: 0.0727, step time: 1.5356\n",
      "232/388, train_loss: 0.2736, step time: 1.5385\n",
      "233/388, train_loss: 0.0682, step time: 1.5336\n",
      "234/388, train_loss: 0.2261, step time: 1.5389\n",
      "235/388, train_loss: 0.1949, step time: 1.5334\n",
      "236/388, train_loss: 0.4579, step time: 1.5315\n",
      "237/388, train_loss: 0.4536, step time: 1.5335\n",
      "238/388, train_loss: 0.0992, step time: 1.5325\n",
      "239/388, train_loss: 0.1190, step time: 1.5360\n",
      "240/388, train_loss: 0.2881, step time: 1.5442\n",
      "241/388, train_loss: 0.0701, step time: 1.5300\n",
      "242/388, train_loss: 0.1087, step time: 1.5468\n",
      "243/388, train_loss: 0.1498, step time: 1.5365\n",
      "244/388, train_loss: 0.2134, step time: 1.5322\n",
      "245/388, train_loss: 0.2692, step time: 1.5327\n",
      "246/388, train_loss: 0.0442, step time: 1.5345\n",
      "247/388, train_loss: 0.3432, step time: 1.5339\n",
      "248/388, train_loss: 0.4815, step time: 1.5350\n",
      "249/388, train_loss: 0.1935, step time: 1.5370\n",
      "250/388, train_loss: 0.1207, step time: 1.5340\n",
      "251/388, train_loss: 0.1711, step time: 1.5300\n",
      "252/388, train_loss: 0.0849, step time: 1.5308\n",
      "253/388, train_loss: 0.4576, step time: 1.5352\n",
      "254/388, train_loss: 0.0735, step time: 1.5330\n",
      "255/388, train_loss: 0.1503, step time: 1.5392\n",
      "256/388, train_loss: 0.2477, step time: 1.5380\n",
      "257/388, train_loss: 0.1401, step time: 1.5312\n",
      "258/388, train_loss: 0.0503, step time: 1.5362\n",
      "259/388, train_loss: 0.1929, step time: 1.5332\n",
      "260/388, train_loss: 0.5931, step time: 1.5317\n",
      "261/388, train_loss: 0.1525, step time: 1.5303\n",
      "262/388, train_loss: 0.1766, step time: 1.5330\n",
      "263/388, train_loss: 0.0633, step time: 1.5401\n",
      "264/388, train_loss: 0.2152, step time: 1.5354\n",
      "265/388, train_loss: 0.1686, step time: 1.5377\n",
      "266/388, train_loss: 0.2631, step time: 1.5396\n",
      "267/388, train_loss: 0.1945, step time: 1.5331\n",
      "268/388, train_loss: 0.0981, step time: 1.5308\n",
      "269/388, train_loss: 0.0725, step time: 1.5301\n",
      "270/388, train_loss: 0.0801, step time: 1.5331\n",
      "271/388, train_loss: 0.4135, step time: 1.5400\n",
      "272/388, train_loss: 0.1473, step time: 1.5344\n",
      "273/388, train_loss: 0.1939, step time: 1.5352\n",
      "274/388, train_loss: 0.1585, step time: 1.5319\n",
      "275/388, train_loss: 0.1565, step time: 1.5324\n",
      "276/388, train_loss: 0.0805, step time: 1.5348\n",
      "277/388, train_loss: 0.1300, step time: 1.5341\n",
      "278/388, train_loss: 0.2101, step time: 1.5344\n",
      "279/388, train_loss: 0.2208, step time: 1.5340\n",
      "280/388, train_loss: 0.6351, step time: 1.5309\n",
      "281/388, train_loss: 0.2066, step time: 1.5347\n",
      "282/388, train_loss: 0.1485, step time: 1.5324\n",
      "283/388, train_loss: 0.1124, step time: 1.5340\n",
      "284/388, train_loss: 0.2033, step time: 1.5620\n",
      "285/388, train_loss: 0.3988, step time: 1.5317\n",
      "286/388, train_loss: 0.1246, step time: 1.5319\n",
      "287/388, train_loss: 0.1840, step time: 1.5630\n",
      "288/388, train_loss: 0.1617, step time: 1.5322\n",
      "289/388, train_loss: 0.2473, step time: 1.5317\n",
      "290/388, train_loss: 0.0742, step time: 1.5323\n",
      "291/388, train_loss: 0.0326, step time: 1.5313\n",
      "292/388, train_loss: 0.1969, step time: 1.5361\n",
      "293/388, train_loss: 0.2928, step time: 1.5359\n",
      "294/388, train_loss: 0.5442, step time: 1.5339\n",
      "295/388, train_loss: 0.0838, step time: 1.5317\n",
      "296/388, train_loss: 0.0695, step time: 1.5326\n",
      "297/388, train_loss: 0.3500, step time: 1.5324\n",
      "298/388, train_loss: 0.1096, step time: 1.5287\n",
      "299/388, train_loss: 0.1215, step time: 1.5417\n",
      "300/388, train_loss: 0.1495, step time: 1.5424\n",
      "301/388, train_loss: 0.2231, step time: 1.5319\n",
      "302/388, train_loss: 0.4096, step time: 1.5299\n",
      "303/388, train_loss: 0.2934, step time: 1.5345\n",
      "304/388, train_loss: 0.1145, step time: 1.5322\n",
      "305/388, train_loss: 0.1318, step time: 1.5340\n",
      "306/388, train_loss: 0.1049, step time: 1.5341\n",
      "307/388, train_loss: 0.4962, step time: 1.5324\n",
      "308/388, train_loss: 0.1795, step time: 1.5329\n",
      "309/388, train_loss: 0.2440, step time: 1.5331\n",
      "310/388, train_loss: 0.4163, step time: 1.5339\n",
      "311/388, train_loss: 0.1642, step time: 1.5336\n",
      "312/388, train_loss: 0.2651, step time: 1.5339\n",
      "313/388, train_loss: 0.1447, step time: 1.5327\n",
      "314/388, train_loss: 0.3909, step time: 1.5341\n",
      "315/388, train_loss: 0.2427, step time: 1.5384\n",
      "316/388, train_loss: 0.2423, step time: 1.5307\n",
      "317/388, train_loss: 0.0934, step time: 1.5325\n",
      "318/388, train_loss: 0.1138, step time: 1.5440\n",
      "319/388, train_loss: 0.2228, step time: 1.5317\n",
      "320/388, train_loss: 0.0999, step time: 1.5268\n",
      "321/388, train_loss: 0.2145, step time: 1.5399\n",
      "322/388, train_loss: 0.2325, step time: 1.5341\n",
      "323/388, train_loss: 0.1705, step time: 1.5374\n",
      "324/388, train_loss: 0.2083, step time: 1.5333\n",
      "325/388, train_loss: 0.2905, step time: 1.5334\n",
      "326/388, train_loss: 0.1846, step time: 1.5323\n",
      "327/388, train_loss: 0.0982, step time: 1.5309\n",
      "328/388, train_loss: 0.2282, step time: 1.5321\n",
      "329/388, train_loss: 0.2776, step time: 1.5353\n",
      "330/388, train_loss: 0.1547, step time: 1.5349\n",
      "331/388, train_loss: 0.1000, step time: 1.5371\n",
      "332/388, train_loss: 0.3839, step time: 1.5321\n",
      "333/388, train_loss: 0.0939, step time: 1.5339\n",
      "334/388, train_loss: 0.0701, step time: 1.5321\n",
      "335/388, train_loss: 0.1220, step time: 1.5326\n",
      "336/388, train_loss: 0.1694, step time: 1.5334\n",
      "337/388, train_loss: 0.2780, step time: 1.5351\n",
      "338/388, train_loss: 0.2030, step time: 1.5340\n",
      "339/388, train_loss: 0.3286, step time: 1.5357\n",
      "340/388, train_loss: 0.1061, step time: 1.5354\n",
      "341/388, train_loss: 0.2483, step time: 1.5302\n",
      "342/388, train_loss: 0.2651, step time: 1.5300\n",
      "343/388, train_loss: 0.1324, step time: 1.5320\n",
      "344/388, train_loss: 0.0787, step time: 1.5294\n",
      "345/388, train_loss: 0.1740, step time: 1.5347\n",
      "346/388, train_loss: 0.5440, step time: 1.5347\n",
      "347/388, train_loss: 0.3072, step time: 1.5348\n",
      "348/388, train_loss: 0.0396, step time: 1.5355\n",
      "349/388, train_loss: 0.2084, step time: 1.5322\n",
      "350/388, train_loss: 0.2112, step time: 1.5355\n",
      "351/388, train_loss: 0.4216, step time: 1.5307\n",
      "352/388, train_loss: 0.1090, step time: 1.5373\n",
      "353/388, train_loss: 0.2040, step time: 1.5358\n",
      "354/388, train_loss: 0.0544, step time: 1.5335\n",
      "355/388, train_loss: 0.1487, step time: 1.5293\n",
      "356/388, train_loss: 0.0918, step time: 1.5292\n",
      "357/388, train_loss: 0.2192, step time: 1.5340\n",
      "358/388, train_loss: 0.1982, step time: 1.5352\n",
      "359/388, train_loss: 0.1281, step time: 1.5365\n",
      "360/388, train_loss: 0.1018, step time: 1.5348\n",
      "361/388, train_loss: 0.1082, step time: 1.5293\n",
      "362/388, train_loss: 0.2010, step time: 1.5321\n",
      "363/388, train_loss: 0.3440, step time: 1.5286\n",
      "364/388, train_loss: 0.1329, step time: 1.5324\n",
      "365/388, train_loss: 0.1226, step time: 1.5310\n",
      "366/388, train_loss: 0.1446, step time: 1.5326\n",
      "367/388, train_loss: 0.1332, step time: 1.5376\n",
      "368/388, train_loss: 0.2063, step time: 1.5340\n",
      "369/388, train_loss: 0.3658, step time: 1.5333\n",
      "370/388, train_loss: 0.0610, step time: 1.5314\n",
      "371/388, train_loss: 0.0522, step time: 1.5347\n",
      "372/388, train_loss: 0.2236, step time: 1.5320\n",
      "373/388, train_loss: 0.1039, step time: 1.5318\n",
      "374/388, train_loss: 0.0945, step time: 1.5295\n",
      "375/388, train_loss: 0.1004, step time: 1.5327\n",
      "376/388, train_loss: 0.2378, step time: 1.5307\n",
      "377/388, train_loss: 0.0925, step time: 1.5310\n",
      "378/388, train_loss: 0.1944, step time: 1.5342\n",
      "379/388, train_loss: 0.0854, step time: 1.5370\n",
      "380/388, train_loss: 0.4173, step time: 1.5338\n",
      "381/388, train_loss: 0.1445, step time: 1.5358\n",
      "382/388, train_loss: 0.1041, step time: 1.5293\n",
      "383/388, train_loss: 0.1252, step time: 1.5320\n",
      "384/388, train_loss: 0.3912, step time: 1.5603\n",
      "385/388, train_loss: 0.0988, step time: 1.5321\n",
      "386/388, train_loss: 0.2176, step time: 1.5323\n",
      "387/388, train_loss: 0.2401, step time: 1.5374\n",
      "388/388, train_loss: 0.2023, step time: 1.5376\n",
      "epoch 33 average loss: 0.2068\n",
      "saved new best metric model\n",
      "current epoch: 33 current mean dice: 0.7590 tc: 0.8086 wt: 0.9003 et: 0.5682\n",
      "best mean dice: 0.7590 at epoch: 33\n",
      "time consuming of epoch 33 is: 704.9446\n",
      "----------\n",
      "epoch 34/100\n",
      "1/388, train_loss: 0.1038, step time: 1.5437\n",
      "2/388, train_loss: 0.1835, step time: 1.5373\n",
      "3/388, train_loss: 0.3048, step time: 1.5378\n",
      "4/388, train_loss: 0.1397, step time: 1.5406\n",
      "5/388, train_loss: 0.0991, step time: 1.5348\n",
      "6/388, train_loss: 0.3543, step time: 1.5376\n",
      "7/388, train_loss: 0.1579, step time: 1.5379\n",
      "8/388, train_loss: 0.0642, step time: 1.5357\n",
      "9/388, train_loss: 0.0533, step time: 1.5362\n",
      "10/388, train_loss: 0.0541, step time: 1.5333\n",
      "11/388, train_loss: 0.2091, step time: 1.5358\n",
      "12/388, train_loss: 0.2684, step time: 1.5348\n",
      "13/388, train_loss: 0.1183, step time: 1.5446\n",
      "14/388, train_loss: 0.1626, step time: 1.5355\n",
      "15/388, train_loss: 0.2853, step time: 1.5344\n",
      "16/388, train_loss: 0.1103, step time: 1.5356\n",
      "17/388, train_loss: 0.1352, step time: 1.5335\n",
      "18/388, train_loss: 0.1793, step time: 1.5342\n",
      "19/388, train_loss: 0.0796, step time: 1.5313\n",
      "20/388, train_loss: 0.0828, step time: 1.5323\n",
      "21/388, train_loss: 0.1402, step time: 1.5315\n",
      "22/388, train_loss: 0.2823, step time: 1.5313\n",
      "23/388, train_loss: 0.4599, step time: 1.5342\n",
      "24/388, train_loss: 0.2835, step time: 1.5371\n",
      "25/388, train_loss: 0.2153, step time: 1.5342\n",
      "26/388, train_loss: 0.2701, step time: 1.5339\n",
      "27/388, train_loss: 0.1244, step time: 1.5357\n",
      "28/388, train_loss: 0.1001, step time: 1.5382\n",
      "29/388, train_loss: 0.1483, step time: 1.5346\n",
      "30/388, train_loss: 0.4121, step time: 1.5342\n",
      "31/388, train_loss: 0.1322, step time: 1.5336\n",
      "32/388, train_loss: 0.2157, step time: 1.5324\n",
      "33/388, train_loss: 0.1710, step time: 1.5329\n",
      "34/388, train_loss: 0.3003, step time: 1.5634\n",
      "35/388, train_loss: 0.0943, step time: 1.5341\n",
      "36/388, train_loss: 0.1356, step time: 1.5301\n",
      "37/388, train_loss: 0.2104, step time: 1.5300\n",
      "38/388, train_loss: 0.1627, step time: 1.5461\n",
      "39/388, train_loss: 0.1002, step time: 1.5395\n",
      "40/388, train_loss: 0.2602, step time: 1.5327\n",
      "41/388, train_loss: 0.2431, step time: 1.5307\n",
      "42/388, train_loss: 0.2299, step time: 1.5335\n",
      "43/388, train_loss: 0.2226, step time: 1.5326\n",
      "44/388, train_loss: 0.2293, step time: 1.5395\n",
      "45/388, train_loss: 0.2585, step time: 1.5303\n",
      "46/388, train_loss: 0.1219, step time: 1.5328\n",
      "47/388, train_loss: 0.1677, step time: 1.5458\n",
      "48/388, train_loss: 0.0878, step time: 1.5368\n",
      "49/388, train_loss: 0.1468, step time: 1.5440\n",
      "50/388, train_loss: 0.1303, step time: 1.5324\n",
      "51/388, train_loss: 0.5553, step time: 1.5323\n",
      "52/388, train_loss: 0.1319, step time: 1.5463\n",
      "53/388, train_loss: 0.1377, step time: 1.5433\n",
      "54/388, train_loss: 0.2694, step time: 1.5345\n",
      "55/388, train_loss: 0.1489, step time: 1.5334\n",
      "56/388, train_loss: 0.1722, step time: 1.5327\n",
      "57/388, train_loss: 0.1261, step time: 1.5342\n",
      "58/388, train_loss: 0.4169, step time: 1.5349\n",
      "59/388, train_loss: 0.4154, step time: 1.5327\n",
      "60/388, train_loss: 0.1175, step time: 1.5355\n",
      "61/388, train_loss: 0.2119, step time: 1.5342\n",
      "62/388, train_loss: 0.1058, step time: 1.5337\n",
      "63/388, train_loss: 0.2118, step time: 1.5569\n",
      "64/388, train_loss: 0.1946, step time: 1.5329\n",
      "65/388, train_loss: 0.1994, step time: 1.5324\n",
      "66/388, train_loss: 0.0921, step time: 1.5346\n",
      "67/388, train_loss: 0.2085, step time: 1.5320\n",
      "68/388, train_loss: 0.2515, step time: 1.5555\n",
      "69/388, train_loss: 0.1129, step time: 1.5331\n",
      "70/388, train_loss: 0.1857, step time: 1.5340\n",
      "71/388, train_loss: 0.1467, step time: 1.5309\n",
      "72/388, train_loss: 0.0987, step time: 1.5326\n",
      "73/388, train_loss: 0.0698, step time: 1.5351\n",
      "74/388, train_loss: 0.2154, step time: 1.5360\n",
      "75/388, train_loss: 0.0770, step time: 1.5332\n",
      "76/388, train_loss: 0.0835, step time: 1.5342\n",
      "77/388, train_loss: 0.1654, step time: 1.5331\n",
      "78/388, train_loss: 0.1359, step time: 1.5355\n",
      "79/388, train_loss: 0.0990, step time: 1.5322\n",
      "80/388, train_loss: 0.2718, step time: 1.5359\n",
      "81/388, train_loss: 0.1182, step time: 1.5374\n",
      "82/388, train_loss: 0.4702, step time: 1.5363\n",
      "83/388, train_loss: 0.2882, step time: 1.5324\n",
      "84/388, train_loss: 0.0372, step time: 1.5343\n",
      "85/388, train_loss: 0.1662, step time: 1.5355\n",
      "86/388, train_loss: 0.1922, step time: 1.5364\n",
      "87/388, train_loss: 0.1258, step time: 1.5356\n",
      "88/388, train_loss: 0.1510, step time: 1.5355\n",
      "89/388, train_loss: 0.3292, step time: 1.5492\n",
      "90/388, train_loss: 0.2538, step time: 1.5340\n",
      "91/388, train_loss: 0.1152, step time: 1.5362\n",
      "92/388, train_loss: 0.1896, step time: 1.5342\n",
      "93/388, train_loss: 0.2527, step time: 1.5324\n",
      "94/388, train_loss: 0.3345, step time: 1.5280\n",
      "95/388, train_loss: 0.3734, step time: 1.5282\n",
      "96/388, train_loss: 0.2600, step time: 1.5317\n",
      "97/388, train_loss: 0.2291, step time: 1.5370\n",
      "98/388, train_loss: 0.2609, step time: 1.5378\n",
      "99/388, train_loss: 0.3157, step time: 1.5334\n",
      "100/388, train_loss: 0.1471, step time: 1.5339\n",
      "101/388, train_loss: 0.2175, step time: 1.5678\n",
      "102/388, train_loss: 0.1503, step time: 1.5326\n",
      "103/388, train_loss: 0.1680, step time: 1.5385\n",
      "104/388, train_loss: 0.1113, step time: 1.5347\n",
      "105/388, train_loss: 0.3251, step time: 1.5336\n",
      "106/388, train_loss: 0.2109, step time: 1.5345\n",
      "107/388, train_loss: 0.1687, step time: 1.5332\n",
      "108/388, train_loss: 0.0990, step time: 1.5289\n",
      "109/388, train_loss: 0.0704, step time: 1.5311\n",
      "110/388, train_loss: 0.0919, step time: 1.5373\n",
      "111/388, train_loss: 0.1976, step time: 1.5337\n",
      "112/388, train_loss: 0.1666, step time: 1.5350\n",
      "113/388, train_loss: 0.0814, step time: 1.5330\n",
      "114/388, train_loss: 0.0772, step time: 1.5296\n",
      "115/388, train_loss: 0.0899, step time: 1.5325\n",
      "116/388, train_loss: 0.2144, step time: 1.5351\n",
      "117/388, train_loss: 0.1266, step time: 1.5335\n",
      "118/388, train_loss: 0.2780, step time: 1.5334\n",
      "119/388, train_loss: 0.3361, step time: 1.5322\n",
      "120/388, train_loss: 0.1886, step time: 1.5348\n",
      "121/388, train_loss: 0.2975, step time: 1.5292\n",
      "122/388, train_loss: 0.2130, step time: 1.5318\n",
      "123/388, train_loss: 0.1938, step time: 1.5325\n",
      "124/388, train_loss: 0.0709, step time: 1.5341\n",
      "125/388, train_loss: 0.3520, step time: 1.5393\n",
      "126/388, train_loss: 0.1464, step time: 1.5351\n",
      "127/388, train_loss: 0.0433, step time: 1.5362\n",
      "128/388, train_loss: 0.3686, step time: 1.5345\n",
      "129/388, train_loss: 0.1544, step time: 1.5275\n",
      "130/388, train_loss: 0.2190, step time: 1.5405\n",
      "131/388, train_loss: 0.2036, step time: 1.5438\n",
      "132/388, train_loss: 0.1382, step time: 1.5331\n",
      "133/388, train_loss: 0.2909, step time: 1.5328\n",
      "134/388, train_loss: 0.2659, step time: 1.5350\n",
      "135/388, train_loss: 0.1820, step time: 1.5347\n",
      "136/388, train_loss: 0.2646, step time: 1.5434\n",
      "137/388, train_loss: 0.0848, step time: 1.5376\n",
      "138/388, train_loss: 0.4439, step time: 1.5400\n",
      "139/388, train_loss: 0.2652, step time: 1.5346\n",
      "140/388, train_loss: 0.0754, step time: 1.5368\n",
      "141/388, train_loss: 0.3438, step time: 1.5596\n",
      "142/388, train_loss: 0.1805, step time: 1.5384\n",
      "143/388, train_loss: 0.1233, step time: 1.5345\n",
      "144/388, train_loss: 0.1820, step time: 1.5371\n",
      "145/388, train_loss: 0.0282, step time: 1.5301\n",
      "146/388, train_loss: 0.4695, step time: 1.5307\n",
      "147/388, train_loss: 0.0748, step time: 1.5304\n",
      "148/388, train_loss: 0.0751, step time: 1.5354\n",
      "149/388, train_loss: 0.1143, step time: 1.5379\n",
      "150/388, train_loss: 0.0979, step time: 1.5431\n",
      "151/388, train_loss: 0.3549, step time: 1.5329\n",
      "152/388, train_loss: 0.0519, step time: 1.5317\n",
      "153/388, train_loss: 0.6221, step time: 1.5372\n",
      "154/388, train_loss: 0.3137, step time: 1.5374\n",
      "155/388, train_loss: 0.2255, step time: 1.5332\n",
      "156/388, train_loss: 0.1911, step time: 1.5347\n",
      "157/388, train_loss: 0.0869, step time: 1.5361\n",
      "158/388, train_loss: 0.1050, step time: 1.5324\n",
      "159/388, train_loss: 0.1628, step time: 1.5516\n",
      "160/388, train_loss: 0.0447, step time: 1.5390\n",
      "161/388, train_loss: 0.2746, step time: 1.5343\n",
      "162/388, train_loss: 0.5538, step time: 1.5425\n",
      "163/388, train_loss: 0.0903, step time: 1.5439\n",
      "164/388, train_loss: 0.1281, step time: 1.5432\n",
      "165/388, train_loss: 0.1473, step time: 1.5348\n",
      "166/388, train_loss: 0.1014, step time: 1.5381\n",
      "167/388, train_loss: 0.1299, step time: 1.5432\n",
      "168/388, train_loss: 0.1100, step time: 1.5407\n",
      "169/388, train_loss: 0.2619, step time: 1.5320\n",
      "170/388, train_loss: 0.0906, step time: 1.5391\n",
      "171/388, train_loss: 0.1504, step time: 1.5432\n",
      "172/388, train_loss: 0.2690, step time: 1.5424\n",
      "173/388, train_loss: 0.3221, step time: 1.5349\n",
      "174/388, train_loss: 0.1739, step time: 1.5395\n",
      "175/388, train_loss: 0.1051, step time: 1.5370\n",
      "176/388, train_loss: 0.1405, step time: 1.5374\n",
      "177/388, train_loss: 0.1236, step time: 1.5362\n",
      "178/388, train_loss: 0.1236, step time: 1.5380\n",
      "179/388, train_loss: 0.1776, step time: 1.5367\n",
      "180/388, train_loss: 0.2971, step time: 1.5379\n",
      "181/388, train_loss: 0.1729, step time: 1.5378\n",
      "182/388, train_loss: 0.1238, step time: 1.5328\n",
      "183/388, train_loss: 0.2125, step time: 1.5335\n",
      "184/388, train_loss: 0.0816, step time: 1.5367\n",
      "185/388, train_loss: 0.2355, step time: 1.5418\n",
      "186/388, train_loss: 0.0991, step time: 1.5365\n",
      "187/388, train_loss: 0.1086, step time: 1.5363\n",
      "188/388, train_loss: 0.1694, step time: 1.5336\n",
      "189/388, train_loss: 0.1958, step time: 1.5355\n",
      "190/388, train_loss: 0.4763, step time: 1.5400\n",
      "191/388, train_loss: 0.4370, step time: 1.5394\n",
      "192/388, train_loss: 0.0805, step time: 1.5440\n",
      "193/388, train_loss: 0.2214, step time: 1.5364\n",
      "194/388, train_loss: 0.1284, step time: 1.5364\n",
      "195/388, train_loss: 0.1632, step time: 1.5346\n",
      "196/388, train_loss: 0.1110, step time: 1.5559\n",
      "197/388, train_loss: 0.1158, step time: 1.5376\n",
      "198/388, train_loss: 0.1821, step time: 1.5299\n",
      "199/388, train_loss: 0.1781, step time: 1.5353\n",
      "200/388, train_loss: 0.2899, step time: 1.5433\n",
      "201/388, train_loss: 0.2239, step time: 1.5361\n",
      "202/388, train_loss: 0.2899, step time: 1.5328\n",
      "203/388, train_loss: 0.0968, step time: 1.5330\n",
      "204/388, train_loss: 0.4834, step time: 1.5492\n",
      "205/388, train_loss: 0.0562, step time: 1.5359\n",
      "206/388, train_loss: 0.1123, step time: 1.5339\n",
      "207/388, train_loss: 0.1318, step time: 1.5379\n",
      "208/388, train_loss: 0.4848, step time: 1.5462\n",
      "209/388, train_loss: 0.0820, step time: 1.5484\n",
      "210/388, train_loss: 0.1741, step time: 1.5358\n",
      "211/388, train_loss: 0.1065, step time: 1.5328\n",
      "212/388, train_loss: 0.0640, step time: 1.5462\n",
      "213/388, train_loss: 0.3169, step time: 1.5339\n",
      "214/388, train_loss: 0.3850, step time: 1.5549\n",
      "215/388, train_loss: 0.2390, step time: 1.5346\n",
      "216/388, train_loss: 0.0422, step time: 1.5468\n",
      "217/388, train_loss: 0.0925, step time: 1.5473\n",
      "218/388, train_loss: 0.1499, step time: 1.5360\n",
      "219/388, train_loss: 0.2588, step time: 1.5390\n",
      "220/388, train_loss: 0.4732, step time: 1.5495\n",
      "221/388, train_loss: 0.3155, step time: 1.5344\n",
      "222/388, train_loss: 0.1588, step time: 1.5325\n",
      "223/388, train_loss: 0.1040, step time: 1.5341\n",
      "224/388, train_loss: 0.3963, step time: 1.5500\n",
      "225/388, train_loss: 0.2028, step time: 1.5343\n",
      "226/388, train_loss: 0.1273, step time: 1.5335\n",
      "227/388, train_loss: 0.3884, step time: 1.5434\n",
      "228/388, train_loss: 0.1348, step time: 1.5483\n",
      "229/388, train_loss: 0.6088, step time: 1.5346\n",
      "230/388, train_loss: 0.3769, step time: 1.5391\n",
      "231/388, train_loss: 0.0891, step time: 1.5338\n",
      "232/388, train_loss: 0.4431, step time: 1.5485\n",
      "233/388, train_loss: 0.1775, step time: 1.5323\n",
      "234/388, train_loss: 0.1242, step time: 1.5358\n",
      "235/388, train_loss: 0.1466, step time: 1.5374\n",
      "236/388, train_loss: 0.1780, step time: 1.5459\n",
      "237/388, train_loss: 0.1988, step time: 1.5376\n",
      "238/388, train_loss: 0.2103, step time: 1.5362\n",
      "239/388, train_loss: 0.2726, step time: 1.5385\n",
      "240/388, train_loss: 0.2401, step time: 1.5501\n",
      "241/388, train_loss: 0.0732, step time: 1.5320\n",
      "242/388, train_loss: 0.2195, step time: 1.5343\n",
      "243/388, train_loss: 0.1384, step time: 1.5360\n",
      "244/388, train_loss: 0.2153, step time: 1.5549\n",
      "245/388, train_loss: 0.1834, step time: 1.5425\n",
      "246/388, train_loss: 0.1984, step time: 1.5314\n",
      "247/388, train_loss: 0.2542, step time: 1.5321\n",
      "248/388, train_loss: 0.4564, step time: 1.5510\n",
      "249/388, train_loss: 0.1231, step time: 1.5394\n",
      "250/388, train_loss: 0.1163, step time: 1.5328\n",
      "251/388, train_loss: 0.2998, step time: 1.5320\n",
      "252/388, train_loss: 0.2187, step time: 1.5437\n",
      "253/388, train_loss: 0.1864, step time: 1.5425\n",
      "254/388, train_loss: 0.3479, step time: 1.5294\n",
      "255/388, train_loss: 0.1693, step time: 1.5333\n",
      "256/388, train_loss: 0.0749, step time: 1.5450\n",
      "257/388, train_loss: 0.5239, step time: 1.5379\n",
      "258/388, train_loss: 0.4485, step time: 1.5382\n",
      "259/388, train_loss: 0.1897, step time: 1.5341\n",
      "260/388, train_loss: 0.4927, step time: 1.5449\n",
      "261/388, train_loss: 0.0753, step time: 1.5408\n",
      "262/388, train_loss: 0.0980, step time: 1.5400\n",
      "263/388, train_loss: 0.4004, step time: 1.5334\n",
      "264/388, train_loss: 0.2578, step time: 1.5435\n",
      "265/388, train_loss: 0.1580, step time: 1.5333\n",
      "266/388, train_loss: 0.3715, step time: 1.5382\n",
      "267/388, train_loss: 0.2198, step time: 1.5476\n",
      "268/388, train_loss: 0.3654, step time: 1.5473\n",
      "269/388, train_loss: 0.4436, step time: 1.5352\n",
      "270/388, train_loss: 0.1993, step time: 1.5363\n",
      "271/388, train_loss: 0.5097, step time: 1.5318\n",
      "272/388, train_loss: 0.0637, step time: 1.5458\n",
      "273/388, train_loss: 0.1895, step time: 1.5360\n",
      "274/388, train_loss: 0.3686, step time: 1.5343\n",
      "275/388, train_loss: 0.1014, step time: 1.5317\n",
      "276/388, train_loss: 0.2104, step time: 1.5489\n",
      "277/388, train_loss: 0.1954, step time: 1.5389\n",
      "278/388, train_loss: 0.1206, step time: 1.5340\n",
      "279/388, train_loss: 0.1742, step time: 1.5315\n",
      "280/388, train_loss: 0.1799, step time: 1.5490\n",
      "281/388, train_loss: 0.3016, step time: 1.5411\n",
      "282/388, train_loss: 0.1316, step time: 1.5361\n",
      "283/388, train_loss: 0.2019, step time: 1.5336\n",
      "284/388, train_loss: 0.2429, step time: 1.5450\n",
      "285/388, train_loss: 0.1712, step time: 1.5366\n",
      "286/388, train_loss: 0.5112, step time: 1.5332\n",
      "287/388, train_loss: 0.5282, step time: 1.5364\n",
      "288/388, train_loss: 0.2306, step time: 1.5469\n",
      "289/388, train_loss: 0.2725, step time: 1.5509\n",
      "290/388, train_loss: 0.2476, step time: 1.5336\n",
      "291/388, train_loss: 0.1715, step time: 1.5411\n",
      "292/388, train_loss: 0.2917, step time: 1.5505\n",
      "293/388, train_loss: 0.1895, step time: 1.5321\n",
      "294/388, train_loss: 0.2010, step time: 1.5334\n",
      "295/388, train_loss: 0.2173, step time: 1.5331\n",
      "296/388, train_loss: 0.3087, step time: 1.5483\n",
      "297/388, train_loss: 0.1442, step time: 1.5485\n",
      "298/388, train_loss: 0.0942, step time: 1.5339\n",
      "299/388, train_loss: 0.2125, step time: 1.5319\n",
      "300/388, train_loss: 0.1056, step time: 1.5508\n",
      "301/388, train_loss: 0.2153, step time: 1.5329\n",
      "302/388, train_loss: 0.1279, step time: 1.5445\n",
      "303/388, train_loss: 0.2728, step time: 1.5331\n",
      "304/388, train_loss: 0.2614, step time: 1.5452\n",
      "305/388, train_loss: 0.2867, step time: 1.5359\n",
      "306/388, train_loss: 0.2437, step time: 1.5420\n",
      "307/388, train_loss: 0.1159, step time: 1.5344\n",
      "308/388, train_loss: 0.2911, step time: 1.5451\n",
      "309/388, train_loss: 0.3411, step time: 1.5382\n",
      "310/388, train_loss: 0.2887, step time: 1.5380\n",
      "311/388, train_loss: 0.0608, step time: 1.5339\n",
      "312/388, train_loss: 0.0578, step time: 1.5421\n",
      "313/388, train_loss: 0.4375, step time: 1.5448\n",
      "314/388, train_loss: 0.1168, step time: 1.5604\n",
      "315/388, train_loss: 0.0678, step time: 1.5457\n",
      "316/388, train_loss: 0.3586, step time: 1.5466\n",
      "317/388, train_loss: 0.4533, step time: 1.5323\n",
      "318/388, train_loss: 0.0423, step time: 1.5367\n",
      "319/388, train_loss: 0.1294, step time: 1.5353\n",
      "320/388, train_loss: 0.1806, step time: 1.5480\n",
      "321/388, train_loss: 0.1664, step time: 1.5319\n",
      "322/388, train_loss: 0.1483, step time: 1.5374\n",
      "323/388, train_loss: 0.1325, step time: 1.5374\n",
      "324/388, train_loss: 0.1226, step time: 1.5450\n",
      "325/388, train_loss: 0.4154, step time: 1.5320\n",
      "326/388, train_loss: 0.2771, step time: 1.5316\n",
      "327/388, train_loss: 0.2700, step time: 1.5384\n",
      "328/388, train_loss: 0.1544, step time: 1.5441\n",
      "329/388, train_loss: 0.1227, step time: 1.5357\n",
      "330/388, train_loss: 0.2859, step time: 1.5433\n",
      "331/388, train_loss: 0.1427, step time: 1.5362\n",
      "332/388, train_loss: 0.0699, step time: 1.5501\n",
      "333/388, train_loss: 0.2926, step time: 1.5363\n",
      "334/388, train_loss: 0.1409, step time: 1.5367\n",
      "335/388, train_loss: 0.1837, step time: 1.5389\n",
      "336/388, train_loss: 0.2717, step time: 1.5488\n",
      "337/388, train_loss: 0.1862, step time: 1.5319\n",
      "338/388, train_loss: 0.0954, step time: 1.5332\n",
      "339/388, train_loss: 0.0896, step time: 1.5421\n",
      "340/388, train_loss: 0.2413, step time: 1.5494\n",
      "341/388, train_loss: 0.1266, step time: 1.5380\n",
      "342/388, train_loss: 0.1225, step time: 1.5353\n",
      "343/388, train_loss: 0.6645, step time: 1.5404\n",
      "344/388, train_loss: 0.0823, step time: 1.5478\n",
      "345/388, train_loss: 0.1107, step time: 1.5350\n",
      "346/388, train_loss: 0.0840, step time: 1.5351\n",
      "347/388, train_loss: 0.0935, step time: 1.5368\n",
      "348/388, train_loss: 0.3010, step time: 1.5468\n",
      "349/388, train_loss: 0.1915, step time: 1.5347\n",
      "350/388, train_loss: 0.5631, step time: 1.5356\n",
      "351/388, train_loss: 0.1190, step time: 1.5354\n",
      "352/388, train_loss: 0.1261, step time: 1.5528\n",
      "353/388, train_loss: 0.1311, step time: 1.5337\n",
      "354/388, train_loss: 0.1489, step time: 1.5335\n",
      "355/388, train_loss: 0.0803, step time: 1.5474\n",
      "356/388, train_loss: 0.2618, step time: 1.5458\n",
      "357/388, train_loss: 0.2625, step time: 1.5372\n",
      "358/388, train_loss: 0.0997, step time: 1.5339\n",
      "359/388, train_loss: 0.1449, step time: 1.5374\n",
      "360/388, train_loss: 0.2073, step time: 1.5498\n",
      "361/388, train_loss: 0.1275, step time: 1.5364\n",
      "362/388, train_loss: 0.1518, step time: 1.5353\n",
      "363/388, train_loss: 0.0839, step time: 1.5410\n",
      "364/388, train_loss: 0.3613, step time: 1.5436\n",
      "365/388, train_loss: 0.1459, step time: 1.5375\n",
      "366/388, train_loss: 0.3008, step time: 1.5322\n",
      "367/388, train_loss: 0.1656, step time: 1.5371\n",
      "368/388, train_loss: 0.0971, step time: 1.5611\n",
      "369/388, train_loss: 0.2896, step time: 1.5317\n",
      "370/388, train_loss: 0.1743, step time: 1.5378\n",
      "371/388, train_loss: 0.0690, step time: 1.5651\n",
      "372/388, train_loss: 0.1503, step time: 1.5427\n",
      "373/388, train_loss: 0.0842, step time: 1.5382\n",
      "374/388, train_loss: 0.1420, step time: 1.5360\n",
      "375/388, train_loss: 0.1616, step time: 1.5335\n",
      "376/388, train_loss: 0.1595, step time: 1.5445\n",
      "377/388, train_loss: 0.1289, step time: 1.5344\n",
      "378/388, train_loss: 0.4389, step time: 1.5395\n",
      "379/388, train_loss: 0.0823, step time: 1.5380\n",
      "380/388, train_loss: 0.2712, step time: 1.5451\n",
      "381/388, train_loss: 0.1049, step time: 1.5309\n",
      "382/388, train_loss: 0.1672, step time: 1.5346\n",
      "383/388, train_loss: 0.2155, step time: 1.5388\n",
      "384/388, train_loss: 0.1422, step time: 1.5465\n",
      "385/388, train_loss: 0.2588, step time: 1.5587\n",
      "386/388, train_loss: 0.1633, step time: 1.5431\n",
      "387/388, train_loss: 0.0584, step time: 1.5344\n",
      "388/388, train_loss: 0.2039, step time: 1.5465\n",
      "epoch 34 average loss: 0.2037\n",
      "current epoch: 34 current mean dice: 0.7582 tc: 0.8070 wt: 0.8969 et: 0.5705\n",
      "best mean dice: 0.7590 at epoch: 33\n",
      "time consuming of epoch 34 is: 705.1435\n",
      "----------\n",
      "epoch 35/100\n",
      "1/388, train_loss: 0.1960, step time: 1.5500\n",
      "2/388, train_loss: 0.5826, step time: 1.5346\n",
      "3/388, train_loss: 0.2629, step time: 1.5378\n",
      "4/388, train_loss: 0.1767, step time: 1.5390\n",
      "5/388, train_loss: 0.2017, step time: 1.5341\n",
      "6/388, train_loss: 0.2800, step time: 1.5364\n",
      "7/388, train_loss: 0.3120, step time: 1.5386\n",
      "8/388, train_loss: 0.1409, step time: 1.5367\n",
      "9/388, train_loss: 0.1203, step time: 1.5368\n",
      "10/388, train_loss: 0.1356, step time: 1.5408\n",
      "11/388, train_loss: 0.0492, step time: 1.5360\n",
      "12/388, train_loss: 0.0746, step time: 1.5348\n",
      "13/388, train_loss: 0.2205, step time: 1.5350\n",
      "14/388, train_loss: 0.6488, step time: 1.5355\n",
      "15/388, train_loss: 0.1228, step time: 1.5449\n",
      "16/388, train_loss: 0.1569, step time: 1.5400\n",
      "17/388, train_loss: 0.3126, step time: 1.5293\n",
      "18/388, train_loss: 0.1871, step time: 1.5403\n",
      "19/388, train_loss: 0.2396, step time: 1.5448\n",
      "20/388, train_loss: 0.3827, step time: 1.5370\n",
      "21/388, train_loss: 0.3929, step time: 1.5399\n",
      "22/388, train_loss: 0.0947, step time: 1.5405\n",
      "23/388, train_loss: 0.1124, step time: 1.5402\n",
      "24/388, train_loss: 0.1259, step time: 1.5345\n",
      "25/388, train_loss: 0.1099, step time: 1.5313\n",
      "26/388, train_loss: 0.2231, step time: 1.5369\n",
      "27/388, train_loss: 0.2289, step time: 1.5330\n",
      "28/388, train_loss: 0.1269, step time: 1.5366\n",
      "29/388, train_loss: 0.2466, step time: 1.5399\n",
      "30/388, train_loss: 0.1497, step time: 1.5520\n",
      "31/388, train_loss: 0.3077, step time: 1.5448\n",
      "32/388, train_loss: 0.2054, step time: 1.5368\n",
      "33/388, train_loss: 0.2113, step time: 1.5401\n",
      "34/388, train_loss: 0.3660, step time: 1.5381\n",
      "35/388, train_loss: 0.3316, step time: 1.5325\n",
      "36/388, train_loss: 0.4960, step time: 1.5316\n",
      "37/388, train_loss: 0.0835, step time: 1.5390\n",
      "38/388, train_loss: 0.2882, step time: 1.5435\n",
      "39/388, train_loss: 0.1908, step time: 1.5358\n",
      "40/388, train_loss: 0.1495, step time: 1.5334\n",
      "41/388, train_loss: 0.1047, step time: 1.5364\n",
      "42/388, train_loss: 0.0715, step time: 1.5382\n",
      "43/388, train_loss: 0.1636, step time: 1.5390\n",
      "44/388, train_loss: 0.2162, step time: 1.5407\n",
      "45/388, train_loss: 0.2902, step time: 1.5354\n",
      "46/388, train_loss: 0.2320, step time: 1.5421\n",
      "47/388, train_loss: 0.1863, step time: 1.5420\n",
      "48/388, train_loss: 0.0637, step time: 1.5300\n",
      "49/388, train_loss: 0.0700, step time: 1.5380\n",
      "50/388, train_loss: 0.4045, step time: 1.5346\n",
      "51/388, train_loss: 0.1854, step time: 1.5359\n",
      "52/388, train_loss: 0.1133, step time: 1.5398\n",
      "53/388, train_loss: 0.1603, step time: 1.5430\n",
      "54/388, train_loss: 0.4352, step time: 1.5387\n",
      "55/388, train_loss: 0.1396, step time: 1.5335\n",
      "56/388, train_loss: 0.2057, step time: 1.5318\n",
      "57/388, train_loss: 0.1093, step time: 1.5418\n",
      "58/388, train_loss: 0.1612, step time: 1.5410\n",
      "59/388, train_loss: 0.1467, step time: 1.5333\n",
      "60/388, train_loss: 0.1549, step time: 1.5396\n",
      "61/388, train_loss: 0.2947, step time: 1.5336\n",
      "62/388, train_loss: 0.2980, step time: 1.5391\n",
      "63/388, train_loss: 0.1661, step time: 1.5466\n",
      "64/388, train_loss: 0.1978, step time: 1.5347\n",
      "65/388, train_loss: 0.2276, step time: 1.5333\n",
      "66/388, train_loss: 0.2425, step time: 1.5322\n",
      "67/388, train_loss: 0.3076, step time: 1.5338\n",
      "68/388, train_loss: 0.0890, step time: 1.5418\n",
      "69/388, train_loss: 0.0941, step time: 1.5431\n",
      "70/388, train_loss: 0.0514, step time: 1.5351\n",
      "71/388, train_loss: 0.1878, step time: 1.5351\n",
      "72/388, train_loss: 0.1956, step time: 1.5358\n",
      "73/388, train_loss: 0.1505, step time: 1.5422\n",
      "74/388, train_loss: 0.2561, step time: 1.5336\n",
      "75/388, train_loss: 0.0471, step time: 1.5348\n",
      "76/388, train_loss: 0.0938, step time: 1.5420\n",
      "77/388, train_loss: 0.2682, step time: 1.5373\n",
      "78/388, train_loss: 0.3846, step time: 1.5427\n",
      "79/388, train_loss: 0.2002, step time: 1.5367\n",
      "80/388, train_loss: 0.0500, step time: 1.5383\n",
      "81/388, train_loss: 0.1097, step time: 1.5315\n",
      "82/388, train_loss: 0.1146, step time: 1.5383\n",
      "83/388, train_loss: 0.4223, step time: 1.5407\n",
      "84/388, train_loss: 0.1417, step time: 1.5341\n",
      "85/388, train_loss: 0.2176, step time: 1.5372\n",
      "86/388, train_loss: 0.1035, step time: 1.5410\n",
      "87/388, train_loss: 0.3281, step time: 1.5342\n",
      "88/388, train_loss: 0.1356, step time: 1.5372\n",
      "89/388, train_loss: 0.2323, step time: 1.5357\n",
      "90/388, train_loss: 0.1534, step time: 1.5443\n",
      "91/388, train_loss: 0.0568, step time: 1.5338\n",
      "92/388, train_loss: 0.1687, step time: 1.5393\n",
      "93/388, train_loss: 0.0907, step time: 1.5375\n",
      "94/388, train_loss: 0.1091, step time: 1.5433\n",
      "95/388, train_loss: 0.2207, step time: 1.5344\n",
      "96/388, train_loss: 0.3310, step time: 1.5320\n",
      "97/388, train_loss: 0.1685, step time: 1.5398\n",
      "98/388, train_loss: 0.0955, step time: 1.5346\n",
      "99/388, train_loss: 0.1506, step time: 1.5418\n",
      "100/388, train_loss: 0.1406, step time: 1.5345\n",
      "101/388, train_loss: 0.1267, step time: 1.5328\n",
      "102/388, train_loss: 0.2400, step time: 1.5329\n",
      "103/388, train_loss: 0.1154, step time: 1.5336\n",
      "104/388, train_loss: 0.4204, step time: 1.5438\n",
      "105/388, train_loss: 0.1131, step time: 1.5353\n",
      "106/388, train_loss: 0.1441, step time: 1.5318\n",
      "107/388, train_loss: 0.0782, step time: 1.5350\n",
      "108/388, train_loss: 0.1902, step time: 1.5397\n",
      "109/388, train_loss: 0.2573, step time: 1.5449\n",
      "110/388, train_loss: 0.2240, step time: 1.5308\n",
      "111/388, train_loss: 0.2027, step time: 1.5324\n",
      "112/388, train_loss: 0.0508, step time: 1.5339\n",
      "113/388, train_loss: 0.4428, step time: 1.5423\n",
      "114/388, train_loss: 0.1021, step time: 1.5597\n",
      "115/388, train_loss: 0.1396, step time: 1.5393\n",
      "116/388, train_loss: 0.1084, step time: 1.5473\n",
      "117/388, train_loss: 0.1054, step time: 1.5366\n",
      "118/388, train_loss: 0.1461, step time: 1.5338\n",
      "119/388, train_loss: 0.1348, step time: 1.5356\n",
      "120/388, train_loss: 0.4216, step time: 1.5357\n",
      "121/388, train_loss: 0.2955, step time: 1.5430\n",
      "122/388, train_loss: 0.1456, step time: 1.5373\n",
      "123/388, train_loss: 0.1615, step time: 1.5360\n",
      "124/388, train_loss: 0.2216, step time: 1.5388\n",
      "125/388, train_loss: 0.1419, step time: 1.5318\n",
      "126/388, train_loss: 0.1151, step time: 1.5392\n",
      "127/388, train_loss: 0.3636, step time: 1.5351\n",
      "128/388, train_loss: 0.1583, step time: 1.5384\n",
      "129/388, train_loss: 0.5168, step time: 1.5413\n",
      "130/388, train_loss: 0.1538, step time: 1.5427\n",
      "131/388, train_loss: 0.1764, step time: 1.5371\n",
      "132/388, train_loss: 0.0888, step time: 1.5364\n",
      "133/388, train_loss: 0.1491, step time: 1.5439\n",
      "134/388, train_loss: 0.0978, step time: 1.5418\n",
      "135/388, train_loss: 0.1537, step time: 1.5311\n",
      "136/388, train_loss: 0.1205, step time: 1.5387\n",
      "137/388, train_loss: 0.2240, step time: 1.5407\n",
      "138/388, train_loss: 0.1505, step time: 1.5334\n",
      "139/388, train_loss: 0.0942, step time: 1.5319\n",
      "140/388, train_loss: 0.1259, step time: 1.5311\n",
      "141/388, train_loss: 0.1307, step time: 1.5394\n",
      "142/388, train_loss: 0.0911, step time: 1.5351\n",
      "143/388, train_loss: 0.1055, step time: 1.5462\n",
      "144/388, train_loss: 0.0801, step time: 1.5315\n",
      "145/388, train_loss: 0.1174, step time: 1.5283\n",
      "146/388, train_loss: 0.3473, step time: 1.5318\n",
      "147/388, train_loss: 0.1583, step time: 1.5332\n",
      "148/388, train_loss: 0.3945, step time: 1.5463\n",
      "149/388, train_loss: 0.1519, step time: 1.5371\n",
      "150/388, train_loss: 0.1203, step time: 1.5375\n",
      "151/388, train_loss: 0.0568, step time: 1.5443\n",
      "152/388, train_loss: 0.2808, step time: 1.5370\n",
      "153/388, train_loss: 0.0819, step time: 1.5326\n",
      "154/388, train_loss: 0.1608, step time: 1.5397\n",
      "155/388, train_loss: 0.1683, step time: 1.5334\n",
      "156/388, train_loss: 0.2110, step time: 1.5354\n",
      "157/388, train_loss: 0.2926, step time: 1.5374\n",
      "158/388, train_loss: 0.1141, step time: 1.5351\n",
      "159/388, train_loss: 0.1009, step time: 1.5365\n",
      "160/388, train_loss: 0.1998, step time: 1.5394\n",
      "161/388, train_loss: 0.0878, step time: 1.5358\n",
      "162/388, train_loss: 0.2261, step time: 1.5341\n",
      "163/388, train_loss: 0.1320, step time: 1.5360\n",
      "164/388, train_loss: 0.1296, step time: 1.5402\n",
      "165/388, train_loss: 0.2908, step time: 1.5347\n",
      "166/388, train_loss: 0.1475, step time: 1.5328\n",
      "167/388, train_loss: 0.0750, step time: 1.5381\n",
      "168/388, train_loss: 0.2214, step time: 1.5394\n",
      "169/388, train_loss: 0.1004, step time: 1.5337\n",
      "170/388, train_loss: 0.1929, step time: 1.5334\n",
      "171/388, train_loss: 0.0941, step time: 1.5330\n",
      "172/388, train_loss: 0.2354, step time: 1.5384\n",
      "173/388, train_loss: 0.2752, step time: 1.5355\n",
      "174/388, train_loss: 0.0953, step time: 1.5392\n",
      "175/388, train_loss: 0.1229, step time: 1.5385\n",
      "176/388, train_loss: 0.0755, step time: 1.5404\n",
      "177/388, train_loss: 0.1359, step time: 1.5318\n",
      "178/388, train_loss: 0.0972, step time: 1.5358\n",
      "179/388, train_loss: 0.2458, step time: 1.5410\n",
      "180/388, train_loss: 0.0520, step time: 1.5345\n",
      "181/388, train_loss: 0.1189, step time: 1.5374\n",
      "182/388, train_loss: 0.6078, step time: 1.5304\n",
      "183/388, train_loss: 0.1203, step time: 1.5350\n",
      "184/388, train_loss: 0.2138, step time: 1.5392\n",
      "185/388, train_loss: 0.1387, step time: 1.5341\n",
      "186/388, train_loss: 0.2943, step time: 1.5386\n",
      "187/388, train_loss: 0.1862, step time: 1.5358\n",
      "188/388, train_loss: 0.0751, step time: 1.5375\n",
      "189/388, train_loss: 0.2321, step time: 1.5405\n",
      "190/388, train_loss: 0.2430, step time: 1.5356\n",
      "191/388, train_loss: 0.1821, step time: 1.5392\n",
      "192/388, train_loss: 0.2699, step time: 1.5372\n",
      "193/388, train_loss: 0.4644, step time: 1.5359\n",
      "194/388, train_loss: 0.3154, step time: 1.5394\n",
      "195/388, train_loss: 0.1433, step time: 1.5395\n",
      "196/388, train_loss: 0.1268, step time: 1.5387\n",
      "197/388, train_loss: 0.5082, step time: 1.5443\n",
      "198/388, train_loss: 0.1964, step time: 1.5352\n",
      "199/388, train_loss: 0.1726, step time: 1.5363\n",
      "200/388, train_loss: 0.1897, step time: 1.5415\n",
      "201/388, train_loss: 0.2302, step time: 1.5340\n",
      "202/388, train_loss: 0.3168, step time: 1.5327\n",
      "203/388, train_loss: 0.1081, step time: 1.5332\n",
      "204/388, train_loss: 0.1241, step time: 1.5401\n",
      "205/388, train_loss: 0.1881, step time: 1.5416\n",
      "206/388, train_loss: 0.5616, step time: 1.5330\n",
      "207/388, train_loss: 0.2146, step time: 1.5424\n",
      "208/388, train_loss: 0.1343, step time: 1.5394\n",
      "209/388, train_loss: 0.1874, step time: 1.5365\n",
      "210/388, train_loss: 0.0845, step time: 1.5331\n",
      "211/388, train_loss: 0.2856, step time: 1.5304\n",
      "212/388, train_loss: 0.2818, step time: 1.5345\n",
      "213/388, train_loss: 0.2765, step time: 1.5441\n",
      "214/388, train_loss: 0.2789, step time: 1.5396\n",
      "215/388, train_loss: 0.0681, step time: 1.5363\n",
      "216/388, train_loss: 0.4003, step time: 1.5465\n",
      "217/388, train_loss: 0.4203, step time: 1.5328\n",
      "218/388, train_loss: 0.1164, step time: 1.5432\n",
      "219/388, train_loss: 0.1968, step time: 1.5393\n",
      "220/388, train_loss: 0.3831, step time: 1.5344\n",
      "221/388, train_loss: 0.1478, step time: 1.5355\n",
      "222/388, train_loss: 0.1113, step time: 1.5410\n",
      "223/388, train_loss: 0.1158, step time: 1.5445\n",
      "224/388, train_loss: 0.2757, step time: 1.5373\n",
      "225/388, train_loss: 0.1660, step time: 1.5309\n",
      "226/388, train_loss: 0.2574, step time: 1.5406\n",
      "227/388, train_loss: 0.1121, step time: 1.5316\n",
      "228/388, train_loss: 0.4247, step time: 1.5437\n",
      "229/388, train_loss: 0.2560, step time: 1.5360\n",
      "230/388, train_loss: 0.0354, step time: 1.5346\n",
      "231/388, train_loss: 0.0968, step time: 1.5341\n",
      "232/388, train_loss: 0.1332, step time: 1.5410\n",
      "233/388, train_loss: 0.2425, step time: 1.5374\n",
      "234/388, train_loss: 0.0906, step time: 1.5351\n",
      "235/388, train_loss: 0.2562, step time: 1.5328\n",
      "236/388, train_loss: 0.3155, step time: 1.5390\n",
      "237/388, train_loss: 0.0948, step time: 1.5377\n",
      "238/388, train_loss: 0.1038, step time: 1.5365\n",
      "239/388, train_loss: 0.3707, step time: 1.5308\n",
      "240/388, train_loss: 0.1606, step time: 1.5341\n",
      "241/388, train_loss: 0.1013, step time: 1.5380\n",
      "242/388, train_loss: 0.2150, step time: 1.5474\n",
      "243/388, train_loss: 0.0985, step time: 1.5321\n",
      "244/388, train_loss: 0.0590, step time: 1.5339\n",
      "245/388, train_loss: 0.0305, step time: 1.5309\n",
      "246/388, train_loss: 0.2860, step time: 1.5333\n",
      "247/388, train_loss: 0.0786, step time: 1.5389\n",
      "248/388, train_loss: 0.1025, step time: 1.5346\n",
      "249/388, train_loss: 0.1913, step time: 1.5447\n",
      "250/388, train_loss: 0.2517, step time: 1.5347\n",
      "251/388, train_loss: 0.1901, step time: 1.5318\n",
      "252/388, train_loss: 0.2421, step time: 1.5317\n",
      "253/388, train_loss: 0.1656, step time: 1.5312\n",
      "254/388, train_loss: 0.0855, step time: 1.5369\n",
      "255/388, train_loss: 0.3378, step time: 1.5356\n",
      "256/388, train_loss: 0.2093, step time: 1.5342\n",
      "257/388, train_loss: 0.1522, step time: 1.5370\n",
      "258/388, train_loss: 0.1071, step time: 1.5296\n",
      "259/388, train_loss: 0.2447, step time: 1.5350\n",
      "260/388, train_loss: 0.2569, step time: 1.5347\n",
      "261/388, train_loss: 0.1444, step time: 1.5321\n",
      "262/388, train_loss: 0.0965, step time: 1.5371\n",
      "263/388, train_loss: 0.1181, step time: 1.5353\n",
      "264/388, train_loss: 0.2754, step time: 1.5341\n",
      "265/388, train_loss: 0.0973, step time: 1.5501\n",
      "266/388, train_loss: 0.2583, step time: 1.5323\n",
      "267/388, train_loss: 0.2115, step time: 1.5302\n",
      "268/388, train_loss: 0.0704, step time: 1.5324\n",
      "269/388, train_loss: 0.2062, step time: 1.5376\n",
      "270/388, train_loss: 0.0953, step time: 1.5376\n",
      "271/388, train_loss: 0.2950, step time: 1.5381\n",
      "272/388, train_loss: 0.1084, step time: 1.5351\n",
      "273/388, train_loss: 0.3130, step time: 1.5332\n",
      "274/388, train_loss: 0.3930, step time: 1.5361\n",
      "275/388, train_loss: 0.0690, step time: 1.5340\n",
      "276/388, train_loss: 0.0729, step time: 1.5370\n",
      "277/388, train_loss: 0.1308, step time: 1.5301\n",
      "278/388, train_loss: 0.1641, step time: 1.5373\n",
      "279/388, train_loss: 0.1679, step time: 1.5317\n",
      "280/388, train_loss: 0.3005, step time: 1.5306\n",
      "281/388, train_loss: 0.1478, step time: 1.5325\n",
      "282/388, train_loss: 0.1959, step time: 1.5467\n",
      "283/388, train_loss: 0.0980, step time: 1.5324\n",
      "284/388, train_loss: 0.0365, step time: 1.5357\n",
      "285/388, train_loss: 0.5211, step time: 1.5327\n",
      "286/388, train_loss: 0.0896, step time: 1.5372\n",
      "287/388, train_loss: 0.0998, step time: 1.5406\n",
      "288/388, train_loss: 0.1071, step time: 1.5313\n",
      "289/388, train_loss: 0.0892, step time: 1.5304\n",
      "290/388, train_loss: 0.5893, step time: 1.5357\n",
      "291/388, train_loss: 0.2728, step time: 1.5356\n",
      "292/388, train_loss: 0.0910, step time: 1.5333\n",
      "293/388, train_loss: 0.0953, step time: 1.5328\n",
      "294/388, train_loss: 0.2864, step time: 1.5331\n",
      "295/388, train_loss: 0.2644, step time: 1.5339\n",
      "296/388, train_loss: 0.3971, step time: 1.5504\n",
      "297/388, train_loss: 0.1438, step time: 1.5375\n",
      "298/388, train_loss: 0.2833, step time: 1.5340\n",
      "299/388, train_loss: 0.2821, step time: 1.5299\n",
      "300/388, train_loss: 0.1883, step time: 1.5327\n",
      "301/388, train_loss: 0.2024, step time: 1.5429\n",
      "302/388, train_loss: 0.3060, step time: 1.5433\n",
      "303/388, train_loss: 0.4300, step time: 1.5422\n",
      "304/388, train_loss: 0.2709, step time: 1.5478\n",
      "305/388, train_loss: 0.5425, step time: 1.5457\n",
      "306/388, train_loss: 0.0682, step time: 1.5456\n",
      "307/388, train_loss: 0.1910, step time: 1.5331\n",
      "308/388, train_loss: 0.1304, step time: 1.5371\n",
      "309/388, train_loss: 0.1284, step time: 1.5453\n",
      "310/388, train_loss: 0.2691, step time: 1.5389\n",
      "311/388, train_loss: 0.0938, step time: 1.5332\n",
      "312/388, train_loss: 0.1035, step time: 1.5509\n",
      "313/388, train_loss: 0.1461, step time: 1.5341\n",
      "314/388, train_loss: 0.3030, step time: 1.5354\n",
      "315/388, train_loss: 0.2440, step time: 1.5329\n",
      "316/388, train_loss: 0.2197, step time: 1.5345\n",
      "317/388, train_loss: 0.0656, step time: 1.5340\n",
      "318/388, train_loss: 0.2870, step time: 1.5405\n",
      "319/388, train_loss: 0.2705, step time: 1.5364\n",
      "320/388, train_loss: 0.3183, step time: 1.5316\n",
      "321/388, train_loss: 0.1037, step time: 1.5332\n",
      "322/388, train_loss: 0.1953, step time: 1.5307\n",
      "323/388, train_loss: 0.2183, step time: 1.5347\n",
      "324/388, train_loss: 0.2507, step time: 1.5338\n",
      "325/388, train_loss: 0.2811, step time: 1.5344\n",
      "326/388, train_loss: 0.1592, step time: 1.5345\n",
      "327/388, train_loss: 0.1633, step time: 1.5361\n",
      "328/388, train_loss: 0.2867, step time: 1.5326\n",
      "329/388, train_loss: 0.1695, step time: 1.5319\n",
      "330/388, train_loss: 0.1609, step time: 1.5336\n",
      "331/388, train_loss: 0.2111, step time: 1.5350\n",
      "332/388, train_loss: 0.1676, step time: 1.5350\n",
      "333/388, train_loss: 0.3689, step time: 1.5355\n",
      "334/388, train_loss: 0.6039, step time: 1.5328\n",
      "335/388, train_loss: 0.2097, step time: 1.5304\n",
      "336/388, train_loss: 0.1072, step time: 1.5325\n",
      "337/388, train_loss: 0.2198, step time: 1.5285\n",
      "338/388, train_loss: 0.0625, step time: 1.5351\n",
      "339/388, train_loss: 0.1834, step time: 1.5421\n",
      "340/388, train_loss: 0.1492, step time: 1.5315\n",
      "341/388, train_loss: 0.3260, step time: 1.5292\n",
      "342/388, train_loss: 0.1018, step time: 1.5351\n",
      "343/388, train_loss: 0.2231, step time: 1.5355\n",
      "344/388, train_loss: 0.3069, step time: 1.5357\n",
      "345/388, train_loss: 0.1419, step time: 1.5359\n",
      "346/388, train_loss: 0.1511, step time: 1.5325\n",
      "347/388, train_loss: 0.4401, step time: 1.5311\n",
      "348/388, train_loss: 0.1644, step time: 1.5317\n",
      "349/388, train_loss: 0.4396, step time: 1.5293\n",
      "350/388, train_loss: 0.1181, step time: 1.5368\n",
      "351/388, train_loss: 0.2122, step time: 1.5356\n",
      "352/388, train_loss: 0.1119, step time: 1.5349\n",
      "353/388, train_loss: 0.2386, step time: 1.5314\n",
      "354/388, train_loss: 0.2234, step time: 1.5300\n",
      "355/388, train_loss: 0.2770, step time: 1.5326\n",
      "356/388, train_loss: 0.1969, step time: 1.5323\n",
      "357/388, train_loss: 0.4582, step time: 1.5452\n",
      "358/388, train_loss: 0.1858, step time: 1.5366\n",
      "359/388, train_loss: 0.1219, step time: 1.5359\n",
      "360/388, train_loss: 0.2453, step time: 1.5326\n",
      "361/388, train_loss: 0.1971, step time: 1.5334\n",
      "362/388, train_loss: 0.2424, step time: 1.5325\n",
      "363/388, train_loss: 0.2317, step time: 1.5321\n",
      "364/388, train_loss: 0.3402, step time: 1.5372\n",
      "365/388, train_loss: 0.3299, step time: 1.5371\n",
      "366/388, train_loss: 0.1674, step time: 1.5342\n",
      "367/388, train_loss: 0.1280, step time: 1.5309\n",
      "368/388, train_loss: 0.0773, step time: 1.5335\n",
      "369/388, train_loss: 0.4763, step time: 1.5292\n",
      "370/388, train_loss: 0.1263, step time: 1.5347\n",
      "371/388, train_loss: 0.0989, step time: 1.5347\n",
      "372/388, train_loss: 0.1725, step time: 1.5313\n",
      "373/388, train_loss: 0.0988, step time: 1.5349\n",
      "374/388, train_loss: 0.2321, step time: 1.5312\n",
      "375/388, train_loss: 0.1189, step time: 1.5332\n",
      "376/388, train_loss: 0.0820, step time: 1.5371\n",
      "377/388, train_loss: 0.1096, step time: 1.5354\n",
      "378/388, train_loss: 0.2585, step time: 1.5347\n",
      "379/388, train_loss: 0.1926, step time: 1.5308\n",
      "380/388, train_loss: 0.0636, step time: 1.5329\n",
      "381/388, train_loss: 0.6399, step time: 1.5333\n",
      "382/388, train_loss: 0.2253, step time: 1.5365\n",
      "383/388, train_loss: 0.0489, step time: 1.5349\n",
      "384/388, train_loss: 0.3240, step time: 1.5363\n",
      "385/388, train_loss: 0.2121, step time: 1.5342\n",
      "386/388, train_loss: 0.0759, step time: 1.5315\n",
      "387/388, train_loss: 0.1052, step time: 1.5319\n",
      "388/388, train_loss: 0.1878, step time: 1.5345\n",
      "epoch 35 average loss: 0.2004\n",
      "current epoch: 35 current mean dice: 0.7551 tc: 0.8047 wt: 0.8967 et: 0.5640\n",
      "best mean dice: 0.7590 at epoch: 33\n",
      "time consuming of epoch 35 is: 703.2036\n",
      "----------\n",
      "epoch 36/100\n",
      "1/388, train_loss: 0.1729, step time: 1.5457\n",
      "2/388, train_loss: 0.0795, step time: 1.5542\n",
      "3/388, train_loss: 0.0907, step time: 1.5643\n",
      "4/388, train_loss: 0.2439, step time: 1.5354\n",
      "5/388, train_loss: 0.1547, step time: 1.5355\n",
      "6/388, train_loss: 0.2054, step time: 1.5407\n",
      "7/388, train_loss: 0.0799, step time: 1.5322\n",
      "8/388, train_loss: 0.2635, step time: 1.5388\n",
      "9/388, train_loss: 0.1180, step time: 1.5374\n",
      "10/388, train_loss: 0.1212, step time: 1.5453\n",
      "11/388, train_loss: 0.1686, step time: 1.5365\n",
      "12/388, train_loss: 0.2033, step time: 1.5333\n",
      "13/388, train_loss: 0.1054, step time: 1.5332\n",
      "14/388, train_loss: 0.2154, step time: 1.5656\n",
      "15/388, train_loss: 0.1473, step time: 1.5359\n",
      "16/388, train_loss: 0.1764, step time: 1.5318\n",
      "17/388, train_loss: 0.2699, step time: 1.5327\n",
      "18/388, train_loss: 0.1452, step time: 1.5378\n",
      "19/388, train_loss: 0.1875, step time: 1.5360\n",
      "20/388, train_loss: 0.1943, step time: 1.5369\n",
      "21/388, train_loss: 0.2372, step time: 1.5324\n",
      "22/388, train_loss: 0.0832, step time: 1.5481\n",
      "23/388, train_loss: 0.2716, step time: 1.5360\n",
      "24/388, train_loss: 0.2089, step time: 1.5371\n",
      "25/388, train_loss: 0.1619, step time: 1.5342\n",
      "26/388, train_loss: 0.2204, step time: 1.5425\n",
      "27/388, train_loss: 0.0915, step time: 1.5310\n",
      "28/388, train_loss: 0.2781, step time: 1.5355\n",
      "29/388, train_loss: 0.3084, step time: 1.5339\n",
      "30/388, train_loss: 0.1108, step time: 1.5394\n",
      "31/388, train_loss: 0.0575, step time: 1.5340\n",
      "32/388, train_loss: 0.1895, step time: 1.5357\n",
      "33/388, train_loss: 0.1476, step time: 1.5336\n",
      "34/388, train_loss: 0.3305, step time: 1.5516\n",
      "35/388, train_loss: 0.1294, step time: 1.5339\n",
      "36/388, train_loss: 0.1074, step time: 1.5335\n",
      "37/388, train_loss: 0.2575, step time: 1.5342\n",
      "38/388, train_loss: 0.2348, step time: 1.5433\n",
      "39/388, train_loss: 0.1993, step time: 1.5316\n",
      "40/388, train_loss: 0.0680, step time: 1.5340\n",
      "41/388, train_loss: 0.4891, step time: 1.5359\n",
      "42/388, train_loss: 0.0983, step time: 1.5434\n",
      "43/388, train_loss: 0.1778, step time: 1.5322\n",
      "44/388, train_loss: 0.1357, step time: 1.5362\n",
      "45/388, train_loss: 0.0858, step time: 1.5346\n",
      "46/388, train_loss: 0.2115, step time: 1.5470\n",
      "47/388, train_loss: 0.3428, step time: 1.5358\n",
      "48/388, train_loss: 0.8506, step time: 1.5349\n",
      "49/388, train_loss: 0.3956, step time: 1.5313\n",
      "50/388, train_loss: 0.2876, step time: 1.5430\n",
      "51/388, train_loss: 0.1355, step time: 1.5452\n",
      "52/388, train_loss: 0.0635, step time: 1.5345\n",
      "53/388, train_loss: 0.0991, step time: 1.5312\n",
      "54/388, train_loss: 0.1201, step time: 1.5446\n",
      "55/388, train_loss: 0.1148, step time: 1.5392\n",
      "56/388, train_loss: 0.5040, step time: 1.5333\n",
      "57/388, train_loss: 0.0860, step time: 1.5340\n",
      "58/388, train_loss: 0.1023, step time: 1.5394\n",
      "59/388, train_loss: 0.2672, step time: 1.5427\n",
      "60/388, train_loss: 0.1260, step time: 1.5338\n",
      "61/388, train_loss: 0.1168, step time: 1.5326\n",
      "62/388, train_loss: 0.1581, step time: 1.5426\n",
      "63/388, train_loss: 0.1197, step time: 1.5382\n",
      "64/388, train_loss: 0.0689, step time: 1.5347\n",
      "65/388, train_loss: 0.5454, step time: 1.5365\n",
      "66/388, train_loss: 0.1063, step time: 1.5426\n",
      "67/388, train_loss: 0.1136, step time: 1.5326\n",
      "68/388, train_loss: 0.1311, step time: 1.5399\n",
      "69/388, train_loss: 0.2107, step time: 1.5401\n",
      "70/388, train_loss: 0.3184, step time: 1.5456\n",
      "71/388, train_loss: 0.0731, step time: 1.5314\n",
      "72/388, train_loss: 0.3848, step time: 1.5337\n",
      "73/388, train_loss: 0.3047, step time: 1.5475\n",
      "74/388, train_loss: 0.2282, step time: 1.5458\n",
      "75/388, train_loss: 0.0856, step time: 1.5346\n",
      "76/388, train_loss: 0.2491, step time: 1.5315\n",
      "77/388, train_loss: 0.0934, step time: 1.5385\n",
      "78/388, train_loss: 0.1662, step time: 1.5540\n",
      "79/388, train_loss: 0.2951, step time: 1.5343\n",
      "80/388, train_loss: 0.2283, step time: 1.5317\n",
      "81/388, train_loss: 0.1368, step time: 1.5322\n",
      "82/388, train_loss: 0.1827, step time: 1.5475\n",
      "83/388, train_loss: 0.1679, step time: 1.5368\n",
      "84/388, train_loss: 0.3466, step time: 1.5340\n",
      "85/388, train_loss: 0.1227, step time: 1.5347\n",
      "86/388, train_loss: 0.1134, step time: 1.5549\n",
      "87/388, train_loss: 0.3377, step time: 1.5362\n",
      "88/388, train_loss: 0.2692, step time: 1.5316\n",
      "89/388, train_loss: 0.1193, step time: 1.5305\n",
      "90/388, train_loss: 0.1365, step time: 1.5526\n",
      "91/388, train_loss: 0.1837, step time: 1.5362\n",
      "92/388, train_loss: 0.1529, step time: 1.5563\n",
      "93/388, train_loss: 0.1507, step time: 1.5322\n",
      "94/388, train_loss: 0.2871, step time: 1.5487\n",
      "95/388, train_loss: 0.3867, step time: 1.5340\n",
      "96/388, train_loss: 0.5636, step time: 1.5330\n",
      "97/388, train_loss: 0.2782, step time: 1.5293\n",
      "98/388, train_loss: 0.2965, step time: 1.5539\n",
      "99/388, train_loss: 0.2872, step time: 1.5382\n",
      "100/388, train_loss: 0.3637, step time: 1.5315\n",
      "101/388, train_loss: 0.0953, step time: 1.5542\n",
      "102/388, train_loss: 0.1548, step time: 1.5549\n",
      "103/388, train_loss: 0.1982, step time: 1.5333\n",
      "104/388, train_loss: 0.2756, step time: 1.5337\n",
      "105/388, train_loss: 0.0823, step time: 1.5302\n",
      "106/388, train_loss: 0.0961, step time: 1.5537\n",
      "107/388, train_loss: 0.2092, step time: 1.5591\n",
      "108/388, train_loss: 0.0714, step time: 1.5340\n",
      "109/388, train_loss: 0.0769, step time: 1.5354\n",
      "110/388, train_loss: 0.3668, step time: 1.5398\n",
      "111/388, train_loss: 0.1169, step time: 1.5300\n",
      "112/388, train_loss: 0.3359, step time: 1.5332\n",
      "113/388, train_loss: 0.2094, step time: 1.5362\n",
      "114/388, train_loss: 0.1014, step time: 1.5553\n",
      "115/388, train_loss: 0.1763, step time: 1.5567\n",
      "116/388, train_loss: 0.0795, step time: 1.5376\n",
      "117/388, train_loss: 0.2345, step time: 1.5351\n",
      "118/388, train_loss: 0.0657, step time: 1.5422\n",
      "119/388, train_loss: 0.6277, step time: 1.5331\n",
      "120/388, train_loss: 0.3051, step time: 1.5361\n",
      "121/388, train_loss: 0.4219, step time: 1.5356\n",
      "122/388, train_loss: 0.1895, step time: 1.5512\n",
      "123/388, train_loss: 0.1379, step time: 1.5375\n",
      "124/388, train_loss: 0.3676, step time: 1.5354\n",
      "125/388, train_loss: 0.0689, step time: 1.5363\n",
      "126/388, train_loss: 0.0984, step time: 1.5461\n",
      "127/388, train_loss: 0.1538, step time: 1.5600\n",
      "128/388, train_loss: 0.2844, step time: 1.5355\n",
      "129/388, train_loss: 0.1422, step time: 1.5317\n",
      "130/388, train_loss: 0.0827, step time: 1.5412\n",
      "131/388, train_loss: 0.2214, step time: 1.5384\n",
      "132/388, train_loss: 0.4572, step time: 1.5396\n",
      "133/388, train_loss: 0.4826, step time: 1.5367\n",
      "134/388, train_loss: 0.1150, step time: 1.5402\n",
      "135/388, train_loss: 0.2840, step time: 1.5309\n",
      "136/388, train_loss: 0.1002, step time: 1.5441\n",
      "137/388, train_loss: 0.0337, step time: 1.5369\n",
      "138/388, train_loss: 0.0834, step time: 1.5496\n",
      "139/388, train_loss: 0.1927, step time: 1.5296\n",
      "140/388, train_loss: 0.1701, step time: 1.5438\n",
      "141/388, train_loss: 0.0877, step time: 1.5371\n",
      "142/388, train_loss: 0.1134, step time: 1.5479\n",
      "143/388, train_loss: 0.1672, step time: 1.5307\n",
      "144/388, train_loss: 0.2460, step time: 1.5334\n",
      "145/388, train_loss: 0.3349, step time: 1.5359\n",
      "146/388, train_loss: 0.1052, step time: 1.5468\n",
      "147/388, train_loss: 0.1462, step time: 1.5364\n",
      "148/388, train_loss: 0.3517, step time: 1.5325\n",
      "149/388, train_loss: 0.0906, step time: 1.5316\n",
      "150/388, train_loss: 0.0827, step time: 1.5396\n",
      "151/388, train_loss: 0.0398, step time: 1.5450\n",
      "152/388, train_loss: 0.1652, step time: 1.5342\n",
      "153/388, train_loss: 0.2187, step time: 1.5334\n",
      "154/388, train_loss: 0.1949, step time: 1.5412\n",
      "155/388, train_loss: 0.1063, step time: 1.5348\n",
      "156/388, train_loss: 0.1111, step time: 1.5378\n",
      "157/388, train_loss: 0.1389, step time: 1.5374\n",
      "158/388, train_loss: 0.2864, step time: 1.5461\n",
      "159/388, train_loss: 0.1712, step time: 1.5317\n",
      "160/388, train_loss: 0.1106, step time: 1.5344\n",
      "161/388, train_loss: 0.3502, step time: 1.5329\n",
      "162/388, train_loss: 0.0837, step time: 1.5462\n",
      "163/388, train_loss: 0.1699, step time: 1.5321\n",
      "164/388, train_loss: 0.0709, step time: 1.5407\n",
      "165/388, train_loss: 0.3184, step time: 1.5343\n",
      "166/388, train_loss: 0.1841, step time: 1.5404\n",
      "167/388, train_loss: 0.1171, step time: 1.5323\n",
      "168/388, train_loss: 0.0941, step time: 1.5313\n",
      "169/388, train_loss: 0.3533, step time: 1.5334\n",
      "170/388, train_loss: 0.1616, step time: 1.5417\n",
      "171/388, train_loss: 0.1685, step time: 1.5336\n",
      "172/388, train_loss: 0.1087, step time: 1.5368\n",
      "173/388, train_loss: 0.0830, step time: 1.5369\n",
      "174/388, train_loss: 0.0915, step time: 1.5466\n",
      "175/388, train_loss: 0.0612, step time: 1.5338\n",
      "176/388, train_loss: 0.1104, step time: 1.5480\n",
      "177/388, train_loss: 0.6204, step time: 1.5378\n",
      "178/388, train_loss: 0.5909, step time: 1.5529\n",
      "179/388, train_loss: 0.1782, step time: 1.5611\n",
      "180/388, train_loss: 0.1789, step time: 1.5307\n",
      "181/388, train_loss: 0.1677, step time: 1.5318\n",
      "182/388, train_loss: 0.1309, step time: 1.5430\n",
      "183/388, train_loss: 0.4147, step time: 1.5337\n",
      "184/388, train_loss: 0.0680, step time: 1.5373\n",
      "185/388, train_loss: 0.1671, step time: 1.5353\n",
      "186/388, train_loss: 0.4663, step time: 1.5461\n",
      "187/388, train_loss: 0.0872, step time: 1.5325\n",
      "188/388, train_loss: 0.1646, step time: 1.5368\n",
      "189/388, train_loss: 0.1099, step time: 1.5369\n",
      "190/388, train_loss: 0.1002, step time: 1.5536\n",
      "191/388, train_loss: 0.2346, step time: 1.5340\n",
      "192/388, train_loss: 0.1078, step time: 1.5377\n",
      "193/388, train_loss: 0.1643, step time: 1.5329\n",
      "194/388, train_loss: 0.0880, step time: 1.5408\n",
      "195/388, train_loss: 0.2297, step time: 1.5442\n",
      "196/388, train_loss: 0.3071, step time: 1.5347\n",
      "197/388, train_loss: 0.1232, step time: 1.5350\n",
      "198/388, train_loss: 0.3236, step time: 1.5420\n",
      "199/388, train_loss: 0.1595, step time: 1.5343\n",
      "200/388, train_loss: 0.2168, step time: 1.5453\n",
      "201/388, train_loss: 0.2720, step time: 1.5428\n",
      "202/388, train_loss: 0.0754, step time: 1.5484\n",
      "203/388, train_loss: 0.1402, step time: 1.5358\n",
      "204/388, train_loss: 0.1393, step time: 1.5320\n",
      "205/388, train_loss: 0.1646, step time: 1.5327\n",
      "206/388, train_loss: 0.1526, step time: 1.5523\n",
      "207/388, train_loss: 0.1602, step time: 1.5347\n",
      "208/388, train_loss: 0.1572, step time: 1.5322\n",
      "209/388, train_loss: 0.2459, step time: 1.5343\n",
      "210/388, train_loss: 0.4162, step time: 1.5457\n",
      "211/388, train_loss: 0.2753, step time: 1.5399\n",
      "212/388, train_loss: 0.1925, step time: 1.5327\n",
      "213/388, train_loss: 0.1509, step time: 1.5322\n",
      "214/388, train_loss: 0.2618, step time: 1.5430\n",
      "215/388, train_loss: 0.3075, step time: 1.5351\n",
      "216/388, train_loss: 0.0615, step time: 1.5345\n",
      "217/388, train_loss: 0.3291, step time: 1.5320\n",
      "218/388, train_loss: 0.1784, step time: 1.5437\n",
      "219/388, train_loss: 0.2948, step time: 1.5343\n",
      "220/388, train_loss: 0.4422, step time: 1.5348\n",
      "221/388, train_loss: 0.4773, step time: 1.5339\n",
      "222/388, train_loss: 0.1832, step time: 1.5485\n",
      "223/388, train_loss: 0.1108, step time: 1.5354\n",
      "224/388, train_loss: 0.1227, step time: 1.5388\n",
      "225/388, train_loss: 0.2893, step time: 1.5326\n",
      "226/388, train_loss: 0.1202, step time: 1.5473\n",
      "227/388, train_loss: 0.1241, step time: 1.5319\n",
      "228/388, train_loss: 0.1717, step time: 1.5466\n",
      "229/388, train_loss: 0.0618, step time: 1.5385\n",
      "230/388, train_loss: 0.1349, step time: 1.5408\n",
      "231/388, train_loss: 0.1394, step time: 1.5363\n",
      "232/388, train_loss: 0.2316, step time: 1.5329\n",
      "233/388, train_loss: 0.1098, step time: 1.5358\n",
      "234/388, train_loss: 0.1347, step time: 1.5511\n",
      "235/388, train_loss: 0.0876, step time: 1.5355\n",
      "236/388, train_loss: 0.0859, step time: 1.5357\n",
      "237/388, train_loss: 0.2675, step time: 1.5368\n",
      "238/388, train_loss: 0.1073, step time: 1.5502\n",
      "239/388, train_loss: 0.3756, step time: 1.5358\n",
      "240/388, train_loss: 0.2436, step time: 1.5339\n",
      "241/388, train_loss: 0.2272, step time: 1.5418\n",
      "242/388, train_loss: 0.2469, step time: 1.5401\n",
      "243/388, train_loss: 0.1002, step time: 1.5354\n",
      "244/388, train_loss: 0.1103, step time: 1.5371\n",
      "245/388, train_loss: 0.2436, step time: 1.5377\n",
      "246/388, train_loss: 0.1971, step time: 1.5439\n",
      "247/388, train_loss: 0.0820, step time: 1.5284\n",
      "248/388, train_loss: 0.1469, step time: 1.5312\n",
      "249/388, train_loss: 0.3932, step time: 1.5369\n",
      "250/388, train_loss: 0.3263, step time: 1.5481\n",
      "251/388, train_loss: 0.1555, step time: 1.5319\n",
      "252/388, train_loss: 0.3127, step time: 1.5345\n",
      "253/388, train_loss: 0.1541, step time: 1.5320\n",
      "254/388, train_loss: 0.2070, step time: 1.5534\n",
      "255/388, train_loss: 0.2290, step time: 1.5320\n",
      "256/388, train_loss: 0.3037, step time: 1.5328\n",
      "257/388, train_loss: 0.1834, step time: 1.5370\n",
      "258/388, train_loss: 0.2181, step time: 1.5557\n",
      "259/388, train_loss: 0.0666, step time: 1.5405\n",
      "260/388, train_loss: 0.0817, step time: 1.5313\n",
      "261/388, train_loss: 0.2781, step time: 1.5541\n",
      "262/388, train_loss: 0.0670, step time: 1.5498\n",
      "263/388, train_loss: 0.2545, step time: 1.5271\n",
      "264/388, train_loss: 0.3746, step time: 1.5331\n",
      "265/388, train_loss: 0.5651, step time: 1.5356\n",
      "266/388, train_loss: 0.1657, step time: 1.5554\n",
      "267/388, train_loss: 0.1387, step time: 1.5336\n",
      "268/388, train_loss: 0.2818, step time: 1.5319\n",
      "269/388, train_loss: 0.1426, step time: 1.5370\n",
      "270/388, train_loss: 0.1470, step time: 1.5461\n",
      "271/388, train_loss: 0.3819, step time: 1.5371\n",
      "272/388, train_loss: 0.1493, step time: 1.5346\n",
      "273/388, train_loss: 0.4629, step time: 1.5317\n",
      "274/388, train_loss: 0.2795, step time: 1.5456\n",
      "275/388, train_loss: 0.1279, step time: 1.5349\n",
      "276/388, train_loss: 0.3111, step time: 1.5329\n",
      "277/388, train_loss: 0.0965, step time: 1.5340\n",
      "278/388, train_loss: 0.4795, step time: 1.5583\n",
      "279/388, train_loss: 0.2122, step time: 1.5344\n",
      "280/388, train_loss: 0.1008, step time: 1.5297\n",
      "281/388, train_loss: 0.3566, step time: 1.5342\n",
      "282/388, train_loss: 0.3439, step time: 1.5442\n",
      "283/388, train_loss: 0.2884, step time: 1.5363\n",
      "284/388, train_loss: 0.2256, step time: 1.5362\n",
      "285/388, train_loss: 0.2665, step time: 1.5321\n",
      "286/388, train_loss: 0.3206, step time: 1.5428\n",
      "287/388, train_loss: 0.2751, step time: 1.5344\n",
      "288/388, train_loss: 0.1269, step time: 1.5340\n",
      "289/388, train_loss: 0.1174, step time: 1.5355\n",
      "290/388, train_loss: 0.1454, step time: 1.5420\n",
      "291/388, train_loss: 0.1824, step time: 1.5285\n",
      "292/388, train_loss: 0.2869, step time: 1.5356\n",
      "293/388, train_loss: 0.1042, step time: 1.5373\n",
      "294/388, train_loss: 0.1915, step time: 1.5436\n",
      "295/388, train_loss: 0.2336, step time: 1.5320\n",
      "296/388, train_loss: 0.1575, step time: 1.5407\n",
      "297/388, train_loss: 0.1390, step time: 1.5343\n",
      "298/388, train_loss: 0.1436, step time: 1.5483\n",
      "299/388, train_loss: 0.1934, step time: 1.5301\n",
      "300/388, train_loss: 0.2335, step time: 1.5318\n",
      "301/388, train_loss: 0.2105, step time: 1.5333\n",
      "302/388, train_loss: 0.0855, step time: 1.5503\n",
      "303/388, train_loss: 0.0727, step time: 1.5301\n",
      "304/388, train_loss: 0.1240, step time: 1.5318\n",
      "305/388, train_loss: 0.0854, step time: 1.5320\n",
      "306/388, train_loss: 0.3837, step time: 1.5442\n",
      "307/388, train_loss: 0.1112, step time: 1.5340\n",
      "308/388, train_loss: 0.1170, step time: 1.5377\n",
      "309/388, train_loss: 0.0538, step time: 1.5318\n",
      "310/388, train_loss: 0.1726, step time: 1.5383\n",
      "311/388, train_loss: 0.3018, step time: 1.5310\n",
      "312/388, train_loss: 0.3886, step time: 1.5346\n",
      "313/388, train_loss: 0.5732, step time: 1.5394\n",
      "314/388, train_loss: 0.1946, step time: 1.5482\n",
      "315/388, train_loss: 0.4815, step time: 1.5319\n",
      "316/388, train_loss: 0.1619, step time: 1.5360\n",
      "317/388, train_loss: 0.4362, step time: 1.5370\n",
      "318/388, train_loss: 0.1034, step time: 1.5527\n",
      "319/388, train_loss: 0.0523, step time: 1.5344\n",
      "320/388, train_loss: 0.1714, step time: 1.5331\n",
      "321/388, train_loss: 0.1334, step time: 1.5339\n",
      "322/388, train_loss: 0.2234, step time: 1.5501\n",
      "323/388, train_loss: 0.1830, step time: 1.5345\n",
      "324/388, train_loss: 0.0999, step time: 1.5359\n",
      "325/388, train_loss: 0.3059, step time: 1.5338\n",
      "326/388, train_loss: 0.0931, step time: 1.5402\n",
      "327/388, train_loss: 0.1026, step time: 1.5386\n",
      "328/388, train_loss: 0.0958, step time: 1.5369\n",
      "329/388, train_loss: 0.0652, step time: 1.5351\n",
      "330/388, train_loss: 0.1407, step time: 1.5495\n",
      "331/388, train_loss: 0.2164, step time: 1.5291\n",
      "332/388, train_loss: 0.1441, step time: 1.5348\n",
      "333/388, train_loss: 0.4087, step time: 1.5362\n",
      "334/388, train_loss: 0.1179, step time: 1.5424\n",
      "335/388, train_loss: 0.2241, step time: 1.5364\n",
      "336/388, train_loss: 0.1947, step time: 1.5366\n",
      "337/388, train_loss: 0.1601, step time: 1.5380\n",
      "338/388, train_loss: 0.2000, step time: 1.5429\n",
      "339/388, train_loss: 0.0697, step time: 1.5314\n",
      "340/388, train_loss: 0.1490, step time: 1.5325\n",
      "341/388, train_loss: 0.1132, step time: 1.5338\n",
      "342/388, train_loss: 0.2156, step time: 1.5463\n",
      "343/388, train_loss: 0.2048, step time: 1.5447\n",
      "344/388, train_loss: 0.2507, step time: 1.5343\n",
      "345/388, train_loss: 0.0645, step time: 1.5331\n",
      "346/388, train_loss: 0.0690, step time: 1.5500\n",
      "347/388, train_loss: 0.3444, step time: 1.5323\n",
      "348/388, train_loss: 0.2440, step time: 1.5322\n",
      "349/388, train_loss: 0.0995, step time: 1.5321\n",
      "350/388, train_loss: 0.2325, step time: 1.5516\n",
      "351/388, train_loss: 0.2061, step time: 1.5553\n",
      "352/388, train_loss: 0.2746, step time: 1.5379\n",
      "353/388, train_loss: 0.2179, step time: 1.5360\n",
      "354/388, train_loss: 0.1382, step time: 1.5520\n",
      "355/388, train_loss: 0.0591, step time: 1.5338\n",
      "356/388, train_loss: 0.2082, step time: 1.5383\n",
      "357/388, train_loss: 0.3945, step time: 1.5364\n",
      "358/388, train_loss: 0.1870, step time: 1.5457\n",
      "359/388, train_loss: 0.0352, step time: 1.5312\n",
      "360/388, train_loss: 0.1123, step time: 1.5618\n",
      "361/388, train_loss: 0.1204, step time: 1.5333\n",
      "362/388, train_loss: 0.3133, step time: 1.5520\n",
      "363/388, train_loss: 0.1135, step time: 1.5352\n",
      "364/388, train_loss: 0.1345, step time: 1.5351\n",
      "365/388, train_loss: 0.1937, step time: 1.5348\n",
      "366/388, train_loss: 0.1321, step time: 1.5440\n",
      "367/388, train_loss: 0.1925, step time: 1.5354\n",
      "368/388, train_loss: 0.3981, step time: 1.5338\n",
      "369/388, train_loss: 0.2609, step time: 1.5305\n",
      "370/388, train_loss: 0.1944, step time: 1.5413\n",
      "371/388, train_loss: 0.2214, step time: 1.5441\n",
      "372/388, train_loss: 0.0490, step time: 1.5371\n",
      "373/388, train_loss: 0.1138, step time: 1.5324\n",
      "374/388, train_loss: 0.1890, step time: 1.5549\n",
      "375/388, train_loss: 0.0857, step time: 1.5367\n",
      "376/388, train_loss: 0.2332, step time: 1.5367\n",
      "377/388, train_loss: 0.3640, step time: 1.5314\n",
      "378/388, train_loss: 0.2459, step time: 1.5533\n",
      "379/388, train_loss: 0.1265, step time: 1.5388\n",
      "380/388, train_loss: 0.1583, step time: 1.5308\n",
      "381/388, train_loss: 0.1136, step time: 1.5306\n",
      "382/388, train_loss: 0.1601, step time: 1.5568\n",
      "383/388, train_loss: 0.2080, step time: 1.5344\n",
      "384/388, train_loss: 0.2345, step time: 1.5321\n",
      "385/388, train_loss: 0.0424, step time: 1.5343\n",
      "386/388, train_loss: 0.1601, step time: 1.5412\n",
      "387/388, train_loss: 0.2038, step time: 1.5372\n",
      "388/388, train_loss: 0.1050, step time: 1.5451\n",
      "epoch 36 average loss: 0.1996\n",
      "saved new best metric model\n",
      "current epoch: 36 current mean dice: 0.7640 tc: 0.8115 wt: 0.9012 et: 0.5793\n",
      "best mean dice: 0.7640 at epoch: 36\n",
      "time consuming of epoch 36 is: 704.7897\n",
      "----------\n",
      "epoch 37/100\n",
      "1/388, train_loss: 0.0722, step time: 1.5565\n",
      "2/388, train_loss: 0.1777, step time: 1.5345\n",
      "3/388, train_loss: 0.0506, step time: 1.5364\n",
      "4/388, train_loss: 0.1125, step time: 1.5383\n",
      "5/388, train_loss: 0.2276, step time: 1.5423\n",
      "6/388, train_loss: 0.1535, step time: 1.5315\n",
      "7/388, train_loss: 0.2028, step time: 1.5372\n",
      "8/388, train_loss: 0.1431, step time: 1.5341\n",
      "9/388, train_loss: 0.1874, step time: 1.5421\n",
      "10/388, train_loss: 0.4215, step time: 1.5327\n",
      "11/388, train_loss: 0.1040, step time: 1.5336\n",
      "12/388, train_loss: 0.1658, step time: 1.5310\n",
      "13/388, train_loss: 0.1635, step time: 1.5459\n",
      "14/388, train_loss: 0.2710, step time: 1.5524\n",
      "15/388, train_loss: 0.3198, step time: 1.5420\n",
      "16/388, train_loss: 0.2023, step time: 1.5355\n",
      "17/388, train_loss: 0.0507, step time: 1.5490\n",
      "18/388, train_loss: 0.1671, step time: 1.5331\n",
      "19/388, train_loss: 0.3350, step time: 1.5396\n",
      "20/388, train_loss: 0.2311, step time: 1.5385\n",
      "21/388, train_loss: 0.1255, step time: 1.5525\n",
      "22/388, train_loss: 0.2035, step time: 1.5368\n",
      "23/388, train_loss: 0.1672, step time: 1.5359\n",
      "24/388, train_loss: 0.0987, step time: 1.5405\n",
      "25/388, train_loss: 0.1880, step time: 1.5459\n",
      "26/388, train_loss: 0.1471, step time: 1.5319\n",
      "27/388, train_loss: 0.2116, step time: 1.5337\n",
      "28/388, train_loss: 0.0657, step time: 1.5357\n",
      "29/388, train_loss: 0.2310, step time: 1.5505\n",
      "30/388, train_loss: 0.1189, step time: 1.5342\n",
      "31/388, train_loss: 0.1342, step time: 1.5341\n",
      "32/388, train_loss: 0.1649, step time: 1.5370\n",
      "33/388, train_loss: 0.0902, step time: 1.5662\n",
      "34/388, train_loss: 0.2441, step time: 1.5367\n",
      "35/388, train_loss: 0.2808, step time: 1.5336\n",
      "36/388, train_loss: 0.3779, step time: 1.5387\n",
      "37/388, train_loss: 0.2398, step time: 1.5449\n",
      "38/388, train_loss: 0.0694, step time: 1.5322\n",
      "39/388, train_loss: 0.0970, step time: 1.5318\n",
      "40/388, train_loss: 0.5207, step time: 1.5332\n",
      "41/388, train_loss: 0.0552, step time: 1.5431\n",
      "42/388, train_loss: 0.2168, step time: 1.5364\n",
      "43/388, train_loss: 0.3123, step time: 1.5377\n",
      "44/388, train_loss: 0.1095, step time: 1.5332\n",
      "45/388, train_loss: 0.3178, step time: 1.5455\n",
      "46/388, train_loss: 0.1019, step time: 1.5436\n",
      "47/388, train_loss: 0.3047, step time: 1.5436\n",
      "48/388, train_loss: 0.5733, step time: 1.5380\n",
      "49/388, train_loss: 0.3536, step time: 1.5418\n",
      "50/388, train_loss: 0.4064, step time: 1.5540\n",
      "51/388, train_loss: 0.3057, step time: 1.5324\n",
      "52/388, train_loss: 0.1240, step time: 1.5337\n",
      "53/388, train_loss: 0.1132, step time: 1.5435\n",
      "54/388, train_loss: 0.3000, step time: 1.5364\n",
      "55/388, train_loss: 0.5443, step time: 1.5372\n",
      "56/388, train_loss: 0.1181, step time: 1.5349\n",
      "57/388, train_loss: 0.0799, step time: 1.5513\n",
      "58/388, train_loss: 0.3847, step time: 1.5375\n",
      "59/388, train_loss: 0.1162, step time: 1.5356\n",
      "60/388, train_loss: 0.3556, step time: 1.5372\n",
      "61/388, train_loss: 0.1779, step time: 1.5456\n",
      "62/388, train_loss: 0.1924, step time: 1.5337\n",
      "63/388, train_loss: 0.0904, step time: 1.5356\n",
      "64/388, train_loss: 0.0870, step time: 1.5384\n",
      "65/388, train_loss: 0.1051, step time: 1.5473\n",
      "66/388, train_loss: 0.0445, step time: 1.5360\n",
      "67/388, train_loss: 0.1387, step time: 1.5346\n",
      "68/388, train_loss: 0.1066, step time: 1.5378\n",
      "69/388, train_loss: 0.1124, step time: 1.5524\n",
      "70/388, train_loss: 0.0980, step time: 1.5370\n",
      "71/388, train_loss: 0.0915, step time: 1.5307\n",
      "72/388, train_loss: 0.1780, step time: 1.5343\n",
      "73/388, train_loss: 0.0890, step time: 1.5490\n",
      "74/388, train_loss: 0.1211, step time: 1.5362\n",
      "75/388, train_loss: 0.1556, step time: 1.5348\n",
      "76/388, train_loss: 0.1381, step time: 1.5334\n",
      "77/388, train_loss: 0.2047, step time: 1.5395\n",
      "78/388, train_loss: 0.2584, step time: 1.5384\n",
      "79/388, train_loss: 0.1748, step time: 1.5345\n",
      "80/388, train_loss: 0.1164, step time: 1.5326\n",
      "81/388, train_loss: 0.2642, step time: 1.5382\n",
      "82/388, train_loss: 0.3627, step time: 1.5350\n",
      "83/388, train_loss: 0.0527, step time: 1.5354\n",
      "84/388, train_loss: 0.3470, step time: 1.5349\n",
      "85/388, train_loss: 0.0973, step time: 1.5384\n",
      "86/388, train_loss: 0.0896, step time: 1.5370\n",
      "87/388, train_loss: 0.3087, step time: 1.5356\n",
      "88/388, train_loss: 0.0959, step time: 1.5380\n",
      "89/388, train_loss: 0.2073, step time: 1.5441\n",
      "90/388, train_loss: 0.1145, step time: 1.5315\n",
      "91/388, train_loss: 0.0897, step time: 1.5371\n",
      "92/388, train_loss: 0.2021, step time: 1.5358\n",
      "93/388, train_loss: 0.2383, step time: 1.5401\n",
      "94/388, train_loss: 0.1832, step time: 1.5322\n",
      "95/388, train_loss: 0.1674, step time: 1.5330\n",
      "96/388, train_loss: 0.2102, step time: 1.5364\n",
      "97/388, train_loss: 0.0919, step time: 1.5473\n",
      "98/388, train_loss: 0.0968, step time: 1.5324\n",
      "99/388, train_loss: 0.1099, step time: 1.5319\n",
      "100/388, train_loss: 0.2395, step time: 1.5364\n",
      "101/388, train_loss: 0.1953, step time: 1.5472\n",
      "102/388, train_loss: 0.0912, step time: 1.5362\n",
      "103/388, train_loss: 0.2126, step time: 1.5327\n",
      "104/388, train_loss: 0.0686, step time: 1.5322\n",
      "105/388, train_loss: 0.3070, step time: 1.5455\n",
      "106/388, train_loss: 0.3018, step time: 1.5374\n",
      "107/388, train_loss: 0.0765, step time: 1.5365\n",
      "108/388, train_loss: 0.3916, step time: 1.5319\n",
      "109/388, train_loss: 0.5172, step time: 1.5440\n",
      "110/388, train_loss: 0.1087, step time: 1.5347\n",
      "111/388, train_loss: 0.1685, step time: 1.5336\n",
      "112/388, train_loss: 0.1542, step time: 1.5350\n",
      "113/388, train_loss: 0.1195, step time: 1.5423\n",
      "114/388, train_loss: 0.0610, step time: 1.5317\n",
      "115/388, train_loss: 0.2020, step time: 1.5317\n",
      "116/388, train_loss: 0.2492, step time: 1.5381\n",
      "117/388, train_loss: 0.1480, step time: 1.5450\n",
      "118/388, train_loss: 0.3680, step time: 1.5351\n",
      "119/388, train_loss: 0.1364, step time: 1.5332\n",
      "120/388, train_loss: 0.2146, step time: 1.5360\n",
      "121/388, train_loss: 0.1326, step time: 1.5490\n",
      "122/388, train_loss: 0.1496, step time: 1.5591\n",
      "123/388, train_loss: 0.3731, step time: 1.5396\n",
      "124/388, train_loss: 0.1842, step time: 1.5333\n",
      "125/388, train_loss: 0.2160, step time: 1.5403\n",
      "126/388, train_loss: 0.4060, step time: 1.5310\n",
      "127/388, train_loss: 0.0721, step time: 1.5474\n",
      "128/388, train_loss: 0.1496, step time: 1.5373\n",
      "129/388, train_loss: 0.1595, step time: 1.5487\n",
      "130/388, train_loss: 0.6191, step time: 1.5321\n",
      "131/388, train_loss: 0.1343, step time: 1.5350\n",
      "132/388, train_loss: 0.0825, step time: 1.5390\n",
      "133/388, train_loss: 0.1740, step time: 1.5440\n",
      "134/388, train_loss: 0.2991, step time: 1.5332\n",
      "135/388, train_loss: 0.1967, step time: 1.5317\n",
      "136/388, train_loss: 0.2550, step time: 1.5438\n",
      "137/388, train_loss: 0.1540, step time: 1.5471\n",
      "138/388, train_loss: 0.0351, step time: 1.5313\n",
      "139/388, train_loss: 0.3127, step time: 1.5303\n",
      "140/388, train_loss: 0.5897, step time: 1.5306\n",
      "141/388, train_loss: 0.4441, step time: 1.5440\n",
      "142/388, train_loss: 0.1921, step time: 1.5466\n",
      "143/388, train_loss: 0.2788, step time: 1.5394\n",
      "144/388, train_loss: 0.2046, step time: 1.5328\n",
      "145/388, train_loss: 0.1932, step time: 1.5376\n",
      "146/388, train_loss: 0.1282, step time: 1.5336\n",
      "147/388, train_loss: 0.2972, step time: 1.5369\n",
      "148/388, train_loss: 0.1822, step time: 1.5383\n",
      "149/388, train_loss: 0.1984, step time: 1.5428\n",
      "150/388, train_loss: 0.2684, step time: 1.5452\n",
      "151/388, train_loss: 0.0573, step time: 1.5599\n",
      "152/388, train_loss: 0.2150, step time: 1.5333\n",
      "153/388, train_loss: 0.0530, step time: 1.5475\n",
      "154/388, train_loss: 0.1423, step time: 1.5380\n",
      "155/388, train_loss: 0.1283, step time: 1.5594\n",
      "156/388, train_loss: 0.2132, step time: 1.5416\n",
      "157/388, train_loss: 0.1915, step time: 1.5452\n",
      "158/388, train_loss: 0.3580, step time: 1.5351\n",
      "159/388, train_loss: 0.2852, step time: 1.5336\n",
      "160/388, train_loss: 0.2250, step time: 1.5383\n",
      "161/388, train_loss: 0.1013, step time: 1.5532\n",
      "162/388, train_loss: 0.0901, step time: 1.5602\n",
      "163/388, train_loss: 0.1147, step time: 1.5387\n",
      "164/388, train_loss: 0.1667, step time: 1.5371\n",
      "165/388, train_loss: 0.0596, step time: 1.5429\n",
      "166/388, train_loss: 0.3658, step time: 1.5349\n",
      "167/388, train_loss: 0.4239, step time: 1.5362\n",
      "168/388, train_loss: 0.1921, step time: 1.5394\n",
      "169/388, train_loss: 0.2657, step time: 1.5489\n",
      "170/388, train_loss: 0.1553, step time: 1.5332\n",
      "171/388, train_loss: 0.1322, step time: 1.5307\n",
      "172/388, train_loss: 0.1926, step time: 1.5394\n",
      "173/388, train_loss: 0.3082, step time: 1.5496\n",
      "174/388, train_loss: 0.2868, step time: 1.5346\n",
      "175/388, train_loss: 0.2223, step time: 1.5350\n",
      "176/388, train_loss: 0.1106, step time: 1.5380\n",
      "177/388, train_loss: 0.1043, step time: 1.5426\n",
      "178/388, train_loss: 0.1416, step time: 1.5353\n",
      "179/388, train_loss: 0.2359, step time: 1.5388\n",
      "180/388, train_loss: 0.0571, step time: 1.5414\n",
      "181/388, train_loss: 0.2745, step time: 1.5440\n",
      "182/388, train_loss: 0.1509, step time: 1.5372\n",
      "183/388, train_loss: 0.1613, step time: 1.5378\n",
      "184/388, train_loss: 0.1065, step time: 1.5384\n",
      "185/388, train_loss: 0.1747, step time: 1.5451\n",
      "186/388, train_loss: 0.4746, step time: 1.5387\n",
      "187/388, train_loss: 0.0952, step time: 1.5345\n",
      "188/388, train_loss: 0.3799, step time: 1.5394\n",
      "189/388, train_loss: 0.0816, step time: 1.5404\n",
      "190/388, train_loss: 0.2501, step time: 1.5339\n",
      "191/388, train_loss: 0.4092, step time: 1.5370\n",
      "192/388, train_loss: 0.1406, step time: 1.5376\n",
      "193/388, train_loss: 0.2891, step time: 1.5517\n",
      "194/388, train_loss: 0.1648, step time: 1.5313\n",
      "195/388, train_loss: 0.2582, step time: 1.5366\n",
      "196/388, train_loss: 0.0694, step time: 1.5374\n",
      "197/388, train_loss: 0.3593, step time: 1.5470\n",
      "198/388, train_loss: 0.1069, step time: 1.5322\n",
      "199/388, train_loss: 0.1074, step time: 1.5328\n",
      "200/388, train_loss: 0.2594, step time: 1.5490\n",
      "201/388, train_loss: 0.5047, step time: 1.5466\n",
      "202/388, train_loss: 0.2687, step time: 1.5354\n",
      "203/388, train_loss: 0.2582, step time: 1.5341\n",
      "204/388, train_loss: 0.1566, step time: 1.5370\n",
      "205/388, train_loss: 0.1191, step time: 1.5481\n",
      "206/388, train_loss: 0.4091, step time: 1.5460\n",
      "207/388, train_loss: 0.2007, step time: 1.5368\n",
      "208/388, train_loss: 0.2186, step time: 1.5449\n",
      "209/388, train_loss: 0.3554, step time: 1.5473\n",
      "210/388, train_loss: 0.2605, step time: 1.5336\n",
      "211/388, train_loss: 0.0940, step time: 1.5329\n",
      "212/388, train_loss: 0.1207, step time: 1.5310\n",
      "213/388, train_loss: 0.2017, step time: 1.5507\n",
      "214/388, train_loss: 0.0513, step time: 1.5338\n",
      "215/388, train_loss: 0.1241, step time: 1.5341\n",
      "216/388, train_loss: 0.1145, step time: 1.5349\n",
      "217/388, train_loss: 0.6012, step time: 1.5531\n",
      "218/388, train_loss: 0.2638, step time: 1.5335\n",
      "219/388, train_loss: 0.1267, step time: 1.5354\n",
      "220/388, train_loss: 0.2460, step time: 1.5336\n",
      "221/388, train_loss: 0.4517, step time: 1.5457\n",
      "222/388, train_loss: 0.2011, step time: 1.5333\n",
      "223/388, train_loss: 0.0922, step time: 1.5330\n",
      "224/388, train_loss: 0.2242, step time: 1.5361\n",
      "225/388, train_loss: 0.1142, step time: 1.5500\n",
      "226/388, train_loss: 0.0881, step time: 1.5390\n",
      "227/388, train_loss: 0.1026, step time: 1.5339\n",
      "228/388, train_loss: 0.2551, step time: 1.5457\n",
      "229/388, train_loss: 0.2148, step time: 1.5445\n",
      "230/388, train_loss: 0.2461, step time: 1.5369\n",
      "231/388, train_loss: 0.1425, step time: 1.5345\n",
      "232/388, train_loss: 0.2803, step time: 1.5325\n",
      "233/388, train_loss: 0.3389, step time: 1.5421\n",
      "234/388, train_loss: 0.1477, step time: 1.5357\n",
      "235/388, train_loss: 0.1963, step time: 1.5368\n",
      "236/388, train_loss: 0.2068, step time: 1.5351\n",
      "237/388, train_loss: 0.2813, step time: 1.5426\n",
      "238/388, train_loss: 0.1716, step time: 1.5371\n",
      "239/388, train_loss: 0.0666, step time: 1.5372\n",
      "240/388, train_loss: 0.0856, step time: 1.5379\n",
      "241/388, train_loss: 0.5649, step time: 1.5441\n",
      "242/388, train_loss: 0.0971, step time: 1.5355\n",
      "243/388, train_loss: 0.0796, step time: 1.5393\n",
      "244/388, train_loss: 0.2440, step time: 1.5375\n",
      "245/388, train_loss: 0.5263, step time: 1.5457\n",
      "246/388, train_loss: 0.1258, step time: 1.5322\n",
      "247/388, train_loss: 0.4603, step time: 1.5341\n",
      "248/388, train_loss: 0.4464, step time: 1.5363\n",
      "249/388, train_loss: 0.2241, step time: 1.5467\n",
      "250/388, train_loss: 0.2013, step time: 1.5335\n",
      "251/388, train_loss: 0.1031, step time: 1.5315\n",
      "252/388, train_loss: 0.2390, step time: 1.5359\n",
      "253/388, train_loss: 0.1386, step time: 1.5459\n",
      "254/388, train_loss: 0.2135, step time: 1.5373\n",
      "255/388, train_loss: 0.1488, step time: 1.5349\n",
      "256/388, train_loss: 0.0846, step time: 1.5315\n",
      "257/388, train_loss: 0.1233, step time: 1.5442\n",
      "258/388, train_loss: 0.0999, step time: 1.5371\n",
      "259/388, train_loss: 0.1459, step time: 1.5613\n",
      "260/388, train_loss: 0.1955, step time: 1.5334\n",
      "261/388, train_loss: 0.2019, step time: 1.5486\n",
      "262/388, train_loss: 0.1442, step time: 1.5370\n",
      "263/388, train_loss: 0.0952, step time: 1.5351\n",
      "264/388, train_loss: 0.2054, step time: 1.5334\n",
      "265/388, train_loss: 0.1039, step time: 1.5497\n",
      "266/388, train_loss: 0.1699, step time: 1.5381\n",
      "267/388, train_loss: 0.0689, step time: 1.5328\n",
      "268/388, train_loss: 0.2861, step time: 1.5379\n",
      "269/388, train_loss: 0.1156, step time: 1.5472\n",
      "270/388, train_loss: 0.3635, step time: 1.5337\n",
      "271/388, train_loss: 0.2207, step time: 1.5320\n",
      "272/388, train_loss: 0.1106, step time: 1.5407\n",
      "273/388, train_loss: 0.1136, step time: 1.5463\n",
      "274/388, train_loss: 0.1797, step time: 1.5355\n",
      "275/388, train_loss: 0.1582, step time: 1.5326\n",
      "276/388, train_loss: 0.1613, step time: 1.5325\n",
      "277/388, train_loss: 0.1481, step time: 1.5433\n",
      "278/388, train_loss: 0.0802, step time: 1.5455\n",
      "279/388, train_loss: 0.0599, step time: 1.5458\n",
      "280/388, train_loss: 0.0503, step time: 1.5332\n",
      "281/388, train_loss: 0.1193, step time: 1.5461\n",
      "282/388, train_loss: 0.1447, step time: 1.5332\n",
      "283/388, train_loss: 0.1292, step time: 1.5418\n",
      "284/388, train_loss: 0.1189, step time: 1.5350\n",
      "285/388, train_loss: 0.2570, step time: 1.5547\n",
      "286/388, train_loss: 0.6382, step time: 1.5314\n",
      "287/388, train_loss: 0.0656, step time: 1.5315\n",
      "288/388, train_loss: 0.1985, step time: 1.5393\n",
      "289/388, train_loss: 0.1834, step time: 1.5450\n",
      "290/388, train_loss: 0.2824, step time: 1.5364\n",
      "291/388, train_loss: 0.2375, step time: 1.5340\n",
      "292/388, train_loss: 0.0966, step time: 1.5365\n",
      "293/388, train_loss: 0.1481, step time: 1.5485\n",
      "294/388, train_loss: 0.1521, step time: 1.5364\n",
      "295/388, train_loss: 0.3190, step time: 1.5343\n",
      "296/388, train_loss: 0.1527, step time: 1.5345\n",
      "297/388, train_loss: 0.1414, step time: 1.5459\n",
      "298/388, train_loss: 0.2754, step time: 1.5364\n",
      "299/388, train_loss: 0.1550, step time: 1.5378\n",
      "300/388, train_loss: 0.2298, step time: 1.5328\n",
      "301/388, train_loss: 0.1504, step time: 1.5482\n",
      "302/388, train_loss: 0.4226, step time: 1.5415\n",
      "303/388, train_loss: 0.1139, step time: 1.5328\n",
      "304/388, train_loss: 0.1508, step time: 1.5313\n",
      "305/388, train_loss: 0.1742, step time: 1.5395\n",
      "306/388, train_loss: 0.1624, step time: 1.5381\n",
      "307/388, train_loss: 0.1112, step time: 1.5370\n",
      "308/388, train_loss: 0.2874, step time: 1.5330\n",
      "309/388, train_loss: 0.2231, step time: 1.5424\n",
      "310/388, train_loss: 0.2951, step time: 1.5300\n",
      "311/388, train_loss: 0.1826, step time: 1.5326\n",
      "312/388, train_loss: 0.1225, step time: 1.5399\n",
      "313/388, train_loss: 0.2114, step time: 1.5470\n",
      "314/388, train_loss: 0.1988, step time: 1.5305\n",
      "315/388, train_loss: 0.1677, step time: 1.5304\n",
      "316/388, train_loss: 0.0756, step time: 1.5433\n",
      "317/388, train_loss: 0.4306, step time: 1.5483\n",
      "318/388, train_loss: 0.2672, step time: 1.5321\n",
      "319/388, train_loss: 0.2525, step time: 1.5351\n",
      "320/388, train_loss: 0.0981, step time: 1.5460\n",
      "321/388, train_loss: 0.1853, step time: 1.5445\n",
      "322/388, train_loss: 0.0972, step time: 1.5343\n",
      "323/388, train_loss: 0.1168, step time: 1.5343\n",
      "324/388, train_loss: 0.0569, step time: 1.5459\n",
      "325/388, train_loss: 0.2230, step time: 1.5463\n",
      "326/388, train_loss: 0.3029, step time: 1.5323\n",
      "327/388, train_loss: 0.4660, step time: 1.5324\n",
      "328/388, train_loss: 0.0302, step time: 1.5422\n",
      "329/388, train_loss: 0.5009, step time: 1.5479\n",
      "330/388, train_loss: 0.1189, step time: 1.5327\n",
      "331/388, train_loss: 0.3815, step time: 1.5325\n",
      "332/388, train_loss: 0.1321, step time: 1.5395\n",
      "333/388, train_loss: 0.1647, step time: 1.5446\n",
      "334/388, train_loss: 0.0874, step time: 1.5357\n",
      "335/388, train_loss: 0.0647, step time: 1.5342\n",
      "336/388, train_loss: 0.1014, step time: 1.5410\n",
      "337/388, train_loss: 0.3853, step time: 1.5417\n",
      "338/388, train_loss: 0.1205, step time: 1.5384\n",
      "339/388, train_loss: 0.1874, step time: 1.5367\n",
      "340/388, train_loss: 0.2030, step time: 1.5462\n",
      "341/388, train_loss: 0.2026, step time: 1.5415\n",
      "342/388, train_loss: 0.0817, step time: 1.5359\n",
      "343/388, train_loss: 0.2205, step time: 1.5340\n",
      "344/388, train_loss: 0.0954, step time: 1.5427\n",
      "345/388, train_loss: 0.0968, step time: 1.5444\n",
      "346/388, train_loss: 0.3957, step time: 1.5346\n",
      "347/388, train_loss: 0.1436, step time: 1.5385\n",
      "348/388, train_loss: 0.1706, step time: 1.5463\n",
      "349/388, train_loss: 0.1462, step time: 1.5424\n",
      "350/388, train_loss: 0.1011, step time: 1.5339\n",
      "351/388, train_loss: 0.1362, step time: 1.5380\n",
      "352/388, train_loss: 0.0851, step time: 1.5480\n",
      "353/388, train_loss: 0.2353, step time: 1.5535\n",
      "354/388, train_loss: 0.1882, step time: 1.5361\n",
      "355/388, train_loss: 0.2124, step time: 1.5361\n",
      "356/388, train_loss: 0.1507, step time: 1.5447\n",
      "357/388, train_loss: 0.1758, step time: 1.5405\n",
      "358/388, train_loss: 0.0950, step time: 1.5328\n",
      "359/388, train_loss: 0.3668, step time: 1.5368\n",
      "360/388, train_loss: 0.2331, step time: 1.5458\n",
      "361/388, train_loss: 0.1263, step time: 1.5439\n",
      "362/388, train_loss: 0.2098, step time: 1.5319\n",
      "363/388, train_loss: 0.1505, step time: 1.5347\n",
      "364/388, train_loss: 0.1148, step time: 1.5449\n",
      "365/388, train_loss: 0.1294, step time: 1.5429\n",
      "366/388, train_loss: 0.2453, step time: 1.5345\n",
      "367/388, train_loss: 0.4047, step time: 1.5341\n",
      "368/388, train_loss: 0.1136, step time: 1.5473\n",
      "369/388, train_loss: 0.0799, step time: 1.5541\n",
      "370/388, train_loss: 0.1241, step time: 1.5324\n",
      "371/388, train_loss: 0.2209, step time: 1.5369\n",
      "372/388, train_loss: 0.1145, step time: 1.5450\n",
      "373/388, train_loss: 0.0627, step time: 1.5443\n",
      "374/388, train_loss: 0.0848, step time: 1.5328\n",
      "375/388, train_loss: 0.0702, step time: 1.5623\n",
      "376/388, train_loss: 0.1624, step time: 1.5450\n",
      "377/388, train_loss: 0.2516, step time: 1.5522\n",
      "378/388, train_loss: 0.1625, step time: 1.5375\n",
      "379/388, train_loss: 0.2630, step time: 1.5367\n",
      "380/388, train_loss: 0.2394, step time: 1.5472\n",
      "381/388, train_loss: 0.2002, step time: 1.5416\n",
      "382/388, train_loss: 0.1415, step time: 1.5344\n",
      "383/388, train_loss: 0.2995, step time: 1.5366\n",
      "384/388, train_loss: 0.2181, step time: 1.5447\n",
      "385/388, train_loss: 0.1085, step time: 1.5423\n",
      "386/388, train_loss: 0.1445, step time: 1.5321\n",
      "387/388, train_loss: 0.2079, step time: 1.5324\n",
      "388/388, train_loss: 0.1846, step time: 1.5474\n",
      "epoch 37 average loss: 0.1974\n",
      "current epoch: 37 current mean dice: 0.7632 tc: 0.8125 wt: 0.9011 et: 0.5761\n",
      "best mean dice: 0.7640 at epoch: 36\n",
      "time consuming of epoch 37 is: 703.8389\n",
      "----------\n",
      "epoch 38/100\n",
      "1/388, train_loss: 0.1161, step time: 1.5463\n",
      "2/388, train_loss: 0.2530, step time: 1.5350\n",
      "3/388, train_loss: 0.1450, step time: 1.5338\n",
      "4/388, train_loss: 0.4171, step time: 1.5375\n",
      "5/388, train_loss: 0.2650, step time: 1.5360\n",
      "6/388, train_loss: 0.0976, step time: 1.5378\n",
      "7/388, train_loss: 0.2439, step time: 1.5392\n",
      "8/388, train_loss: 0.2545, step time: 1.5382\n",
      "9/388, train_loss: 0.0977, step time: 1.5338\n",
      "10/388, train_loss: 0.3351, step time: 1.5372\n",
      "11/388, train_loss: 0.2624, step time: 1.5329\n",
      "12/388, train_loss: 0.2845, step time: 1.5303\n",
      "13/388, train_loss: 0.3115, step time: 1.5327\n",
      "14/388, train_loss: 0.2249, step time: 1.5654\n",
      "15/388, train_loss: 0.2687, step time: 1.5591\n",
      "16/388, train_loss: 0.1023, step time: 1.5610\n",
      "17/388, train_loss: 0.3321, step time: 1.5285\n",
      "18/388, train_loss: 0.3416, step time: 1.5330\n",
      "19/388, train_loss: 0.2225, step time: 1.5357\n",
      "20/388, train_loss: 0.1128, step time: 1.5375\n",
      "21/388, train_loss: 0.1728, step time: 1.5362\n",
      "22/388, train_loss: 0.2229, step time: 1.5352\n",
      "23/388, train_loss: 0.2280, step time: 1.5443\n",
      "24/388, train_loss: 0.2046, step time: 1.5380\n",
      "25/388, train_loss: 0.2257, step time: 1.5333\n",
      "26/388, train_loss: 0.2936, step time: 1.5341\n",
      "27/388, train_loss: 0.0893, step time: 1.5321\n",
      "28/388, train_loss: 0.2154, step time: 1.5342\n",
      "29/388, train_loss: 0.0927, step time: 1.5293\n",
      "30/388, train_loss: 0.1734, step time: 1.5357\n",
      "31/388, train_loss: 0.2584, step time: 1.5350\n",
      "32/388, train_loss: 0.2335, step time: 1.5357\n",
      "33/388, train_loss: 0.1087, step time: 1.5338\n",
      "34/388, train_loss: 0.1649, step time: 1.5337\n",
      "35/388, train_loss: 0.1114, step time: 1.5330\n",
      "36/388, train_loss: 0.1829, step time: 1.5336\n",
      "37/388, train_loss: 0.1939, step time: 1.5343\n",
      "38/388, train_loss: 0.2138, step time: 1.5352\n",
      "39/388, train_loss: 0.1755, step time: 1.5351\n",
      "40/388, train_loss: 0.0869, step time: 1.5323\n",
      "41/388, train_loss: 0.1205, step time: 1.5321\n",
      "42/388, train_loss: 0.1442, step time: 1.5321\n",
      "43/388, train_loss: 0.2446, step time: 1.5321\n",
      "44/388, train_loss: 0.1058, step time: 1.5342\n",
      "45/388, train_loss: 0.1502, step time: 1.5337\n",
      "46/388, train_loss: 0.1816, step time: 1.5366\n",
      "47/388, train_loss: 0.1763, step time: 1.5324\n",
      "48/388, train_loss: 0.2690, step time: 1.5320\n",
      "49/388, train_loss: 0.1325, step time: 1.5311\n",
      "50/388, train_loss: 0.1167, step time: 1.5308\n",
      "51/388, train_loss: 0.1821, step time: 1.5346\n",
      "52/388, train_loss: 0.1755, step time: 1.5347\n",
      "53/388, train_loss: 0.0900, step time: 1.5369\n",
      "54/388, train_loss: 0.1405, step time: 1.5331\n",
      "55/388, train_loss: 0.2204, step time: 1.5326\n",
      "56/388, train_loss: 0.0974, step time: 1.5315\n",
      "57/388, train_loss: 0.1876, step time: 1.5297\n",
      "58/388, train_loss: 0.1659, step time: 1.5368\n",
      "59/388, train_loss: 0.4193, step time: 1.5351\n",
      "60/388, train_loss: 0.2132, step time: 1.5367\n",
      "61/388, train_loss: 0.1078, step time: 1.5323\n",
      "62/388, train_loss: 0.4016, step time: 1.5344\n",
      "63/388, train_loss: 0.2309, step time: 1.5324\n",
      "64/388, train_loss: 0.1057, step time: 1.5322\n",
      "65/388, train_loss: 0.1220, step time: 1.5406\n",
      "66/388, train_loss: 0.5285, step time: 1.5345\n",
      "67/388, train_loss: 0.1125, step time: 1.5331\n",
      "68/388, train_loss: 0.3450, step time: 1.5294\n",
      "69/388, train_loss: 0.2750, step time: 1.5321\n",
      "70/388, train_loss: 0.4019, step time: 1.5389\n",
      "71/388, train_loss: 0.1282, step time: 1.5377\n",
      "72/388, train_loss: 0.3394, step time: 1.5356\n",
      "73/388, train_loss: 0.2474, step time: 1.5320\n",
      "74/388, train_loss: 0.0927, step time: 1.5367\n",
      "75/388, train_loss: 0.2984, step time: 1.5325\n",
      "76/388, train_loss: 0.2875, step time: 1.5334\n",
      "77/388, train_loss: 0.2119, step time: 1.5322\n",
      "78/388, train_loss: 0.1047, step time: 1.5373\n",
      "79/388, train_loss: 0.4913, step time: 1.5368\n",
      "80/388, train_loss: 0.0680, step time: 1.5349\n",
      "81/388, train_loss: 0.1818, step time: 1.5349\n",
      "82/388, train_loss: 0.1850, step time: 1.5323\n",
      "83/388, train_loss: 0.1314, step time: 1.5348\n",
      "84/388, train_loss: 0.1833, step time: 1.5352\n",
      "85/388, train_loss: 0.1959, step time: 1.5344\n",
      "86/388, train_loss: 0.1758, step time: 1.5324\n",
      "87/388, train_loss: 0.0510, step time: 1.5352\n",
      "88/388, train_loss: 0.4404, step time: 1.5335\n",
      "89/388, train_loss: 0.1310, step time: 1.5323\n",
      "90/388, train_loss: 0.0559, step time: 1.5389\n",
      "91/388, train_loss: 0.1652, step time: 1.5375\n",
      "92/388, train_loss: 0.0641, step time: 1.5382\n",
      "93/388, train_loss: 0.3926, step time: 1.5333\n",
      "94/388, train_loss: 0.1047, step time: 1.5317\n",
      "95/388, train_loss: 0.5108, step time: 1.5296\n",
      "96/388, train_loss: 0.0674, step time: 1.5343\n",
      "97/388, train_loss: 0.1199, step time: 1.5355\n",
      "98/388, train_loss: 0.1083, step time: 1.5365\n",
      "99/388, train_loss: 0.3109, step time: 1.5328\n",
      "100/388, train_loss: 0.1622, step time: 1.5357\n",
      "101/388, train_loss: 0.1759, step time: 1.5324\n",
      "102/388, train_loss: 0.3905, step time: 1.5327\n",
      "103/388, train_loss: 0.2977, step time: 1.5333\n",
      "104/388, train_loss: 0.1123, step time: 1.5374\n",
      "105/388, train_loss: 0.1462, step time: 1.5350\n",
      "106/388, train_loss: 0.4025, step time: 1.5331\n",
      "107/388, train_loss: 0.3719, step time: 1.5322\n",
      "108/388, train_loss: 0.2093, step time: 1.5318\n",
      "109/388, train_loss: 0.0638, step time: 1.5381\n",
      "110/388, train_loss: 0.1974, step time: 1.5346\n",
      "111/388, train_loss: 0.2046, step time: 1.5311\n",
      "112/388, train_loss: 0.1946, step time: 1.5309\n",
      "113/388, train_loss: 0.1372, step time: 1.5329\n",
      "114/388, train_loss: 0.1002, step time: 1.5312\n",
      "115/388, train_loss: 0.2227, step time: 1.5356\n",
      "116/388, train_loss: 0.1915, step time: 1.5375\n",
      "117/388, train_loss: 0.0726, step time: 1.5332\n",
      "118/388, train_loss: 0.1253, step time: 1.5298\n",
      "119/388, train_loss: 0.4345, step time: 1.5338\n",
      "120/388, train_loss: 0.1904, step time: 1.5424\n",
      "121/388, train_loss: 0.0479, step time: 1.5352\n",
      "122/388, train_loss: 0.3039, step time: 1.5337\n",
      "123/388, train_loss: 0.2744, step time: 1.5351\n",
      "124/388, train_loss: 0.0802, step time: 1.5323\n",
      "125/388, train_loss: 0.2880, step time: 1.5323\n",
      "126/388, train_loss: 0.0988, step time: 1.5331\n",
      "127/388, train_loss: 0.1762, step time: 1.5347\n",
      "128/388, train_loss: 0.3423, step time: 1.5394\n",
      "129/388, train_loss: 0.1992, step time: 1.5316\n",
      "130/388, train_loss: 0.1166, step time: 1.5341\n",
      "131/388, train_loss: 0.1140, step time: 1.5354\n",
      "132/388, train_loss: 0.2798, step time: 1.5359\n",
      "133/388, train_loss: 0.2779, step time: 1.5366\n",
      "134/388, train_loss: 0.2441, step time: 1.5372\n",
      "135/388, train_loss: 0.5694, step time: 1.5359\n",
      "136/388, train_loss: 0.1819, step time: 1.5365\n",
      "137/388, train_loss: 0.1082, step time: 1.5343\n",
      "138/388, train_loss: 0.1052, step time: 1.5614\n",
      "139/388, train_loss: 0.4344, step time: 1.5287\n",
      "140/388, train_loss: 0.0655, step time: 1.5293\n",
      "141/388, train_loss: 0.0381, step time: 1.5319\n",
      "142/388, train_loss: 0.2304, step time: 1.5322\n",
      "143/388, train_loss: 0.3634, step time: 1.5371\n",
      "144/388, train_loss: 0.1688, step time: 1.5337\n",
      "145/388, train_loss: 0.3201, step time: 1.5322\n",
      "146/388, train_loss: 0.2180, step time: 1.5332\n",
      "147/388, train_loss: 0.0649, step time: 1.5337\n",
      "148/388, train_loss: 0.6012, step time: 1.5340\n",
      "149/388, train_loss: 0.2119, step time: 1.5368\n",
      "150/388, train_loss: 0.2406, step time: 1.5378\n",
      "151/388, train_loss: 0.0825, step time: 1.5317\n",
      "152/388, train_loss: 0.1435, step time: 1.5291\n",
      "153/388, train_loss: 0.0890, step time: 1.5332\n",
      "154/388, train_loss: 0.2113, step time: 1.5315\n",
      "155/388, train_loss: 0.0697, step time: 1.5338\n",
      "156/388, train_loss: 0.4010, step time: 1.5354\n",
      "157/388, train_loss: 0.1350, step time: 1.5318\n",
      "158/388, train_loss: 0.4626, step time: 1.5321\n",
      "159/388, train_loss: 0.4351, step time: 1.5322\n",
      "160/388, train_loss: 0.0895, step time: 1.5303\n",
      "161/388, train_loss: 0.1592, step time: 1.5343\n",
      "162/388, train_loss: 0.1100, step time: 1.5358\n",
      "163/388, train_loss: 0.0690, step time: 1.5368\n",
      "164/388, train_loss: 0.2654, step time: 1.5344\n",
      "165/388, train_loss: 0.0725, step time: 1.5322\n",
      "166/388, train_loss: 0.1616, step time: 1.5325\n",
      "167/388, train_loss: 0.0988, step time: 1.5327\n",
      "168/388, train_loss: 0.3462, step time: 1.5375\n",
      "169/388, train_loss: 0.1226, step time: 1.5374\n",
      "170/388, train_loss: 0.1131, step time: 1.5333\n",
      "171/388, train_loss: 0.2880, step time: 1.5317\n",
      "172/388, train_loss: 0.0671, step time: 1.5336\n",
      "173/388, train_loss: 0.1053, step time: 1.5367\n",
      "174/388, train_loss: 0.2092, step time: 1.5361\n",
      "175/388, train_loss: 0.2594, step time: 1.5380\n",
      "176/388, train_loss: 0.0962, step time: 1.5332\n",
      "177/388, train_loss: 0.2026, step time: 1.5313\n",
      "178/388, train_loss: 0.2287, step time: 1.5361\n",
      "179/388, train_loss: 0.2725, step time: 1.5363\n",
      "180/388, train_loss: 0.1870, step time: 1.5538\n",
      "181/388, train_loss: 0.1792, step time: 1.5346\n",
      "182/388, train_loss: 0.0963, step time: 1.5323\n",
      "183/388, train_loss: 0.1225, step time: 1.5345\n",
      "184/388, train_loss: 0.1339, step time: 1.5345\n",
      "185/388, train_loss: 0.3195, step time: 1.5352\n",
      "186/388, train_loss: 0.1673, step time: 1.5364\n",
      "187/388, train_loss: 0.1534, step time: 1.5303\n",
      "188/388, train_loss: 0.2412, step time: 1.5380\n",
      "189/388, train_loss: 0.0512, step time: 1.5371\n",
      "190/388, train_loss: 0.1421, step time: 1.5345\n",
      "191/388, train_loss: 0.5838, step time: 1.5361\n",
      "192/388, train_loss: 0.2301, step time: 1.5314\n",
      "193/388, train_loss: 0.1413, step time: 1.5444\n",
      "194/388, train_loss: 0.2051, step time: 1.5371\n",
      "195/388, train_loss: 0.0660, step time: 1.5384\n",
      "196/388, train_loss: 0.0987, step time: 1.5312\n",
      "197/388, train_loss: 0.3854, step time: 1.5342\n",
      "198/388, train_loss: 0.2413, step time: 1.5470\n",
      "199/388, train_loss: 0.1281, step time: 1.5380\n",
      "200/388, train_loss: 0.1213, step time: 1.5423\n",
      "201/388, train_loss: 0.0522, step time: 1.5349\n",
      "202/388, train_loss: 0.1362, step time: 1.5381\n",
      "203/388, train_loss: 0.1506, step time: 1.5380\n",
      "204/388, train_loss: 0.1802, step time: 1.5364\n",
      "205/388, train_loss: 0.2125, step time: 1.5317\n",
      "206/388, train_loss: 0.1153, step time: 1.5318\n",
      "207/388, train_loss: 0.2358, step time: 1.5474\n",
      "208/388, train_loss: 0.1187, step time: 1.5348\n",
      "209/388, train_loss: 0.1513, step time: 1.5378\n",
      "210/388, train_loss: 0.2087, step time: 1.5330\n",
      "211/388, train_loss: 0.3412, step time: 1.5310\n",
      "212/388, train_loss: 0.1483, step time: 1.5333\n",
      "213/388, train_loss: 0.1918, step time: 1.5380\n",
      "214/388, train_loss: 0.3384, step time: 1.5333\n",
      "215/388, train_loss: 0.0992, step time: 1.5323\n",
      "216/388, train_loss: 0.1463, step time: 1.5313\n",
      "217/388, train_loss: 0.1275, step time: 1.5392\n",
      "218/388, train_loss: 0.1277, step time: 1.5326\n",
      "219/388, train_loss: 0.3593, step time: 1.5334\n",
      "220/388, train_loss: 0.1628, step time: 1.5325\n",
      "221/388, train_loss: 0.0690, step time: 1.5323\n",
      "222/388, train_loss: 0.2054, step time: 1.5345\n",
      "223/388, train_loss: 0.1186, step time: 1.5375\n",
      "224/388, train_loss: 0.0912, step time: 1.5333\n",
      "225/388, train_loss: 0.2583, step time: 1.5318\n",
      "226/388, train_loss: 0.1836, step time: 1.5312\n",
      "227/388, train_loss: 0.0900, step time: 1.5321\n",
      "228/388, train_loss: 0.0850, step time: 1.5356\n",
      "229/388, train_loss: 0.1422, step time: 1.5358\n",
      "230/388, train_loss: 0.1884, step time: 1.5364\n",
      "231/388, train_loss: 0.0746, step time: 1.5310\n",
      "232/388, train_loss: 0.1095, step time: 1.5332\n",
      "233/388, train_loss: 0.1912, step time: 1.5346\n",
      "234/388, train_loss: 0.1292, step time: 1.5339\n",
      "235/388, train_loss: 0.0942, step time: 1.5653\n",
      "236/388, train_loss: 0.2379, step time: 1.5344\n",
      "237/388, train_loss: 0.1338, step time: 1.5340\n",
      "238/388, train_loss: 0.0670, step time: 1.5320\n",
      "239/388, train_loss: 0.1059, step time: 1.5321\n",
      "240/388, train_loss: 0.0686, step time: 1.5348\n",
      "241/388, train_loss: 0.1683, step time: 1.5478\n",
      "242/388, train_loss: 0.0490, step time: 1.5325\n",
      "243/388, train_loss: 0.0919, step time: 1.5331\n",
      "244/388, train_loss: 0.2025, step time: 1.5349\n",
      "245/388, train_loss: 0.1091, step time: 1.5487\n",
      "246/388, train_loss: 0.1863, step time: 1.5339\n",
      "247/388, train_loss: 0.2502, step time: 1.5324\n",
      "248/388, train_loss: 0.7141, step time: 1.5336\n",
      "249/388, train_loss: 0.1063, step time: 1.5436\n",
      "250/388, train_loss: 0.1689, step time: 1.5364\n",
      "251/388, train_loss: 0.0285, step time: 1.5322\n",
      "252/388, train_loss: 0.0952, step time: 1.5360\n",
      "253/388, train_loss: 0.1062, step time: 1.5443\n",
      "254/388, train_loss: 0.1329, step time: 1.5352\n",
      "255/388, train_loss: 0.1108, step time: 1.5358\n",
      "256/388, train_loss: 0.4210, step time: 1.5331\n",
      "257/388, train_loss: 0.2217, step time: 1.5412\n",
      "258/388, train_loss: 0.1217, step time: 1.5352\n",
      "259/388, train_loss: 0.0671, step time: 1.5365\n",
      "260/388, train_loss: 0.1891, step time: 1.5336\n",
      "261/388, train_loss: 0.3423, step time: 1.5391\n",
      "262/388, train_loss: 0.1417, step time: 1.5316\n",
      "263/388, train_loss: 0.0733, step time: 1.5506\n",
      "264/388, train_loss: 0.5219, step time: 1.5339\n",
      "265/388, train_loss: 0.1422, step time: 1.5454\n",
      "266/388, train_loss: 0.2886, step time: 1.5331\n",
      "267/388, train_loss: 0.0601, step time: 1.5338\n",
      "268/388, train_loss: 0.0919, step time: 1.5377\n",
      "269/388, train_loss: 0.2093, step time: 1.5481\n",
      "270/388, train_loss: 0.1127, step time: 1.5333\n",
      "271/388, train_loss: 0.2212, step time: 1.5334\n",
      "272/388, train_loss: 0.2648, step time: 1.5339\n",
      "273/388, train_loss: 0.0817, step time: 1.5447\n",
      "274/388, train_loss: 0.1305, step time: 1.5380\n",
      "275/388, train_loss: 0.1922, step time: 1.5318\n",
      "276/388, train_loss: 0.1139, step time: 1.5347\n",
      "277/388, train_loss: 0.0739, step time: 1.5535\n",
      "278/388, train_loss: 0.1218, step time: 1.5348\n",
      "279/388, train_loss: 0.1270, step time: 1.5607\n",
      "280/388, train_loss: 0.1986, step time: 1.5364\n",
      "281/388, train_loss: 0.1710, step time: 1.5469\n",
      "282/388, train_loss: 0.2110, step time: 1.5300\n",
      "283/388, train_loss: 0.1974, step time: 1.5325\n",
      "284/388, train_loss: 0.1000, step time: 1.5333\n",
      "285/388, train_loss: 0.3563, step time: 1.5468\n",
      "286/388, train_loss: 0.1160, step time: 1.5375\n",
      "287/388, train_loss: 0.2695, step time: 1.5338\n",
      "288/388, train_loss: 0.2010, step time: 1.5308\n",
      "289/388, train_loss: 0.1511, step time: 1.5463\n",
      "290/388, train_loss: 0.4712, step time: 1.5366\n",
      "291/388, train_loss: 0.0635, step time: 1.5340\n",
      "292/388, train_loss: 0.2873, step time: 1.5339\n",
      "293/388, train_loss: 0.1097, step time: 1.5372\n",
      "294/388, train_loss: 0.2928, step time: 1.5336\n",
      "295/388, train_loss: 0.1709, step time: 1.5359\n",
      "296/388, train_loss: 0.2234, step time: 1.5374\n",
      "297/388, train_loss: 0.1602, step time: 1.5452\n",
      "298/388, train_loss: 0.1624, step time: 1.5347\n",
      "299/388, train_loss: 0.1473, step time: 1.5347\n",
      "300/388, train_loss: 0.2235, step time: 1.5374\n",
      "301/388, train_loss: 0.4027, step time: 1.5477\n",
      "302/388, train_loss: 0.1074, step time: 1.5317\n",
      "303/388, train_loss: 0.2545, step time: 1.5335\n",
      "304/388, train_loss: 0.0612, step time: 1.5337\n",
      "305/388, train_loss: 0.0670, step time: 1.5492\n",
      "306/388, train_loss: 0.0506, step time: 1.5383\n",
      "307/388, train_loss: 0.2183, step time: 1.5327\n",
      "308/388, train_loss: 0.1481, step time: 1.5345\n",
      "309/388, train_loss: 0.1196, step time: 1.5411\n",
      "310/388, train_loss: 0.3074, step time: 1.5386\n",
      "311/388, train_loss: 0.0933, step time: 1.5345\n",
      "312/388, train_loss: 0.3999, step time: 1.5332\n",
      "313/388, train_loss: 0.2270, step time: 1.5406\n",
      "314/388, train_loss: 0.1097, step time: 1.5363\n",
      "315/388, train_loss: 0.1778, step time: 1.5361\n",
      "316/388, train_loss: 0.1065, step time: 1.5337\n",
      "317/388, train_loss: 0.0994, step time: 1.5426\n",
      "318/388, train_loss: 0.0921, step time: 1.5325\n",
      "319/388, train_loss: 0.1487, step time: 1.5337\n",
      "320/388, train_loss: 0.2623, step time: 1.5327\n",
      "321/388, train_loss: 0.2685, step time: 1.5419\n",
      "322/388, train_loss: 0.0917, step time: 1.5370\n",
      "323/388, train_loss: 0.2825, step time: 1.5324\n",
      "324/388, train_loss: 0.5284, step time: 1.5333\n",
      "325/388, train_loss: 0.1426, step time: 1.5376\n",
      "326/388, train_loss: 0.1396, step time: 1.5347\n",
      "327/388, train_loss: 0.1724, step time: 1.5357\n",
      "328/388, train_loss: 0.1978, step time: 1.5360\n",
      "329/388, train_loss: 0.0911, step time: 1.5417\n",
      "330/388, train_loss: 0.2422, step time: 1.5303\n",
      "331/388, train_loss: 0.1730, step time: 1.5331\n",
      "332/388, train_loss: 0.0960, step time: 1.5366\n",
      "333/388, train_loss: 0.1419, step time: 1.5442\n",
      "334/388, train_loss: 0.0964, step time: 1.5324\n",
      "335/388, train_loss: 0.2181, step time: 1.5324\n",
      "336/388, train_loss: 0.1060, step time: 1.5337\n",
      "337/388, train_loss: 0.0920, step time: 1.5458\n",
      "338/388, train_loss: 0.5563, step time: 1.5494\n",
      "339/388, train_loss: 0.1970, step time: 1.5322\n",
      "340/388, train_loss: 0.1140, step time: 1.5316\n",
      "341/388, train_loss: 0.3354, step time: 1.5391\n",
      "342/388, train_loss: 0.2019, step time: 1.5345\n",
      "343/388, train_loss: 0.3741, step time: 1.5358\n",
      "344/388, train_loss: 0.2381, step time: 1.5362\n",
      "345/388, train_loss: 0.6538, step time: 1.5400\n",
      "346/388, train_loss: 0.2419, step time: 1.5347\n",
      "347/388, train_loss: 0.1475, step time: 1.5377\n",
      "348/388, train_loss: 0.1629, step time: 1.5381\n",
      "349/388, train_loss: 0.6937, step time: 1.5465\n",
      "350/388, train_loss: 0.0933, step time: 1.5549\n",
      "351/388, train_loss: 0.1691, step time: 1.5458\n",
      "352/388, train_loss: 0.2616, step time: 1.5360\n",
      "353/388, train_loss: 0.4454, step time: 1.5435\n",
      "354/388, train_loss: 0.2031, step time: 1.5395\n",
      "355/388, train_loss: 0.2345, step time: 1.5538\n",
      "356/388, train_loss: 0.2339, step time: 1.5343\n",
      "357/388, train_loss: 0.2852, step time: 1.5456\n",
      "358/388, train_loss: 0.3693, step time: 1.5358\n",
      "359/388, train_loss: 0.1269, step time: 1.5326\n",
      "360/388, train_loss: 0.2271, step time: 1.5362\n",
      "361/388, train_loss: 0.1368, step time: 1.5467\n",
      "362/388, train_loss: 0.5535, step time: 1.5355\n",
      "363/388, train_loss: 0.2955, step time: 1.5330\n",
      "364/388, train_loss: 0.1014, step time: 1.5323\n",
      "365/388, train_loss: 0.0907, step time: 1.5414\n",
      "366/388, train_loss: 0.1274, step time: 1.5392\n",
      "367/388, train_loss: 0.2182, step time: 1.5381\n",
      "368/388, train_loss: 0.0856, step time: 1.5339\n",
      "369/388, train_loss: 0.2094, step time: 1.5410\n",
      "370/388, train_loss: 0.2048, step time: 1.5343\n",
      "371/388, train_loss: 0.2202, step time: 1.5359\n",
      "372/388, train_loss: 0.3081, step time: 1.5354\n",
      "373/388, train_loss: 0.1280, step time: 1.5404\n",
      "374/388, train_loss: 0.0891, step time: 1.5308\n",
      "375/388, train_loss: 0.2522, step time: 1.5290\n",
      "376/388, train_loss: 0.2096, step time: 1.5371\n",
      "377/388, train_loss: 0.1240, step time: 1.5485\n",
      "378/388, train_loss: 0.2893, step time: 1.5309\n",
      "379/388, train_loss: 0.2758, step time: 1.5342\n",
      "380/388, train_loss: 0.1411, step time: 1.5330\n",
      "381/388, train_loss: 0.1018, step time: 1.5474\n",
      "382/388, train_loss: 0.3929, step time: 1.5368\n",
      "383/388, train_loss: 0.1718, step time: 1.5297\n",
      "384/388, train_loss: 0.1975, step time: 1.5342\n",
      "385/388, train_loss: 0.2944, step time: 1.5460\n",
      "386/388, train_loss: 0.3643, step time: 1.5348\n",
      "387/388, train_loss: 0.2730, step time: 1.5341\n",
      "388/388, train_loss: 0.2847, step time: 1.5329\n",
      "epoch 38 average loss: 0.1997\n",
      "current epoch: 38 current mean dice: 0.7561 tc: 0.8075 wt: 0.8981 et: 0.5626\n",
      "best mean dice: 0.7640 at epoch: 36\n",
      "time consuming of epoch 38 is: 703.1672\n",
      "----------\n",
      "epoch 39/100\n",
      "1/388, train_loss: 0.5850, step time: 1.5469\n",
      "2/388, train_loss: 0.2633, step time: 1.5426\n",
      "3/388, train_loss: 0.1940, step time: 1.5341\n",
      "4/388, train_loss: 0.3576, step time: 1.5437\n",
      "5/388, train_loss: 0.5587, step time: 1.5438\n",
      "6/388, train_loss: 0.3212, step time: 1.5318\n",
      "7/388, train_loss: 0.4474, step time: 1.5331\n",
      "8/388, train_loss: 0.1880, step time: 1.5333\n",
      "9/388, train_loss: 0.0848, step time: 1.5310\n",
      "10/388, train_loss: 0.2255, step time: 1.5385\n",
      "11/388, train_loss: 0.5773, step time: 1.5383\n",
      "12/388, train_loss: 0.1935, step time: 1.5382\n",
      "13/388, train_loss: 0.0718, step time: 1.5358\n",
      "14/388, train_loss: 0.0836, step time: 1.5345\n",
      "15/388, train_loss: 0.1529, step time: 1.5317\n",
      "16/388, train_loss: 0.0881, step time: 1.5341\n",
      "17/388, train_loss: 0.1173, step time: 1.5350\n",
      "18/388, train_loss: 0.2099, step time: 1.5387\n",
      "19/388, train_loss: 0.2459, step time: 1.5311\n",
      "20/388, train_loss: 0.1743, step time: 1.5343\n",
      "21/388, train_loss: 0.2088, step time: 1.5330\n",
      "22/388, train_loss: 0.1604, step time: 1.5359\n",
      "23/388, train_loss: 0.3758, step time: 1.5351\n",
      "24/388, train_loss: 0.1987, step time: 1.5316\n",
      "25/388, train_loss: 0.2338, step time: 1.5342\n",
      "26/388, train_loss: 0.5356, step time: 1.5363\n",
      "27/388, train_loss: 0.4307, step time: 1.5381\n",
      "28/388, train_loss: 0.2413, step time: 1.5358\n",
      "29/388, train_loss: 0.1851, step time: 1.5330\n",
      "30/388, train_loss: 0.1517, step time: 1.5351\n",
      "31/388, train_loss: 0.0725, step time: 1.5318\n",
      "32/388, train_loss: 0.1791, step time: 1.5319\n",
      "33/388, train_loss: 0.2588, step time: 1.5419\n",
      "34/388, train_loss: 0.1855, step time: 1.5331\n",
      "35/388, train_loss: 0.1328, step time: 1.5356\n",
      "36/388, train_loss: 0.0802, step time: 1.5353\n",
      "37/388, train_loss: 0.1102, step time: 1.5333\n",
      "38/388, train_loss: 0.3416, step time: 1.5332\n",
      "39/388, train_loss: 0.1817, step time: 1.5291\n",
      "40/388, train_loss: 0.1758, step time: 1.5331\n",
      "41/388, train_loss: 0.3181, step time: 1.5363\n",
      "42/388, train_loss: 0.1010, step time: 1.5377\n",
      "43/388, train_loss: 0.1254, step time: 1.5362\n",
      "44/388, train_loss: 0.2508, step time: 1.5353\n",
      "45/388, train_loss: 0.2418, step time: 1.5296\n",
      "46/388, train_loss: 0.1455, step time: 1.5319\n",
      "47/388, train_loss: 0.1292, step time: 1.5309\n",
      "48/388, train_loss: 0.2543, step time: 1.5389\n",
      "49/388, train_loss: 0.1132, step time: 1.5380\n",
      "50/388, train_loss: 0.2299, step time: 1.5352\n",
      "51/388, train_loss: 0.0975, step time: 1.5330\n",
      "52/388, train_loss: 0.1408, step time: 1.5456\n",
      "53/388, train_loss: 0.1666, step time: 1.5347\n",
      "54/388, train_loss: 0.1552, step time: 1.5352\n",
      "55/388, train_loss: 0.1700, step time: 1.5335\n",
      "56/388, train_loss: 0.0514, step time: 1.5351\n",
      "57/388, train_loss: 0.0976, step time: 1.5324\n",
      "58/388, train_loss: 0.1200, step time: 1.5341\n",
      "59/388, train_loss: 0.3357, step time: 1.5369\n",
      "60/388, train_loss: 0.1835, step time: 1.5387\n",
      "61/388, train_loss: 0.1000, step time: 1.5330\n",
      "62/388, train_loss: 0.1921, step time: 1.5312\n",
      "63/388, train_loss: 0.1141, step time: 1.5299\n",
      "64/388, train_loss: 0.2416, step time: 1.5304\n",
      "65/388, train_loss: 0.1616, step time: 1.5303\n",
      "66/388, train_loss: 0.0956, step time: 1.5321\n",
      "67/388, train_loss: 0.2367, step time: 1.5359\n",
      "68/388, train_loss: 0.2248, step time: 1.5465\n",
      "69/388, train_loss: 0.0998, step time: 1.5341\n",
      "70/388, train_loss: 0.1072, step time: 1.5313\n",
      "71/388, train_loss: 0.1026, step time: 1.5323\n",
      "72/388, train_loss: 0.2140, step time: 1.5389\n",
      "73/388, train_loss: 0.1694, step time: 1.5357\n",
      "74/388, train_loss: 0.0888, step time: 1.5339\n",
      "75/388, train_loss: 0.2232, step time: 1.5317\n",
      "76/388, train_loss: 0.0528, step time: 1.5299\n",
      "77/388, train_loss: 0.2150, step time: 1.5314\n",
      "78/388, train_loss: 0.1246, step time: 1.5340\n",
      "79/388, train_loss: 0.2146, step time: 1.5346\n",
      "80/388, train_loss: 0.2707, step time: 1.5353\n",
      "81/388, train_loss: 0.1744, step time: 1.5338\n",
      "82/388, train_loss: 0.2935, step time: 1.5305\n",
      "83/388, train_loss: 0.2237, step time: 1.5336\n",
      "84/388, train_loss: 0.1899, step time: 1.5296\n",
      "85/388, train_loss: 0.1589, step time: 1.5329\n",
      "86/388, train_loss: 0.1952, step time: 1.5376\n",
      "87/388, train_loss: 0.3460, step time: 1.5362\n",
      "88/388, train_loss: 0.2536, step time: 1.5342\n",
      "89/388, train_loss: 0.2547, step time: 1.5350\n",
      "90/388, train_loss: 0.1643, step time: 1.5323\n",
      "91/388, train_loss: 0.2225, step time: 1.5303\n",
      "92/388, train_loss: 0.2909, step time: 1.5350\n",
      "93/388, train_loss: 0.2521, step time: 1.5328\n",
      "94/388, train_loss: 0.1276, step time: 1.5368\n",
      "95/388, train_loss: 0.4246, step time: 1.5354\n",
      "96/388, train_loss: 0.0450, step time: 1.5335\n",
      "97/388, train_loss: 0.2203, step time: 1.5335\n",
      "98/388, train_loss: 0.2332, step time: 1.5471\n",
      "99/388, train_loss: 0.1262, step time: 1.5353\n",
      "100/388, train_loss: 0.1803, step time: 1.5348\n",
      "101/388, train_loss: 0.3109, step time: 1.5332\n",
      "102/388, train_loss: 0.4386, step time: 1.5330\n",
      "103/388, train_loss: 0.1486, step time: 1.5314\n",
      "104/388, train_loss: 0.1444, step time: 1.5317\n",
      "105/388, train_loss: 0.1175, step time: 1.5334\n",
      "106/388, train_loss: 0.0363, step time: 1.5365\n",
      "107/388, train_loss: 0.0578, step time: 1.5339\n",
      "108/388, train_loss: 0.1969, step time: 1.5316\n",
      "109/388, train_loss: 0.2636, step time: 1.5328\n",
      "110/388, train_loss: 0.3713, step time: 1.5305\n",
      "111/388, train_loss: 0.3437, step time: 1.5333\n",
      "112/388, train_loss: 0.1299, step time: 1.5372\n",
      "113/388, train_loss: 0.3548, step time: 1.5373\n",
      "114/388, train_loss: 0.2184, step time: 1.5371\n",
      "115/388, train_loss: 0.1051, step time: 1.5317\n",
      "116/388, train_loss: 0.2270, step time: 1.5332\n",
      "117/388, train_loss: 0.0719, step time: 1.5341\n",
      "118/388, train_loss: 0.1771, step time: 1.5377\n",
      "119/388, train_loss: 0.2080, step time: 1.5329\n",
      "120/388, train_loss: 0.1496, step time: 1.5338\n",
      "121/388, train_loss: 0.2035, step time: 1.5310\n",
      "122/388, train_loss: 0.2231, step time: 1.5321\n",
      "123/388, train_loss: 0.1639, step time: 1.5330\n",
      "124/388, train_loss: 0.3034, step time: 1.5367\n",
      "125/388, train_loss: 0.1264, step time: 1.5350\n",
      "126/388, train_loss: 0.2022, step time: 1.5359\n",
      "127/388, train_loss: 0.4680, step time: 1.5345\n",
      "128/388, train_loss: 0.1493, step time: 1.5339\n",
      "129/388, train_loss: 0.2702, step time: 1.5329\n",
      "130/388, train_loss: 0.4409, step time: 1.5366\n",
      "131/388, train_loss: 0.1037, step time: 1.5364\n",
      "132/388, train_loss: 0.1342, step time: 1.5355\n",
      "133/388, train_loss: 0.2183, step time: 1.5351\n",
      "134/388, train_loss: 0.2592, step time: 1.5350\n",
      "135/388, train_loss: 0.3433, step time: 1.5304\n",
      "136/388, train_loss: 0.0945, step time: 1.5275\n",
      "137/388, train_loss: 0.1055, step time: 1.5344\n",
      "138/388, train_loss: 0.3013, step time: 1.5382\n",
      "139/388, train_loss: 0.0846, step time: 1.5373\n",
      "140/388, train_loss: 0.1504, step time: 1.5318\n",
      "141/388, train_loss: 0.2987, step time: 1.5377\n",
      "142/388, train_loss: 0.4738, step time: 1.5330\n",
      "143/388, train_loss: 0.1808, step time: 1.5336\n",
      "144/388, train_loss: 0.1120, step time: 1.5360\n",
      "145/388, train_loss: 0.1112, step time: 1.5321\n",
      "146/388, train_loss: 0.3466, step time: 1.5640\n",
      "147/388, train_loss: 0.1210, step time: 1.5338\n",
      "148/388, train_loss: 0.0939, step time: 1.5322\n",
      "149/388, train_loss: 0.2122, step time: 1.5349\n",
      "150/388, train_loss: 0.1030, step time: 1.5359\n",
      "151/388, train_loss: 0.0901, step time: 1.5400\n",
      "152/388, train_loss: 0.1615, step time: 1.5381\n",
      "153/388, train_loss: 0.1890, step time: 1.5324\n",
      "154/388, train_loss: 0.2117, step time: 1.5320\n",
      "155/388, train_loss: 0.2191, step time: 1.5318\n",
      "156/388, train_loss: 0.1868, step time: 1.5304\n",
      "157/388, train_loss: 0.1216, step time: 1.5339\n",
      "158/388, train_loss: 0.2377, step time: 1.5347\n",
      "159/388, train_loss: 0.1934, step time: 1.5365\n",
      "160/388, train_loss: 0.2148, step time: 1.5347\n",
      "161/388, train_loss: 0.0624, step time: 1.5330\n",
      "162/388, train_loss: 0.1485, step time: 1.5291\n",
      "163/388, train_loss: 0.0758, step time: 1.5345\n",
      "164/388, train_loss: 0.3191, step time: 1.5338\n",
      "165/388, train_loss: 0.1137, step time: 1.5505\n",
      "166/388, train_loss: 0.2509, step time: 1.5343\n",
      "167/388, train_loss: 0.1835, step time: 1.5337\n",
      "168/388, train_loss: 0.3024, step time: 1.5341\n",
      "169/388, train_loss: 0.0739, step time: 1.5317\n",
      "170/388, train_loss: 0.4078, step time: 1.5346\n",
      "171/388, train_loss: 0.1916, step time: 1.5364\n",
      "172/388, train_loss: 0.4837, step time: 1.5313\n",
      "173/388, train_loss: 0.2077, step time: 1.5330\n",
      "174/388, train_loss: 0.2184, step time: 1.5332\n",
      "175/388, train_loss: 0.1019, step time: 1.5338\n",
      "176/388, train_loss: 0.0934, step time: 1.5416\n",
      "177/388, train_loss: 0.0606, step time: 1.5403\n",
      "178/388, train_loss: 0.2257, step time: 1.5354\n",
      "179/388, train_loss: 0.0342, step time: 1.5304\n",
      "180/388, train_loss: 0.1123, step time: 1.5322\n",
      "181/388, train_loss: 0.4323, step time: 1.5300\n",
      "182/388, train_loss: 0.1521, step time: 1.5311\n",
      "183/388, train_loss: 0.2970, step time: 1.5359\n",
      "184/388, train_loss: 0.3210, step time: 1.5357\n",
      "185/388, train_loss: 0.2165, step time: 1.5322\n",
      "186/388, train_loss: 0.2650, step time: 1.5340\n",
      "187/388, train_loss: 0.1614, step time: 1.5307\n",
      "188/388, train_loss: 0.1482, step time: 1.5308\n",
      "189/388, train_loss: 0.2239, step time: 1.5327\n",
      "190/388, train_loss: 0.0666, step time: 1.5358\n",
      "191/388, train_loss: 0.3842, step time: 1.5342\n",
      "192/388, train_loss: 0.0634, step time: 1.5344\n",
      "193/388, train_loss: 0.0669, step time: 1.5339\n",
      "194/388, train_loss: 0.0553, step time: 1.5334\n",
      "195/388, train_loss: 0.2944, step time: 1.5307\n",
      "196/388, train_loss: 0.2387, step time: 1.5321\n",
      "197/388, train_loss: 0.0816, step time: 1.5317\n",
      "198/388, train_loss: 0.1006, step time: 1.5322\n",
      "199/388, train_loss: 0.0886, step time: 1.5373\n",
      "200/388, train_loss: 0.1822, step time: 1.5363\n",
      "201/388, train_loss: 0.0605, step time: 1.5346\n",
      "202/388, train_loss: 0.1291, step time: 1.5355\n",
      "203/388, train_loss: 0.0892, step time: 1.5321\n",
      "204/388, train_loss: 0.2070, step time: 1.5360\n",
      "205/388, train_loss: 0.1027, step time: 1.5335\n",
      "206/388, train_loss: 0.1447, step time: 1.5366\n",
      "207/388, train_loss: 0.5349, step time: 1.5300\n",
      "208/388, train_loss: 0.2124, step time: 1.5325\n",
      "209/388, train_loss: 0.1317, step time: 1.5325\n",
      "210/388, train_loss: 0.0905, step time: 1.5385\n",
      "211/388, train_loss: 0.1229, step time: 1.5351\n",
      "212/388, train_loss: 0.2654, step time: 1.5331\n",
      "213/388, train_loss: 0.0795, step time: 1.5303\n",
      "214/388, train_loss: 0.2073, step time: 1.5306\n",
      "215/388, train_loss: 0.1741, step time: 1.5379\n",
      "216/388, train_loss: 0.0598, step time: 1.5367\n",
      "217/388, train_loss: 0.0864, step time: 1.5616\n",
      "218/388, train_loss: 0.4627, step time: 1.5356\n",
      "219/388, train_loss: 0.1066, step time: 1.5345\n",
      "220/388, train_loss: 0.2155, step time: 1.5379\n",
      "221/388, train_loss: 0.5096, step time: 1.5332\n",
      "222/388, train_loss: 0.1014, step time: 1.5332\n",
      "223/388, train_loss: 0.1891, step time: 1.5338\n",
      "224/388, train_loss: 0.2168, step time: 1.5326\n",
      "225/388, train_loss: 0.1352, step time: 1.5368\n",
      "226/388, train_loss: 0.1894, step time: 1.5366\n",
      "227/388, train_loss: 0.2993, step time: 1.5328\n",
      "228/388, train_loss: 0.3541, step time: 1.5343\n",
      "229/388, train_loss: 0.6217, step time: 1.5311\n",
      "230/388, train_loss: 0.0900, step time: 1.5336\n",
      "231/388, train_loss: 0.1451, step time: 1.5349\n",
      "232/388, train_loss: 0.3407, step time: 1.5373\n",
      "233/388, train_loss: 0.1241, step time: 1.5384\n",
      "234/388, train_loss: 0.0737, step time: 1.5350\n",
      "235/388, train_loss: 0.1818, step time: 1.5339\n",
      "236/388, train_loss: 0.1065, step time: 1.5327\n",
      "237/388, train_loss: 0.1392, step time: 1.5363\n",
      "238/388, train_loss: 0.2806, step time: 1.5521\n",
      "239/388, train_loss: 0.1397, step time: 1.5318\n",
      "240/388, train_loss: 0.4316, step time: 1.5320\n",
      "241/388, train_loss: 0.1888, step time: 1.5289\n",
      "242/388, train_loss: 0.2437, step time: 1.5363\n",
      "243/388, train_loss: 0.2697, step time: 1.5361\n",
      "244/388, train_loss: 0.2118, step time: 1.5351\n",
      "245/388, train_loss: 0.2987, step time: 1.5319\n",
      "246/388, train_loss: 0.1169, step time: 1.5315\n",
      "247/388, train_loss: 0.1514, step time: 1.5314\n",
      "248/388, train_loss: 0.0514, step time: 1.5320\n",
      "249/388, train_loss: 0.5361, step time: 1.5359\n",
      "250/388, train_loss: 0.1896, step time: 1.5393\n",
      "251/388, train_loss: 0.0839, step time: 1.5354\n",
      "252/388, train_loss: 0.1702, step time: 1.5337\n",
      "253/388, train_loss: 0.0911, step time: 1.5315\n",
      "254/388, train_loss: 0.2188, step time: 1.5310\n",
      "255/388, train_loss: 0.1444, step time: 1.5376\n",
      "256/388, train_loss: 0.1307, step time: 1.5355\n",
      "257/388, train_loss: 0.2209, step time: 1.5327\n",
      "258/388, train_loss: 0.0581, step time: 1.5341\n",
      "259/388, train_loss: 0.1085, step time: 1.5322\n",
      "260/388, train_loss: 0.3043, step time: 1.5322\n",
      "261/388, train_loss: 0.1344, step time: 1.5313\n",
      "262/388, train_loss: 0.0971, step time: 1.5325\n",
      "263/388, train_loss: 0.2477, step time: 1.5375\n",
      "264/388, train_loss: 0.1056, step time: 1.5367\n",
      "265/388, train_loss: 0.0991, step time: 1.5320\n",
      "266/388, train_loss: 0.2944, step time: 1.5320\n",
      "267/388, train_loss: 0.2642, step time: 1.5326\n",
      "268/388, train_loss: 0.3551, step time: 1.5594\n",
      "269/388, train_loss: 0.3137, step time: 1.5324\n",
      "270/388, train_loss: 0.1115, step time: 1.5354\n",
      "271/388, train_loss: 0.1979, step time: 1.5349\n",
      "272/388, train_loss: 0.2619, step time: 1.5354\n",
      "273/388, train_loss: 0.1468, step time: 1.5378\n",
      "274/388, train_loss: 0.0902, step time: 1.5340\n",
      "275/388, train_loss: 0.1568, step time: 1.5320\n",
      "276/388, train_loss: 0.0963, step time: 1.5333\n",
      "277/388, train_loss: 0.0589, step time: 1.5374\n",
      "278/388, train_loss: 0.1266, step time: 1.5389\n",
      "279/388, train_loss: 0.1173, step time: 1.5320\n",
      "280/388, train_loss: 0.2833, step time: 1.5304\n",
      "281/388, train_loss: 0.2762, step time: 1.5310\n",
      "282/388, train_loss: 0.1570, step time: 1.5309\n",
      "283/388, train_loss: 0.2489, step time: 1.5350\n",
      "284/388, train_loss: 0.0936, step time: 1.5352\n",
      "285/388, train_loss: 0.1515, step time: 1.5340\n",
      "286/388, train_loss: 0.1688, step time: 1.5353\n",
      "287/388, train_loss: 0.6366, step time: 1.5347\n",
      "288/388, train_loss: 0.0617, step time: 1.5321\n",
      "289/388, train_loss: 0.1135, step time: 1.5330\n",
      "290/388, train_loss: 0.1060, step time: 1.5346\n",
      "291/388, train_loss: 0.1282, step time: 1.5348\n",
      "292/388, train_loss: 0.0896, step time: 1.5369\n",
      "293/388, train_loss: 0.1972, step time: 1.5348\n",
      "294/388, train_loss: 0.1583, step time: 1.5348\n",
      "295/388, train_loss: 0.2603, step time: 1.5317\n",
      "296/388, train_loss: 0.2709, step time: 1.5344\n",
      "297/388, train_loss: 0.1407, step time: 1.5425\n",
      "298/388, train_loss: 0.1732, step time: 1.5301\n",
      "299/388, train_loss: 0.2386, step time: 1.5326\n",
      "300/388, train_loss: 0.2595, step time: 1.5334\n",
      "301/388, train_loss: 0.1400, step time: 1.5300\n",
      "302/388, train_loss: 0.5279, step time: 1.5367\n",
      "303/388, train_loss: 0.1321, step time: 1.5362\n",
      "304/388, train_loss: 0.4543, step time: 1.5365\n",
      "305/388, train_loss: 0.0994, step time: 1.5310\n",
      "306/388, train_loss: 0.0723, step time: 1.5330\n",
      "307/388, train_loss: 0.1535, step time: 1.5311\n",
      "308/388, train_loss: 0.2860, step time: 1.5327\n",
      "309/388, train_loss: 0.6637, step time: 1.5349\n",
      "310/388, train_loss: 0.1243, step time: 1.5384\n",
      "311/388, train_loss: 0.1266, step time: 1.5377\n",
      "312/388, train_loss: 0.0714, step time: 1.5345\n",
      "313/388, train_loss: 0.0891, step time: 1.5331\n",
      "314/388, train_loss: 0.1970, step time: 1.5318\n",
      "315/388, train_loss: 0.0901, step time: 1.5328\n",
      "316/388, train_loss: 0.1189, step time: 1.5358\n",
      "317/388, train_loss: 0.5025, step time: 1.5331\n",
      "318/388, train_loss: 0.1819, step time: 1.5364\n",
      "319/388, train_loss: 0.2027, step time: 1.5334\n",
      "320/388, train_loss: 0.1316, step time: 1.5319\n",
      "321/388, train_loss: 0.1363, step time: 1.5317\n",
      "322/388, train_loss: 0.0685, step time: 1.5355\n",
      "323/388, train_loss: 0.2893, step time: 1.5344\n",
      "324/388, train_loss: 0.0882, step time: 1.5360\n",
      "325/388, train_loss: 0.1353, step time: 1.5369\n",
      "326/388, train_loss: 0.2595, step time: 1.5368\n",
      "327/388, train_loss: 0.0555, step time: 1.5301\n",
      "328/388, train_loss: 0.3803, step time: 1.5335\n",
      "329/388, train_loss: 0.1666, step time: 1.5361\n",
      "330/388, train_loss: 0.1352, step time: 1.5344\n",
      "331/388, train_loss: 0.2004, step time: 1.5337\n",
      "332/388, train_loss: 0.2215, step time: 1.5355\n",
      "333/388, train_loss: 0.1819, step time: 1.5312\n",
      "334/388, train_loss: 0.0973, step time: 1.5313\n",
      "335/388, train_loss: 0.1909, step time: 1.5295\n",
      "336/388, train_loss: 0.1118, step time: 1.5337\n",
      "337/388, train_loss: 0.1103, step time: 1.5504\n",
      "338/388, train_loss: 0.0584, step time: 1.5378\n",
      "339/388, train_loss: 0.0863, step time: 1.5340\n",
      "340/388, train_loss: 0.1002, step time: 1.5340\n",
      "341/388, train_loss: 0.2902, step time: 1.5323\n",
      "342/388, train_loss: 0.0917, step time: 1.5343\n",
      "343/388, train_loss: 0.3997, step time: 1.5360\n",
      "344/388, train_loss: 0.2551, step time: 1.5374\n",
      "345/388, train_loss: 0.2798, step time: 1.5353\n",
      "346/388, train_loss: 0.1778, step time: 1.5342\n",
      "347/388, train_loss: 0.0521, step time: 1.5315\n",
      "348/388, train_loss: 0.1776, step time: 1.5369\n",
      "349/388, train_loss: 0.1798, step time: 1.5365\n",
      "350/388, train_loss: 0.2794, step time: 1.5365\n",
      "351/388, train_loss: 0.1174, step time: 1.5346\n",
      "352/388, train_loss: 0.0879, step time: 1.5344\n",
      "353/388, train_loss: 0.1011, step time: 1.5354\n",
      "354/388, train_loss: 0.1540, step time: 1.5370\n",
      "355/388, train_loss: 0.2720, step time: 1.5389\n",
      "356/388, train_loss: 0.2253, step time: 1.5327\n",
      "357/388, train_loss: 0.1946, step time: 1.5322\n",
      "358/388, train_loss: 0.2222, step time: 1.5288\n",
      "359/388, train_loss: 0.1658, step time: 1.5353\n",
      "360/388, train_loss: 0.2223, step time: 1.5384\n",
      "361/388, train_loss: 0.0327, step time: 1.5442\n",
      "362/388, train_loss: 0.2146, step time: 1.5343\n",
      "363/388, train_loss: 0.1386, step time: 1.5331\n",
      "364/388, train_loss: 0.1782, step time: 1.5328\n",
      "365/388, train_loss: 0.2603, step time: 1.5375\n",
      "366/388, train_loss: 0.2418, step time: 1.5376\n",
      "367/388, train_loss: 0.1138, step time: 1.5312\n",
      "368/388, train_loss: 0.2200, step time: 1.5312\n",
      "369/388, train_loss: 0.1429, step time: 1.5337\n",
      "370/388, train_loss: 0.0807, step time: 1.5324\n",
      "371/388, train_loss: 0.4250, step time: 1.5341\n",
      "372/388, train_loss: 0.0726, step time: 1.5367\n",
      "373/388, train_loss: 0.2863, step time: 1.5345\n",
      "374/388, train_loss: 0.1005, step time: 1.5302\n",
      "375/388, train_loss: 0.1623, step time: 1.5323\n",
      "376/388, train_loss: 0.2124, step time: 1.5303\n",
      "377/388, train_loss: 0.1529, step time: 1.5318\n",
      "378/388, train_loss: 0.3593, step time: 1.5343\n",
      "379/388, train_loss: 0.1044, step time: 1.5332\n",
      "380/388, train_loss: 0.5117, step time: 1.5341\n",
      "381/388, train_loss: 0.1090, step time: 1.5306\n",
      "382/388, train_loss: 0.0718, step time: 1.5323\n",
      "383/388, train_loss: 0.2911, step time: 1.5353\n",
      "384/388, train_loss: 0.0875, step time: 1.5362\n",
      "385/388, train_loss: 0.1532, step time: 1.5370\n",
      "386/388, train_loss: 0.0903, step time: 1.5348\n",
      "387/388, train_loss: 0.1057, step time: 1.5387\n",
      "388/388, train_loss: 0.1457, step time: 1.5370\n",
      "epoch 39 average loss: 0.1967\n",
      "current epoch: 39 current mean dice: 0.7540 tc: 0.7925 wt: 0.9005 et: 0.5690\n",
      "best mean dice: 0.7640 at epoch: 36\n",
      "time consuming of epoch 39 is: 702.6384\n",
      "----------\n",
      "epoch 40/100\n",
      "1/388, train_loss: 0.0992, step time: 1.5613\n",
      "2/388, train_loss: 0.1070, step time: 1.5356\n",
      "3/388, train_loss: 0.1074, step time: 1.5338\n",
      "4/388, train_loss: 0.0879, step time: 1.5381\n",
      "5/388, train_loss: 0.1567, step time: 1.5336\n",
      "6/388, train_loss: 0.2206, step time: 1.5337\n",
      "7/388, train_loss: 0.0975, step time: 1.5367\n",
      "8/388, train_loss: 0.0893, step time: 1.5346\n",
      "9/388, train_loss: 0.1027, step time: 1.5361\n",
      "10/388, train_loss: 0.1551, step time: 1.5303\n",
      "11/388, train_loss: 0.3000, step time: 1.5338\n",
      "12/388, train_loss: 0.1557, step time: 1.5287\n",
      "13/388, train_loss: 0.4566, step time: 1.5349\n",
      "14/388, train_loss: 0.1532, step time: 1.5350\n",
      "15/388, train_loss: 0.1214, step time: 1.5433\n",
      "16/388, train_loss: 0.3989, step time: 1.5335\n",
      "17/388, train_loss: 0.2095, step time: 1.5286\n",
      "18/388, train_loss: 0.1040, step time: 1.5324\n",
      "19/388, train_loss: 0.2690, step time: 1.5441\n",
      "20/388, train_loss: 0.1621, step time: 1.5339\n",
      "21/388, train_loss: 0.1754, step time: 1.5307\n",
      "22/388, train_loss: 0.1036, step time: 1.5332\n",
      "23/388, train_loss: 0.2348, step time: 1.5348\n",
      "24/388, train_loss: 0.1193, step time: 1.5385\n",
      "25/388, train_loss: 0.2658, step time: 1.5341\n",
      "26/388, train_loss: 0.2307, step time: 1.5337\n",
      "27/388, train_loss: 0.1078, step time: 1.5306\n",
      "28/388, train_loss: 0.2084, step time: 1.5361\n",
      "29/388, train_loss: 0.2677, step time: 1.5342\n",
      "30/388, train_loss: 0.1427, step time: 1.5360\n",
      "31/388, train_loss: 0.0475, step time: 1.5340\n",
      "32/388, train_loss: 0.1044, step time: 1.5346\n",
      "33/388, train_loss: 0.2598, step time: 1.5369\n",
      "34/388, train_loss: 0.0917, step time: 1.5443\n",
      "35/388, train_loss: 0.1969, step time: 1.5352\n",
      "36/388, train_loss: 0.2937, step time: 1.5363\n",
      "37/388, train_loss: 0.0835, step time: 1.5300\n",
      "38/388, train_loss: 0.1651, step time: 1.5319\n",
      "39/388, train_loss: 0.0662, step time: 1.5337\n",
      "40/388, train_loss: 0.2570, step time: 1.5322\n",
      "41/388, train_loss: 0.1707, step time: 1.5378\n",
      "42/388, train_loss: 0.2418, step time: 1.5332\n",
      "43/388, train_loss: 0.1175, step time: 1.5330\n",
      "44/388, train_loss: 0.1264, step time: 1.5304\n",
      "45/388, train_loss: 0.2312, step time: 1.5329\n",
      "46/388, train_loss: 0.1781, step time: 1.5344\n",
      "47/388, train_loss: 0.3524, step time: 1.5338\n",
      "48/388, train_loss: 0.0530, step time: 1.5365\n",
      "49/388, train_loss: 0.1504, step time: 1.5352\n",
      "50/388, train_loss: 0.0360, step time: 1.5325\n",
      "51/388, train_loss: 0.1867, step time: 1.5296\n",
      "52/388, train_loss: 0.1988, step time: 1.5337\n",
      "53/388, train_loss: 0.1797, step time: 1.5326\n",
      "54/388, train_loss: 0.1087, step time: 1.5400\n",
      "55/388, train_loss: 0.1536, step time: 1.5339\n",
      "56/388, train_loss: 0.1158, step time: 1.5354\n",
      "57/388, train_loss: 0.2899, step time: 1.5314\n",
      "58/388, train_loss: 0.2466, step time: 1.5320\n",
      "59/388, train_loss: 0.0974, step time: 1.5332\n",
      "60/388, train_loss: 0.0631, step time: 1.5342\n",
      "61/388, train_loss: 0.3029, step time: 1.5335\n",
      "62/388, train_loss: 0.2528, step time: 1.5323\n",
      "63/388, train_loss: 0.2165, step time: 1.5304\n",
      "64/388, train_loss: 0.1179, step time: 1.5320\n",
      "65/388, train_loss: 0.3209, step time: 1.5370\n",
      "66/388, train_loss: 0.0872, step time: 1.5344\n",
      "67/388, train_loss: 0.0484, step time: 1.5347\n",
      "68/388, train_loss: 0.1459, step time: 1.5356\n",
      "69/388, train_loss: 0.0637, step time: 1.5322\n",
      "70/388, train_loss: 0.1386, step time: 1.5302\n",
      "71/388, train_loss: 0.2420, step time: 1.5341\n",
      "72/388, train_loss: 0.2965, step time: 1.5336\n",
      "73/388, train_loss: 0.3838, step time: 1.5308\n",
      "74/388, train_loss: 0.2106, step time: 1.5301\n",
      "75/388, train_loss: 0.2331, step time: 1.5294\n",
      "76/388, train_loss: 0.1881, step time: 1.5316\n",
      "77/388, train_loss: 0.6265, step time: 1.5324\n",
      "78/388, train_loss: 0.1535, step time: 1.5349\n",
      "79/388, train_loss: 0.4104, step time: 1.5365\n",
      "80/388, train_loss: 0.1794, step time: 1.5325\n",
      "81/388, train_loss: 0.1618, step time: 1.5309\n",
      "82/388, train_loss: 0.1728, step time: 1.5318\n",
      "83/388, train_loss: 0.0580, step time: 1.5332\n",
      "84/388, train_loss: 0.0844, step time: 1.5338\n",
      "85/388, train_loss: 0.0655, step time: 1.5366\n",
      "86/388, train_loss: 0.0858, step time: 1.5375\n",
      "87/388, train_loss: 0.0954, step time: 1.5321\n",
      "88/388, train_loss: 0.1651, step time: 1.5349\n",
      "89/388, train_loss: 0.1442, step time: 1.5343\n",
      "90/388, train_loss: 0.1295, step time: 1.5330\n",
      "91/388, train_loss: 0.1709, step time: 1.5375\n",
      "92/388, train_loss: 0.1449, step time: 1.5356\n",
      "93/388, train_loss: 0.1576, step time: 1.5319\n",
      "94/388, train_loss: 0.1523, step time: 1.5301\n",
      "95/388, train_loss: 0.2286, step time: 1.5317\n",
      "96/388, train_loss: 0.4929, step time: 1.5320\n",
      "97/388, train_loss: 0.4502, step time: 1.5345\n",
      "98/388, train_loss: 0.0889, step time: 1.5327\n",
      "99/388, train_loss: 0.2823, step time: 1.5339\n",
      "100/388, train_loss: 0.1241, step time: 1.5346\n",
      "101/388, train_loss: 0.0965, step time: 1.5316\n",
      "102/388, train_loss: 0.0590, step time: 1.5336\n",
      "103/388, train_loss: 0.1411, step time: 1.5307\n",
      "104/388, train_loss: 0.0869, step time: 1.5455\n",
      "105/388, train_loss: 0.2675, step time: 1.5372\n",
      "106/388, train_loss: 0.3132, step time: 1.5317\n",
      "107/388, train_loss: 0.1122, step time: 1.5338\n",
      "108/388, train_loss: 0.2120, step time: 1.5330\n",
      "109/388, train_loss: 0.2209, step time: 1.5291\n",
      "110/388, train_loss: 0.1018, step time: 1.5365\n",
      "111/388, train_loss: 0.0790, step time: 1.5440\n",
      "112/388, train_loss: 0.0905, step time: 1.5341\n",
      "113/388, train_loss: 0.1821, step time: 1.5291\n",
      "114/388, train_loss: 0.2236, step time: 1.5351\n",
      "115/388, train_loss: 0.3943, step time: 1.5359\n",
      "116/388, train_loss: 0.1503, step time: 1.5365\n",
      "117/388, train_loss: 0.2778, step time: 1.5341\n",
      "118/388, train_loss: 0.1105, step time: 1.5340\n",
      "119/388, train_loss: 0.0686, step time: 1.5307\n",
      "120/388, train_loss: 0.2360, step time: 1.5337\n",
      "121/388, train_loss: 0.1819, step time: 1.5381\n",
      "122/388, train_loss: 0.2614, step time: 1.5417\n",
      "123/388, train_loss: 0.2866, step time: 1.5342\n",
      "124/388, train_loss: 0.1243, step time: 1.5390\n",
      "125/388, train_loss: 0.1211, step time: 1.5370\n",
      "126/388, train_loss: 0.1730, step time: 1.5401\n",
      "127/388, train_loss: 0.4000, step time: 1.5321\n",
      "128/388, train_loss: 0.2593, step time: 1.5383\n",
      "129/388, train_loss: 0.2227, step time: 1.5344\n",
      "130/388, train_loss: 0.1211, step time: 1.5342\n",
      "131/388, train_loss: 0.1651, step time: 1.5337\n",
      "132/388, train_loss: 0.0958, step time: 1.5357\n",
      "133/388, train_loss: 0.3196, step time: 1.5330\n",
      "134/388, train_loss: 0.1913, step time: 1.5331\n",
      "135/388, train_loss: 0.0742, step time: 1.5319\n",
      "136/388, train_loss: 0.1006, step time: 1.5324\n",
      "137/388, train_loss: 0.1041, step time: 1.5370\n",
      "138/388, train_loss: 0.2653, step time: 1.5366\n",
      "139/388, train_loss: 0.2482, step time: 1.5334\n",
      "140/388, train_loss: 0.2404, step time: 1.5313\n",
      "141/388, train_loss: 0.0574, step time: 1.5327\n",
      "142/388, train_loss: 0.3054, step time: 1.5319\n",
      "143/388, train_loss: 0.0629, step time: 1.5310\n",
      "144/388, train_loss: 0.1094, step time: 1.5413\n",
      "145/388, train_loss: 0.1831, step time: 1.5350\n",
      "146/388, train_loss: 0.2348, step time: 1.5316\n",
      "147/388, train_loss: 0.0924, step time: 1.5334\n",
      "148/388, train_loss: 0.3159, step time: 1.5340\n",
      "149/388, train_loss: 0.2630, step time: 1.5363\n",
      "150/388, train_loss: 0.2010, step time: 1.5589\n",
      "151/388, train_loss: 0.1167, step time: 1.5377\n",
      "152/388, train_loss: 0.0969, step time: 1.5354\n",
      "153/388, train_loss: 0.1860, step time: 1.5345\n",
      "154/388, train_loss: 0.2526, step time: 1.5322\n",
      "155/388, train_loss: 0.1452, step time: 1.5415\n",
      "156/388, train_loss: 0.1479, step time: 1.5443\n",
      "157/388, train_loss: 0.0283, step time: 1.5341\n",
      "158/388, train_loss: 0.2289, step time: 1.5399\n",
      "159/388, train_loss: 0.1337, step time: 1.5369\n",
      "160/388, train_loss: 0.1281, step time: 1.5322\n",
      "161/388, train_loss: 0.1393, step time: 1.5334\n",
      "162/388, train_loss: 0.1855, step time: 1.5307\n",
      "163/388, train_loss: 0.1168, step time: 1.5315\n",
      "164/388, train_loss: 0.2338, step time: 1.5386\n",
      "165/388, train_loss: 0.2158, step time: 1.5343\n",
      "166/388, train_loss: 0.0747, step time: 1.5359\n",
      "167/388, train_loss: 0.1824, step time: 1.5322\n",
      "168/388, train_loss: 0.0687, step time: 1.5325\n",
      "169/388, train_loss: 0.3150, step time: 1.5335\n",
      "170/388, train_loss: 0.2590, step time: 1.5360\n",
      "171/388, train_loss: 0.1494, step time: 1.5399\n",
      "172/388, train_loss: 0.2391, step time: 1.5373\n",
      "173/388, train_loss: 0.2016, step time: 1.5339\n",
      "174/388, train_loss: 0.0843, step time: 1.5338\n",
      "175/388, train_loss: 0.1687, step time: 1.5334\n",
      "176/388, train_loss: 0.0718, step time: 1.5367\n",
      "177/388, train_loss: 0.1706, step time: 1.5372\n",
      "178/388, train_loss: 0.1542, step time: 1.5352\n",
      "179/388, train_loss: 0.0988, step time: 1.5337\n",
      "180/388, train_loss: 0.1630, step time: 1.5304\n",
      "181/388, train_loss: 0.2645, step time: 1.5339\n",
      "182/388, train_loss: 0.2069, step time: 1.5357\n",
      "183/388, train_loss: 0.1381, step time: 1.5388\n",
      "184/388, train_loss: 0.3070, step time: 1.5322\n",
      "185/388, train_loss: 0.2671, step time: 1.5343\n",
      "186/388, train_loss: 0.1711, step time: 1.5337\n",
      "187/388, train_loss: 0.5105, step time: 1.5313\n",
      "188/388, train_loss: 0.2273, step time: 1.5350\n",
      "189/388, train_loss: 0.1418, step time: 1.5334\n",
      "190/388, train_loss: 0.0870, step time: 1.5363\n",
      "191/388, train_loss: 0.3119, step time: 1.5360\n",
      "192/388, train_loss: 0.1108, step time: 1.5352\n",
      "193/388, train_loss: 0.1552, step time: 1.5313\n",
      "194/388, train_loss: 0.4450, step time: 1.5327\n",
      "195/388, train_loss: 0.2698, step time: 1.5322\n",
      "196/388, train_loss: 0.1013, step time: 1.5354\n",
      "197/388, train_loss: 0.1309, step time: 1.5380\n",
      "198/388, train_loss: 0.2011, step time: 1.5373\n",
      "199/388, train_loss: 0.0756, step time: 1.5345\n",
      "200/388, train_loss: 0.2837, step time: 1.5316\n",
      "201/388, train_loss: 0.2565, step time: 1.5325\n",
      "202/388, train_loss: 0.1855, step time: 1.5446\n",
      "203/388, train_loss: 0.1169, step time: 1.5334\n",
      "204/388, train_loss: 0.5190, step time: 1.5354\n",
      "205/388, train_loss: 0.1186, step time: 1.5337\n",
      "206/388, train_loss: 0.2651, step time: 1.5335\n",
      "207/388, train_loss: 0.2185, step time: 1.5336\n",
      "208/388, train_loss: 0.4576, step time: 1.5392\n",
      "209/388, train_loss: 0.0516, step time: 1.5344\n",
      "210/388, train_loss: 0.1205, step time: 1.5318\n",
      "211/388, train_loss: 0.1995, step time: 1.5328\n",
      "212/388, train_loss: 0.0765, step time: 1.5325\n",
      "213/388, train_loss: 0.0856, step time: 1.5324\n",
      "214/388, train_loss: 0.1639, step time: 1.5359\n",
      "215/388, train_loss: 0.0931, step time: 1.5321\n",
      "216/388, train_loss: 0.1326, step time: 1.5323\n",
      "217/388, train_loss: 0.1035, step time: 1.5339\n",
      "218/388, train_loss: 0.1374, step time: 1.5376\n",
      "219/388, train_loss: 0.1830, step time: 1.5355\n",
      "220/388, train_loss: 0.0566, step time: 1.5349\n",
      "221/388, train_loss: 0.1070, step time: 1.5311\n",
      "222/388, train_loss: 0.3047, step time: 1.5348\n",
      "223/388, train_loss: 0.1339, step time: 1.5368\n",
      "224/388, train_loss: 0.1701, step time: 1.5341\n",
      "225/388, train_loss: 0.2658, step time: 1.5344\n",
      "226/388, train_loss: 0.2096, step time: 1.5347\n",
      "227/388, train_loss: 0.1711, step time: 1.5319\n",
      "228/388, train_loss: 0.1862, step time: 1.5382\n",
      "229/388, train_loss: 0.4663, step time: 1.5322\n",
      "230/388, train_loss: 0.2104, step time: 1.5351\n",
      "231/388, train_loss: 0.1708, step time: 1.5334\n",
      "232/388, train_loss: 0.3604, step time: 1.5361\n",
      "233/388, train_loss: 0.0925, step time: 1.5370\n",
      "234/388, train_loss: 0.0960, step time: 1.5357\n",
      "235/388, train_loss: 0.0907, step time: 1.5336\n",
      "236/388, train_loss: 0.2158, step time: 1.5357\n",
      "237/388, train_loss: 0.0887, step time: 1.5336\n",
      "238/388, train_loss: 0.1013, step time: 1.5344\n",
      "239/388, train_loss: 0.2305, step time: 1.5370\n",
      "240/388, train_loss: 0.0666, step time: 1.5357\n",
      "241/388, train_loss: 0.1202, step time: 1.5335\n",
      "242/388, train_loss: 0.0899, step time: 1.5315\n",
      "243/388, train_loss: 0.2212, step time: 1.5323\n",
      "244/388, train_loss: 0.2667, step time: 1.5329\n",
      "245/388, train_loss: 0.0660, step time: 1.5341\n",
      "246/388, train_loss: 0.1578, step time: 1.5381\n",
      "247/388, train_loss: 0.0755, step time: 1.5356\n",
      "248/388, train_loss: 0.2825, step time: 1.5336\n",
      "249/388, train_loss: 0.2411, step time: 1.5337\n",
      "250/388, train_loss: 0.1994, step time: 1.5293\n",
      "251/388, train_loss: 0.0756, step time: 1.5312\n",
      "252/388, train_loss: 0.1245, step time: 1.5346\n",
      "253/388, train_loss: 0.0915, step time: 1.5367\n",
      "254/388, train_loss: 0.1040, step time: 1.5371\n",
      "255/388, train_loss: 0.0914, step time: 1.5360\n",
      "256/388, train_loss: 0.1135, step time: 1.5327\n",
      "257/388, train_loss: 0.1853, step time: 1.5294\n",
      "258/388, train_loss: 0.1264, step time: 1.5325\n",
      "259/388, train_loss: 0.0397, step time: 1.5415\n",
      "260/388, train_loss: 0.2229, step time: 1.5353\n",
      "261/388, train_loss: 0.1997, step time: 1.5335\n",
      "262/388, train_loss: 0.1136, step time: 1.5354\n",
      "263/388, train_loss: 0.2234, step time: 1.5306\n",
      "264/388, train_loss: 0.5503, step time: 1.5320\n",
      "265/388, train_loss: 0.2434, step time: 1.5372\n",
      "266/388, train_loss: 0.2175, step time: 1.5378\n",
      "267/388, train_loss: 0.1363, step time: 1.5311\n",
      "268/388, train_loss: 0.2181, step time: 1.5328\n",
      "269/388, train_loss: 0.0964, step time: 1.5310\n",
      "270/388, train_loss: 0.2125, step time: 1.5348\n",
      "271/388, train_loss: 0.3519, step time: 1.5376\n",
      "272/388, train_loss: 0.1701, step time: 1.5385\n",
      "273/388, train_loss: 0.3889, step time: 1.5320\n",
      "274/388, train_loss: 0.1138, step time: 1.5321\n",
      "275/388, train_loss: 0.2966, step time: 1.5347\n",
      "276/388, train_loss: 0.0612, step time: 1.5336\n",
      "277/388, train_loss: 0.1546, step time: 1.5349\n",
      "278/388, train_loss: 0.5459, step time: 1.5478\n",
      "279/388, train_loss: 0.2555, step time: 1.5322\n",
      "280/388, train_loss: 0.1458, step time: 1.5326\n",
      "281/388, train_loss: 0.2831, step time: 1.5296\n",
      "282/388, train_loss: 0.0578, step time: 1.5325\n",
      "283/388, train_loss: 0.1254, step time: 1.5367\n",
      "284/388, train_loss: 0.1410, step time: 1.5353\n",
      "285/388, train_loss: 0.2701, step time: 1.5320\n",
      "286/388, train_loss: 0.1957, step time: 1.5303\n",
      "287/388, train_loss: 0.2731, step time: 1.5324\n",
      "288/388, train_loss: 0.2913, step time: 1.5396\n",
      "289/388, train_loss: 0.2645, step time: 1.5370\n",
      "290/388, train_loss: 0.3087, step time: 1.5346\n",
      "291/388, train_loss: 0.2344, step time: 1.5303\n",
      "292/388, train_loss: 0.2566, step time: 1.5364\n",
      "293/388, train_loss: 0.1226, step time: 1.5325\n",
      "294/388, train_loss: 0.1216, step time: 1.5386\n",
      "295/388, train_loss: 0.0639, step time: 1.5346\n",
      "296/388, train_loss: 0.2757, step time: 1.5340\n",
      "297/388, train_loss: 0.4427, step time: 1.5344\n",
      "298/388, train_loss: 0.1281, step time: 1.5306\n",
      "299/388, train_loss: 0.1157, step time: 1.5335\n",
      "300/388, train_loss: 0.1997, step time: 1.5331\n",
      "301/388, train_loss: 0.2419, step time: 1.5377\n",
      "302/388, train_loss: 0.2104, step time: 1.5364\n",
      "303/388, train_loss: 0.0960, step time: 1.5329\n",
      "304/388, train_loss: 0.5252, step time: 1.5400\n",
      "305/388, train_loss: 0.2004, step time: 1.5307\n",
      "306/388, train_loss: 0.0661, step time: 1.5416\n",
      "307/388, train_loss: 0.1497, step time: 1.5328\n",
      "308/388, train_loss: 0.1994, step time: 1.5350\n",
      "309/388, train_loss: 0.3120, step time: 1.5324\n",
      "310/388, train_loss: 0.3285, step time: 1.5314\n",
      "311/388, train_loss: 0.0882, step time: 1.5353\n",
      "312/388, train_loss: 0.0824, step time: 1.5350\n",
      "313/388, train_loss: 0.1960, step time: 1.5379\n",
      "314/388, train_loss: 0.1844, step time: 1.5312\n",
      "315/388, train_loss: 0.1748, step time: 1.5317\n",
      "316/388, train_loss: 0.0957, step time: 1.5409\n",
      "317/388, train_loss: 0.1433, step time: 1.5372\n",
      "318/388, train_loss: 0.2390, step time: 1.5425\n",
      "319/388, train_loss: 0.0897, step time: 1.5327\n",
      "320/388, train_loss: 0.2790, step time: 1.5342\n",
      "321/388, train_loss: 0.1172, step time: 1.5363\n",
      "322/388, train_loss: 0.2035, step time: 1.5428\n",
      "323/388, train_loss: 0.1858, step time: 1.5329\n",
      "324/388, train_loss: 0.2852, step time: 1.5305\n",
      "325/388, train_loss: 0.1712, step time: 1.5397\n",
      "326/388, train_loss: 0.2429, step time: 1.5466\n",
      "327/388, train_loss: 0.1882, step time: 1.5348\n",
      "328/388, train_loss: 0.2048, step time: 1.5382\n",
      "329/388, train_loss: 0.4105, step time: 1.5321\n",
      "330/388, train_loss: 0.0942, step time: 1.5447\n",
      "331/388, train_loss: 0.1624, step time: 1.5341\n",
      "332/388, train_loss: 0.1984, step time: 1.5327\n",
      "333/388, train_loss: 0.4278, step time: 1.5351\n",
      "334/388, train_loss: 0.4034, step time: 1.5474\n",
      "335/388, train_loss: 0.0878, step time: 1.5370\n",
      "336/388, train_loss: 0.2738, step time: 1.5351\n",
      "337/388, train_loss: 0.1516, step time: 1.5331\n",
      "338/388, train_loss: 0.1452, step time: 1.5368\n",
      "339/388, train_loss: 0.1986, step time: 1.5362\n",
      "340/388, train_loss: 0.4636, step time: 1.5401\n",
      "341/388, train_loss: 0.3911, step time: 1.5371\n",
      "342/388, train_loss: 0.0753, step time: 1.5320\n",
      "343/388, train_loss: 0.2293, step time: 1.5340\n",
      "344/388, train_loss: 0.2998, step time: 1.5306\n",
      "345/388, train_loss: 0.1193, step time: 1.5353\n",
      "346/388, train_loss: 0.1330, step time: 1.5443\n",
      "347/388, train_loss: 0.0979, step time: 1.5316\n",
      "348/388, train_loss: 0.0912, step time: 1.5354\n",
      "349/388, train_loss: 0.5333, step time: 1.5334\n",
      "350/388, train_loss: 0.1682, step time: 1.5443\n",
      "351/388, train_loss: 0.3683, step time: 1.5367\n",
      "352/388, train_loss: 0.3354, step time: 1.5335\n",
      "353/388, train_loss: 0.1208, step time: 1.5333\n",
      "354/388, train_loss: 0.3511, step time: 1.5364\n",
      "355/388, train_loss: 0.1247, step time: 1.5349\n",
      "356/388, train_loss: 0.0757, step time: 1.5381\n",
      "357/388, train_loss: 0.0676, step time: 1.5317\n",
      "358/388, train_loss: 0.2453, step time: 1.5330\n",
      "359/388, train_loss: 0.0999, step time: 1.5362\n",
      "360/388, train_loss: 0.2056, step time: 1.5374\n",
      "361/388, train_loss: 0.1569, step time: 1.5354\n",
      "362/388, train_loss: 0.1920, step time: 1.5360\n",
      "363/388, train_loss: 0.3448, step time: 1.5336\n",
      "364/388, train_loss: 0.1314, step time: 1.5353\n",
      "365/388, train_loss: 0.1886, step time: 1.5355\n",
      "366/388, train_loss: 0.2161, step time: 1.5479\n",
      "367/388, train_loss: 0.1340, step time: 1.5308\n",
      "368/388, train_loss: 0.1250, step time: 1.5340\n",
      "369/388, train_loss: 0.3119, step time: 1.5371\n",
      "370/388, train_loss: 0.1420, step time: 1.5374\n",
      "371/388, train_loss: 0.0671, step time: 1.5321\n",
      "372/388, train_loss: 0.4439, step time: 1.5319\n",
      "373/388, train_loss: 0.2647, step time: 1.5501\n",
      "374/388, train_loss: 0.0902, step time: 1.5379\n",
      "375/388, train_loss: 0.3767, step time: 1.5366\n",
      "376/388, train_loss: 0.1394, step time: 1.5355\n",
      "377/388, train_loss: 0.0756, step time: 1.5310\n",
      "378/388, train_loss: 0.1459, step time: 1.5365\n",
      "379/388, train_loss: 0.2326, step time: 1.5312\n",
      "380/388, train_loss: 0.1359, step time: 1.5367\n",
      "381/388, train_loss: 0.2941, step time: 1.5358\n",
      "382/388, train_loss: 0.1796, step time: 1.5458\n",
      "383/388, train_loss: 0.1843, step time: 1.5303\n",
      "384/388, train_loss: 0.0856, step time: 1.5323\n",
      "385/388, train_loss: 0.0943, step time: 1.5333\n",
      "386/388, train_loss: 0.1099, step time: 1.5509\n",
      "387/388, train_loss: 0.2865, step time: 1.5305\n",
      "388/388, train_loss: 0.0810, step time: 1.5295\n",
      "epoch 40 average loss: 0.1898\n",
      "current epoch: 40 current mean dice: 0.7563 tc: 0.8009 wt: 0.9003 et: 0.5676\n",
      "best mean dice: 0.7640 at epoch: 36\n",
      "time consuming of epoch 40 is: 701.9502\n",
      "----------\n",
      "epoch 41/100\n",
      "1/388, train_loss: 0.3176, step time: 1.5376\n",
      "2/388, train_loss: 0.2120, step time: 1.5346\n",
      "3/388, train_loss: 0.0705, step time: 1.5477\n",
      "4/388, train_loss: 0.1534, step time: 1.5371\n",
      "5/388, train_loss: 0.1786, step time: 1.5306\n",
      "6/388, train_loss: 0.1333, step time: 1.5475\n",
      "7/388, train_loss: 0.2566, step time: 1.5376\n",
      "8/388, train_loss: 0.0532, step time: 1.5340\n",
      "9/388, train_loss: 0.1174, step time: 1.5340\n",
      "10/388, train_loss: 0.1948, step time: 1.5378\n",
      "11/388, train_loss: 0.0833, step time: 1.5340\n",
      "12/388, train_loss: 0.3221, step time: 1.5318\n",
      "13/388, train_loss: 0.3130, step time: 1.5346\n",
      "14/388, train_loss: 0.0800, step time: 1.5333\n",
      "15/388, train_loss: 0.0942, step time: 1.5423\n",
      "16/388, train_loss: 0.1136, step time: 1.5384\n",
      "17/388, train_loss: 0.1172, step time: 1.5354\n",
      "18/388, train_loss: 0.1243, step time: 1.5308\n",
      "19/388, train_loss: 0.2292, step time: 1.5337\n",
      "20/388, train_loss: 0.4798, step time: 1.5326\n",
      "21/388, train_loss: 0.2319, step time: 1.5337\n",
      "22/388, train_loss: 0.2426, step time: 1.5319\n",
      "23/388, train_loss: 0.1203, step time: 1.5310\n",
      "24/388, train_loss: 0.0920, step time: 1.5333\n",
      "25/388, train_loss: 0.1330, step time: 1.5373\n",
      "26/388, train_loss: 0.0722, step time: 1.5347\n",
      "27/388, train_loss: 0.1213, step time: 1.5339\n",
      "28/388, train_loss: 0.0389, step time: 1.5327\n",
      "29/388, train_loss: 0.0909, step time: 1.5321\n",
      "30/388, train_loss: 0.1001, step time: 1.5349\n",
      "31/388, train_loss: 0.0643, step time: 1.5366\n",
      "32/388, train_loss: 0.2737, step time: 1.5326\n",
      "33/388, train_loss: 0.1508, step time: 1.5324\n",
      "34/388, train_loss: 0.2830, step time: 1.5335\n",
      "35/388, train_loss: 0.1818, step time: 1.5355\n",
      "36/388, train_loss: 0.0606, step time: 1.5397\n",
      "37/388, train_loss: 0.0950, step time: 1.5335\n",
      "38/388, train_loss: 0.4216, step time: 1.5301\n",
      "39/388, train_loss: 0.2061, step time: 1.5404\n",
      "40/388, train_loss: 0.1768, step time: 1.5373\n",
      "41/388, train_loss: 0.2061, step time: 1.5413\n",
      "42/388, train_loss: 0.1447, step time: 1.5395\n",
      "43/388, train_loss: 0.2365, step time: 1.5341\n",
      "44/388, train_loss: 0.1200, step time: 1.5304\n",
      "45/388, train_loss: 0.1283, step time: 1.5326\n",
      "46/388, train_loss: 0.0663, step time: 1.5330\n",
      "47/388, train_loss: 0.5448, step time: 1.5378\n",
      "48/388, train_loss: 0.0762, step time: 1.5356\n",
      "49/388, train_loss: 0.2285, step time: 1.5361\n",
      "50/388, train_loss: 0.0629, step time: 1.5325\n",
      "51/388, train_loss: 0.1029, step time: 1.5298\n",
      "52/388, train_loss: 0.1218, step time: 1.5340\n",
      "53/388, train_loss: 0.1796, step time: 1.5358\n",
      "54/388, train_loss: 0.1483, step time: 1.5341\n",
      "55/388, train_loss: 0.2252, step time: 1.5321\n",
      "56/388, train_loss: 0.2116, step time: 1.5327\n",
      "57/388, train_loss: 0.1009, step time: 1.5341\n",
      "58/388, train_loss: 0.1948, step time: 1.5338\n",
      "59/388, train_loss: 0.1871, step time: 1.5330\n",
      "60/388, train_loss: 0.2752, step time: 1.5376\n",
      "61/388, train_loss: 0.4601, step time: 1.5319\n",
      "62/388, train_loss: 0.0660, step time: 1.5350\n",
      "63/388, train_loss: 0.4658, step time: 1.5358\n",
      "64/388, train_loss: 0.0909, step time: 1.5384\n",
      "65/388, train_loss: 0.2698, step time: 1.5321\n",
      "66/388, train_loss: 0.0904, step time: 1.5319\n",
      "67/388, train_loss: 0.1657, step time: 1.5355\n",
      "68/388, train_loss: 0.0891, step time: 1.5320\n",
      "69/388, train_loss: 0.2183, step time: 1.5324\n",
      "70/388, train_loss: 0.0376, step time: 1.5327\n",
      "71/388, train_loss: 0.1011, step time: 1.5328\n",
      "72/388, train_loss: 0.3070, step time: 1.5360\n",
      "73/388, train_loss: 0.1513, step time: 1.5328\n",
      "74/388, train_loss: 0.0679, step time: 1.5318\n",
      "75/388, train_loss: 0.2221, step time: 1.5303\n",
      "76/388, train_loss: 0.0893, step time: 1.5335\n",
      "77/388, train_loss: 0.1683, step time: 1.5373\n",
      "78/388, train_loss: 0.2151, step time: 1.5366\n",
      "79/388, train_loss: 0.4996, step time: 1.5365\n",
      "80/388, train_loss: 0.0964, step time: 1.5351\n",
      "81/388, train_loss: 0.3654, step time: 1.5329\n",
      "82/388, train_loss: 0.1471, step time: 1.5306\n",
      "83/388, train_loss: 0.1165, step time: 1.5352\n",
      "84/388, train_loss: 0.2294, step time: 1.5346\n",
      "85/388, train_loss: 0.0892, step time: 1.5334\n",
      "86/388, train_loss: 0.1360, step time: 1.5343\n",
      "87/388, train_loss: 0.1121, step time: 1.5283\n",
      "88/388, train_loss: 0.1457, step time: 1.5343\n",
      "89/388, train_loss: 0.1690, step time: 1.5326\n",
      "90/388, train_loss: 0.2098, step time: 1.5383\n",
      "91/388, train_loss: 0.2690, step time: 1.5376\n",
      "92/388, train_loss: 0.0703, step time: 1.5415\n",
      "93/388, train_loss: 0.1023, step time: 1.5327\n",
      "94/388, train_loss: 0.1429, step time: 1.5349\n",
      "95/388, train_loss: 0.0680, step time: 1.5342\n",
      "96/388, train_loss: 0.1651, step time: 1.5375\n",
      "97/388, train_loss: 0.2262, step time: 1.5333\n",
      "98/388, train_loss: 0.3509, step time: 1.5327\n",
      "99/388, train_loss: 0.1607, step time: 1.5302\n",
      "100/388, train_loss: 0.2193, step time: 1.5344\n",
      "101/388, train_loss: 0.0900, step time: 1.5430\n",
      "102/388, train_loss: 0.3522, step time: 1.5346\n",
      "103/388, train_loss: 0.2774, step time: 1.5358\n",
      "104/388, train_loss: 0.1916, step time: 1.5328\n",
      "105/388, train_loss: 0.2058, step time: 1.5328\n",
      "106/388, train_loss: 0.1506, step time: 1.5335\n",
      "107/388, train_loss: 0.2147, step time: 1.5415\n",
      "108/388, train_loss: 0.2015, step time: 1.5347\n",
      "109/388, train_loss: 0.0881, step time: 1.5345\n",
      "110/388, train_loss: 0.1534, step time: 1.5320\n",
      "111/388, train_loss: 0.2489, step time: 1.5328\n",
      "112/388, train_loss: 0.2844, step time: 1.5342\n",
      "113/388, train_loss: 0.1718, step time: 1.5353\n",
      "114/388, train_loss: 0.4018, step time: 1.5357\n",
      "115/388, train_loss: 0.1510, step time: 1.5313\n",
      "116/388, train_loss: 0.3656, step time: 1.5372\n",
      "117/388, train_loss: 0.0471, step time: 1.5311\n",
      "118/388, train_loss: 0.2549, step time: 1.5321\n",
      "119/388, train_loss: 0.3069, step time: 1.5336\n",
      "120/388, train_loss: 0.2092, step time: 1.5457\n",
      "121/388, train_loss: 0.2163, step time: 1.5347\n",
      "122/388, train_loss: 0.1246, step time: 1.5334\n",
      "123/388, train_loss: 0.1284, step time: 1.5353\n",
      "124/388, train_loss: 0.2252, step time: 1.5383\n",
      "125/388, train_loss: 0.2831, step time: 1.5348\n",
      "126/388, train_loss: 0.1551, step time: 1.5393\n",
      "127/388, train_loss: 0.1057, step time: 1.5316\n",
      "128/388, train_loss: 0.1031, step time: 1.5317\n",
      "129/388, train_loss: 0.2427, step time: 1.5552\n",
      "130/388, train_loss: 0.0844, step time: 1.5382\n",
      "131/388, train_loss: 0.0977, step time: 1.5362\n",
      "132/388, train_loss: 0.0951, step time: 1.5316\n",
      "133/388, train_loss: 0.6105, step time: 1.5339\n",
      "134/388, train_loss: 0.1158, step time: 1.5347\n",
      "135/388, train_loss: 0.3766, step time: 1.5348\n",
      "136/388, train_loss: 0.0993, step time: 1.5379\n",
      "137/388, train_loss: 0.1899, step time: 1.5370\n",
      "138/388, train_loss: 0.1314, step time: 1.5329\n",
      "139/388, train_loss: 0.2147, step time: 1.5392\n",
      "140/388, train_loss: 0.0987, step time: 1.5322\n",
      "141/388, train_loss: 0.1222, step time: 1.5307\n",
      "142/388, train_loss: 0.6171, step time: 1.5336\n",
      "143/388, train_loss: 0.3119, step time: 1.5345\n",
      "144/388, train_loss: 0.2324, step time: 1.5365\n",
      "145/388, train_loss: 0.3862, step time: 1.5313\n",
      "146/388, train_loss: 0.1576, step time: 1.5467\n",
      "147/388, train_loss: 0.0893, step time: 1.5354\n",
      "148/388, train_loss: 0.1206, step time: 1.5278\n",
      "149/388, train_loss: 0.1482, step time: 1.5303\n",
      "150/388, train_loss: 0.0456, step time: 1.5306\n",
      "151/388, train_loss: 0.1915, step time: 1.5334\n",
      "152/388, train_loss: 0.3303, step time: 1.5491\n",
      "153/388, train_loss: 0.2301, step time: 1.5339\n",
      "154/388, train_loss: 0.4431, step time: 1.5303\n",
      "155/388, train_loss: 0.2266, step time: 1.5303\n",
      "156/388, train_loss: 0.1030, step time: 1.5305\n",
      "157/388, train_loss: 0.1432, step time: 1.5343\n",
      "158/388, train_loss: 0.1739, step time: 1.5354\n",
      "159/388, train_loss: 0.3035, step time: 1.5362\n",
      "160/388, train_loss: 0.1665, step time: 1.5364\n",
      "161/388, train_loss: 0.4122, step time: 1.5321\n",
      "162/388, train_loss: 0.1535, step time: 1.5561\n",
      "163/388, train_loss: 0.1766, step time: 1.5376\n",
      "164/388, train_loss: 0.0800, step time: 1.5345\n",
      "165/388, train_loss: 0.0886, step time: 1.5346\n",
      "166/388, train_loss: 0.0835, step time: 1.5342\n",
      "167/388, train_loss: 0.1095, step time: 1.5296\n",
      "168/388, train_loss: 0.3603, step time: 1.5346\n",
      "169/388, train_loss: 0.0743, step time: 1.5351\n",
      "170/388, train_loss: 0.1412, step time: 1.5356\n",
      "171/388, train_loss: 0.1076, step time: 1.5384\n",
      "172/388, train_loss: 0.0725, step time: 1.5305\n",
      "173/388, train_loss: 0.1709, step time: 1.5303\n",
      "174/388, train_loss: 0.2131, step time: 1.5337\n",
      "175/388, train_loss: 0.1485, step time: 1.5334\n",
      "176/388, train_loss: 0.1462, step time: 1.5373\n",
      "177/388, train_loss: 0.2654, step time: 1.5365\n",
      "178/388, train_loss: 0.0940, step time: 1.5343\n",
      "179/388, train_loss: 0.0617, step time: 1.5336\n",
      "180/388, train_loss: 0.1132, step time: 1.5308\n",
      "181/388, train_loss: 0.1513, step time: 1.5327\n",
      "182/388, train_loss: 0.1581, step time: 1.5350\n",
      "183/388, train_loss: 0.3052, step time: 1.5380\n",
      "184/388, train_loss: 0.1518, step time: 1.5335\n",
      "185/388, train_loss: 0.1084, step time: 1.5374\n",
      "186/388, train_loss: 0.0947, step time: 1.5317\n",
      "187/388, train_loss: 0.1226, step time: 1.5495\n",
      "188/388, train_loss: 0.2031, step time: 1.5332\n",
      "189/388, train_loss: 0.1684, step time: 1.5295\n",
      "190/388, train_loss: 0.2123, step time: 1.5346\n",
      "191/388, train_loss: 0.1697, step time: 1.5328\n",
      "192/388, train_loss: 0.2077, step time: 1.5352\n",
      "193/388, train_loss: 0.2244, step time: 1.5333\n",
      "194/388, train_loss: 0.3781, step time: 1.5335\n",
      "195/388, train_loss: 0.1927, step time: 1.5309\n",
      "196/388, train_loss: 0.1351, step time: 1.5286\n",
      "197/388, train_loss: 0.4109, step time: 1.5303\n",
      "198/388, train_loss: 0.5072, step time: 1.5303\n",
      "199/388, train_loss: 0.2137, step time: 1.5326\n",
      "200/388, train_loss: 0.2060, step time: 1.5359\n",
      "201/388, train_loss: 0.1019, step time: 1.5372\n",
      "202/388, train_loss: 0.3872, step time: 1.5337\n",
      "203/388, train_loss: 0.1374, step time: 1.5312\n",
      "204/388, train_loss: 0.2121, step time: 1.5322\n",
      "205/388, train_loss: 0.1362, step time: 1.5314\n",
      "206/388, train_loss: 0.1336, step time: 1.5355\n",
      "207/388, train_loss: 0.1750, step time: 1.5340\n",
      "208/388, train_loss: 0.2536, step time: 1.5301\n",
      "209/388, train_loss: 0.1345, step time: 1.5318\n",
      "210/388, train_loss: 0.3089, step time: 1.5307\n",
      "211/388, train_loss: 0.1855, step time: 1.5283\n",
      "212/388, train_loss: 0.4075, step time: 1.5314\n",
      "213/388, train_loss: 0.1852, step time: 1.5350\n",
      "214/388, train_loss: 0.2937, step time: 1.5352\n",
      "215/388, train_loss: 0.1227, step time: 1.5311\n",
      "216/388, train_loss: 0.1545, step time: 1.5337\n",
      "217/388, train_loss: 0.1376, step time: 1.5295\n",
      "218/388, train_loss: 0.0558, step time: 1.5304\n",
      "219/388, train_loss: 0.1461, step time: 1.5308\n",
      "220/388, train_loss: 0.2119, step time: 1.5280\n",
      "221/388, train_loss: 0.0998, step time: 1.5367\n",
      "222/388, train_loss: 0.1757, step time: 1.5368\n",
      "223/388, train_loss: 0.3390, step time: 1.5376\n",
      "224/388, train_loss: 0.1701, step time: 1.5354\n",
      "225/388, train_loss: 0.4871, step time: 1.5333\n",
      "226/388, train_loss: 0.5700, step time: 1.5298\n",
      "227/388, train_loss: 0.1169, step time: 1.5322\n",
      "228/388, train_loss: 0.1424, step time: 1.5357\n",
      "229/388, train_loss: 0.2256, step time: 1.5342\n",
      "230/388, train_loss: 0.3319, step time: 1.5350\n",
      "231/388, train_loss: 0.3859, step time: 1.5288\n",
      "232/388, train_loss: 0.1365, step time: 1.5290\n",
      "233/388, train_loss: 0.0959, step time: 1.5290\n",
      "234/388, train_loss: 0.3939, step time: 1.5325\n",
      "235/388, train_loss: 0.1479, step time: 1.5320\n",
      "236/388, train_loss: 0.1094, step time: 1.5360\n",
      "237/388, train_loss: 0.2691, step time: 1.5388\n",
      "238/388, train_loss: 0.1870, step time: 1.5422\n",
      "239/388, train_loss: 0.1598, step time: 1.5311\n",
      "240/388, train_loss: 0.2218, step time: 1.5322\n",
      "241/388, train_loss: 0.1279, step time: 1.5335\n",
      "242/388, train_loss: 0.0703, step time: 1.5324\n",
      "243/388, train_loss: 0.1196, step time: 1.5364\n",
      "244/388, train_loss: 0.3513, step time: 1.5426\n",
      "245/388, train_loss: 0.1264, step time: 1.5331\n",
      "246/388, train_loss: 0.1540, step time: 1.5312\n",
      "247/388, train_loss: 0.1069, step time: 1.5324\n",
      "248/388, train_loss: 0.1206, step time: 1.5286\n",
      "249/388, train_loss: 0.1662, step time: 1.5285\n",
      "250/388, train_loss: 0.4809, step time: 1.5383\n",
      "251/388, train_loss: 0.1650, step time: 1.5343\n",
      "252/388, train_loss: 0.1085, step time: 1.5331\n",
      "253/388, train_loss: 0.1273, step time: 1.5282\n",
      "254/388, train_loss: 0.1976, step time: 1.5297\n",
      "255/388, train_loss: 0.1664, step time: 1.5301\n",
      "256/388, train_loss: 0.2154, step time: 1.5325\n",
      "257/388, train_loss: 0.2806, step time: 1.5326\n",
      "258/388, train_loss: 0.1002, step time: 1.5321\n",
      "259/388, train_loss: 0.2996, step time: 1.5311\n",
      "260/388, train_loss: 0.0690, step time: 1.5427\n",
      "261/388, train_loss: 0.0916, step time: 1.5349\n",
      "262/388, train_loss: 0.3525, step time: 1.5343\n",
      "263/388, train_loss: 0.3454, step time: 1.5317\n",
      "264/388, train_loss: 0.1983, step time: 1.5330\n",
      "265/388, train_loss: 0.1905, step time: 1.5306\n",
      "266/388, train_loss: 0.1000, step time: 1.5345\n",
      "267/388, train_loss: 0.3316, step time: 1.5341\n",
      "268/388, train_loss: 0.1020, step time: 1.5358\n",
      "269/388, train_loss: 0.1085, step time: 1.5342\n",
      "270/388, train_loss: 0.4248, step time: 1.5359\n",
      "271/388, train_loss: 0.0952, step time: 1.5308\n",
      "272/388, train_loss: 0.1480, step time: 1.5309\n",
      "273/388, train_loss: 0.1701, step time: 1.5313\n",
      "274/388, train_loss: 0.1911, step time: 1.5313\n",
      "275/388, train_loss: 0.2421, step time: 1.5326\n",
      "276/388, train_loss: 0.1531, step time: 1.5341\n",
      "277/388, train_loss: 0.1417, step time: 1.5348\n",
      "278/388, train_loss: 0.2602, step time: 1.5338\n",
      "279/388, train_loss: 0.1913, step time: 1.5325\n",
      "280/388, train_loss: 0.0973, step time: 1.5302\n",
      "281/388, train_loss: 0.1398, step time: 1.5303\n",
      "282/388, train_loss: 0.1199, step time: 1.5318\n",
      "283/388, train_loss: 0.2692, step time: 1.5395\n",
      "284/388, train_loss: 0.2498, step time: 1.5368\n",
      "285/388, train_loss: 0.1580, step time: 1.5320\n",
      "286/388, train_loss: 0.1729, step time: 1.5327\n",
      "287/388, train_loss: 0.0601, step time: 1.5316\n",
      "288/388, train_loss: 0.1323, step time: 1.5339\n",
      "289/388, train_loss: 0.1525, step time: 1.5352\n",
      "290/388, train_loss: 0.0902, step time: 1.5356\n",
      "291/388, train_loss: 0.1611, step time: 1.5334\n",
      "292/388, train_loss: 0.1472, step time: 1.5347\n",
      "293/388, train_loss: 0.1505, step time: 1.5332\n",
      "294/388, train_loss: 0.4518, step time: 1.5316\n",
      "295/388, train_loss: 0.1575, step time: 1.5344\n",
      "296/388, train_loss: 0.0961, step time: 1.5366\n",
      "297/388, train_loss: 0.1516, step time: 1.5409\n",
      "298/388, train_loss: 0.2028, step time: 1.5321\n",
      "299/388, train_loss: 0.4201, step time: 1.5318\n",
      "300/388, train_loss: 0.3532, step time: 1.5332\n",
      "301/388, train_loss: 0.1033, step time: 1.5512\n",
      "302/388, train_loss: 0.3038, step time: 1.5285\n",
      "303/388, train_loss: 0.1847, step time: 1.5292\n",
      "304/388, train_loss: 0.2329, step time: 1.5331\n",
      "305/388, train_loss: 0.5657, step time: 1.5342\n",
      "306/388, train_loss: 0.1705, step time: 1.5357\n",
      "307/388, train_loss: 0.3357, step time: 1.5346\n",
      "308/388, train_loss: 0.0862, step time: 1.5336\n",
      "309/388, train_loss: 0.0636, step time: 1.5307\n",
      "310/388, train_loss: 0.2783, step time: 1.5337\n",
      "311/388, train_loss: 0.0979, step time: 1.5305\n",
      "312/388, train_loss: 0.6876, step time: 1.5343\n",
      "313/388, train_loss: 0.2763, step time: 1.5342\n",
      "314/388, train_loss: 0.8849, step time: 1.5348\n",
      "315/388, train_loss: 0.4288, step time: 1.5302\n",
      "316/388, train_loss: 0.0881, step time: 1.5374\n",
      "317/388, train_loss: 0.1616, step time: 1.5336\n",
      "318/388, train_loss: 0.2194, step time: 1.5580\n",
      "319/388, train_loss: 0.3800, step time: 1.5583\n",
      "320/388, train_loss: 0.2228, step time: 1.5410\n",
      "321/388, train_loss: 0.2328, step time: 1.5359\n",
      "322/388, train_loss: 0.2103, step time: 1.5362\n",
      "323/388, train_loss: 0.0350, step time: 1.5328\n",
      "324/388, train_loss: 0.0944, step time: 1.5328\n",
      "325/388, train_loss: 0.3113, step time: 1.5314\n",
      "326/388, train_loss: 0.3224, step time: 1.5607\n",
      "327/388, train_loss: 0.2882, step time: 1.5307\n",
      "328/388, train_loss: 0.0963, step time: 1.5310\n",
      "329/388, train_loss: 0.1175, step time: 1.5316\n",
      "330/388, train_loss: 0.2190, step time: 1.5322\n",
      "331/388, train_loss: 0.2720, step time: 1.5312\n",
      "332/388, train_loss: 0.0901, step time: 1.5355\n",
      "333/388, train_loss: 0.2445, step time: 1.5325\n",
      "334/388, train_loss: 0.2083, step time: 1.5340\n",
      "335/388, train_loss: 0.3002, step time: 1.5287\n",
      "336/388, train_loss: 0.0522, step time: 1.5337\n",
      "337/388, train_loss: 0.2509, step time: 1.5318\n",
      "338/388, train_loss: 0.2075, step time: 1.5344\n",
      "339/388, train_loss: 0.0733, step time: 1.5344\n",
      "340/388, train_loss: 0.1038, step time: 1.5330\n",
      "341/388, train_loss: 0.2396, step time: 1.5312\n",
      "342/388, train_loss: 0.1564, step time: 1.5292\n",
      "343/388, train_loss: 0.0593, step time: 1.5330\n",
      "344/388, train_loss: 0.1425, step time: 1.5290\n",
      "345/388, train_loss: 0.0754, step time: 1.5340\n",
      "346/388, train_loss: 0.1446, step time: 1.5345\n",
      "347/388, train_loss: 0.0825, step time: 1.5342\n",
      "348/388, train_loss: 0.2544, step time: 1.5335\n",
      "349/388, train_loss: 0.2360, step time: 1.5341\n",
      "350/388, train_loss: 0.1765, step time: 1.5325\n",
      "351/388, train_loss: 0.1161, step time: 1.5305\n",
      "352/388, train_loss: 0.2051, step time: 1.5380\n",
      "353/388, train_loss: 0.0944, step time: 1.5338\n",
      "354/388, train_loss: 0.0702, step time: 1.5360\n",
      "355/388, train_loss: 0.0927, step time: 1.5589\n",
      "356/388, train_loss: 0.2913, step time: 1.5332\n",
      "357/388, train_loss: 0.2889, step time: 1.5358\n",
      "358/388, train_loss: 0.1771, step time: 1.5370\n",
      "359/388, train_loss: 0.3684, step time: 1.5415\n",
      "360/388, train_loss: 0.5363, step time: 1.5324\n",
      "361/388, train_loss: 0.0528, step time: 1.5332\n",
      "362/388, train_loss: 0.4547, step time: 1.5342\n",
      "363/388, train_loss: 0.0846, step time: 1.5347\n",
      "364/388, train_loss: 0.0615, step time: 1.5348\n",
      "365/388, train_loss: 0.1780, step time: 1.5302\n",
      "366/388, train_loss: 0.1718, step time: 1.5313\n",
      "367/388, train_loss: 0.1255, step time: 1.5322\n",
      "368/388, train_loss: 0.0865, step time: 1.5461\n",
      "369/388, train_loss: 0.2141, step time: 1.5367\n",
      "370/388, train_loss: 0.1971, step time: 1.5364\n",
      "371/388, train_loss: 0.2788, step time: 1.5323\n",
      "372/388, train_loss: 0.0543, step time: 1.5313\n",
      "373/388, train_loss: 0.2660, step time: 1.5334\n",
      "374/388, train_loss: 0.0326, step time: 1.5361\n",
      "375/388, train_loss: 0.1255, step time: 1.5330\n",
      "376/388, train_loss: 0.3124, step time: 1.5367\n",
      "377/388, train_loss: 0.0789, step time: 1.5353\n",
      "378/388, train_loss: 0.1632, step time: 1.5321\n",
      "379/388, train_loss: 0.2764, step time: 1.5329\n",
      "380/388, train_loss: 0.3089, step time: 1.5325\n",
      "381/388, train_loss: 0.1588, step time: 1.5351\n",
      "382/388, train_loss: 0.0949, step time: 1.5327\n",
      "383/388, train_loss: 0.1556, step time: 1.5324\n",
      "384/388, train_loss: 0.1947, step time: 1.5357\n",
      "385/388, train_loss: 0.1517, step time: 1.5326\n",
      "386/388, train_loss: 0.2808, step time: 1.5320\n",
      "387/388, train_loss: 0.1295, step time: 1.5354\n",
      "388/388, train_loss: 0.1468, step time: 1.5303\n",
      "epoch 41 average loss: 0.1955\n",
      "current epoch: 41 current mean dice: 0.7525 tc: 0.7992 wt: 0.8860 et: 0.5722\n",
      "best mean dice: 0.7640 at epoch: 36\n",
      "time consuming of epoch 41 is: 702.7718\n",
      "----------\n",
      "epoch 42/100\n",
      "1/388, train_loss: 0.2137, step time: 1.5443\n",
      "2/388, train_loss: 0.1480, step time: 1.5373\n",
      "3/388, train_loss: 0.3108, step time: 1.5369\n",
      "4/388, train_loss: 0.0862, step time: 1.5344\n",
      "5/388, train_loss: 0.1262, step time: 1.5312\n",
      "6/388, train_loss: 0.2550, step time: 1.5366\n",
      "7/388, train_loss: 0.3114, step time: 1.5316\n",
      "8/388, train_loss: 0.1965, step time: 1.5309\n",
      "9/388, train_loss: 0.1268, step time: 1.5300\n",
      "10/388, train_loss: 0.1810, step time: 1.5387\n",
      "11/388, train_loss: 0.2227, step time: 1.5339\n",
      "12/388, train_loss: 0.0964, step time: 1.5365\n",
      "13/388, train_loss: 0.2593, step time: 1.5301\n",
      "14/388, train_loss: 0.0856, step time: 1.5330\n",
      "15/388, train_loss: 0.0858, step time: 1.5349\n",
      "16/388, train_loss: 0.2687, step time: 1.5331\n",
      "17/388, train_loss: 0.1020, step time: 1.5383\n",
      "18/388, train_loss: 0.2028, step time: 1.5378\n",
      "19/388, train_loss: 0.0874, step time: 1.5326\n",
      "20/388, train_loss: 0.1991, step time: 1.5336\n",
      "21/388, train_loss: 0.3553, step time: 1.5299\n",
      "22/388, train_loss: 0.1484, step time: 1.5331\n",
      "23/388, train_loss: 0.1501, step time: 1.5342\n",
      "24/388, train_loss: 0.1114, step time: 1.5353\n",
      "25/388, train_loss: 0.2734, step time: 1.5373\n",
      "26/388, train_loss: 0.0891, step time: 1.5322\n",
      "27/388, train_loss: 0.4304, step time: 1.5347\n",
      "28/388, train_loss: 0.1927, step time: 1.5320\n",
      "29/388, train_loss: 0.1981, step time: 1.5298\n",
      "30/388, train_loss: 0.1674, step time: 1.5372\n",
      "31/388, train_loss: 0.1876, step time: 1.5345\n",
      "32/388, train_loss: 0.1266, step time: 1.5331\n",
      "33/388, train_loss: 0.0666, step time: 1.5358\n",
      "34/388, train_loss: 0.2221, step time: 1.5298\n",
      "35/388, train_loss: 0.4841, step time: 1.5324\n",
      "36/388, train_loss: 0.1735, step time: 1.5356\n",
      "37/388, train_loss: 0.2464, step time: 1.5355\n",
      "38/388, train_loss: 0.2691, step time: 1.5351\n",
      "39/388, train_loss: 0.1581, step time: 1.5358\n",
      "40/388, train_loss: 0.1855, step time: 1.5321\n",
      "41/388, train_loss: 0.0740, step time: 1.5346\n",
      "42/388, train_loss: 0.1119, step time: 1.5349\n",
      "43/388, train_loss: 0.2388, step time: 1.5352\n",
      "44/388, train_loss: 0.5033, step time: 1.5375\n",
      "45/388, train_loss: 0.0555, step time: 1.5344\n",
      "46/388, train_loss: 0.1296, step time: 1.5338\n",
      "47/388, train_loss: 0.1527, step time: 1.5473\n",
      "48/388, train_loss: 0.3065, step time: 1.5329\n",
      "49/388, train_loss: 0.1264, step time: 1.5328\n",
      "50/388, train_loss: 0.0995, step time: 1.5314\n",
      "51/388, train_loss: 0.2924, step time: 1.5300\n",
      "52/388, train_loss: 0.1205, step time: 1.5292\n",
      "53/388, train_loss: 0.1220, step time: 1.5332\n",
      "54/388, train_loss: 0.2005, step time: 1.5305\n",
      "55/388, train_loss: 0.0833, step time: 1.5337\n",
      "56/388, train_loss: 0.1348, step time: 1.5391\n",
      "57/388, train_loss: 0.1947, step time: 1.5376\n",
      "58/388, train_loss: 0.1025, step time: 1.5317\n",
      "59/388, train_loss: 0.2187, step time: 1.5333\n",
      "60/388, train_loss: 0.4942, step time: 1.5310\n",
      "61/388, train_loss: 0.5188, step time: 1.5340\n",
      "62/388, train_loss: 0.2794, step time: 1.5361\n",
      "63/388, train_loss: 0.2982, step time: 1.5361\n",
      "64/388, train_loss: 0.0674, step time: 1.5373\n",
      "65/388, train_loss: 0.0834, step time: 1.5349\n",
      "66/388, train_loss: 0.1265, step time: 1.5321\n",
      "67/388, train_loss: 0.1127, step time: 1.5302\n",
      "68/388, train_loss: 0.0991, step time: 1.5340\n",
      "69/388, train_loss: 0.0296, step time: 1.5366\n",
      "70/388, train_loss: 0.3990, step time: 1.5341\n",
      "71/388, train_loss: 0.0890, step time: 1.5329\n",
      "72/388, train_loss: 0.0878, step time: 1.5326\n",
      "73/388, train_loss: 0.2264, step time: 1.5338\n",
      "74/388, train_loss: 0.2228, step time: 1.5343\n",
      "75/388, train_loss: 0.2873, step time: 1.5351\n",
      "76/388, train_loss: 0.1735, step time: 1.5329\n",
      "77/388, train_loss: 0.2764, step time: 1.5330\n",
      "78/388, train_loss: 0.1356, step time: 1.5312\n",
      "79/388, train_loss: 0.0852, step time: 1.5304\n",
      "80/388, train_loss: 0.0909, step time: 1.5336\n",
      "81/388, train_loss: 0.4159, step time: 1.5370\n",
      "82/388, train_loss: 0.1764, step time: 1.5424\n",
      "83/388, train_loss: 0.0943, step time: 1.5312\n",
      "84/388, train_loss: 0.1212, step time: 1.5335\n",
      "85/388, train_loss: 0.1885, step time: 1.5363\n",
      "86/388, train_loss: 0.0969, step time: 1.5327\n",
      "87/388, train_loss: 0.3052, step time: 1.5367\n",
      "88/388, train_loss: 0.0608, step time: 1.5315\n",
      "89/388, train_loss: 0.1162, step time: 1.5442\n",
      "90/388, train_loss: 0.2281, step time: 1.5369\n",
      "91/388, train_loss: 0.2870, step time: 1.5345\n",
      "92/388, train_loss: 0.0954, step time: 1.5371\n",
      "93/388, train_loss: 0.0879, step time: 1.5304\n",
      "94/388, train_loss: 0.1968, step time: 1.5345\n",
      "95/388, train_loss: 0.0836, step time: 1.5336\n",
      "96/388, train_loss: 0.2069, step time: 1.5352\n",
      "97/388, train_loss: 0.2078, step time: 1.5363\n",
      "98/388, train_loss: 0.0836, step time: 1.5331\n",
      "99/388, train_loss: 0.1167, step time: 1.5299\n",
      "100/388, train_loss: 0.1142, step time: 1.5290\n",
      "101/388, train_loss: 0.2577, step time: 1.5288\n",
      "102/388, train_loss: 0.1815, step time: 1.5471\n",
      "103/388, train_loss: 0.1730, step time: 1.5337\n",
      "104/388, train_loss: 0.1250, step time: 1.5385\n",
      "105/388, train_loss: 0.2269, step time: 1.5351\n",
      "106/388, train_loss: 0.3665, step time: 1.5292\n",
      "107/388, train_loss: 0.1390, step time: 1.5333\n",
      "108/388, train_loss: 0.1977, step time: 1.5401\n",
      "109/388, train_loss: 0.4418, step time: 1.5339\n",
      "110/388, train_loss: 0.1488, step time: 1.5395\n",
      "111/388, train_loss: 0.3125, step time: 1.5334\n",
      "112/388, train_loss: 0.0608, step time: 1.5348\n",
      "113/388, train_loss: 0.0987, step time: 1.5406\n",
      "114/388, train_loss: 0.3430, step time: 1.5356\n",
      "115/388, train_loss: 0.1423, step time: 1.5367\n",
      "116/388, train_loss: 0.1533, step time: 1.5334\n",
      "117/388, train_loss: 0.1401, step time: 1.5315\n",
      "118/388, train_loss: 0.2070, step time: 1.5309\n",
      "119/388, train_loss: 0.0444, step time: 1.5358\n",
      "120/388, train_loss: 0.1940, step time: 1.5347\n",
      "121/388, train_loss: 0.2547, step time: 1.5358\n",
      "122/388, train_loss: 0.1508, step time: 1.5333\n",
      "123/388, train_loss: 0.3069, step time: 1.5326\n",
      "124/388, train_loss: 0.1532, step time: 1.5358\n",
      "125/388, train_loss: 0.1343, step time: 1.5374\n",
      "126/388, train_loss: 0.4009, step time: 1.5324\n",
      "127/388, train_loss: 0.4599, step time: 1.5326\n",
      "128/388, train_loss: 0.0814, step time: 1.5318\n",
      "129/388, train_loss: 0.1296, step time: 1.5344\n",
      "130/388, train_loss: 0.1591, step time: 1.5367\n",
      "131/388, train_loss: 0.4873, step time: 1.5377\n",
      "132/388, train_loss: 0.1418, step time: 1.5418\n",
      "133/388, train_loss: 0.3348, step time: 1.5334\n",
      "134/388, train_loss: 0.1162, step time: 1.5335\n",
      "135/388, train_loss: 0.2653, step time: 1.5351\n",
      "136/388, train_loss: 0.0801, step time: 1.5336\n",
      "137/388, train_loss: 0.1292, step time: 1.5350\n",
      "138/388, train_loss: 0.2379, step time: 1.5323\n",
      "139/388, train_loss: 0.3090, step time: 1.5313\n",
      "140/388, train_loss: 0.2130, step time: 1.5351\n",
      "141/388, train_loss: 0.0971, step time: 1.5374\n",
      "142/388, train_loss: 0.2237, step time: 1.5367\n",
      "143/388, train_loss: 0.0608, step time: 1.5318\n",
      "144/388, train_loss: 0.2271, step time: 1.5322\n",
      "145/388, train_loss: 0.0921, step time: 1.5282\n",
      "146/388, train_loss: 0.0782, step time: 1.5366\n",
      "147/388, train_loss: 0.0470, step time: 1.5568\n",
      "148/388, train_loss: 0.1576, step time: 1.5355\n",
      "149/388, train_loss: 0.0571, step time: 1.5341\n",
      "150/388, train_loss: 0.1659, step time: 1.5360\n",
      "151/388, train_loss: 0.0997, step time: 1.5316\n",
      "152/388, train_loss: 0.1489, step time: 1.5324\n",
      "153/388, train_loss: 0.2841, step time: 1.5339\n",
      "154/388, train_loss: 0.1771, step time: 1.5378\n",
      "155/388, train_loss: 0.0975, step time: 1.5411\n",
      "156/388, train_loss: 0.1661, step time: 1.5361\n",
      "157/388, train_loss: 0.1410, step time: 1.5366\n",
      "158/388, train_loss: 0.1744, step time: 1.5349\n",
      "159/388, train_loss: 0.2761, step time: 1.5366\n",
      "160/388, train_loss: 0.0921, step time: 1.5377\n",
      "161/388, train_loss: 0.1197, step time: 1.5353\n",
      "162/388, train_loss: 0.2221, step time: 1.5348\n",
      "163/388, train_loss: 0.2870, step time: 1.5324\n",
      "164/388, train_loss: 0.1467, step time: 1.5340\n",
      "165/388, train_loss: 0.1508, step time: 1.5359\n",
      "166/388, train_loss: 0.3987, step time: 1.5373\n",
      "167/388, train_loss: 0.0859, step time: 1.5350\n",
      "168/388, train_loss: 0.2919, step time: 1.5335\n",
      "169/388, train_loss: 0.2605, step time: 1.5344\n",
      "170/388, train_loss: 0.2057, step time: 1.5365\n",
      "171/388, train_loss: 0.3955, step time: 1.5334\n",
      "172/388, train_loss: 0.4045, step time: 1.5328\n",
      "173/388, train_loss: 0.1332, step time: 1.5358\n",
      "174/388, train_loss: 0.1465, step time: 1.5333\n",
      "175/388, train_loss: 0.1890, step time: 1.5313\n",
      "176/388, train_loss: 0.2625, step time: 1.5308\n",
      "177/388, train_loss: 0.0984, step time: 1.5310\n",
      "178/388, train_loss: 0.1750, step time: 1.5298\n",
      "179/388, train_loss: 0.3147, step time: 1.5375\n",
      "180/388, train_loss: 0.1996, step time: 1.5373\n",
      "181/388, train_loss: 0.1735, step time: 1.5382\n",
      "182/388, train_loss: 0.0804, step time: 1.5317\n",
      "183/388, train_loss: 0.1640, step time: 1.5309\n",
      "184/388, train_loss: 0.4191, step time: 1.5432\n",
      "185/388, train_loss: 0.1691, step time: 1.5322\n",
      "186/388, train_loss: 0.5283, step time: 1.5372\n",
      "187/388, train_loss: 0.2680, step time: 1.5362\n",
      "188/388, train_loss: 0.1392, step time: 1.5370\n",
      "189/388, train_loss: 0.1663, step time: 1.5336\n",
      "190/388, train_loss: 0.2642, step time: 1.5304\n",
      "191/388, train_loss: 0.0972, step time: 1.5353\n",
      "192/388, train_loss: 0.1164, step time: 1.5369\n",
      "193/388, train_loss: 0.1485, step time: 1.5355\n",
      "194/388, train_loss: 0.0541, step time: 1.5296\n",
      "195/388, train_loss: 0.1307, step time: 1.5320\n",
      "196/388, train_loss: 0.1885, step time: 1.5317\n",
      "197/388, train_loss: 0.4979, step time: 1.5337\n",
      "198/388, train_loss: 0.2467, step time: 1.5371\n",
      "199/388, train_loss: 0.1637, step time: 1.5340\n",
      "200/388, train_loss: 0.1620, step time: 1.5557\n",
      "201/388, train_loss: 0.1156, step time: 1.5338\n",
      "202/388, train_loss: 0.1720, step time: 1.5332\n",
      "203/388, train_loss: 0.2132, step time: 1.5362\n",
      "204/388, train_loss: 0.0583, step time: 1.5362\n",
      "205/388, train_loss: 0.1106, step time: 1.5318\n",
      "206/388, train_loss: 0.1157, step time: 1.5323\n",
      "207/388, train_loss: 0.3542, step time: 1.5305\n",
      "208/388, train_loss: 0.1870, step time: 1.5389\n",
      "209/388, train_loss: 0.1360, step time: 1.5375\n",
      "210/388, train_loss: 0.0903, step time: 1.5358\n",
      "211/388, train_loss: 0.0637, step time: 1.5312\n",
      "212/388, train_loss: 0.1041, step time: 1.5289\n",
      "213/388, train_loss: 0.1266, step time: 1.5306\n",
      "214/388, train_loss: 0.4365, step time: 1.5312\n",
      "215/388, train_loss: 0.2376, step time: 1.5375\n",
      "216/388, train_loss: 0.1069, step time: 1.5346\n",
      "217/388, train_loss: 0.1332, step time: 1.5333\n",
      "218/388, train_loss: 0.3011, step time: 1.5346\n",
      "219/388, train_loss: 0.1968, step time: 1.5310\n",
      "220/388, train_loss: 0.1268, step time: 1.5316\n",
      "221/388, train_loss: 0.2134, step time: 1.5308\n",
      "222/388, train_loss: 0.2790, step time: 1.5379\n",
      "223/388, train_loss: 0.2169, step time: 1.5354\n",
      "224/388, train_loss: 0.1422, step time: 1.5342\n",
      "225/388, train_loss: 0.2888, step time: 1.5322\n",
      "226/388, train_loss: 0.0900, step time: 1.5312\n",
      "227/388, train_loss: 0.6434, step time: 1.5374\n",
      "228/388, train_loss: 0.2403, step time: 1.5365\n",
      "229/388, train_loss: 0.0645, step time: 1.5339\n",
      "230/388, train_loss: 0.3778, step time: 1.5341\n",
      "231/388, train_loss: 0.2169, step time: 1.5342\n",
      "232/388, train_loss: 0.0553, step time: 1.5336\n",
      "233/388, train_loss: 0.0713, step time: 1.5358\n",
      "234/388, train_loss: 0.0646, step time: 1.5347\n",
      "235/388, train_loss: 0.1422, step time: 1.5359\n",
      "236/388, train_loss: 0.1993, step time: 1.5338\n",
      "237/388, train_loss: 0.1165, step time: 1.5319\n",
      "238/388, train_loss: 0.2111, step time: 1.5313\n",
      "239/388, train_loss: 0.1271, step time: 1.5315\n",
      "240/388, train_loss: 0.1438, step time: 1.5338\n",
      "241/388, train_loss: 0.1929, step time: 1.5345\n",
      "242/388, train_loss: 0.2022, step time: 1.5371\n",
      "243/388, train_loss: 0.0868, step time: 1.5349\n",
      "244/388, train_loss: 0.2426, step time: 1.5359\n",
      "245/388, train_loss: 0.1896, step time: 1.5334\n",
      "246/388, train_loss: 0.2260, step time: 1.5325\n",
      "247/388, train_loss: 0.0685, step time: 1.5363\n",
      "248/388, train_loss: 0.2121, step time: 1.5320\n",
      "249/388, train_loss: 0.4256, step time: 1.5289\n",
      "250/388, train_loss: 0.1368, step time: 1.5326\n",
      "251/388, train_loss: 0.1216, step time: 1.5319\n",
      "252/388, train_loss: 0.0727, step time: 1.5346\n",
      "253/388, train_loss: 0.3868, step time: 1.5359\n",
      "254/388, train_loss: 0.1505, step time: 1.5319\n",
      "255/388, train_loss: 0.2057, step time: 1.5343\n",
      "256/388, train_loss: 0.2063, step time: 1.5369\n",
      "257/388, train_loss: 0.1324, step time: 1.5347\n",
      "258/388, train_loss: 0.2669, step time: 1.5388\n",
      "259/388, train_loss: 0.1337, step time: 1.5340\n",
      "260/388, train_loss: 0.5089, step time: 1.5410\n",
      "261/388, train_loss: 0.3200, step time: 1.5294\n",
      "262/388, train_loss: 0.1872, step time: 1.5333\n",
      "263/388, train_loss: 0.1104, step time: 1.5296\n",
      "264/388, train_loss: 0.0973, step time: 1.5362\n",
      "265/388, train_loss: 0.0921, step time: 1.5546\n",
      "266/388, train_loss: 0.2823, step time: 1.5347\n",
      "267/388, train_loss: 0.3110, step time: 1.5322\n",
      "268/388, train_loss: 0.2153, step time: 1.5329\n",
      "269/388, train_loss: 0.2960, step time: 1.5351\n",
      "270/388, train_loss: 0.0932, step time: 1.5356\n",
      "271/388, train_loss: 0.2262, step time: 1.5333\n",
      "272/388, train_loss: 0.1624, step time: 1.5323\n",
      "273/388, train_loss: 0.2812, step time: 1.5311\n",
      "274/388, train_loss: 0.0623, step time: 1.5321\n",
      "275/388, train_loss: 0.4002, step time: 1.5448\n",
      "276/388, train_loss: 0.2559, step time: 1.5355\n",
      "277/388, train_loss: 0.2006, step time: 1.5359\n",
      "278/388, train_loss: 0.1669, step time: 1.5322\n",
      "279/388, train_loss: 0.1142, step time: 1.5338\n",
      "280/388, train_loss: 0.1383, step time: 1.5343\n",
      "281/388, train_loss: 0.2216, step time: 1.5387\n",
      "282/388, train_loss: 0.0363, step time: 1.5401\n",
      "283/388, train_loss: 0.1891, step time: 1.5385\n",
      "284/388, train_loss: 0.1834, step time: 1.5359\n",
      "285/388, train_loss: 0.0526, step time: 1.5331\n",
      "286/388, train_loss: 0.2385, step time: 1.5355\n",
      "287/388, train_loss: 0.3226, step time: 1.5382\n",
      "288/388, train_loss: 0.1890, step time: 1.5389\n",
      "289/388, train_loss: 0.2600, step time: 1.5345\n",
      "290/388, train_loss: 0.1492, step time: 1.5345\n",
      "291/388, train_loss: 0.0999, step time: 1.5347\n",
      "292/388, train_loss: 0.1688, step time: 1.5375\n",
      "293/388, train_loss: 0.1084, step time: 1.5359\n",
      "294/388, train_loss: 0.2157, step time: 1.5325\n",
      "295/388, train_loss: 0.1173, step time: 1.5324\n",
      "296/388, train_loss: 0.1742, step time: 1.5358\n",
      "297/388, train_loss: 0.1986, step time: 1.5356\n",
      "298/388, train_loss: 0.1602, step time: 1.5540\n",
      "299/388, train_loss: 0.6077, step time: 1.5331\n",
      "300/388, train_loss: 0.1046, step time: 1.5306\n",
      "301/388, train_loss: 0.1010, step time: 1.5542\n",
      "302/388, train_loss: 0.0974, step time: 1.5379\n",
      "303/388, train_loss: 0.0648, step time: 1.5363\n",
      "304/388, train_loss: 0.4709, step time: 1.5343\n",
      "305/388, train_loss: 0.0706, step time: 1.5331\n",
      "306/388, train_loss: 0.2364, step time: 1.5428\n",
      "307/388, train_loss: 0.1480, step time: 1.5350\n",
      "308/388, train_loss: 0.2957, step time: 1.5331\n",
      "309/388, train_loss: 0.2098, step time: 1.5313\n",
      "310/388, train_loss: 0.3982, step time: 1.5331\n",
      "311/388, train_loss: 0.0951, step time: 1.5417\n",
      "312/388, train_loss: 0.2933, step time: 1.5381\n",
      "313/388, train_loss: 0.2517, step time: 1.5375\n",
      "314/388, train_loss: 0.2382, step time: 1.5364\n",
      "315/388, train_loss: 0.2474, step time: 1.5359\n",
      "316/388, train_loss: 0.0738, step time: 1.5385\n",
      "317/388, train_loss: 0.2006, step time: 1.5454\n",
      "318/388, train_loss: 0.1017, step time: 1.5380\n",
      "319/388, train_loss: 0.1788, step time: 1.5371\n",
      "320/388, train_loss: 0.1537, step time: 1.5400\n",
      "321/388, train_loss: 0.1078, step time: 1.5336\n",
      "322/388, train_loss: 0.2163, step time: 1.5382\n",
      "323/388, train_loss: 0.1651, step time: 1.5402\n",
      "324/388, train_loss: 0.2147, step time: 1.5342\n",
      "325/388, train_loss: 0.0647, step time: 1.5323\n",
      "326/388, train_loss: 0.1395, step time: 1.5619\n",
      "327/388, train_loss: 0.2726, step time: 1.5317\n",
      "328/388, train_loss: 0.1883, step time: 1.5330\n",
      "329/388, train_loss: 0.2002, step time: 1.5392\n",
      "330/388, train_loss: 0.2559, step time: 1.5386\n",
      "331/388, train_loss: 0.1243, step time: 1.5330\n",
      "332/388, train_loss: 0.3340, step time: 1.5326\n",
      "333/388, train_loss: 0.1106, step time: 1.5341\n",
      "334/388, train_loss: 0.1095, step time: 1.5384\n",
      "335/388, train_loss: 0.1269, step time: 1.5390\n",
      "336/388, train_loss: 0.2209, step time: 1.5340\n",
      "337/388, train_loss: 0.2927, step time: 1.5327\n",
      "338/388, train_loss: 0.2601, step time: 1.5311\n",
      "339/388, train_loss: 0.1424, step time: 1.5300\n",
      "340/388, train_loss: 0.0781, step time: 1.5323\n",
      "341/388, train_loss: 0.3491, step time: 1.5393\n",
      "342/388, train_loss: 0.0893, step time: 1.5341\n",
      "343/388, train_loss: 0.1154, step time: 1.5337\n",
      "344/388, train_loss: 0.2472, step time: 1.5352\n",
      "345/388, train_loss: 0.2646, step time: 1.5324\n",
      "346/388, train_loss: 0.1059, step time: 1.5424\n",
      "347/388, train_loss: 0.1030, step time: 1.5356\n",
      "348/388, train_loss: 0.3667, step time: 1.5356\n",
      "349/388, train_loss: 0.2826, step time: 1.5336\n",
      "350/388, train_loss: 0.0880, step time: 1.5356\n",
      "351/388, train_loss: 0.1239, step time: 1.5364\n",
      "352/388, train_loss: 0.1785, step time: 1.5365\n",
      "353/388, train_loss: 0.1297, step time: 1.5326\n",
      "354/388, train_loss: 0.3082, step time: 1.5348\n",
      "355/388, train_loss: 0.2512, step time: 1.5402\n",
      "356/388, train_loss: 0.1419, step time: 1.5390\n",
      "357/388, train_loss: 0.0298, step time: 1.5327\n",
      "358/388, train_loss: 0.0599, step time: 1.5376\n",
      "359/388, train_loss: 0.1087, step time: 1.5380\n",
      "360/388, train_loss: 0.2560, step time: 1.5372\n",
      "361/388, train_loss: 0.1079, step time: 1.5350\n",
      "362/388, train_loss: 0.0743, step time: 1.5667\n",
      "363/388, train_loss: 0.1059, step time: 1.5370\n",
      "364/388, train_loss: 0.1081, step time: 1.5365\n",
      "365/388, train_loss: 0.2461, step time: 1.5442\n",
      "366/388, train_loss: 0.3801, step time: 1.5345\n",
      "367/388, train_loss: 0.1081, step time: 1.5359\n",
      "368/388, train_loss: 0.0894, step time: 1.5375\n",
      "369/388, train_loss: 0.1427, step time: 1.5360\n",
      "370/388, train_loss: 0.2250, step time: 1.5383\n",
      "371/388, train_loss: 0.1104, step time: 1.5355\n",
      "372/388, train_loss: 0.1433, step time: 1.5357\n",
      "373/388, train_loss: 0.3725, step time: 1.5319\n",
      "374/388, train_loss: 0.1156, step time: 1.5352\n",
      "375/388, train_loss: 0.5777, step time: 1.5348\n",
      "376/388, train_loss: 0.1010, step time: 1.5355\n",
      "377/388, train_loss: 0.1915, step time: 1.5345\n",
      "378/388, train_loss: 0.2083, step time: 1.5298\n",
      "379/388, train_loss: 0.1113, step time: 1.5307\n",
      "380/388, train_loss: 0.1919, step time: 1.5320\n",
      "381/388, train_loss: 0.2226, step time: 1.5337\n",
      "382/388, train_loss: 0.2065, step time: 1.5353\n",
      "383/388, train_loss: 0.1021, step time: 1.5360\n",
      "384/388, train_loss: 0.0489, step time: 1.5347\n",
      "385/388, train_loss: 0.0932, step time: 1.5332\n",
      "386/388, train_loss: 0.1173, step time: 1.5332\n",
      "387/388, train_loss: 0.1258, step time: 1.5351\n",
      "388/388, train_loss: 0.0946, step time: 1.5326\n",
      "epoch 42 average loss: 0.1900\n",
      "current epoch: 42 current mean dice: 0.7631 tc: 0.8138 wt: 0.9024 et: 0.5729\n",
      "best mean dice: 0.7640 at epoch: 36\n",
      "time consuming of epoch 42 is: 703.4567\n",
      "----------\n",
      "epoch 43/100\n",
      "1/388, train_loss: 0.4712, step time: 1.5468\n",
      "2/388, train_loss: 0.1105, step time: 1.5359\n",
      "3/388, train_loss: 0.1012, step time: 1.5326\n",
      "4/388, train_loss: 0.1422, step time: 1.5310\n",
      "5/388, train_loss: 0.2365, step time: 1.5382\n",
      "6/388, train_loss: 0.2653, step time: 1.5320\n",
      "7/388, train_loss: 0.0848, step time: 1.5318\n",
      "8/388, train_loss: 0.1543, step time: 1.5361\n",
      "9/388, train_loss: 0.1216, step time: 1.5368\n",
      "10/388, train_loss: 0.3151, step time: 1.5314\n",
      "11/388, train_loss: 0.5025, step time: 1.5290\n",
      "12/388, train_loss: 0.1356, step time: 1.5348\n",
      "13/388, train_loss: 0.1020, step time: 1.5375\n",
      "14/388, train_loss: 0.0684, step time: 1.5362\n",
      "15/388, train_loss: 0.1015, step time: 1.5375\n",
      "16/388, train_loss: 0.1704, step time: 1.5339\n",
      "17/388, train_loss: 0.1235, step time: 1.5321\n",
      "18/388, train_loss: 0.1996, step time: 1.5358\n",
      "19/388, train_loss: 0.3752, step time: 1.5386\n",
      "20/388, train_loss: 0.1681, step time: 1.5359\n",
      "21/388, train_loss: 0.4288, step time: 1.5342\n",
      "22/388, train_loss: 0.3917, step time: 1.5332\n",
      "23/388, train_loss: 0.2408, step time: 1.5318\n",
      "24/388, train_loss: 0.2862, step time: 1.5389\n",
      "25/388, train_loss: 0.1438, step time: 1.5421\n",
      "26/388, train_loss: 0.1181, step time: 1.5362\n",
      "27/388, train_loss: 0.2628, step time: 1.5359\n",
      "28/388, train_loss: 0.0963, step time: 1.5352\n",
      "29/388, train_loss: 0.2243, step time: 1.5343\n",
      "30/388, train_loss: 0.1315, step time: 1.5337\n",
      "31/388, train_loss: 0.2115, step time: 1.5409\n",
      "32/388, train_loss: 0.0881, step time: 1.5584\n",
      "33/388, train_loss: 0.1475, step time: 1.5372\n",
      "34/388, train_loss: 0.0712, step time: 1.5356\n",
      "35/388, train_loss: 0.1818, step time: 1.5378\n",
      "36/388, train_loss: 0.1799, step time: 1.5387\n",
      "37/388, train_loss: 0.1694, step time: 1.5368\n",
      "38/388, train_loss: 0.2435, step time: 1.5326\n",
      "39/388, train_loss: 0.1795, step time: 1.5334\n",
      "40/388, train_loss: 0.1182, step time: 1.5331\n",
      "41/388, train_loss: 0.4408, step time: 1.5348\n",
      "42/388, train_loss: 0.0490, step time: 1.5399\n",
      "43/388, train_loss: 0.3083, step time: 1.5358\n",
      "44/388, train_loss: 0.1371, step time: 1.5341\n",
      "45/388, train_loss: 0.0783, step time: 1.5568\n",
      "46/388, train_loss: 0.0962, step time: 1.5341\n",
      "47/388, train_loss: 0.0644, step time: 1.5341\n",
      "48/388, train_loss: 0.1401, step time: 1.5546\n",
      "49/388, train_loss: 0.0341, step time: 1.5363\n",
      "50/388, train_loss: 0.1612, step time: 1.5386\n",
      "51/388, train_loss: 0.0978, step time: 1.5336\n",
      "52/388, train_loss: 0.1316, step time: 1.5351\n",
      "53/388, train_loss: 0.1232, step time: 1.5358\n",
      "54/388, train_loss: 0.2024, step time: 1.5382\n",
      "55/388, train_loss: 0.1560, step time: 1.5331\n",
      "56/388, train_loss: 0.1738, step time: 1.5349\n",
      "57/388, train_loss: 0.1124, step time: 1.5319\n",
      "58/388, train_loss: 0.3207, step time: 1.5387\n",
      "59/388, train_loss: 0.1223, step time: 1.5397\n",
      "60/388, train_loss: 0.0962, step time: 1.5370\n",
      "61/388, train_loss: 0.1994, step time: 1.5316\n",
      "62/388, train_loss: 0.1233, step time: 1.5314\n",
      "63/388, train_loss: 0.2104, step time: 1.5300\n",
      "64/388, train_loss: 0.0984, step time: 1.5371\n",
      "65/388, train_loss: 0.2054, step time: 1.5333\n",
      "66/388, train_loss: 0.0836, step time: 1.5328\n",
      "67/388, train_loss: 0.1543, step time: 1.5323\n",
      "68/388, train_loss: 0.0756, step time: 1.5324\n",
      "69/388, train_loss: 0.2584, step time: 1.5306\n",
      "70/388, train_loss: 0.1428, step time: 1.5333\n",
      "71/388, train_loss: 0.0961, step time: 1.5391\n",
      "72/388, train_loss: 0.1063, step time: 1.5331\n",
      "73/388, train_loss: 0.3101, step time: 1.5325\n",
      "74/388, train_loss: 0.1588, step time: 1.5300\n",
      "75/388, train_loss: 0.1229, step time: 1.5321\n",
      "76/388, train_loss: 0.1981, step time: 1.5326\n",
      "77/388, train_loss: 0.2480, step time: 1.5371\n",
      "78/388, train_loss: 0.3828, step time: 1.5355\n",
      "79/388, train_loss: 0.1566, step time: 1.5357\n",
      "80/388, train_loss: 0.1437, step time: 1.5647\n",
      "81/388, train_loss: 0.2557, step time: 1.5345\n",
      "82/388, train_loss: 0.1650, step time: 1.5414\n",
      "83/388, train_loss: 0.0725, step time: 1.5336\n",
      "84/388, train_loss: 0.1244, step time: 1.5345\n",
      "85/388, train_loss: 0.1934, step time: 1.5407\n",
      "86/388, train_loss: 0.2537, step time: 1.5344\n",
      "87/388, train_loss: 0.3731, step time: 1.5370\n",
      "88/388, train_loss: 0.2107, step time: 1.5319\n",
      "89/388, train_loss: 0.4729, step time: 1.5289\n",
      "90/388, train_loss: 0.2685, step time: 1.5337\n",
      "91/388, train_loss: 0.2311, step time: 1.5358\n",
      "92/388, train_loss: 0.1388, step time: 1.5361\n",
      "93/388, train_loss: 0.0460, step time: 1.5362\n",
      "94/388, train_loss: 0.0785, step time: 1.5325\n",
      "95/388, train_loss: 0.0932, step time: 1.5298\n",
      "96/388, train_loss: 0.1629, step time: 1.5345\n",
      "97/388, train_loss: 0.1298, step time: 1.5378\n",
      "98/388, train_loss: 0.2313, step time: 1.5351\n",
      "99/388, train_loss: 0.1951, step time: 1.5341\n",
      "100/388, train_loss: 0.0407, step time: 1.5331\n",
      "101/388, train_loss: 0.1154, step time: 1.5293\n",
      "102/388, train_loss: 0.0846, step time: 1.5320\n",
      "103/388, train_loss: 0.5986, step time: 1.5346\n",
      "104/388, train_loss: 0.0263, step time: 1.5366\n",
      "105/388, train_loss: 0.0915, step time: 1.5358\n",
      "106/388, train_loss: 0.1806, step time: 1.5332\n",
      "107/388, train_loss: 0.2658, step time: 1.5336\n",
      "108/388, train_loss: 0.2156, step time: 1.5316\n",
      "109/388, train_loss: 0.4054, step time: 1.5348\n",
      "110/388, train_loss: 0.4405, step time: 1.5366\n",
      "111/388, train_loss: 0.1445, step time: 1.5328\n",
      "112/388, train_loss: 0.1537, step time: 1.5356\n",
      "113/388, train_loss: 0.1412, step time: 1.5344\n",
      "114/388, train_loss: 0.1254, step time: 1.5318\n",
      "115/388, train_loss: 0.1893, step time: 1.5293\n",
      "116/388, train_loss: 0.0711, step time: 1.5327\n",
      "117/388, train_loss: 0.3036, step time: 1.5365\n",
      "118/388, train_loss: 0.4057, step time: 1.5355\n",
      "119/388, train_loss: 0.0876, step time: 1.5340\n",
      "120/388, train_loss: 0.0752, step time: 1.5339\n",
      "121/388, train_loss: 0.1219, step time: 1.5302\n",
      "122/388, train_loss: 0.1145, step time: 1.5330\n",
      "123/388, train_loss: 0.2935, step time: 1.5284\n",
      "124/388, train_loss: 0.2023, step time: 1.5370\n",
      "125/388, train_loss: 0.0910, step time: 1.5373\n",
      "126/388, train_loss: 0.3854, step time: 1.5328\n",
      "127/388, train_loss: 0.0996, step time: 1.5335\n",
      "128/388, train_loss: 0.2458, step time: 1.5319\n",
      "129/388, train_loss: 0.1054, step time: 1.5316\n",
      "130/388, train_loss: 0.2235, step time: 1.5309\n",
      "131/388, train_loss: 0.0977, step time: 1.5328\n",
      "132/388, train_loss: 0.3255, step time: 1.5308\n",
      "133/388, train_loss: 0.4001, step time: 1.5347\n",
      "134/388, train_loss: 0.1153, step time: 1.5396\n",
      "135/388, train_loss: 0.0504, step time: 1.5319\n",
      "136/388, train_loss: 0.1716, step time: 1.5341\n",
      "137/388, train_loss: 0.1511, step time: 1.5349\n",
      "138/388, train_loss: 0.1967, step time: 1.5350\n",
      "139/388, train_loss: 0.1299, step time: 1.5350\n",
      "140/388, train_loss: 0.2747, step time: 1.5370\n",
      "141/388, train_loss: 0.1786, step time: 1.5330\n",
      "142/388, train_loss: 0.0966, step time: 1.5304\n",
      "143/388, train_loss: 0.0945, step time: 1.5292\n",
      "144/388, train_loss: 0.0582, step time: 1.5337\n",
      "145/388, train_loss: 0.6084, step time: 1.5321\n",
      "146/388, train_loss: 0.1690, step time: 1.5374\n",
      "147/388, train_loss: 0.5193, step time: 1.5405\n",
      "148/388, train_loss: 0.2489, step time: 1.5343\n",
      "149/388, train_loss: 0.4399, step time: 1.5361\n",
      "150/388, train_loss: 0.1468, step time: 1.5388\n",
      "151/388, train_loss: 0.2054, step time: 1.5311\n",
      "152/388, train_loss: 0.4065, step time: 1.5368\n",
      "153/388, train_loss: 0.0680, step time: 1.5356\n",
      "154/388, train_loss: 0.1303, step time: 1.5369\n",
      "155/388, train_loss: 0.2039, step time: 1.5345\n",
      "156/388, train_loss: 0.0899, step time: 1.5344\n",
      "157/388, train_loss: 0.2066, step time: 1.5320\n",
      "158/388, train_loss: 0.1427, step time: 1.5328\n",
      "159/388, train_loss: 0.0896, step time: 1.5353\n",
      "160/388, train_loss: 0.0673, step time: 1.5390\n",
      "161/388, train_loss: 0.1410, step time: 1.5320\n",
      "162/388, train_loss: 0.3624, step time: 1.5334\n",
      "163/388, train_loss: 0.1656, step time: 1.5323\n",
      "164/388, train_loss: 0.1562, step time: 1.5335\n",
      "165/388, train_loss: 0.1123, step time: 1.5367\n",
      "166/388, train_loss: 0.1822, step time: 1.5374\n",
      "167/388, train_loss: 0.1138, step time: 1.5368\n",
      "168/388, train_loss: 0.0972, step time: 1.5365\n",
      "169/388, train_loss: 0.1054, step time: 1.5330\n",
      "170/388, train_loss: 0.0871, step time: 1.5327\n",
      "171/388, train_loss: 0.1388, step time: 1.5317\n",
      "172/388, train_loss: 0.1753, step time: 1.5336\n",
      "173/388, train_loss: 0.2302, step time: 1.5341\n",
      "174/388, train_loss: 0.1729, step time: 1.5372\n",
      "175/388, train_loss: 0.1781, step time: 1.5341\n",
      "176/388, train_loss: 0.0478, step time: 1.5313\n",
      "177/388, train_loss: 0.1583, step time: 1.5310\n",
      "178/388, train_loss: 0.2818, step time: 1.5302\n",
      "179/388, train_loss: 0.1720, step time: 1.5489\n",
      "180/388, train_loss: 0.2513, step time: 1.5390\n",
      "181/388, train_loss: 0.1197, step time: 1.5371\n",
      "182/388, train_loss: 0.2441, step time: 1.5330\n",
      "183/388, train_loss: 0.2223, step time: 1.5319\n",
      "184/388, train_loss: 0.2305, step time: 1.5313\n",
      "185/388, train_loss: 0.1111, step time: 1.5343\n",
      "186/388, train_loss: 0.2209, step time: 1.5376\n",
      "187/388, train_loss: 0.2009, step time: 1.5353\n",
      "188/388, train_loss: 0.2996, step time: 1.5358\n",
      "189/388, train_loss: 0.2322, step time: 1.5326\n",
      "190/388, train_loss: 0.1184, step time: 1.5324\n",
      "191/388, train_loss: 0.1709, step time: 1.5336\n",
      "192/388, train_loss: 0.0907, step time: 1.5375\n",
      "193/388, train_loss: 0.2111, step time: 1.5364\n",
      "194/388, train_loss: 0.0958, step time: 1.5331\n",
      "195/388, train_loss: 0.1359, step time: 1.5334\n",
      "196/388, train_loss: 0.2611, step time: 1.5346\n",
      "197/388, train_loss: 0.1437, step time: 1.5360\n",
      "198/388, train_loss: 0.1658, step time: 1.5385\n",
      "199/388, train_loss: 0.1338, step time: 1.5337\n",
      "200/388, train_loss: 0.1673, step time: 1.5316\n",
      "201/388, train_loss: 0.2860, step time: 1.5411\n",
      "202/388, train_loss: 0.2318, step time: 1.5385\n",
      "203/388, train_loss: 0.1011, step time: 1.5452\n",
      "204/388, train_loss: 0.0639, step time: 1.5335\n",
      "205/388, train_loss: 0.4004, step time: 1.5316\n",
      "206/388, train_loss: 0.1058, step time: 1.5346\n",
      "207/388, train_loss: 0.2428, step time: 1.5321\n",
      "208/388, train_loss: 0.3479, step time: 1.5386\n",
      "209/388, train_loss: 0.1080, step time: 1.5343\n",
      "210/388, train_loss: 0.2979, step time: 1.5355\n",
      "211/388, train_loss: 0.0862, step time: 1.5328\n",
      "212/388, train_loss: 0.1282, step time: 1.5341\n",
      "213/388, train_loss: 0.2105, step time: 1.5306\n",
      "214/388, train_loss: 0.2020, step time: 1.5343\n",
      "215/388, train_loss: 0.0839, step time: 1.5377\n",
      "216/388, train_loss: 0.3084, step time: 1.5383\n",
      "217/388, train_loss: 0.0961, step time: 1.5333\n",
      "218/388, train_loss: 0.1411, step time: 1.5329\n",
      "219/388, train_loss: 0.1118, step time: 1.5303\n",
      "220/388, train_loss: 0.2610, step time: 1.5323\n",
      "221/388, train_loss: 0.2905, step time: 1.5375\n",
      "222/388, train_loss: 0.2997, step time: 1.5372\n",
      "223/388, train_loss: 0.1469, step time: 1.5351\n",
      "224/388, train_loss: 0.1895, step time: 1.5338\n",
      "225/388, train_loss: 0.2250, step time: 1.5346\n",
      "226/388, train_loss: 0.2536, step time: 1.5353\n",
      "227/388, train_loss: 0.1886, step time: 1.5356\n",
      "228/388, train_loss: 0.2160, step time: 1.5332\n",
      "229/388, train_loss: 0.2977, step time: 1.5342\n",
      "230/388, train_loss: 0.2834, step time: 1.5376\n",
      "231/388, train_loss: 0.4042, step time: 1.5320\n",
      "232/388, train_loss: 0.3204, step time: 1.5303\n",
      "233/388, train_loss: 0.1462, step time: 1.5365\n",
      "234/388, train_loss: 0.2881, step time: 1.5385\n",
      "235/388, train_loss: 0.0913, step time: 1.5337\n",
      "236/388, train_loss: 0.1433, step time: 1.5338\n",
      "237/388, train_loss: 0.1926, step time: 1.5296\n",
      "238/388, train_loss: 0.1049, step time: 1.5320\n",
      "239/388, train_loss: 0.3875, step time: 1.5380\n",
      "240/388, train_loss: 0.1634, step time: 1.5358\n",
      "241/388, train_loss: 0.2772, step time: 1.5325\n",
      "242/388, train_loss: 0.1073, step time: 1.5322\n",
      "243/388, train_loss: 0.1923, step time: 1.5339\n",
      "244/388, train_loss: 0.1812, step time: 1.5414\n",
      "245/388, train_loss: 0.1942, step time: 1.5601\n",
      "246/388, train_loss: 0.2082, step time: 1.5331\n",
      "247/388, train_loss: 0.0689, step time: 1.5348\n",
      "248/388, train_loss: 0.0782, step time: 1.5371\n",
      "249/388, train_loss: 0.1415, step time: 1.5367\n",
      "250/388, train_loss: 0.4200, step time: 1.5362\n",
      "251/388, train_loss: 0.5291, step time: 1.5320\n",
      "252/388, train_loss: 0.2512, step time: 1.5318\n",
      "253/388, train_loss: 0.2073, step time: 1.5314\n",
      "254/388, train_loss: 0.0693, step time: 1.5335\n",
      "255/388, train_loss: 0.1394, step time: 1.5360\n",
      "256/388, train_loss: 0.3974, step time: 1.5351\n",
      "257/388, train_loss: 0.0975, step time: 1.5350\n",
      "258/388, train_loss: 0.1465, step time: 1.5312\n",
      "259/388, train_loss: 0.0392, step time: 1.5328\n",
      "260/388, train_loss: 0.1185, step time: 1.5319\n",
      "261/388, train_loss: 0.0983, step time: 1.5390\n",
      "262/388, train_loss: 0.0895, step time: 1.5336\n",
      "263/388, train_loss: 0.2562, step time: 1.5369\n",
      "264/388, train_loss: 0.2924, step time: 1.5365\n",
      "265/388, train_loss: 0.3792, step time: 1.5318\n",
      "266/388, train_loss: 0.0943, step time: 1.5323\n",
      "267/388, train_loss: 0.0341, step time: 1.5314\n",
      "268/388, train_loss: 0.1157, step time: 1.5363\n",
      "269/388, train_loss: 0.1113, step time: 1.5356\n",
      "270/388, train_loss: 0.0988, step time: 1.5343\n",
      "271/388, train_loss: 0.3046, step time: 1.5337\n",
      "272/388, train_loss: 0.3004, step time: 1.5318\n",
      "273/388, train_loss: 0.2336, step time: 1.5309\n",
      "274/388, train_loss: 0.0952, step time: 1.5335\n",
      "275/388, train_loss: 0.3778, step time: 1.5320\n",
      "276/388, train_loss: 0.0659, step time: 1.5345\n",
      "277/388, train_loss: 0.1502, step time: 1.5356\n",
      "278/388, train_loss: 0.2704, step time: 1.5341\n",
      "279/388, train_loss: 0.3653, step time: 1.5341\n",
      "280/388, train_loss: 0.1843, step time: 1.5371\n",
      "281/388, train_loss: 0.2465, step time: 1.5330\n",
      "282/388, train_loss: 0.1937, step time: 1.5359\n",
      "283/388, train_loss: 0.2181, step time: 1.5378\n",
      "284/388, train_loss: 0.1580, step time: 1.5344\n",
      "285/388, train_loss: 0.3408, step time: 1.5329\n",
      "286/388, train_loss: 0.0664, step time: 1.5313\n",
      "287/388, train_loss: 0.1426, step time: 1.5328\n",
      "288/388, train_loss: 0.1004, step time: 1.5362\n",
      "289/388, train_loss: 0.1979, step time: 1.5389\n",
      "290/388, train_loss: 0.1873, step time: 1.5339\n",
      "291/388, train_loss: 0.2876, step time: 1.5295\n",
      "292/388, train_loss: 0.1276, step time: 1.5329\n",
      "293/388, train_loss: 0.2154, step time: 1.5313\n",
      "294/388, train_loss: 0.1057, step time: 1.5340\n",
      "295/388, train_loss: 0.1885, step time: 1.5511\n",
      "296/388, train_loss: 0.0810, step time: 1.5351\n",
      "297/388, train_loss: 0.1927, step time: 1.5331\n",
      "298/388, train_loss: 0.3591, step time: 1.5350\n",
      "299/388, train_loss: 0.1919, step time: 1.5366\n",
      "300/388, train_loss: 0.0685, step time: 1.5366\n",
      "301/388, train_loss: 0.0841, step time: 1.5361\n",
      "302/388, train_loss: 0.1731, step time: 1.5317\n",
      "303/388, train_loss: 0.1388, step time: 1.5325\n",
      "304/388, train_loss: 0.2031, step time: 1.5309\n",
      "305/388, train_loss: 0.0810, step time: 1.5317\n",
      "306/388, train_loss: 0.1695, step time: 1.5469\n",
      "307/388, train_loss: 0.5147, step time: 1.5357\n",
      "308/388, train_loss: 0.1841, step time: 1.5325\n",
      "309/388, train_loss: 0.0806, step time: 1.5311\n",
      "310/388, train_loss: 0.2221, step time: 1.5353\n",
      "311/388, train_loss: 0.2829, step time: 1.5338\n",
      "312/388, train_loss: 0.2053, step time: 1.5327\n",
      "313/388, train_loss: 0.1154, step time: 1.5350\n",
      "314/388, train_loss: 0.3362, step time: 1.5357\n",
      "315/388, train_loss: 0.2033, step time: 1.5373\n",
      "316/388, train_loss: 0.1206, step time: 1.5341\n",
      "317/388, train_loss: 0.1965, step time: 1.5304\n",
      "318/388, train_loss: 0.3548, step time: 1.5312\n",
      "319/388, train_loss: 0.2195, step time: 1.5386\n",
      "320/388, train_loss: 0.2252, step time: 1.5367\n",
      "321/388, train_loss: 0.1421, step time: 1.5376\n",
      "322/388, train_loss: 0.1113, step time: 1.5314\n",
      "323/388, train_loss: 0.0962, step time: 1.5311\n",
      "324/388, train_loss: 0.1487, step time: 1.5325\n",
      "325/388, train_loss: 0.0887, step time: 1.5333\n",
      "326/388, train_loss: 0.1799, step time: 1.5372\n",
      "327/388, train_loss: 0.0872, step time: 1.5348\n",
      "328/388, train_loss: 0.1210, step time: 1.5348\n",
      "329/388, train_loss: 0.0854, step time: 1.5422\n",
      "330/388, train_loss: 0.2941, step time: 1.5683\n",
      "331/388, train_loss: 0.2968, step time: 1.5337\n",
      "332/388, train_loss: 0.1224, step time: 1.5307\n",
      "333/388, train_loss: 0.1313, step time: 1.5311\n",
      "334/388, train_loss: 0.2210, step time: 1.5359\n",
      "335/388, train_loss: 0.2072, step time: 1.5358\n",
      "336/388, train_loss: 0.1068, step time: 1.5351\n",
      "337/388, train_loss: 0.2158, step time: 1.5348\n",
      "338/388, train_loss: 0.1186, step time: 1.5351\n",
      "339/388, train_loss: 0.0904, step time: 1.5335\n",
      "340/388, train_loss: 0.2193, step time: 1.5380\n",
      "341/388, train_loss: 0.1106, step time: 1.5382\n",
      "342/388, train_loss: 0.0622, step time: 1.5325\n",
      "343/388, train_loss: 0.1285, step time: 1.5344\n",
      "344/388, train_loss: 0.3760, step time: 1.5421\n",
      "345/388, train_loss: 0.1361, step time: 1.5334\n",
      "346/388, train_loss: 0.2061, step time: 1.5370\n",
      "347/388, train_loss: 0.1780, step time: 1.5322\n",
      "348/388, train_loss: 0.0801, step time: 1.5322\n",
      "349/388, train_loss: 0.3962, step time: 1.5397\n",
      "350/388, train_loss: 0.3523, step time: 1.5351\n",
      "351/388, train_loss: 0.2606, step time: 1.5352\n",
      "352/388, train_loss: 0.1029, step time: 1.5324\n",
      "353/388, train_loss: 0.1744, step time: 1.5326\n",
      "354/388, train_loss: 0.0932, step time: 1.5354\n",
      "355/388, train_loss: 0.1275, step time: 1.5368\n",
      "356/388, train_loss: 0.0927, step time: 1.5357\n",
      "357/388, train_loss: 0.6120, step time: 1.5343\n",
      "358/388, train_loss: 0.7103, step time: 1.5343\n",
      "359/388, train_loss: 0.0764, step time: 1.5330\n",
      "360/388, train_loss: 0.1835, step time: 1.5373\n",
      "361/388, train_loss: 0.1295, step time: 1.5334\n",
      "362/388, train_loss: 0.1120, step time: 1.5356\n",
      "363/388, train_loss: 0.0764, step time: 1.5354\n",
      "364/388, train_loss: 0.4987, step time: 1.5359\n",
      "365/388, train_loss: 0.2254, step time: 1.5315\n",
      "366/388, train_loss: 0.0733, step time: 1.5330\n",
      "367/388, train_loss: 0.3863, step time: 1.5313\n",
      "368/388, train_loss: 0.1856, step time: 1.5350\n",
      "369/388, train_loss: 0.1175, step time: 1.5445\n",
      "370/388, train_loss: 0.5421, step time: 1.5353\n",
      "371/388, train_loss: 0.2707, step time: 1.5325\n",
      "372/388, train_loss: 0.3123, step time: 1.5323\n",
      "373/388, train_loss: 0.1873, step time: 1.5298\n",
      "374/388, train_loss: 0.1074, step time: 1.5377\n",
      "375/388, train_loss: 0.3176, step time: 1.5374\n",
      "376/388, train_loss: 0.1506, step time: 1.5342\n",
      "377/388, train_loss: 0.1820, step time: 1.5324\n",
      "378/388, train_loss: 0.1607, step time: 1.5327\n",
      "379/388, train_loss: 0.1407, step time: 1.5337\n",
      "380/388, train_loss: 0.1627, step time: 1.5384\n",
      "381/388, train_loss: 0.1450, step time: 1.5372\n",
      "382/388, train_loss: 0.1780, step time: 1.5340\n",
      "383/388, train_loss: 0.2404, step time: 1.5315\n",
      "384/388, train_loss: 0.2940, step time: 1.5322\n",
      "385/388, train_loss: 0.0989, step time: 1.5345\n",
      "386/388, train_loss: 0.1839, step time: 1.5295\n",
      "387/388, train_loss: 0.0799, step time: 1.5363\n",
      "388/388, train_loss: 0.3717, step time: 1.5364\n",
      "epoch 43 average loss: 0.1923\n",
      "current epoch: 43 current mean dice: 0.7478 tc: 0.7973 wt: 0.8974 et: 0.5488\n",
      "best mean dice: 0.7640 at epoch: 36\n",
      "time consuming of epoch 43 is: 702.6894\n",
      "----------\n",
      "epoch 44/100\n",
      "1/388, train_loss: 0.0877, step time: 1.5512\n",
      "2/388, train_loss: 0.2418, step time: 1.5344\n",
      "3/388, train_loss: 0.0523, step time: 1.5341\n",
      "4/388, train_loss: 0.3705, step time: 1.5371\n",
      "5/388, train_loss: 0.0508, step time: 1.5343\n",
      "6/388, train_loss: 0.3273, step time: 1.5330\n",
      "7/388, train_loss: 0.1185, step time: 1.5401\n",
      "8/388, train_loss: 0.1004, step time: 1.5416\n",
      "9/388, train_loss: 0.3377, step time: 1.5319\n",
      "10/388, train_loss: 0.0910, step time: 1.5354\n",
      "11/388, train_loss: 0.1234, step time: 1.5328\n",
      "12/388, train_loss: 0.0733, step time: 1.5345\n",
      "13/388, train_loss: 0.1385, step time: 1.5338\n",
      "14/388, train_loss: 0.0945, step time: 1.5387\n",
      "15/388, train_loss: 0.0658, step time: 1.5376\n",
      "16/388, train_loss: 0.1002, step time: 1.5344\n",
      "17/388, train_loss: 0.1118, step time: 1.5333\n",
      "18/388, train_loss: 0.3264, step time: 1.5598\n",
      "19/388, train_loss: 0.1263, step time: 1.5319\n",
      "20/388, train_loss: 0.0791, step time: 1.5332\n",
      "21/388, train_loss: 0.2580, step time: 1.5373\n",
      "22/388, train_loss: 0.2591, step time: 1.5353\n",
      "23/388, train_loss: 0.2221, step time: 1.5309\n",
      "24/388, train_loss: 0.2414, step time: 1.5320\n",
      "25/388, train_loss: 0.3832, step time: 1.5362\n",
      "26/388, train_loss: 0.1662, step time: 1.5359\n",
      "27/388, train_loss: 0.1598, step time: 1.5313\n",
      "28/388, train_loss: 0.6094, step time: 1.5356\n",
      "29/388, train_loss: 0.2326, step time: 1.5319\n",
      "30/388, train_loss: 0.3905, step time: 1.5343\n",
      "31/388, train_loss: 0.2606, step time: 1.5361\n",
      "32/388, train_loss: 0.0763, step time: 1.5368\n",
      "33/388, train_loss: 0.1811, step time: 1.5361\n",
      "34/388, train_loss: 0.0843, step time: 1.5358\n",
      "35/388, train_loss: 0.1026, step time: 1.5366\n",
      "36/388, train_loss: 0.3023, step time: 1.5353\n",
      "37/388, train_loss: 0.1528, step time: 1.5332\n",
      "38/388, train_loss: 0.1058, step time: 1.5325\n",
      "39/388, train_loss: 0.0664, step time: 1.5315\n",
      "40/388, train_loss: 0.2376, step time: 1.5345\n",
      "41/388, train_loss: 0.1090, step time: 1.5312\n",
      "42/388, train_loss: 0.1388, step time: 1.5359\n",
      "43/388, train_loss: 0.2617, step time: 1.5445\n",
      "44/388, train_loss: 0.1098, step time: 1.5327\n",
      "45/388, train_loss: 0.2272, step time: 1.5343\n",
      "46/388, train_loss: 0.5752, step time: 1.5394\n",
      "47/388, train_loss: 0.1054, step time: 1.5314\n",
      "48/388, train_loss: 0.2612, step time: 1.5332\n",
      "49/388, train_loss: 0.6010, step time: 1.5628\n",
      "50/388, train_loss: 0.2904, step time: 1.5609\n",
      "51/388, train_loss: 0.0785, step time: 1.5348\n",
      "52/388, train_loss: 0.0608, step time: 1.5372\n",
      "53/388, train_loss: 0.2018, step time: 1.5294\n",
      "54/388, train_loss: 0.2271, step time: 1.5339\n",
      "55/388, train_loss: 0.0742, step time: 1.5344\n",
      "56/388, train_loss: 0.2479, step time: 1.5395\n",
      "57/388, train_loss: 0.1002, step time: 1.5334\n",
      "58/388, train_loss: 0.5088, step time: 1.5311\n",
      "59/388, train_loss: 0.1059, step time: 1.5347\n",
      "60/388, train_loss: 0.1374, step time: 1.5424\n",
      "61/388, train_loss: 0.1335, step time: 1.5346\n",
      "62/388, train_loss: 0.0664, step time: 1.5325\n",
      "63/388, train_loss: 0.1652, step time: 1.5336\n",
      "64/388, train_loss: 0.1883, step time: 1.5336\n",
      "65/388, train_loss: 0.2315, step time: 1.5395\n",
      "66/388, train_loss: 0.2232, step time: 1.5354\n",
      "67/388, train_loss: 0.0479, step time: 1.5338\n",
      "68/388, train_loss: 0.0843, step time: 1.5345\n",
      "69/388, train_loss: 0.1421, step time: 1.5356\n",
      "70/388, train_loss: 0.0890, step time: 1.5421\n",
      "71/388, train_loss: 0.0354, step time: 1.5351\n",
      "72/388, train_loss: 0.3334, step time: 1.5315\n",
      "73/388, train_loss: 0.1916, step time: 1.5359\n",
      "74/388, train_loss: 0.2661, step time: 1.5326\n",
      "75/388, train_loss: 0.1332, step time: 1.5318\n",
      "76/388, train_loss: 0.1747, step time: 1.5304\n",
      "77/388, train_loss: 0.2597, step time: 1.5330\n",
      "78/388, train_loss: 0.2082, step time: 1.5361\n",
      "79/388, train_loss: 0.0915, step time: 1.5357\n",
      "80/388, train_loss: 0.1763, step time: 1.5345\n",
      "81/388, train_loss: 0.2317, step time: 1.5342\n",
      "82/388, train_loss: 0.1359, step time: 1.5324\n",
      "83/388, train_loss: 0.1614, step time: 1.5307\n",
      "84/388, train_loss: 0.1585, step time: 1.5385\n",
      "85/388, train_loss: 0.2832, step time: 1.5409\n",
      "86/388, train_loss: 0.3404, step time: 1.5348\n",
      "87/388, train_loss: 0.1503, step time: 1.5335\n",
      "88/388, train_loss: 0.1685, step time: 1.5331\n",
      "89/388, train_loss: 0.2440, step time: 1.5355\n",
      "90/388, train_loss: 0.1722, step time: 1.5374\n",
      "91/388, train_loss: 0.1007, step time: 1.5349\n",
      "92/388, train_loss: 0.0545, step time: 1.5319\n",
      "93/388, train_loss: 0.0324, step time: 1.5297\n",
      "94/388, train_loss: 0.2485, step time: 1.5310\n",
      "95/388, train_loss: 0.2372, step time: 1.5355\n",
      "96/388, train_loss: 0.2830, step time: 1.5377\n",
      "97/388, train_loss: 0.1278, step time: 1.5315\n",
      "98/388, train_loss: 0.1145, step time: 1.5326\n",
      "99/388, train_loss: 0.1544, step time: 1.5339\n",
      "100/388, train_loss: 0.2104, step time: 1.5322\n",
      "101/388, train_loss: 0.3647, step time: 1.5371\n",
      "102/388, train_loss: 0.2849, step time: 1.5350\n",
      "103/388, train_loss: 0.1269, step time: 1.5323\n",
      "104/388, train_loss: 0.1615, step time: 1.5338\n",
      "105/388, train_loss: 0.4739, step time: 1.5316\n",
      "106/388, train_loss: 0.0747, step time: 1.5367\n",
      "107/388, train_loss: 0.2855, step time: 1.5359\n",
      "108/388, train_loss: 0.1166, step time: 1.5344\n",
      "109/388, train_loss: 0.1941, step time: 1.5465\n",
      "110/388, train_loss: 0.1678, step time: 1.5337\n",
      "111/388, train_loss: 0.1057, step time: 1.5338\n",
      "112/388, train_loss: 0.0837, step time: 1.5362\n",
      "113/388, train_loss: 0.0791, step time: 1.5311\n",
      "114/388, train_loss: 0.1971, step time: 1.5360\n",
      "115/388, train_loss: 0.3676, step time: 1.5353\n",
      "116/388, train_loss: 0.2266, step time: 1.5336\n",
      "117/388, train_loss: 0.2461, step time: 1.5399\n",
      "118/388, train_loss: 0.1435, step time: 1.5324\n",
      "119/388, train_loss: 0.1934, step time: 1.5340\n",
      "120/388, train_loss: 0.1929, step time: 1.5388\n",
      "121/388, train_loss: 0.2670, step time: 1.5391\n",
      "122/388, train_loss: 0.1579, step time: 1.5355\n",
      "123/388, train_loss: 0.4578, step time: 1.5350\n",
      "124/388, train_loss: 0.2426, step time: 1.5349\n",
      "125/388, train_loss: 0.0939, step time: 1.5340\n",
      "126/388, train_loss: 0.2266, step time: 1.5375\n",
      "127/388, train_loss: 0.1995, step time: 1.5394\n",
      "128/388, train_loss: 0.1234, step time: 1.5366\n",
      "129/388, train_loss: 0.1893, step time: 1.5332\n",
      "130/388, train_loss: 0.0579, step time: 1.5347\n",
      "131/388, train_loss: 0.1309, step time: 1.5316\n",
      "132/388, train_loss: 0.2948, step time: 1.5333\n",
      "133/388, train_loss: 0.1026, step time: 1.5362\n",
      "134/388, train_loss: 0.1223, step time: 1.5335\n",
      "135/388, train_loss: 0.1212, step time: 1.5335\n",
      "136/388, train_loss: 0.2676, step time: 1.5365\n",
      "137/388, train_loss: 0.3325, step time: 1.5334\n",
      "138/388, train_loss: 0.1741, step time: 1.5364\n",
      "139/388, train_loss: 0.1882, step time: 1.5351\n",
      "140/388, train_loss: 0.2633, step time: 1.5402\n",
      "141/388, train_loss: 0.1560, step time: 1.5347\n",
      "142/388, train_loss: 0.0934, step time: 1.5363\n",
      "143/388, train_loss: 0.1245, step time: 1.5332\n",
      "144/388, train_loss: 0.1184, step time: 1.5390\n",
      "145/388, train_loss: 0.0996, step time: 1.5353\n",
      "146/388, train_loss: 0.1520, step time: 1.5357\n",
      "147/388, train_loss: 0.1811, step time: 1.5350\n",
      "148/388, train_loss: 0.1058, step time: 1.5325\n",
      "149/388, train_loss: 0.4406, step time: 1.5319\n",
      "150/388, train_loss: 0.1581, step time: 1.5331\n",
      "151/388, train_loss: 0.4461, step time: 1.5361\n",
      "152/388, train_loss: 0.1687, step time: 1.5385\n",
      "153/388, train_loss: 0.1925, step time: 1.5337\n",
      "154/388, train_loss: 0.2524, step time: 1.5331\n",
      "155/388, train_loss: 0.1921, step time: 1.5333\n",
      "156/388, train_loss: 0.5943, step time: 1.5393\n",
      "157/388, train_loss: 0.0855, step time: 1.5359\n",
      "158/388, train_loss: 0.3825, step time: 1.5401\n",
      "159/388, train_loss: 0.1987, step time: 1.5438\n",
      "160/388, train_loss: 0.1873, step time: 1.5338\n",
      "161/388, train_loss: 0.2774, step time: 1.5376\n",
      "162/388, train_loss: 0.0477, step time: 1.5389\n",
      "163/388, train_loss: 0.2834, step time: 1.5392\n",
      "164/388, train_loss: 0.0586, step time: 1.5347\n",
      "165/388, train_loss: 0.1004, step time: 1.5396\n",
      "166/388, train_loss: 0.2278, step time: 1.5402\n",
      "167/388, train_loss: 0.1052, step time: 1.5373\n",
      "168/388, train_loss: 0.5507, step time: 1.5495\n",
      "169/388, train_loss: 0.1173, step time: 1.5305\n",
      "170/388, train_loss: 0.0779, step time: 1.5354\n",
      "171/388, train_loss: 0.2406, step time: 1.5364\n",
      "172/388, train_loss: 0.3764, step time: 1.5344\n",
      "173/388, train_loss: 0.2027, step time: 1.5355\n",
      "174/388, train_loss: 0.3122, step time: 1.5352\n",
      "175/388, train_loss: 0.0916, step time: 1.5336\n",
      "176/388, train_loss: 0.0891, step time: 1.5363\n",
      "177/388, train_loss: 0.1853, step time: 1.5387\n",
      "178/388, train_loss: 0.2411, step time: 1.5368\n",
      "179/388, train_loss: 0.2761, step time: 1.5377\n",
      "180/388, train_loss: 0.1417, step time: 1.5583\n",
      "181/388, train_loss: 0.4310, step time: 1.5385\n",
      "182/388, train_loss: 0.1472, step time: 1.5346\n",
      "183/388, train_loss: 0.3497, step time: 1.5356\n",
      "184/388, train_loss: 0.2061, step time: 1.5351\n",
      "185/388, train_loss: 0.2114, step time: 1.5325\n",
      "186/388, train_loss: 0.2769, step time: 1.5332\n",
      "187/388, train_loss: 0.0732, step time: 1.5344\n",
      "188/388, train_loss: 0.1055, step time: 1.5357\n",
      "189/388, train_loss: 0.1274, step time: 1.5381\n",
      "190/388, train_loss: 0.5021, step time: 1.5330\n",
      "191/388, train_loss: 0.2240, step time: 1.5326\n",
      "192/388, train_loss: 0.4651, step time: 1.5358\n",
      "193/388, train_loss: 0.0647, step time: 1.5352\n",
      "194/388, train_loss: 0.1730, step time: 1.5384\n",
      "195/388, train_loss: 0.2003, step time: 1.5366\n",
      "196/388, train_loss: 0.2683, step time: 1.5346\n",
      "197/388, train_loss: 0.2183, step time: 1.5324\n",
      "198/388, train_loss: 0.1730, step time: 1.5324\n",
      "199/388, train_loss: 0.1886, step time: 1.5348\n",
      "200/388, train_loss: 0.1288, step time: 1.5389\n",
      "201/388, train_loss: 0.1368, step time: 1.5363\n",
      "202/388, train_loss: 0.1178, step time: 1.5343\n",
      "203/388, train_loss: 0.1030, step time: 1.5341\n",
      "204/388, train_loss: 0.1887, step time: 1.5334\n",
      "205/388, train_loss: 0.1288, step time: 1.5339\n",
      "206/388, train_loss: 0.1596, step time: 1.5378\n",
      "207/388, train_loss: 0.0622, step time: 1.5371\n",
      "208/388, train_loss: 0.0939, step time: 1.5372\n",
      "209/388, train_loss: 0.2052, step time: 1.5343\n",
      "210/388, train_loss: 0.1640, step time: 1.5414\n",
      "211/388, train_loss: 0.4027, step time: 1.5364\n",
      "212/388, train_loss: 0.0923, step time: 1.5382\n",
      "213/388, train_loss: 0.1632, step time: 1.5350\n",
      "214/388, train_loss: 0.1306, step time: 1.5315\n",
      "215/388, train_loss: 0.1073, step time: 1.5320\n",
      "216/388, train_loss: 0.0971, step time: 1.5347\n",
      "217/388, train_loss: 0.2056, step time: 1.5423\n",
      "218/388, train_loss: 0.5254, step time: 1.5354\n",
      "219/388, train_loss: 0.1193, step time: 1.5336\n",
      "220/388, train_loss: 0.3335, step time: 1.5353\n",
      "221/388, train_loss: 0.2072, step time: 1.5339\n",
      "222/388, train_loss: 0.2695, step time: 1.5335\n",
      "223/388, train_loss: 0.0681, step time: 1.5382\n",
      "224/388, train_loss: 0.3611, step time: 1.5373\n",
      "225/388, train_loss: 0.1583, step time: 1.5364\n",
      "226/388, train_loss: 0.1850, step time: 1.5342\n",
      "227/388, train_loss: 0.3092, step time: 1.5340\n",
      "228/388, train_loss: 0.1414, step time: 1.5317\n",
      "229/388, train_loss: 0.5124, step time: 1.5763\n",
      "230/388, train_loss: 0.2331, step time: 1.5340\n",
      "231/388, train_loss: 0.1652, step time: 1.5308\n",
      "232/388, train_loss: 0.1772, step time: 1.5402\n",
      "233/388, train_loss: 0.1576, step time: 1.5369\n",
      "234/388, train_loss: 0.1863, step time: 1.5353\n",
      "235/388, train_loss: 0.1170, step time: 1.5353\n",
      "236/388, train_loss: 0.2626, step time: 1.5354\n",
      "237/388, train_loss: 0.0666, step time: 1.5374\n",
      "238/388, train_loss: 0.2590, step time: 1.5356\n",
      "239/388, train_loss: 0.2794, step time: 1.5364\n",
      "240/388, train_loss: 0.1248, step time: 1.5347\n",
      "241/388, train_loss: 0.1841, step time: 1.5318\n",
      "242/388, train_loss: 0.1982, step time: 1.5307\n",
      "243/388, train_loss: 0.2299, step time: 1.5379\n",
      "244/388, train_loss: 0.1784, step time: 1.5345\n",
      "245/388, train_loss: 0.4202, step time: 1.5408\n",
      "246/388, train_loss: 0.2524, step time: 1.5329\n",
      "247/388, train_loss: 0.2341, step time: 1.5317\n",
      "248/388, train_loss: 0.3255, step time: 1.5322\n",
      "249/388, train_loss: 0.1793, step time: 1.5314\n",
      "250/388, train_loss: 0.2593, step time: 1.5360\n",
      "251/388, train_loss: 0.1694, step time: 1.5319\n",
      "252/388, train_loss: 0.2224, step time: 1.5310\n",
      "253/388, train_loss: 0.1666, step time: 1.5322\n",
      "254/388, train_loss: 0.1328, step time: 1.5319\n",
      "255/388, train_loss: 0.0923, step time: 1.5308\n",
      "256/388, train_loss: 0.0859, step time: 1.5394\n",
      "257/388, train_loss: 0.2501, step time: 1.5371\n",
      "258/388, train_loss: 0.1783, step time: 1.5358\n",
      "259/388, train_loss: 0.1244, step time: 1.5360\n",
      "260/388, train_loss: 0.0859, step time: 1.5370\n",
      "261/388, train_loss: 0.3100, step time: 1.5327\n",
      "262/388, train_loss: 0.1628, step time: 1.5355\n",
      "263/388, train_loss: 0.1618, step time: 1.5379\n",
      "264/388, train_loss: 0.1932, step time: 1.5351\n",
      "265/388, train_loss: 0.2011, step time: 1.5330\n",
      "266/388, train_loss: 0.0647, step time: 1.5323\n",
      "267/388, train_loss: 0.2624, step time: 1.5331\n",
      "268/388, train_loss: 0.4564, step time: 1.5351\n",
      "269/388, train_loss: 0.2312, step time: 1.5373\n",
      "270/388, train_loss: 0.2535, step time: 1.5335\n",
      "271/388, train_loss: 0.1523, step time: 1.5364\n",
      "272/388, train_loss: 0.0762, step time: 1.5354\n",
      "273/388, train_loss: 0.3212, step time: 1.5348\n",
      "274/388, train_loss: 0.0705, step time: 1.5338\n",
      "275/388, train_loss: 0.2029, step time: 1.5360\n",
      "276/388, train_loss: 0.1738, step time: 1.5376\n",
      "277/388, train_loss: 0.2998, step time: 1.5360\n",
      "278/388, train_loss: 0.2328, step time: 1.5370\n",
      "279/388, train_loss: 0.2340, step time: 1.5362\n",
      "280/388, train_loss: 0.1368, step time: 1.5377\n",
      "281/388, train_loss: 0.3942, step time: 1.5360\n",
      "282/388, train_loss: 0.0307, step time: 1.5364\n",
      "283/388, train_loss: 0.1159, step time: 1.5329\n",
      "284/388, train_loss: 0.0856, step time: 1.5339\n",
      "285/388, train_loss: 0.0966, step time: 1.5346\n",
      "286/388, train_loss: 0.1672, step time: 1.5308\n",
      "287/388, train_loss: 0.2998, step time: 1.5361\n",
      "288/388, train_loss: 0.1966, step time: 1.5374\n",
      "289/388, train_loss: 0.3687, step time: 1.5371\n",
      "290/388, train_loss: 0.1456, step time: 1.5403\n",
      "291/388, train_loss: 0.1761, step time: 1.5316\n",
      "292/388, train_loss: 0.2075, step time: 1.5318\n",
      "293/388, train_loss: 0.1257, step time: 1.5313\n",
      "294/388, train_loss: 0.1570, step time: 1.5371\n",
      "295/388, train_loss: 0.3951, step time: 1.5360\n",
      "296/388, train_loss: 0.1681, step time: 1.5380\n",
      "297/388, train_loss: 0.0791, step time: 1.5337\n",
      "298/388, train_loss: 0.3170, step time: 1.5486\n",
      "299/388, train_loss: 0.1404, step time: 1.5390\n",
      "300/388, train_loss: 0.2884, step time: 1.5378\n",
      "301/388, train_loss: 0.1406, step time: 1.5354\n",
      "302/388, train_loss: 0.1215, step time: 1.5336\n",
      "303/388, train_loss: 0.0724, step time: 1.5342\n",
      "304/388, train_loss: 0.1450, step time: 1.5358\n",
      "305/388, train_loss: 0.3032, step time: 1.5392\n",
      "306/388, train_loss: 0.1293, step time: 1.5386\n",
      "307/388, train_loss: 0.0985, step time: 1.5367\n",
      "308/388, train_loss: 0.3522, step time: 1.5317\n",
      "309/388, train_loss: 0.2210, step time: 1.5321\n",
      "310/388, train_loss: 0.3247, step time: 1.5344\n",
      "311/388, train_loss: 0.1833, step time: 1.5359\n",
      "312/388, train_loss: 0.1049, step time: 1.5362\n",
      "313/388, train_loss: 0.1078, step time: 1.5359\n",
      "314/388, train_loss: 0.3234, step time: 1.5358\n",
      "315/388, train_loss: 0.1982, step time: 1.5321\n",
      "316/388, train_loss: 0.1686, step time: 1.5351\n",
      "317/388, train_loss: 0.1816, step time: 1.5324\n",
      "318/388, train_loss: 0.1754, step time: 1.5556\n",
      "319/388, train_loss: 0.1422, step time: 1.5386\n",
      "320/388, train_loss: 0.0866, step time: 1.5345\n",
      "321/388, train_loss: 0.2765, step time: 1.5374\n",
      "322/388, train_loss: 0.1260, step time: 1.5361\n",
      "323/388, train_loss: 0.4299, step time: 1.5378\n",
      "324/388, train_loss: 0.1085, step time: 1.5404\n",
      "325/388, train_loss: 0.2307, step time: 1.5366\n",
      "326/388, train_loss: 0.1768, step time: 1.5360\n",
      "327/388, train_loss: 0.1928, step time: 1.5336\n",
      "328/388, train_loss: 0.1022, step time: 1.5341\n",
      "329/388, train_loss: 0.2300, step time: 1.5373\n",
      "330/388, train_loss: 0.0889, step time: 1.5385\n",
      "331/388, train_loss: 0.2137, step time: 1.5344\n",
      "332/388, train_loss: 0.0690, step time: 1.5325\n",
      "333/388, train_loss: 0.1046, step time: 1.5361\n",
      "334/388, train_loss: 0.2883, step time: 1.5460\n",
      "335/388, train_loss: 0.3051, step time: 1.5363\n",
      "336/388, train_loss: 0.1432, step time: 1.5315\n",
      "337/388, train_loss: 0.0971, step time: 1.5317\n",
      "338/388, train_loss: 0.1200, step time: 1.5389\n",
      "339/388, train_loss: 0.0827, step time: 1.5350\n",
      "340/388, train_loss: 0.0925, step time: 1.5381\n",
      "341/388, train_loss: 0.0934, step time: 1.5346\n",
      "342/388, train_loss: 0.0961, step time: 1.5329\n",
      "343/388, train_loss: 0.0864, step time: 1.5322\n",
      "344/388, train_loss: 0.1267, step time: 1.5322\n",
      "345/388, train_loss: 0.2155, step time: 1.5388\n",
      "346/388, train_loss: 0.1043, step time: 1.5388\n",
      "347/388, train_loss: 0.2794, step time: 1.5375\n",
      "348/388, train_loss: 0.0704, step time: 1.5341\n",
      "349/388, train_loss: 0.2905, step time: 1.5360\n",
      "350/388, train_loss: 0.0806, step time: 1.5324\n",
      "351/388, train_loss: 0.2688, step time: 1.5354\n",
      "352/388, train_loss: 0.3521, step time: 1.5364\n",
      "353/388, train_loss: 0.3238, step time: 1.5364\n",
      "354/388, train_loss: 0.1126, step time: 1.5347\n",
      "355/388, train_loss: 0.1030, step time: 1.5339\n",
      "356/388, train_loss: 0.1977, step time: 1.5343\n",
      "357/388, train_loss: 0.1160, step time: 1.5330\n",
      "358/388, train_loss: 0.0663, step time: 1.5361\n",
      "359/388, train_loss: 0.0678, step time: 1.5374\n",
      "360/388, train_loss: 0.3515, step time: 1.5328\n",
      "361/388, train_loss: 0.0948, step time: 1.5348\n",
      "362/388, train_loss: 0.0883, step time: 1.5380\n",
      "363/388, train_loss: 0.6238, step time: 1.5372\n",
      "364/388, train_loss: 0.1614, step time: 1.5341\n",
      "365/388, train_loss: 0.0734, step time: 1.5343\n",
      "366/388, train_loss: 0.1415, step time: 1.5356\n",
      "367/388, train_loss: 0.2829, step time: 1.5336\n",
      "368/388, train_loss: 0.1759, step time: 1.5350\n",
      "369/388, train_loss: 0.2667, step time: 1.5367\n",
      "370/388, train_loss: 0.3066, step time: 1.5379\n",
      "371/388, train_loss: 0.2540, step time: 1.5358\n",
      "372/388, train_loss: 0.0577, step time: 1.5303\n",
      "373/388, train_loss: 0.0711, step time: 1.5329\n",
      "374/388, train_loss: 0.0918, step time: 1.5356\n",
      "375/388, train_loss: 0.1801, step time: 1.5329\n",
      "376/388, train_loss: 0.0829, step time: 1.5357\n",
      "377/388, train_loss: 0.0800, step time: 1.5343\n",
      "378/388, train_loss: 0.1996, step time: 1.5325\n",
      "379/388, train_loss: 0.2237, step time: 1.5318\n",
      "380/388, train_loss: 0.2014, step time: 1.5316\n",
      "381/388, train_loss: 0.0775, step time: 1.5309\n",
      "382/388, train_loss: 0.1533, step time: 1.5315\n",
      "383/388, train_loss: 0.1174, step time: 1.5372\n",
      "384/388, train_loss: 0.1968, step time: 1.5380\n",
      "385/388, train_loss: 0.5291, step time: 1.5355\n",
      "386/388, train_loss: 0.0608, step time: 1.5314\n",
      "387/388, train_loss: 0.1342, step time: 1.5316\n",
      "388/388, train_loss: 0.1810, step time: 1.5345\n",
      "epoch 44 average loss: 0.1939\n",
      "current epoch: 44 current mean dice: 0.7612 tc: 0.8123 wt: 0.9007 et: 0.5705\n",
      "best mean dice: 0.7640 at epoch: 36\n",
      "time consuming of epoch 44 is: 703.1349\n",
      "----------\n",
      "epoch 45/100\n",
      "1/388, train_loss: 0.1831, step time: 1.5480\n",
      "2/388, train_loss: 0.1674, step time: 1.5348\n",
      "3/388, train_loss: 0.1530, step time: 1.5306\n",
      "4/388, train_loss: 0.1678, step time: 1.5372\n",
      "5/388, train_loss: 0.1060, step time: 1.5344\n",
      "6/388, train_loss: 0.1441, step time: 1.5336\n",
      "7/388, train_loss: 0.0492, step time: 1.5367\n",
      "8/388, train_loss: 0.1070, step time: 1.5399\n",
      "9/388, train_loss: 0.3079, step time: 1.5351\n",
      "10/388, train_loss: 0.0689, step time: 1.5345\n",
      "11/388, train_loss: 0.3250, step time: 1.5325\n",
      "12/388, train_loss: 0.1258, step time: 1.5354\n",
      "13/388, train_loss: 0.1863, step time: 1.5385\n",
      "14/388, train_loss: 0.1700, step time: 1.5372\n",
      "15/388, train_loss: 0.2643, step time: 1.5348\n",
      "16/388, train_loss: 0.1952, step time: 1.5356\n",
      "17/388, train_loss: 0.1941, step time: 1.5348\n",
      "18/388, train_loss: 0.1130, step time: 1.5331\n",
      "19/388, train_loss: 0.1557, step time: 1.5337\n",
      "20/388, train_loss: 0.1091, step time: 1.5317\n",
      "21/388, train_loss: 0.3370, step time: 1.5343\n",
      "22/388, train_loss: 0.1133, step time: 1.5344\n",
      "23/388, train_loss: 0.0715, step time: 1.5306\n",
      "24/388, train_loss: 0.1016, step time: 1.5302\n",
      "25/388, train_loss: 0.0830, step time: 1.5341\n",
      "26/388, train_loss: 0.1366, step time: 1.5355\n",
      "27/388, train_loss: 0.0999, step time: 1.5327\n",
      "28/388, train_loss: 0.1734, step time: 1.5321\n",
      "29/388, train_loss: 0.1148, step time: 1.5339\n",
      "30/388, train_loss: 0.0573, step time: 1.5355\n",
      "31/388, train_loss: 0.2125, step time: 1.5353\n",
      "32/388, train_loss: 0.1442, step time: 1.5397\n",
      "33/388, train_loss: 0.0866, step time: 1.5331\n",
      "34/388, train_loss: 0.2634, step time: 1.5360\n",
      "35/388, train_loss: 0.1133, step time: 1.5372\n",
      "36/388, train_loss: 0.1651, step time: 1.5611\n",
      "37/388, train_loss: 0.2170, step time: 1.5347\n",
      "38/388, train_loss: 0.1330, step time: 1.5350\n",
      "39/388, train_loss: 0.0645, step time: 1.5349\n",
      "40/388, train_loss: 0.2735, step time: 1.5378\n",
      "41/388, train_loss: 0.1192, step time: 1.5312\n",
      "42/388, train_loss: 0.0955, step time: 1.5338\n",
      "43/388, train_loss: 0.0871, step time: 1.5315\n",
      "44/388, train_loss: 0.0331, step time: 1.5376\n",
      "45/388, train_loss: 0.3563, step time: 1.5357\n",
      "46/388, train_loss: 0.0885, step time: 1.5391\n",
      "47/388, train_loss: 0.4208, step time: 1.5349\n",
      "48/388, train_loss: 0.1220, step time: 1.5346\n",
      "49/388, train_loss: 0.2321, step time: 1.5378\n",
      "50/388, train_loss: 0.0690, step time: 1.5369\n",
      "51/388, train_loss: 0.0818, step time: 1.5344\n",
      "52/388, train_loss: 0.1927, step time: 1.5316\n",
      "53/388, train_loss: 0.3258, step time: 1.5332\n",
      "54/388, train_loss: 0.1009, step time: 1.5392\n",
      "55/388, train_loss: 0.0994, step time: 1.5364\n",
      "56/388, train_loss: 0.1029, step time: 1.5357\n",
      "57/388, train_loss: 0.1207, step time: 1.5349\n",
      "58/388, train_loss: 0.1823, step time: 1.5376\n",
      "59/388, train_loss: 0.5758, step time: 1.5405\n",
      "60/388, train_loss: 0.0605, step time: 1.5394\n",
      "61/388, train_loss: 0.1385, step time: 1.5341\n",
      "62/388, train_loss: 0.2007, step time: 1.5329\n",
      "63/388, train_loss: 0.1263, step time: 1.5338\n",
      "64/388, train_loss: 0.2734, step time: 1.5350\n",
      "65/388, train_loss: 0.0721, step time: 1.5358\n",
      "66/388, train_loss: 0.3634, step time: 1.5373\n",
      "67/388, train_loss: 0.1779, step time: 1.5365\n",
      "68/388, train_loss: 0.0947, step time: 1.5342\n",
      "69/388, train_loss: 0.0650, step time: 1.5291\n",
      "70/388, train_loss: 0.0698, step time: 1.5294\n",
      "71/388, train_loss: 0.2348, step time: 1.5330\n",
      "72/388, train_loss: 0.3594, step time: 1.5369\n",
      "73/388, train_loss: 0.0707, step time: 1.5355\n",
      "74/388, train_loss: 0.2239, step time: 1.5373\n",
      "75/388, train_loss: 0.4401, step time: 1.5324\n",
      "76/388, train_loss: 0.2056, step time: 1.5336\n",
      "77/388, train_loss: 0.2688, step time: 1.5323\n",
      "78/388, train_loss: 0.1085, step time: 1.5348\n",
      "79/388, train_loss: 0.2829, step time: 1.5368\n",
      "80/388, train_loss: 0.2935, step time: 1.5330\n",
      "81/388, train_loss: 0.1057, step time: 1.5361\n",
      "82/388, train_loss: 0.1083, step time: 1.5320\n",
      "83/388, train_loss: 0.1040, step time: 1.5329\n",
      "84/388, train_loss: 0.1552, step time: 1.5346\n",
      "85/388, train_loss: 0.0981, step time: 1.5372\n",
      "86/388, train_loss: 0.2484, step time: 1.5384\n",
      "87/388, train_loss: 0.2574, step time: 1.5394\n",
      "88/388, train_loss: 0.1130, step time: 1.5342\n",
      "89/388, train_loss: 0.1042, step time: 1.5328\n",
      "90/388, train_loss: 0.1439, step time: 1.5374\n",
      "91/388, train_loss: 0.2175, step time: 1.5366\n",
      "92/388, train_loss: 0.4021, step time: 1.5589\n",
      "93/388, train_loss: 0.2296, step time: 1.5357\n",
      "94/388, train_loss: 0.1270, step time: 1.5378\n",
      "95/388, train_loss: 0.1022, step time: 1.5331\n",
      "96/388, train_loss: 0.0851, step time: 1.5344\n",
      "97/388, train_loss: 0.1212, step time: 1.5346\n",
      "98/388, train_loss: 0.0962, step time: 1.5334\n",
      "99/388, train_loss: 0.1494, step time: 1.5370\n",
      "100/388, train_loss: 0.1223, step time: 1.5364\n",
      "101/388, train_loss: 0.1179, step time: 1.5328\n",
      "102/388, train_loss: 0.1424, step time: 1.5333\n",
      "103/388, train_loss: 0.0850, step time: 1.5332\n",
      "104/388, train_loss: 0.3935, step time: 1.5339\n",
      "105/388, train_loss: 0.0749, step time: 1.5390\n",
      "106/388, train_loss: 0.2592, step time: 1.5360\n",
      "107/388, train_loss: 0.1854, step time: 1.5332\n",
      "108/388, train_loss: 0.1028, step time: 1.5361\n",
      "109/388, train_loss: 0.2114, step time: 1.5332\n",
      "110/388, train_loss: 0.0283, step time: 1.5332\n",
      "111/388, train_loss: 0.1455, step time: 1.5418\n",
      "112/388, train_loss: 0.0844, step time: 1.5334\n",
      "113/388, train_loss: 0.3195, step time: 1.5359\n",
      "114/388, train_loss: 0.2804, step time: 1.5326\n",
      "115/388, train_loss: 0.3412, step time: 1.5362\n",
      "116/388, train_loss: 0.0948, step time: 1.5342\n",
      "117/388, train_loss: 0.2033, step time: 1.5367\n",
      "118/388, train_loss: 0.2446, step time: 1.5352\n",
      "119/388, train_loss: 0.2760, step time: 1.5354\n",
      "120/388, train_loss: 0.1339, step time: 1.5320\n",
      "121/388, train_loss: 0.1683, step time: 1.5371\n",
      "122/388, train_loss: 0.0785, step time: 1.5361\n",
      "123/388, train_loss: 0.0941, step time: 1.5361\n",
      "124/388, train_loss: 0.2501, step time: 1.5328\n",
      "125/388, train_loss: 0.1383, step time: 1.5338\n",
      "126/388, train_loss: 0.2174, step time: 1.5333\n",
      "127/388, train_loss: 0.1827, step time: 1.5322\n",
      "128/388, train_loss: 0.1782, step time: 1.5369\n",
      "129/388, train_loss: 0.1329, step time: 1.5497\n",
      "130/388, train_loss: 0.1609, step time: 1.5321\n",
      "131/388, train_loss: 0.1992, step time: 1.5341\n",
      "132/388, train_loss: 0.5099, step time: 1.5321\n",
      "133/388, train_loss: 0.2598, step time: 1.5348\n",
      "134/388, train_loss: 0.3261, step time: 1.5378\n",
      "135/388, train_loss: 0.2352, step time: 1.5362\n",
      "136/388, train_loss: 0.1566, step time: 1.5324\n",
      "137/388, train_loss: 0.1070, step time: 1.5325\n",
      "138/388, train_loss: 0.2360, step time: 1.5307\n",
      "139/388, train_loss: 0.0519, step time: 1.5352\n",
      "140/388, train_loss: 0.3517, step time: 1.5346\n",
      "141/388, train_loss: 0.1589, step time: 1.5359\n",
      "142/388, train_loss: 0.2705, step time: 1.5341\n",
      "143/388, train_loss: 0.2081, step time: 1.5336\n",
      "144/388, train_loss: 0.1759, step time: 1.5369\n",
      "145/388, train_loss: 0.2740, step time: 1.5333\n",
      "146/388, train_loss: 0.0629, step time: 1.5384\n",
      "147/388, train_loss: 0.1772, step time: 1.5354\n",
      "148/388, train_loss: 0.2187, step time: 1.5334\n",
      "149/388, train_loss: 0.0953, step time: 1.5318\n",
      "150/388, train_loss: 0.1571, step time: 1.5299\n",
      "151/388, train_loss: 0.2143, step time: 1.5342\n",
      "152/388, train_loss: 0.1502, step time: 1.5368\n",
      "153/388, train_loss: 0.3066, step time: 1.5360\n",
      "154/388, train_loss: 0.3280, step time: 1.5367\n",
      "155/388, train_loss: 0.0540, step time: 1.5342\n",
      "156/388, train_loss: 0.1263, step time: 1.5321\n",
      "157/388, train_loss: 0.3171, step time: 1.5424\n",
      "158/388, train_loss: 0.0994, step time: 1.5356\n",
      "159/388, train_loss: 0.0904, step time: 1.5369\n",
      "160/388, train_loss: 0.1036, step time: 1.5341\n",
      "161/388, train_loss: 0.2694, step time: 1.5390\n",
      "162/388, train_loss: 0.1324, step time: 1.5418\n",
      "163/388, train_loss: 0.1407, step time: 1.5372\n",
      "164/388, train_loss: 0.0850, step time: 1.5374\n",
      "165/388, train_loss: 0.0852, step time: 1.5326\n",
      "166/388, train_loss: 0.0489, step time: 1.5323\n",
      "167/388, train_loss: 0.2793, step time: 1.5314\n",
      "168/388, train_loss: 0.1091, step time: 1.5326\n",
      "169/388, train_loss: 0.1367, step time: 1.5354\n",
      "170/388, train_loss: 0.2659, step time: 1.5342\n",
      "171/388, train_loss: 0.2163, step time: 1.5411\n",
      "172/388, train_loss: 0.3589, step time: 1.5388\n",
      "173/388, train_loss: 0.1838, step time: 1.5319\n",
      "174/388, train_loss: 0.1603, step time: 1.5619\n",
      "175/388, train_loss: 0.0627, step time: 1.5296\n",
      "176/388, train_loss: 0.2847, step time: 1.5318\n",
      "177/388, train_loss: 0.0853, step time: 1.5314\n",
      "178/388, train_loss: 0.2304, step time: 1.5365\n",
      "179/388, train_loss: 0.1280, step time: 1.5347\n",
      "180/388, train_loss: 0.1541, step time: 1.5345\n",
      "181/388, train_loss: 0.1032, step time: 1.5290\n",
      "182/388, train_loss: 0.4084, step time: 1.5332\n",
      "183/388, train_loss: 0.1844, step time: 1.5318\n",
      "184/388, train_loss: 0.1514, step time: 1.5355\n",
      "185/388, train_loss: 0.1121, step time: 1.5357\n",
      "186/388, train_loss: 0.1194, step time: 1.5374\n",
      "187/388, train_loss: 0.1581, step time: 1.5374\n",
      "188/388, train_loss: 0.0935, step time: 1.5322\n",
      "189/388, train_loss: 0.2051, step time: 1.5346\n",
      "190/388, train_loss: 0.6101, step time: 1.5393\n",
      "191/388, train_loss: 0.2091, step time: 1.5302\n",
      "192/388, train_loss: 0.5002, step time: 1.5321\n",
      "193/388, train_loss: 0.0842, step time: 1.5312\n",
      "194/388, train_loss: 0.0631, step time: 1.5374\n",
      "195/388, train_loss: 0.2059, step time: 1.5372\n",
      "196/388, train_loss: 0.2324, step time: 1.5406\n",
      "197/388, train_loss: 0.1128, step time: 1.5333\n",
      "198/388, train_loss: 0.4157, step time: 1.5401\n",
      "199/388, train_loss: 0.2109, step time: 1.5347\n",
      "200/388, train_loss: 0.0972, step time: 1.5382\n",
      "201/388, train_loss: 0.3590, step time: 1.5372\n",
      "202/388, train_loss: 0.4524, step time: 1.5364\n",
      "203/388, train_loss: 0.1627, step time: 1.5377\n",
      "204/388, train_loss: 0.1132, step time: 1.5322\n",
      "205/388, train_loss: 0.2270, step time: 1.5363\n",
      "206/388, train_loss: 0.2733, step time: 1.5394\n",
      "207/388, train_loss: 0.5555, step time: 1.5323\n",
      "208/388, train_loss: 0.2593, step time: 1.5335\n",
      "209/388, train_loss: 0.2123, step time: 1.5348\n",
      "210/388, train_loss: 0.0754, step time: 1.5338\n",
      "211/388, train_loss: 0.2563, step time: 1.5355\n",
      "212/388, train_loss: 0.1486, step time: 1.5339\n",
      "213/388, train_loss: 0.0351, step time: 1.5335\n",
      "214/388, train_loss: 0.0659, step time: 1.5328\n",
      "215/388, train_loss: 0.1939, step time: 1.5335\n",
      "216/388, train_loss: 0.1530, step time: 1.5332\n",
      "217/388, train_loss: 0.2746, step time: 1.5582\n",
      "218/388, train_loss: 0.2574, step time: 1.5369\n",
      "219/388, train_loss: 0.2122, step time: 1.5350\n",
      "220/388, train_loss: 0.1476, step time: 1.5310\n",
      "221/388, train_loss: 0.1526, step time: 1.5341\n",
      "222/388, train_loss: 0.1830, step time: 1.5320\n",
      "223/388, train_loss: 0.3935, step time: 1.5360\n",
      "224/388, train_loss: 0.1533, step time: 1.5375\n",
      "225/388, train_loss: 0.2365, step time: 1.5343\n",
      "226/388, train_loss: 0.1192, step time: 1.5319\n",
      "227/388, train_loss: 0.1672, step time: 1.5330\n",
      "228/388, train_loss: 0.1589, step time: 1.5366\n",
      "229/388, train_loss: 0.4153, step time: 1.5351\n",
      "230/388, train_loss: 0.1241, step time: 1.5362\n",
      "231/388, train_loss: 0.2511, step time: 1.5322\n",
      "232/388, train_loss: 0.0661, step time: 1.5347\n",
      "233/388, train_loss: 0.2751, step time: 1.5350\n",
      "234/388, train_loss: 0.0556, step time: 1.5562\n",
      "235/388, train_loss: 0.2844, step time: 1.5329\n",
      "236/388, train_loss: 0.1548, step time: 1.5356\n",
      "237/388, train_loss: 0.1634, step time: 1.5341\n",
      "238/388, train_loss: 0.2319, step time: 1.5370\n",
      "239/388, train_loss: 0.1714, step time: 1.5378\n",
      "240/388, train_loss: 0.1772, step time: 1.5366\n",
      "241/388, train_loss: 0.2371, step time: 1.5348\n",
      "242/388, train_loss: 0.0477, step time: 1.5353\n",
      "243/388, train_loss: 0.2135, step time: 1.5367\n",
      "244/388, train_loss: 0.0746, step time: 1.5431\n",
      "245/388, train_loss: 0.1710, step time: 1.5350\n",
      "246/388, train_loss: 0.1313, step time: 1.5326\n",
      "247/388, train_loss: 0.1575, step time: 1.5333\n",
      "248/388, train_loss: 0.2478, step time: 1.5330\n",
      "249/388, train_loss: 0.3750, step time: 1.5357\n",
      "250/388, train_loss: 0.0963, step time: 1.5378\n",
      "251/388, train_loss: 0.0867, step time: 1.5343\n",
      "252/388, train_loss: 0.3064, step time: 1.5350\n",
      "253/388, train_loss: 0.1904, step time: 1.5311\n",
      "254/388, train_loss: 0.2299, step time: 1.5308\n",
      "255/388, train_loss: 0.0834, step time: 1.5300\n",
      "256/388, train_loss: 0.2328, step time: 1.5362\n",
      "257/388, train_loss: 0.1119, step time: 1.5383\n",
      "258/388, train_loss: 0.1285, step time: 1.5349\n",
      "259/388, train_loss: 0.1241, step time: 1.5334\n",
      "260/388, train_loss: 0.5312, step time: 1.5332\n",
      "261/388, train_loss: 0.2804, step time: 1.5400\n",
      "262/388, train_loss: 0.2855, step time: 1.5379\n",
      "263/388, train_loss: 0.2015, step time: 1.5364\n",
      "264/388, train_loss: 0.1591, step time: 1.5347\n",
      "265/388, train_loss: 0.3764, step time: 1.5322\n",
      "266/388, train_loss: 0.2086, step time: 1.5339\n",
      "267/388, train_loss: 0.1438, step time: 1.5375\n",
      "268/388, train_loss: 0.2400, step time: 1.5352\n",
      "269/388, train_loss: 0.2153, step time: 1.5347\n",
      "270/388, train_loss: 0.2858, step time: 1.5312\n",
      "271/388, train_loss: 0.3858, step time: 1.5347\n",
      "272/388, train_loss: 0.1171, step time: 1.5421\n",
      "273/388, train_loss: 0.1573, step time: 1.5318\n",
      "274/388, train_loss: 0.0764, step time: 1.5311\n",
      "275/388, train_loss: 0.1571, step time: 1.5327\n",
      "276/388, train_loss: 0.2146, step time: 1.5337\n",
      "277/388, train_loss: 0.3683, step time: 1.5470\n",
      "278/388, train_loss: 0.2295, step time: 1.5345\n",
      "279/388, train_loss: 0.2137, step time: 1.5359\n",
      "280/388, train_loss: 0.0451, step time: 1.5362\n",
      "281/388, train_loss: 0.1843, step time: 1.5391\n",
      "282/388, train_loss: 0.6505, step time: 1.5340\n",
      "283/388, train_loss: 0.1055, step time: 1.5325\n",
      "284/388, train_loss: 0.2451, step time: 1.5333\n",
      "285/388, train_loss: 0.0957, step time: 1.5353\n",
      "286/388, train_loss: 0.2271, step time: 1.5330\n",
      "287/388, train_loss: 0.1299, step time: 1.5322\n",
      "288/388, train_loss: 0.0996, step time: 1.5627\n",
      "289/388, train_loss: 0.2112, step time: 1.5384\n",
      "290/388, train_loss: 0.2290, step time: 1.5329\n",
      "291/388, train_loss: 0.0895, step time: 1.5334\n",
      "292/388, train_loss: 0.2842, step time: 1.5341\n",
      "293/388, train_loss: 0.1367, step time: 1.5348\n",
      "294/388, train_loss: 0.0967, step time: 1.5356\n",
      "295/388, train_loss: 0.2765, step time: 1.5339\n",
      "296/388, train_loss: 0.1434, step time: 1.5311\n",
      "297/388, train_loss: 0.5846, step time: 1.5429\n",
      "298/388, train_loss: 0.2710, step time: 1.5374\n",
      "299/388, train_loss: 0.0769, step time: 1.5291\n",
      "300/388, train_loss: 0.3662, step time: 1.5294\n",
      "301/388, train_loss: 0.0961, step time: 1.5313\n",
      "302/388, train_loss: 0.0468, step time: 1.5331\n",
      "303/388, train_loss: 0.2103, step time: 1.5346\n",
      "304/388, train_loss: 0.0724, step time: 1.5358\n",
      "305/388, train_loss: 0.2573, step time: 1.5355\n",
      "306/388, train_loss: 0.0904, step time: 1.5372\n",
      "307/388, train_loss: 0.0894, step time: 1.5320\n",
      "308/388, train_loss: 0.1305, step time: 1.5345\n",
      "309/388, train_loss: 0.0902, step time: 1.5321\n",
      "310/388, train_loss: 0.0664, step time: 1.5330\n",
      "311/388, train_loss: 0.3722, step time: 1.5371\n",
      "312/388, train_loss: 0.1804, step time: 1.5364\n",
      "313/388, train_loss: 0.1401, step time: 1.5337\n",
      "314/388, train_loss: 0.2979, step time: 1.5345\n",
      "315/388, train_loss: 0.1295, step time: 1.5317\n",
      "316/388, train_loss: 0.1918, step time: 1.5334\n",
      "317/388, train_loss: 0.2968, step time: 1.5376\n",
      "318/388, train_loss: 0.2216, step time: 1.5349\n",
      "319/388, train_loss: 0.1390, step time: 1.5353\n",
      "320/388, train_loss: 0.1515, step time: 1.5340\n",
      "321/388, train_loss: 0.5457, step time: 1.5318\n",
      "322/388, train_loss: 0.0552, step time: 1.5294\n",
      "323/388, train_loss: 0.1097, step time: 1.5345\n",
      "324/388, train_loss: 0.2204, step time: 1.5370\n",
      "325/388, train_loss: 0.5264, step time: 1.5380\n",
      "326/388, train_loss: 0.0899, step time: 1.5332\n",
      "327/388, train_loss: 0.0965, step time: 1.5342\n",
      "328/388, train_loss: 0.3414, step time: 1.5329\n",
      "329/388, train_loss: 0.3866, step time: 1.5329\n",
      "330/388, train_loss: 0.3095, step time: 1.5306\n",
      "331/388, train_loss: 0.7245, step time: 1.5378\n",
      "332/388, train_loss: 0.1972, step time: 1.5361\n",
      "333/388, train_loss: 0.1798, step time: 1.5300\n",
      "334/388, train_loss: 0.1216, step time: 1.5298\n",
      "335/388, train_loss: 0.0888, step time: 1.5332\n",
      "336/388, train_loss: 0.2239, step time: 1.5348\n",
      "337/388, train_loss: 0.2515, step time: 1.5361\n",
      "338/388, train_loss: 0.1126, step time: 1.5355\n",
      "339/388, train_loss: 0.2716, step time: 1.5329\n",
      "340/388, train_loss: 0.1506, step time: 1.5331\n",
      "341/388, train_loss: 0.3060, step time: 1.5332\n",
      "342/388, train_loss: 0.2262, step time: 1.5322\n",
      "343/388, train_loss: 0.1948, step time: 1.5347\n",
      "344/388, train_loss: 0.2281, step time: 1.5369\n",
      "345/388, train_loss: 0.2677, step time: 1.5358\n",
      "346/388, train_loss: 0.1922, step time: 1.5352\n",
      "347/388, train_loss: 0.0649, step time: 1.5320\n",
      "348/388, train_loss: 0.2181, step time: 1.5349\n",
      "349/388, train_loss: 0.2925, step time: 1.5317\n",
      "350/388, train_loss: 0.1561, step time: 1.5385\n",
      "351/388, train_loss: 0.1502, step time: 1.5362\n",
      "352/388, train_loss: 0.1544, step time: 1.5313\n",
      "353/388, train_loss: 0.0800, step time: 1.5327\n",
      "354/388, train_loss: 0.5261, step time: 1.5306\n",
      "355/388, train_loss: 0.1746, step time: 1.5344\n",
      "356/388, train_loss: 0.1201, step time: 1.5377\n",
      "357/388, train_loss: 0.1646, step time: 1.5342\n",
      "358/388, train_loss: 0.1354, step time: 1.5338\n",
      "359/388, train_loss: 0.1322, step time: 1.5336\n",
      "360/388, train_loss: 0.2477, step time: 1.5309\n",
      "361/388, train_loss: 0.1611, step time: 1.5358\n",
      "362/388, train_loss: 0.1579, step time: 1.5385\n",
      "363/388, train_loss: 0.2672, step time: 1.5369\n",
      "364/388, train_loss: 0.2705, step time: 1.5348\n",
      "365/388, train_loss: 0.2290, step time: 1.5334\n",
      "366/388, train_loss: 0.3037, step time: 1.5313\n",
      "367/388, train_loss: 0.0951, step time: 1.5337\n",
      "368/388, train_loss: 0.2835, step time: 1.5349\n",
      "369/388, train_loss: 0.2464, step time: 1.5333\n",
      "370/388, train_loss: 0.2098, step time: 1.5333\n",
      "371/388, train_loss: 0.1159, step time: 1.5339\n",
      "372/388, train_loss: 0.3934, step time: 1.5335\n",
      "373/388, train_loss: 0.1048, step time: 1.5344\n",
      "374/388, train_loss: 0.1604, step time: 1.5376\n",
      "375/388, train_loss: 0.2982, step time: 1.5383\n",
      "376/388, train_loss: 0.1094, step time: 1.5622\n",
      "377/388, train_loss: 0.1378, step time: 1.5366\n",
      "378/388, train_loss: 0.0420, step time: 1.5348\n",
      "379/388, train_loss: 0.1673, step time: 1.5372\n",
      "380/388, train_loss: 0.1166, step time: 1.5346\n",
      "381/388, train_loss: 0.1112, step time: 1.5340\n",
      "382/388, train_loss: 0.2651, step time: 1.5330\n",
      "383/388, train_loss: 0.2396, step time: 1.5331\n",
      "384/388, train_loss: 0.0798, step time: 1.5388\n",
      "385/388, train_loss: 0.1283, step time: 1.5346\n",
      "386/388, train_loss: 0.0678, step time: 1.5384\n",
      "387/388, train_loss: 0.2264, step time: 1.5376\n",
      "388/388, train_loss: 0.4387, step time: 1.5332\n",
      "epoch 45 average loss: 0.1924\n",
      "current epoch: 45 current mean dice: 0.7543 tc: 0.7967 wt: 0.9014 et: 0.5647\n",
      "best mean dice: 0.7640 at epoch: 36\n",
      "time consuming of epoch 45 is: 702.2859\n",
      "----------\n",
      "epoch 46/100\n",
      "1/388, train_loss: 0.2086, step time: 1.5608\n",
      "2/388, train_loss: 0.2416, step time: 1.5348\n",
      "3/388, train_loss: 0.3081, step time: 1.5313\n",
      "4/388, train_loss: 0.1454, step time: 1.5358\n",
      "5/388, train_loss: 0.0688, step time: 1.5354\n",
      "6/388, train_loss: 0.2112, step time: 1.5372\n",
      "7/388, train_loss: 0.0958, step time: 1.5340\n",
      "8/388, train_loss: 0.1145, step time: 1.5330\n",
      "9/388, train_loss: 0.1132, step time: 1.5349\n",
      "10/388, train_loss: 0.1760, step time: 1.5374\n",
      "11/388, train_loss: 0.0964, step time: 1.5354\n",
      "12/388, train_loss: 0.2900, step time: 1.5350\n",
      "13/388, train_loss: 0.1090, step time: 1.5333\n",
      "14/388, train_loss: 0.2926, step time: 1.5347\n",
      "15/388, train_loss: 0.1060, step time: 1.5331\n",
      "16/388, train_loss: 0.2747, step time: 1.5343\n",
      "17/388, train_loss: 0.2520, step time: 1.5359\n",
      "18/388, train_loss: 0.2475, step time: 1.5367\n",
      "19/388, train_loss: 0.4647, step time: 1.5343\n",
      "20/388, train_loss: 0.1225, step time: 1.5360\n",
      "21/388, train_loss: 0.1888, step time: 1.5312\n",
      "22/388, train_loss: 0.1352, step time: 1.5370\n",
      "23/388, train_loss: 0.2415, step time: 1.5384\n",
      "24/388, train_loss: 0.0872, step time: 1.5365\n",
      "25/388, train_loss: 0.2727, step time: 1.5354\n",
      "26/388, train_loss: 0.1257, step time: 1.5322\n",
      "27/388, train_loss: 0.2698, step time: 1.5306\n",
      "28/388, train_loss: 0.1025, step time: 1.5348\n",
      "29/388, train_loss: 0.1412, step time: 1.5352\n",
      "30/388, train_loss: 0.2039, step time: 1.5384\n",
      "31/388, train_loss: 0.3697, step time: 1.5345\n",
      "32/388, train_loss: 0.1057, step time: 1.5318\n",
      "33/388, train_loss: 0.1935, step time: 1.5313\n",
      "34/388, train_loss: 0.2845, step time: 1.5339\n",
      "35/388, train_loss: 0.1246, step time: 1.5315\n",
      "36/388, train_loss: 0.1659, step time: 1.5353\n",
      "37/388, train_loss: 0.1088, step time: 1.5361\n",
      "38/388, train_loss: 0.2753, step time: 1.5348\n",
      "39/388, train_loss: 0.0889, step time: 1.5374\n",
      "40/388, train_loss: 0.1150, step time: 1.5348\n",
      "41/388, train_loss: 0.0423, step time: 1.5349\n",
      "42/388, train_loss: 0.1692, step time: 1.5325\n",
      "43/388, train_loss: 0.0900, step time: 1.5337\n",
      "44/388, train_loss: 0.0992, step time: 1.5510\n",
      "45/388, train_loss: 0.0948, step time: 1.5358\n",
      "46/388, train_loss: 0.1341, step time: 1.5331\n",
      "47/388, train_loss: 0.3433, step time: 1.5317\n",
      "48/388, train_loss: 0.1101, step time: 1.5345\n",
      "49/388, train_loss: 0.0480, step time: 1.5357\n",
      "50/388, train_loss: 0.2055, step time: 1.5363\n",
      "51/388, train_loss: 0.0983, step time: 1.5363\n",
      "52/388, train_loss: 0.0784, step time: 1.5347\n",
      "53/388, train_loss: 0.0999, step time: 1.5394\n",
      "54/388, train_loss: 0.1988, step time: 1.5348\n",
      "55/388, train_loss: 0.0669, step time: 1.5356\n",
      "56/388, train_loss: 0.1178, step time: 1.5368\n",
      "57/388, train_loss: 0.1299, step time: 1.5338\n",
      "58/388, train_loss: 0.1647, step time: 1.5343\n",
      "59/388, train_loss: 0.4443, step time: 1.5306\n",
      "60/388, train_loss: 0.5217, step time: 1.5328\n",
      "61/388, train_loss: 0.3428, step time: 1.5358\n",
      "62/388, train_loss: 0.4800, step time: 1.5383\n",
      "63/388, train_loss: 0.3362, step time: 1.5319\n",
      "64/388, train_loss: 0.2627, step time: 1.5410\n",
      "65/388, train_loss: 0.2804, step time: 1.5353\n",
      "66/388, train_loss: 0.3794, step time: 1.5372\n",
      "67/388, train_loss: 0.1406, step time: 1.5347\n",
      "68/388, train_loss: 0.2493, step time: 1.5344\n",
      "69/388, train_loss: 0.2695, step time: 1.5334\n",
      "70/388, train_loss: 0.1241, step time: 1.5335\n",
      "71/388, train_loss: 0.1089, step time: 1.5323\n",
      "72/388, train_loss: 0.2458, step time: 1.5370\n",
      "73/388, train_loss: 0.0656, step time: 1.5341\n",
      "74/388, train_loss: 0.0912, step time: 1.5349\n",
      "75/388, train_loss: 0.1556, step time: 1.5337\n",
      "76/388, train_loss: 0.2109, step time: 1.5325\n",
      "77/388, train_loss: 0.1272, step time: 1.5342\n",
      "78/388, train_loss: 0.0989, step time: 1.5369\n",
      "79/388, train_loss: 0.3094, step time: 1.5378\n",
      "80/388, train_loss: 0.1173, step time: 1.5312\n",
      "81/388, train_loss: 0.1680, step time: 1.5299\n",
      "82/388, train_loss: 0.0860, step time: 1.5313\n",
      "83/388, train_loss: 0.1974, step time: 1.5346\n",
      "84/388, train_loss: 0.3266, step time: 1.5358\n",
      "85/388, train_loss: 0.1534, step time: 1.5345\n",
      "86/388, train_loss: 0.0884, step time: 1.5351\n",
      "87/388, train_loss: 0.2000, step time: 1.5397\n",
      "88/388, train_loss: 0.1457, step time: 1.5292\n",
      "89/388, train_loss: 0.4861, step time: 1.5465\n",
      "90/388, train_loss: 0.1662, step time: 1.5330\n",
      "91/388, train_loss: 0.2364, step time: 1.5339\n",
      "92/388, train_loss: 0.6748, step time: 1.5325\n",
      "93/388, train_loss: 0.0769, step time: 1.5368\n",
      "94/388, train_loss: 0.4586, step time: 1.5315\n",
      "95/388, train_loss: 0.0333, step time: 1.5338\n",
      "96/388, train_loss: 0.1890, step time: 1.5328\n",
      "97/388, train_loss: 0.0882, step time: 1.5323\n",
      "98/388, train_loss: 0.2245, step time: 1.5370\n",
      "99/388, train_loss: 0.1277, step time: 1.5361\n",
      "100/388, train_loss: 0.1481, step time: 1.5333\n",
      "101/388, train_loss: 0.1153, step time: 1.5326\n",
      "102/388, train_loss: 0.2669, step time: 1.5312\n",
      "103/388, train_loss: 0.3780, step time: 1.5356\n",
      "104/388, train_loss: 0.0841, step time: 1.5317\n",
      "105/388, train_loss: 0.1696, step time: 1.5348\n",
      "106/388, train_loss: 0.1288, step time: 1.5363\n",
      "107/388, train_loss: 0.0839, step time: 1.5339\n",
      "108/388, train_loss: 0.2027, step time: 1.5339\n",
      "109/388, train_loss: 0.1308, step time: 1.5342\n",
      "110/388, train_loss: 0.1219, step time: 1.5344\n",
      "111/388, train_loss: 0.1015, step time: 1.5323\n",
      "112/388, train_loss: 0.2500, step time: 1.5334\n",
      "113/388, train_loss: 0.1864, step time: 1.5350\n",
      "114/388, train_loss: 0.1444, step time: 1.5340\n",
      "115/388, train_loss: 0.0811, step time: 1.5329\n",
      "116/388, train_loss: 0.1661, step time: 1.5318\n",
      "117/388, train_loss: 0.2224, step time: 1.5292\n",
      "118/388, train_loss: 0.2204, step time: 1.5317\n",
      "119/388, train_loss: 0.2262, step time: 1.5310\n",
      "120/388, train_loss: 0.0788, step time: 1.5308\n",
      "121/388, train_loss: 0.2792, step time: 1.5364\n",
      "122/388, train_loss: 0.0984, step time: 1.5370\n",
      "123/388, train_loss: 0.1296, step time: 1.5358\n",
      "124/388, train_loss: 0.3028, step time: 1.5312\n",
      "125/388, train_loss: 0.2584, step time: 1.5338\n",
      "126/388, train_loss: 0.3628, step time: 1.5312\n",
      "127/388, train_loss: 0.3902, step time: 1.5327\n",
      "128/388, train_loss: 0.1272, step time: 1.5398\n",
      "129/388, train_loss: 0.1390, step time: 1.5339\n",
      "130/388, train_loss: 0.2308, step time: 1.5376\n",
      "131/388, train_loss: 0.1864, step time: 1.5331\n",
      "132/388, train_loss: 0.1657, step time: 1.5322\n",
      "133/388, train_loss: 0.1513, step time: 1.5330\n",
      "134/388, train_loss: 0.3527, step time: 1.5321\n",
      "135/388, train_loss: 0.2865, step time: 1.5335\n",
      "136/388, train_loss: 0.3104, step time: 1.5341\n",
      "137/388, train_loss: 0.0968, step time: 1.5352\n",
      "138/388, train_loss: 0.0312, step time: 1.5295\n",
      "139/388, train_loss: 0.4192, step time: 1.5300\n",
      "140/388, train_loss: 0.1935, step time: 1.5292\n",
      "141/388, train_loss: 0.1712, step time: 1.5535\n",
      "142/388, train_loss: 0.2028, step time: 1.5371\n",
      "143/388, train_loss: 0.0901, step time: 1.5336\n",
      "144/388, train_loss: 0.2089, step time: 1.5326\n",
      "145/388, train_loss: 0.2965, step time: 1.5330\n",
      "146/388, train_loss: 0.4786, step time: 1.5356\n",
      "147/388, train_loss: 0.2107, step time: 1.5317\n",
      "148/388, train_loss: 0.1960, step time: 1.5360\n",
      "149/388, train_loss: 0.1197, step time: 1.5376\n",
      "150/388, train_loss: 0.1499, step time: 1.5339\n",
      "151/388, train_loss: 0.1637, step time: 1.5296\n",
      "152/388, train_loss: 0.2971, step time: 1.5321\n",
      "153/388, train_loss: 0.3900, step time: 1.5307\n",
      "154/388, train_loss: 0.2862, step time: 1.5314\n",
      "155/388, train_loss: 0.3506, step time: 1.5343\n",
      "156/388, train_loss: 0.0956, step time: 1.5345\n",
      "157/388, train_loss: 0.2026, step time: 1.5328\n",
      "158/388, train_loss: 0.0749, step time: 1.5350\n",
      "159/388, train_loss: 0.1693, step time: 1.5337\n",
      "160/388, train_loss: 0.1410, step time: 1.5318\n",
      "161/388, train_loss: 0.2315, step time: 1.5312\n",
      "162/388, train_loss: 0.2140, step time: 1.5350\n",
      "163/388, train_loss: 0.2021, step time: 1.5364\n",
      "164/388, train_loss: 0.1718, step time: 1.5339\n",
      "165/388, train_loss: 0.0911, step time: 1.5330\n",
      "166/388, train_loss: 0.1830, step time: 1.5312\n",
      "167/388, train_loss: 0.2488, step time: 1.5333\n",
      "168/388, train_loss: 0.1043, step time: 1.5354\n",
      "169/388, train_loss: 0.3572, step time: 1.5388\n",
      "170/388, train_loss: 0.1724, step time: 1.5338\n",
      "171/388, train_loss: 0.0998, step time: 1.5331\n",
      "172/388, train_loss: 0.1242, step time: 1.5605\n",
      "173/388, train_loss: 0.2117, step time: 1.5360\n",
      "174/388, train_loss: 0.1090, step time: 1.5351\n",
      "175/388, train_loss: 0.0951, step time: 1.5392\n",
      "176/388, train_loss: 0.4147, step time: 1.5315\n",
      "177/388, train_loss: 0.2820, step time: 1.5329\n",
      "178/388, train_loss: 0.5659, step time: 1.5335\n",
      "179/388, train_loss: 0.1197, step time: 1.5355\n",
      "180/388, train_loss: 0.0707, step time: 1.5297\n",
      "181/388, train_loss: 0.0973, step time: 1.5309\n",
      "182/388, train_loss: 0.1805, step time: 1.5318\n",
      "183/388, train_loss: 0.2843, step time: 1.5306\n",
      "184/388, train_loss: 0.2633, step time: 1.5357\n",
      "185/388, train_loss: 0.1974, step time: 1.5355\n",
      "186/388, train_loss: 0.2214, step time: 1.5354\n",
      "187/388, train_loss: 0.2167, step time: 1.5305\n",
      "188/388, train_loss: 0.1184, step time: 1.5359\n",
      "189/388, train_loss: 0.2411, step time: 1.5349\n",
      "190/388, train_loss: 0.1083, step time: 1.5358\n",
      "191/388, train_loss: 0.2390, step time: 1.5341\n",
      "192/388, train_loss: 0.1224, step time: 1.5382\n",
      "193/388, train_loss: 0.1843, step time: 1.5371\n",
      "194/388, train_loss: 0.1199, step time: 1.5353\n",
      "195/388, train_loss: 0.1725, step time: 1.5309\n",
      "196/388, train_loss: 0.0574, step time: 1.5324\n",
      "197/388, train_loss: 0.1629, step time: 1.5307\n",
      "198/388, train_loss: 0.1630, step time: 1.5381\n",
      "199/388, train_loss: 0.1501, step time: 1.5363\n",
      "200/388, train_loss: 0.0964, step time: 1.5323\n",
      "201/388, train_loss: 0.0852, step time: 1.5325\n",
      "202/388, train_loss: 0.2053, step time: 1.5327\n",
      "203/388, train_loss: 0.4062, step time: 1.5329\n",
      "204/388, train_loss: 0.3453, step time: 1.5350\n",
      "205/388, train_loss: 0.1435, step time: 1.5342\n",
      "206/388, train_loss: 0.1367, step time: 1.5339\n",
      "207/388, train_loss: 0.1130, step time: 1.5316\n",
      "208/388, train_loss: 0.0884, step time: 1.5338\n",
      "209/388, train_loss: 0.0969, step time: 1.5309\n",
      "210/388, train_loss: 0.4681, step time: 1.5317\n",
      "211/388, train_loss: 0.5017, step time: 1.5339\n",
      "212/388, train_loss: 0.0958, step time: 1.5380\n",
      "213/388, train_loss: 0.1034, step time: 1.5351\n",
      "214/388, train_loss: 0.1937, step time: 1.5350\n",
      "215/388, train_loss: 0.0767, step time: 1.5303\n",
      "216/388, train_loss: 0.2189, step time: 1.5324\n",
      "217/388, train_loss: 0.1511, step time: 1.5379\n",
      "218/388, train_loss: 0.0531, step time: 1.5399\n",
      "219/388, train_loss: 0.0751, step time: 1.5325\n",
      "220/388, train_loss: 0.3052, step time: 1.5314\n",
      "221/388, train_loss: 0.0959, step time: 1.5308\n",
      "222/388, train_loss: 0.2080, step time: 1.5396\n",
      "223/388, train_loss: 0.3123, step time: 1.5361\n",
      "224/388, train_loss: 0.1273, step time: 1.5319\n",
      "225/388, train_loss: 0.2489, step time: 1.5342\n",
      "226/388, train_loss: 0.2487, step time: 1.5340\n",
      "227/388, train_loss: 0.3588, step time: 1.5336\n",
      "228/388, train_loss: 0.3500, step time: 1.5368\n",
      "229/388, train_loss: 0.0597, step time: 1.5350\n",
      "230/388, train_loss: 0.1023, step time: 1.5336\n",
      "231/388, train_loss: 0.1602, step time: 1.5336\n",
      "232/388, train_loss: 0.4845, step time: 1.5373\n",
      "233/388, train_loss: 0.0926, step time: 1.5368\n",
      "234/388, train_loss: 0.2386, step time: 1.5364\n",
      "235/388, train_loss: 0.1584, step time: 1.5343\n",
      "236/388, train_loss: 0.4353, step time: 1.5329\n",
      "237/388, train_loss: 0.0770, step time: 1.5300\n",
      "238/388, train_loss: 0.2076, step time: 1.5371\n",
      "239/388, train_loss: 0.2227, step time: 1.5349\n",
      "240/388, train_loss: 0.1056, step time: 1.5324\n",
      "241/388, train_loss: 0.3342, step time: 1.5329\n",
      "242/388, train_loss: 0.1071, step time: 1.5319\n",
      "243/388, train_loss: 0.1759, step time: 1.5338\n",
      "244/388, train_loss: 0.1701, step time: 1.5352\n",
      "245/388, train_loss: 0.1760, step time: 1.5322\n",
      "246/388, train_loss: 0.1046, step time: 1.5342\n",
      "247/388, train_loss: 0.1559, step time: 1.5318\n",
      "248/388, train_loss: 0.1198, step time: 1.5345\n",
      "249/388, train_loss: 0.1131, step time: 1.5359\n",
      "250/388, train_loss: 0.1389, step time: 1.5371\n",
      "251/388, train_loss: 0.0998, step time: 1.5339\n",
      "252/388, train_loss: 0.1443, step time: 1.5315\n",
      "253/388, train_loss: 0.0735, step time: 1.5319\n",
      "254/388, train_loss: 0.2986, step time: 1.5319\n",
      "255/388, train_loss: 0.1114, step time: 1.5318\n",
      "256/388, train_loss: 0.1374, step time: 1.5341\n",
      "257/388, train_loss: 0.1368, step time: 1.5359\n",
      "258/388, train_loss: 0.1139, step time: 1.5339\n",
      "259/388, train_loss: 0.6287, step time: 1.5335\n",
      "260/388, train_loss: 0.2033, step time: 1.5318\n",
      "261/388, train_loss: 0.1446, step time: 1.5331\n",
      "262/388, train_loss: 0.1816, step time: 1.5336\n",
      "263/388, train_loss: 0.2796, step time: 1.5394\n",
      "264/388, train_loss: 0.1598, step time: 1.5352\n",
      "265/388, train_loss: 0.2380, step time: 1.5328\n",
      "266/388, train_loss: 0.2024, step time: 1.5321\n",
      "267/388, train_loss: 0.0659, step time: 1.5355\n",
      "268/388, train_loss: 0.1958, step time: 1.5324\n",
      "269/388, train_loss: 0.1910, step time: 1.5351\n",
      "270/388, train_loss: 0.2687, step time: 1.5335\n",
      "271/388, train_loss: 0.0648, step time: 1.5301\n",
      "272/388, train_loss: 0.1702, step time: 1.5321\n",
      "273/388, train_loss: 0.0643, step time: 1.5336\n",
      "274/388, train_loss: 0.1144, step time: 1.5363\n",
      "275/388, train_loss: 0.0509, step time: 1.5341\n",
      "276/388, train_loss: 0.1296, step time: 1.5325\n",
      "277/388, train_loss: 0.1137, step time: 1.5464\n",
      "278/388, train_loss: 0.0967, step time: 1.5322\n",
      "279/388, train_loss: 0.1496, step time: 1.5354\n",
      "280/388, train_loss: 0.2970, step time: 1.5434\n",
      "281/388, train_loss: 0.1244, step time: 1.5421\n",
      "282/388, train_loss: 0.3239, step time: 1.5350\n",
      "283/388, train_loss: 0.2988, step time: 1.5374\n",
      "284/388, train_loss: 0.0910, step time: 1.5342\n",
      "285/388, train_loss: 0.2013, step time: 1.5335\n",
      "286/388, train_loss: 0.1328, step time: 1.5303\n",
      "287/388, train_loss: 0.1765, step time: 1.5316\n",
      "288/388, train_loss: 0.2340, step time: 1.5326\n",
      "289/388, train_loss: 0.0454, step time: 1.5361\n",
      "290/388, train_loss: 0.2339, step time: 1.5346\n",
      "291/388, train_loss: 0.2341, step time: 1.5344\n",
      "292/388, train_loss: 0.2028, step time: 1.5321\n",
      "293/388, train_loss: 0.2210, step time: 1.5316\n",
      "294/388, train_loss: 0.0759, step time: 1.5311\n",
      "295/388, train_loss: 0.1206, step time: 1.5359\n",
      "296/388, train_loss: 0.1720, step time: 1.5362\n",
      "297/388, train_loss: 0.1876, step time: 1.5327\n",
      "298/388, train_loss: 0.2934, step time: 1.5308\n",
      "299/388, train_loss: 0.1310, step time: 1.5323\n",
      "300/388, train_loss: 0.4487, step time: 1.5375\n",
      "301/388, train_loss: 0.0964, step time: 1.5369\n",
      "302/388, train_loss: 0.3612, step time: 1.5369\n",
      "303/388, train_loss: 0.2858, step time: 1.5349\n",
      "304/388, train_loss: 0.0614, step time: 1.5316\n",
      "305/388, train_loss: 0.1889, step time: 1.5336\n",
      "306/388, train_loss: 0.0570, step time: 1.5354\n",
      "307/388, train_loss: 0.3011, step time: 1.5372\n",
      "308/388, train_loss: 0.1209, step time: 1.5355\n",
      "309/388, train_loss: 0.1474, step time: 1.5300\n",
      "310/388, train_loss: 0.2170, step time: 1.5324\n",
      "311/388, train_loss: 0.0725, step time: 1.5321\n",
      "312/388, train_loss: 0.1520, step time: 1.5338\n",
      "313/388, train_loss: 0.1023, step time: 1.5339\n",
      "314/388, train_loss: 0.1604, step time: 1.5316\n",
      "315/388, train_loss: 0.1207, step time: 1.5333\n",
      "316/388, train_loss: 0.2961, step time: 1.5329\n",
      "317/388, train_loss: 0.0654, step time: 1.5333\n",
      "318/388, train_loss: 0.1342, step time: 1.5328\n",
      "319/388, train_loss: 0.3599, step time: 1.5326\n",
      "320/388, train_loss: 0.2452, step time: 1.5361\n",
      "321/388, train_loss: 0.3863, step time: 1.5345\n",
      "322/388, train_loss: 0.1484, step time: 1.5351\n",
      "323/388, train_loss: 0.1415, step time: 1.5294\n",
      "324/388, train_loss: 0.0948, step time: 1.5296\n",
      "325/388, train_loss: 0.1883, step time: 1.5328\n",
      "326/388, train_loss: 0.0479, step time: 1.5326\n",
      "327/388, train_loss: 0.2384, step time: 1.5350\n",
      "328/388, train_loss: 0.5223, step time: 1.5362\n",
      "329/388, train_loss: 0.1273, step time: 1.5343\n",
      "330/388, train_loss: 0.2281, step time: 1.5312\n",
      "331/388, train_loss: 0.3379, step time: 1.5387\n",
      "332/388, train_loss: 0.1979, step time: 1.5315\n",
      "333/388, train_loss: 0.3168, step time: 1.5343\n",
      "334/388, train_loss: 0.1791, step time: 1.5380\n",
      "335/388, train_loss: 0.2097, step time: 1.5359\n",
      "336/388, train_loss: 0.1037, step time: 1.5320\n",
      "337/388, train_loss: 0.1585, step time: 1.5315\n",
      "338/388, train_loss: 0.1668, step time: 1.5311\n",
      "339/388, train_loss: 0.2200, step time: 1.5345\n",
      "340/388, train_loss: 0.2729, step time: 1.5364\n",
      "341/388, train_loss: 0.3420, step time: 1.5377\n",
      "342/388, train_loss: 0.0323, step time: 1.5349\n",
      "343/388, train_loss: 0.1838, step time: 1.5320\n",
      "344/388, train_loss: 0.2157, step time: 1.5599\n",
      "345/388, train_loss: 0.0509, step time: 1.5346\n",
      "346/388, train_loss: 0.1155, step time: 1.5334\n",
      "347/388, train_loss: 0.0955, step time: 1.5317\n",
      "348/388, train_loss: 0.0893, step time: 1.5338\n",
      "349/388, train_loss: 0.0915, step time: 1.5330\n",
      "350/388, train_loss: 0.0461, step time: 1.5347\n",
      "351/388, train_loss: 0.1698, step time: 1.5315\n",
      "352/388, train_loss: 0.0955, step time: 1.5311\n",
      "353/388, train_loss: 0.1702, step time: 1.5311\n",
      "354/388, train_loss: 0.0715, step time: 1.5339\n",
      "355/388, train_loss: 0.0823, step time: 1.5364\n",
      "356/388, train_loss: 0.0848, step time: 1.5373\n",
      "357/388, train_loss: 0.3751, step time: 1.5348\n",
      "358/388, train_loss: 0.0574, step time: 1.5336\n",
      "359/388, train_loss: 0.1171, step time: 1.5293\n",
      "360/388, train_loss: 0.1167, step time: 1.5312\n",
      "361/388, train_loss: 0.1559, step time: 1.5361\n",
      "362/388, train_loss: 0.1142, step time: 1.5356\n",
      "363/388, train_loss: 0.2403, step time: 1.5456\n",
      "364/388, train_loss: 0.1799, step time: 1.5350\n",
      "365/388, train_loss: 0.2540, step time: 1.5353\n",
      "366/388, train_loss: 0.1037, step time: 1.5367\n",
      "367/388, train_loss: 0.3775, step time: 1.5373\n",
      "368/388, train_loss: 0.0780, step time: 1.5347\n",
      "369/388, train_loss: 0.2238, step time: 1.5338\n",
      "370/388, train_loss: 0.1870, step time: 1.5381\n",
      "371/388, train_loss: 0.2271, step time: 1.5343\n",
      "372/388, train_loss: 0.1149, step time: 1.5354\n",
      "373/388, train_loss: 0.2566, step time: 1.5354\n",
      "374/388, train_loss: 0.1306, step time: 1.5308\n",
      "375/388, train_loss: 0.2566, step time: 1.5313\n",
      "376/388, train_loss: 0.1782, step time: 1.5306\n",
      "377/388, train_loss: 0.0618, step time: 1.5407\n",
      "378/388, train_loss: 0.1516, step time: 1.5384\n",
      "379/388, train_loss: 0.2265, step time: 1.5364\n",
      "380/388, train_loss: 0.1808, step time: 1.5335\n",
      "381/388, train_loss: 0.1723, step time: 1.5327\n",
      "382/388, train_loss: 0.3541, step time: 1.5392\n",
      "383/388, train_loss: 0.1710, step time: 1.5355\n",
      "384/388, train_loss: 0.0894, step time: 1.5354\n",
      "385/388, train_loss: 0.1394, step time: 1.5344\n",
      "386/388, train_loss: 0.1921, step time: 1.5331\n",
      "387/388, train_loss: 0.2350, step time: 1.5327\n",
      "388/388, train_loss: 0.1604, step time: 1.5437\n",
      "epoch 46 average loss: 0.1896\n",
      "current epoch: 46 current mean dice: 0.7598 tc: 0.8062 wt: 0.9010 et: 0.5721\n",
      "best mean dice: 0.7640 at epoch: 36\n",
      "time consuming of epoch 46 is: 704.2793\n",
      "----------\n",
      "epoch 47/100\n",
      "1/388, train_loss: 0.0837, step time: 1.5476\n",
      "2/388, train_loss: 0.0900, step time: 1.5365\n",
      "3/388, train_loss: 0.2533, step time: 1.5355\n",
      "4/388, train_loss: 0.1726, step time: 1.5349\n",
      "5/388, train_loss: 0.1501, step time: 1.5376\n",
      "6/388, train_loss: 0.1000, step time: 1.5371\n",
      "7/388, train_loss: 0.0912, step time: 1.5349\n",
      "8/388, train_loss: 0.1479, step time: 1.5330\n",
      "9/388, train_loss: 0.2724, step time: 1.5339\n",
      "10/388, train_loss: 0.1253, step time: 1.5355\n",
      "11/388, train_loss: 0.2392, step time: 1.5332\n",
      "12/388, train_loss: 0.5055, step time: 1.5336\n",
      "13/388, train_loss: 0.2257, step time: 1.5332\n",
      "14/388, train_loss: 0.1534, step time: 1.5373\n",
      "15/388, train_loss: 0.0344, step time: 1.5368\n",
      "16/388, train_loss: 0.2119, step time: 1.5354\n",
      "17/388, train_loss: 0.4328, step time: 1.5304\n",
      "18/388, train_loss: 0.2924, step time: 1.5324\n",
      "19/388, train_loss: 0.0965, step time: 1.5312\n",
      "20/388, train_loss: 0.2696, step time: 1.5313\n",
      "21/388, train_loss: 0.0982, step time: 1.5365\n",
      "22/388, train_loss: 0.1164, step time: 1.5344\n",
      "23/388, train_loss: 0.0820, step time: 1.5348\n",
      "24/388, train_loss: 0.1660, step time: 1.5341\n",
      "25/388, train_loss: 0.2101, step time: 1.5350\n",
      "26/388, train_loss: 0.0960, step time: 1.5357\n",
      "27/388, train_loss: 0.2193, step time: 1.5353\n",
      "28/388, train_loss: 0.0926, step time: 1.5333\n",
      "29/388, train_loss: 0.0663, step time: 1.5322\n",
      "30/388, train_loss: 0.2796, step time: 1.5339\n",
      "31/388, train_loss: 0.2474, step time: 1.5403\n",
      "32/388, train_loss: 0.1630, step time: 1.5400\n",
      "33/388, train_loss: 0.0525, step time: 1.5357\n",
      "34/388, train_loss: 0.1245, step time: 1.5357\n",
      "35/388, train_loss: 0.1198, step time: 1.5299\n",
      "36/388, train_loss: 0.1861, step time: 1.5343\n",
      "37/388, train_loss: 0.1343, step time: 1.5371\n",
      "38/388, train_loss: 0.3350, step time: 1.5337\n",
      "39/388, train_loss: 0.2176, step time: 1.5450\n",
      "40/388, train_loss: 0.1768, step time: 1.5584\n",
      "41/388, train_loss: 0.3321, step time: 1.5416\n",
      "42/388, train_loss: 0.2170, step time: 1.5374\n",
      "43/388, train_loss: 0.1361, step time: 1.5347\n",
      "44/388, train_loss: 0.1651, step time: 1.5335\n",
      "45/388, train_loss: 0.2227, step time: 1.5387\n",
      "46/388, train_loss: 0.2422, step time: 1.5372\n",
      "47/388, train_loss: 0.0476, step time: 1.5343\n",
      "48/388, train_loss: 0.3314, step time: 1.5349\n",
      "49/388, train_loss: 0.1389, step time: 1.5364\n",
      "50/388, train_loss: 0.0793, step time: 1.5316\n",
      "51/388, train_loss: 0.1112, step time: 1.5329\n",
      "52/388, train_loss: 0.3669, step time: 1.5355\n",
      "53/388, train_loss: 0.3348, step time: 1.5426\n",
      "54/388, train_loss: 0.1697, step time: 1.5366\n",
      "55/388, train_loss: 0.0929, step time: 1.5321\n",
      "56/388, train_loss: 0.2059, step time: 1.5349\n",
      "57/388, train_loss: 0.2227, step time: 1.5507\n",
      "58/388, train_loss: 0.0731, step time: 1.5384\n",
      "59/388, train_loss: 0.2043, step time: 1.5350\n",
      "60/388, train_loss: 0.3479, step time: 1.5346\n",
      "61/388, train_loss: 0.3655, step time: 1.5436\n",
      "62/388, train_loss: 0.2790, step time: 1.5385\n",
      "63/388, train_loss: 0.0709, step time: 1.5343\n",
      "64/388, train_loss: 0.1276, step time: 1.5350\n",
      "65/388, train_loss: 0.2547, step time: 1.5424\n",
      "66/388, train_loss: 0.1915, step time: 1.5354\n",
      "67/388, train_loss: 0.1576, step time: 1.5332\n",
      "68/388, train_loss: 0.2078, step time: 1.5320\n",
      "69/388, train_loss: 0.1714, step time: 1.5413\n",
      "70/388, train_loss: 0.1093, step time: 1.5322\n",
      "71/388, train_loss: 0.4069, step time: 1.5369\n",
      "72/388, train_loss: 0.1304, step time: 1.5482\n",
      "73/388, train_loss: 0.0966, step time: 1.5383\n",
      "74/388, train_loss: 0.1959, step time: 1.5301\n",
      "75/388, train_loss: 0.0963, step time: 1.5358\n",
      "76/388, train_loss: 0.0705, step time: 1.5343\n",
      "77/388, train_loss: 0.1652, step time: 1.5361\n",
      "78/388, train_loss: 0.1135, step time: 1.5336\n",
      "79/388, train_loss: 0.1087, step time: 1.5361\n",
      "80/388, train_loss: 0.0604, step time: 1.5364\n",
      "81/388, train_loss: 0.2769, step time: 1.5404\n",
      "82/388, train_loss: 0.0930, step time: 1.5333\n",
      "83/388, train_loss: 0.2850, step time: 1.5313\n",
      "84/388, train_loss: 0.0997, step time: 1.5371\n",
      "85/388, train_loss: 0.1770, step time: 1.5448\n",
      "86/388, train_loss: 0.2498, step time: 1.5354\n",
      "87/388, train_loss: 0.1518, step time: 1.5372\n",
      "88/388, train_loss: 0.2141, step time: 1.5309\n",
      "89/388, train_loss: 0.2788, step time: 1.5433\n",
      "90/388, train_loss: 0.2225, step time: 1.5368\n",
      "91/388, train_loss: 0.2134, step time: 1.5365\n",
      "92/388, train_loss: 0.3040, step time: 1.5331\n",
      "93/388, train_loss: 0.2722, step time: 1.5410\n",
      "94/388, train_loss: 0.1765, step time: 1.5347\n",
      "95/388, train_loss: 0.0957, step time: 1.5428\n",
      "96/388, train_loss: 0.1438, step time: 1.5343\n",
      "97/388, train_loss: 0.2204, step time: 1.5443\n",
      "98/388, train_loss: 0.0896, step time: 1.5319\n",
      "99/388, train_loss: 0.1953, step time: 1.5396\n",
      "100/388, train_loss: 0.0629, step time: 1.5356\n",
      "101/388, train_loss: 0.1479, step time: 1.5450\n",
      "102/388, train_loss: 0.0981, step time: 1.5314\n",
      "103/388, train_loss: 0.2329, step time: 1.5406\n",
      "104/388, train_loss: 0.2192, step time: 1.5350\n",
      "105/388, train_loss: 0.1092, step time: 1.5443\n",
      "106/388, train_loss: 0.3751, step time: 1.5351\n",
      "107/388, train_loss: 0.1423, step time: 1.5301\n",
      "108/388, train_loss: 0.2787, step time: 1.5321\n",
      "109/388, train_loss: 0.0759, step time: 1.5478\n",
      "110/388, train_loss: 0.0720, step time: 1.5559\n",
      "111/388, train_loss: 0.1498, step time: 1.5307\n",
      "112/388, train_loss: 0.1664, step time: 1.5425\n",
      "113/388, train_loss: 0.1618, step time: 1.5454\n",
      "114/388, train_loss: 0.1543, step time: 1.5320\n",
      "115/388, train_loss: 0.3048, step time: 1.5315\n",
      "116/388, train_loss: 0.0903, step time: 1.5376\n",
      "117/388, train_loss: 0.2827, step time: 1.5497\n",
      "118/388, train_loss: 0.4062, step time: 1.5325\n",
      "119/388, train_loss: 0.0976, step time: 1.5316\n",
      "120/388, train_loss: 0.0687, step time: 1.5363\n",
      "121/388, train_loss: 0.2008, step time: 1.5489\n",
      "122/388, train_loss: 0.1871, step time: 1.5604\n",
      "123/388, train_loss: 0.1290, step time: 1.5348\n",
      "124/388, train_loss: 0.3964, step time: 1.5405\n",
      "125/388, train_loss: 0.4915, step time: 1.5408\n",
      "126/388, train_loss: 0.3173, step time: 1.5301\n",
      "127/388, train_loss: 0.2038, step time: 1.5310\n",
      "128/388, train_loss: 0.1995, step time: 1.5340\n",
      "129/388, train_loss: 0.2209, step time: 1.5410\n",
      "130/388, train_loss: 0.2943, step time: 1.5378\n",
      "131/388, train_loss: 0.0850, step time: 1.5341\n",
      "132/388, train_loss: 0.1712, step time: 1.5342\n",
      "133/388, train_loss: 0.2304, step time: 1.5457\n",
      "134/388, train_loss: 0.1847, step time: 1.5418\n",
      "135/388, train_loss: 0.0846, step time: 1.5373\n",
      "136/388, train_loss: 0.3431, step time: 1.5321\n",
      "137/388, train_loss: 0.3072, step time: 1.5462\n",
      "138/388, train_loss: 0.0857, step time: 1.5338\n",
      "139/388, train_loss: 0.1623, step time: 1.5370\n",
      "140/388, train_loss: 0.3606, step time: 1.5335\n",
      "141/388, train_loss: 0.1316, step time: 1.5410\n",
      "142/388, train_loss: 0.0266, step time: 1.5337\n",
      "143/388, train_loss: 0.0619, step time: 1.5360\n",
      "144/388, train_loss: 0.3806, step time: 1.5342\n",
      "145/388, train_loss: 0.3440, step time: 1.5470\n",
      "146/388, train_loss: 0.1419, step time: 1.5325\n",
      "147/388, train_loss: 0.3575, step time: 1.5302\n",
      "148/388, train_loss: 0.1542, step time: 1.5321\n",
      "149/388, train_loss: 0.1230, step time: 1.5504\n",
      "150/388, train_loss: 0.1524, step time: 1.5355\n",
      "151/388, train_loss: 0.1085, step time: 1.5341\n",
      "152/388, train_loss: 0.2378, step time: 1.5347\n",
      "153/388, train_loss: 0.1561, step time: 1.5319\n",
      "154/388, train_loss: 0.2649, step time: 1.5363\n",
      "155/388, train_loss: 0.0821, step time: 1.5330\n",
      "156/388, train_loss: 0.1269, step time: 1.5347\n",
      "157/388, train_loss: 0.2596, step time: 1.5461\n",
      "158/388, train_loss: 0.1355, step time: 1.5357\n",
      "159/388, train_loss: 0.1373, step time: 1.5359\n",
      "160/388, train_loss: 0.5007, step time: 1.5326\n",
      "161/388, train_loss: 0.0431, step time: 1.5397\n",
      "162/388, train_loss: 0.2320, step time: 1.5406\n",
      "163/388, train_loss: 0.0720, step time: 1.5383\n",
      "164/388, train_loss: 0.0622, step time: 1.5367\n",
      "165/388, train_loss: 0.0654, step time: 1.5438\n",
      "166/388, train_loss: 0.1114, step time: 1.5327\n",
      "167/388, train_loss: 0.0973, step time: 1.5331\n",
      "168/388, train_loss: 0.1233, step time: 1.5372\n",
      "169/388, train_loss: 0.0651, step time: 1.5425\n",
      "170/388, train_loss: 0.1446, step time: 1.5321\n",
      "171/388, train_loss: 0.1818, step time: 1.5302\n",
      "172/388, train_loss: 0.2032, step time: 1.5361\n",
      "173/388, train_loss: 0.2522, step time: 1.5476\n",
      "174/388, train_loss: 0.2362, step time: 1.5330\n",
      "175/388, train_loss: 0.0842, step time: 1.5336\n",
      "176/388, train_loss: 0.0705, step time: 1.5310\n",
      "177/388, train_loss: 0.0714, step time: 1.5445\n",
      "178/388, train_loss: 0.3915, step time: 1.5367\n",
      "179/388, train_loss: 0.2060, step time: 1.5380\n",
      "180/388, train_loss: 0.1010, step time: 1.5350\n",
      "181/388, train_loss: 0.2591, step time: 1.5499\n",
      "182/388, train_loss: 0.2807, step time: 1.5369\n",
      "183/388, train_loss: 0.1720, step time: 1.5398\n",
      "184/388, train_loss: 0.0466, step time: 1.5335\n",
      "185/388, train_loss: 0.1206, step time: 1.5438\n",
      "186/388, train_loss: 0.2575, step time: 1.5381\n",
      "187/388, train_loss: 0.2159, step time: 1.5388\n",
      "188/388, train_loss: 0.1165, step time: 1.5328\n",
      "189/388, train_loss: 0.2747, step time: 1.5396\n",
      "190/388, train_loss: 0.1373, step time: 1.5353\n",
      "191/388, train_loss: 0.1260, step time: 1.5328\n",
      "192/388, train_loss: 0.1170, step time: 1.5348\n",
      "193/388, train_loss: 0.1324, step time: 1.5441\n",
      "194/388, train_loss: 0.1936, step time: 1.5302\n",
      "195/388, train_loss: 0.2282, step time: 1.5327\n",
      "196/388, train_loss: 0.1108, step time: 1.5349\n",
      "197/388, train_loss: 0.1358, step time: 1.5456\n",
      "198/388, train_loss: 0.2517, step time: 1.5338\n",
      "199/388, train_loss: 0.2893, step time: 1.5325\n",
      "200/388, train_loss: 0.0984, step time: 1.5369\n",
      "201/388, train_loss: 0.1012, step time: 1.5417\n",
      "202/388, train_loss: 0.3558, step time: 1.5339\n",
      "203/388, train_loss: 0.4601, step time: 1.5460\n",
      "204/388, train_loss: 0.1054, step time: 1.5317\n",
      "205/388, train_loss: 0.2701, step time: 1.5450\n",
      "206/388, train_loss: 0.4402, step time: 1.5366\n",
      "207/388, train_loss: 0.1614, step time: 1.5348\n",
      "208/388, train_loss: 0.0923, step time: 1.5347\n",
      "209/388, train_loss: 0.1222, step time: 1.5373\n",
      "210/388, train_loss: 0.2615, step time: 1.5316\n",
      "211/388, train_loss: 0.0586, step time: 1.5347\n",
      "212/388, train_loss: 0.2038, step time: 1.5383\n",
      "213/388, train_loss: 0.1345, step time: 1.5442\n",
      "214/388, train_loss: 0.1214, step time: 1.5315\n",
      "215/388, train_loss: 0.1512, step time: 1.5387\n",
      "216/388, train_loss: 0.0817, step time: 1.5354\n",
      "217/388, train_loss: 0.0461, step time: 1.5443\n",
      "218/388, train_loss: 0.3332, step time: 1.5344\n",
      "219/388, train_loss: 0.2042, step time: 1.5295\n",
      "220/388, train_loss: 0.5673, step time: 1.5314\n",
      "221/388, train_loss: 0.0883, step time: 1.5517\n",
      "222/388, train_loss: 0.1312, step time: 1.5351\n",
      "223/388, train_loss: 0.1979, step time: 1.5304\n",
      "224/388, train_loss: 0.1102, step time: 1.5311\n",
      "225/388, train_loss: 0.3978, step time: 1.5501\n",
      "226/388, train_loss: 0.1606, step time: 1.5309\n",
      "227/388, train_loss: 0.1956, step time: 1.5337\n",
      "228/388, train_loss: 0.2189, step time: 1.5389\n",
      "229/388, train_loss: 0.2730, step time: 1.5443\n",
      "230/388, train_loss: 0.5886, step time: 1.5352\n",
      "231/388, train_loss: 0.0900, step time: 1.5340\n",
      "232/388, train_loss: 0.2844, step time: 1.5393\n",
      "233/388, train_loss: 0.0348, step time: 1.5417\n",
      "234/388, train_loss: 0.2350, step time: 1.5309\n",
      "235/388, train_loss: 0.2227, step time: 1.5307\n",
      "236/388, train_loss: 0.0792, step time: 1.5367\n",
      "237/388, train_loss: 0.3996, step time: 1.5475\n",
      "238/388, train_loss: 0.2943, step time: 1.5336\n",
      "239/388, train_loss: 0.0846, step time: 1.5325\n",
      "240/388, train_loss: 0.1945, step time: 1.5340\n",
      "241/388, train_loss: 0.2003, step time: 1.5452\n",
      "242/388, train_loss: 0.1061, step time: 1.5363\n",
      "243/388, train_loss: 0.1924, step time: 1.5329\n",
      "244/388, train_loss: 0.2638, step time: 1.5322\n",
      "245/388, train_loss: 0.1145, step time: 1.5366\n",
      "246/388, train_loss: 0.5454, step time: 1.5356\n",
      "247/388, train_loss: 0.5370, step time: 1.5365\n",
      "248/388, train_loss: 0.2872, step time: 1.5382\n",
      "249/388, train_loss: 0.0701, step time: 1.5421\n",
      "250/388, train_loss: 0.0643, step time: 1.5333\n",
      "251/388, train_loss: 0.1311, step time: 1.5366\n",
      "252/388, train_loss: 0.1045, step time: 1.5362\n",
      "253/388, train_loss: 0.1956, step time: 1.5475\n",
      "254/388, train_loss: 0.0857, step time: 1.5358\n",
      "255/388, train_loss: 0.1019, step time: 1.5313\n",
      "256/388, train_loss: 0.3109, step time: 1.5380\n",
      "257/388, train_loss: 0.2156, step time: 1.5520\n",
      "258/388, train_loss: 0.0981, step time: 1.5332\n",
      "259/388, train_loss: 0.0936, step time: 1.5322\n",
      "260/388, train_loss: 0.1450, step time: 1.5317\n",
      "261/388, train_loss: 0.0880, step time: 1.5487\n",
      "262/388, train_loss: 0.5105, step time: 1.5352\n",
      "263/388, train_loss: 0.1163, step time: 1.5321\n",
      "264/388, train_loss: 0.0577, step time: 1.5353\n",
      "265/388, train_loss: 0.2317, step time: 1.5470\n",
      "266/388, train_loss: 0.2961, step time: 1.5477\n",
      "267/388, train_loss: 0.1557, step time: 1.5345\n",
      "268/388, train_loss: 0.1365, step time: 1.5318\n",
      "269/388, train_loss: 0.1235, step time: 1.5414\n",
      "270/388, train_loss: 0.2291, step time: 1.5425\n",
      "271/388, train_loss: 0.0991, step time: 1.5370\n",
      "272/388, train_loss: 0.0994, step time: 1.5370\n",
      "273/388, train_loss: 0.4134, step time: 1.5460\n",
      "274/388, train_loss: 0.1709, step time: 1.5352\n",
      "275/388, train_loss: 0.1045, step time: 1.5473\n",
      "276/388, train_loss: 0.0999, step time: 1.5317\n",
      "277/388, train_loss: 0.1597, step time: 1.5442\n",
      "278/388, train_loss: 0.0601, step time: 1.5454\n",
      "279/388, train_loss: 0.1590, step time: 1.5387\n",
      "280/388, train_loss: 0.0517, step time: 1.5355\n",
      "281/388, train_loss: 0.1442, step time: 1.5443\n",
      "282/388, train_loss: 0.2212, step time: 1.5394\n",
      "283/388, train_loss: 0.1738, step time: 1.5335\n",
      "284/388, train_loss: 0.2444, step time: 1.5366\n",
      "285/388, train_loss: 0.1834, step time: 1.5428\n",
      "286/388, train_loss: 0.1190, step time: 1.5350\n",
      "287/388, train_loss: 0.2166, step time: 1.5375\n",
      "288/388, train_loss: 0.1829, step time: 1.5386\n",
      "289/388, train_loss: 0.2896, step time: 1.5438\n",
      "290/388, train_loss: 0.0846, step time: 1.5432\n",
      "291/388, train_loss: 0.1152, step time: 1.5500\n",
      "292/388, train_loss: 0.0950, step time: 1.5313\n",
      "293/388, train_loss: 0.2839, step time: 1.5498\n",
      "294/388, train_loss: 0.1668, step time: 1.5369\n",
      "295/388, train_loss: 0.2214, step time: 1.5362\n",
      "296/388, train_loss: 0.1314, step time: 1.5342\n",
      "297/388, train_loss: 0.1015, step time: 1.5414\n",
      "298/388, train_loss: 0.1476, step time: 1.5361\n",
      "299/388, train_loss: 0.0794, step time: 1.5350\n",
      "300/388, train_loss: 0.1009, step time: 1.5429\n",
      "301/388, train_loss: 0.2386, step time: 1.5521\n",
      "302/388, train_loss: 0.2076, step time: 1.5399\n",
      "303/388, train_loss: 0.2656, step time: 1.5328\n",
      "304/388, train_loss: 0.6011, step time: 1.5341\n",
      "305/388, train_loss: 0.2110, step time: 1.5451\n",
      "306/388, train_loss: 0.0767, step time: 1.5326\n",
      "307/388, train_loss: 0.1973, step time: 1.5328\n",
      "308/388, train_loss: 0.2680, step time: 1.5353\n",
      "309/388, train_loss: 0.1544, step time: 1.5448\n",
      "310/388, train_loss: 0.2317, step time: 1.5465\n",
      "311/388, train_loss: 0.2178, step time: 1.5362\n",
      "312/388, train_loss: 0.2222, step time: 1.5348\n",
      "313/388, train_loss: 0.0925, step time: 1.5419\n",
      "314/388, train_loss: 0.1396, step time: 1.5371\n",
      "315/388, train_loss: 0.1686, step time: 1.5353\n",
      "316/388, train_loss: 0.1400, step time: 1.5364\n",
      "317/388, train_loss: 0.5492, step time: 1.5394\n",
      "318/388, train_loss: 0.1757, step time: 1.5320\n",
      "319/388, train_loss: 0.1171, step time: 1.5337\n",
      "320/388, train_loss: 0.0693, step time: 1.5373\n",
      "321/388, train_loss: 0.4500, step time: 1.5478\n",
      "322/388, train_loss: 0.1866, step time: 1.5338\n",
      "323/388, train_loss: 0.1036, step time: 1.5342\n",
      "324/388, train_loss: 0.1774, step time: 1.5361\n",
      "325/388, train_loss: 0.0972, step time: 1.5476\n",
      "326/388, train_loss: 0.1432, step time: 1.5369\n",
      "327/388, train_loss: 0.2283, step time: 1.5337\n",
      "328/388, train_loss: 0.0631, step time: 1.5363\n",
      "329/388, train_loss: 0.0820, step time: 1.5488\n",
      "330/388, train_loss: 0.0624, step time: 1.5338\n",
      "331/388, train_loss: 0.0942, step time: 1.5361\n",
      "332/388, train_loss: 0.2620, step time: 1.5379\n",
      "333/388, train_loss: 0.0630, step time: 1.5476\n",
      "334/388, train_loss: 0.1108, step time: 1.5352\n",
      "335/388, train_loss: 0.2167, step time: 1.5401\n",
      "336/388, train_loss: 0.1910, step time: 1.5356\n",
      "337/388, train_loss: 0.1446, step time: 1.5471\n",
      "338/388, train_loss: 0.0990, step time: 1.5333\n",
      "339/388, train_loss: 0.1443, step time: 1.5312\n",
      "340/388, train_loss: 0.0879, step time: 1.5408\n",
      "341/388, train_loss: 0.1438, step time: 1.5471\n",
      "342/388, train_loss: 0.0801, step time: 1.5315\n",
      "343/388, train_loss: 0.1825, step time: 1.5348\n",
      "344/388, train_loss: 0.1303, step time: 1.5380\n",
      "345/388, train_loss: 0.1533, step time: 1.5438\n",
      "346/388, train_loss: 0.2585, step time: 1.5343\n",
      "347/388, train_loss: 0.1009, step time: 1.5331\n",
      "348/388, train_loss: 0.2333, step time: 1.5332\n",
      "349/388, train_loss: 0.1047, step time: 1.5503\n",
      "350/388, train_loss: 0.0708, step time: 1.5368\n",
      "351/388, train_loss: 0.4823, step time: 1.5342\n",
      "352/388, train_loss: 0.2439, step time: 1.5335\n",
      "353/388, train_loss: 0.1954, step time: 1.5447\n",
      "354/388, train_loss: 0.2770, step time: 1.5386\n",
      "355/388, train_loss: 0.2164, step time: 1.5333\n",
      "356/388, train_loss: 0.1274, step time: 1.5333\n",
      "357/388, train_loss: 0.1107, step time: 1.5459\n",
      "358/388, train_loss: 0.2090, step time: 1.5383\n",
      "359/388, train_loss: 0.1044, step time: 1.5387\n",
      "360/388, train_loss: 0.3044, step time: 1.5314\n",
      "361/388, train_loss: 0.1608, step time: 1.5441\n",
      "362/388, train_loss: 0.1105, step time: 1.5376\n",
      "363/388, train_loss: 0.1530, step time: 1.5377\n",
      "364/388, train_loss: 0.2051, step time: 1.5367\n",
      "365/388, train_loss: 0.1684, step time: 1.5406\n",
      "366/388, train_loss: 0.1507, step time: 1.5357\n",
      "367/388, train_loss: 0.1604, step time: 1.5381\n",
      "368/388, train_loss: 0.2522, step time: 1.5393\n",
      "369/388, train_loss: 0.0879, step time: 1.5424\n",
      "370/388, train_loss: 0.1074, step time: 1.5363\n",
      "371/388, train_loss: 0.1124, step time: 1.5385\n",
      "372/388, train_loss: 0.2349, step time: 1.5373\n",
      "373/388, train_loss: 0.2500, step time: 1.5489\n",
      "374/388, train_loss: 0.1625, step time: 1.5358\n",
      "375/388, train_loss: 0.1410, step time: 1.5364\n",
      "376/388, train_loss: 0.2831, step time: 1.5359\n",
      "377/388, train_loss: 0.1809, step time: 1.5474\n",
      "378/388, train_loss: 0.3086, step time: 1.5340\n",
      "379/388, train_loss: 0.2026, step time: 1.5331\n",
      "380/388, train_loss: 0.3409, step time: 1.5372\n",
      "381/388, train_loss: 0.0933, step time: 1.5494\n",
      "382/388, train_loss: 0.3099, step time: 1.5356\n",
      "383/388, train_loss: 0.1178, step time: 1.5302\n",
      "384/388, train_loss: 0.2132, step time: 1.5332\n",
      "385/388, train_loss: 0.0896, step time: 1.5491\n",
      "386/388, train_loss: 0.3700, step time: 1.5381\n",
      "387/388, train_loss: 0.0625, step time: 1.5320\n",
      "388/388, train_loss: 0.1674, step time: 1.5326\n",
      "epoch 47 average loss: 0.1863\n",
      "current epoch: 47 current mean dice: 0.7597 tc: 0.8073 wt: 0.9037 et: 0.5681\n",
      "best mean dice: 0.7640 at epoch: 36\n",
      "time consuming of epoch 47 is: 702.7020\n",
      "----------\n",
      "epoch 48/100\n",
      "1/388, train_loss: 0.2338, step time: 1.5684\n",
      "2/388, train_loss: 0.1326, step time: 1.5351\n",
      "3/388, train_loss: 0.2265, step time: 1.5330\n",
      "4/388, train_loss: 0.1017, step time: 1.5376\n",
      "5/388, train_loss: 0.2163, step time: 1.5448\n",
      "6/388, train_loss: 0.1051, step time: 1.5364\n",
      "7/388, train_loss: 0.1340, step time: 1.5322\n",
      "8/388, train_loss: 0.2577, step time: 1.5349\n",
      "9/388, train_loss: 0.1079, step time: 1.5435\n",
      "10/388, train_loss: 0.4792, step time: 1.5325\n",
      "11/388, train_loss: 0.2171, step time: 1.5316\n",
      "12/388, train_loss: 0.1711, step time: 1.5320\n",
      "13/388, train_loss: 0.1174, step time: 1.5398\n",
      "14/388, train_loss: 0.2017, step time: 1.5327\n",
      "15/388, train_loss: 0.1995, step time: 1.5375\n",
      "16/388, train_loss: 0.3555, step time: 1.5318\n",
      "17/388, train_loss: 0.0953, step time: 1.5467\n",
      "18/388, train_loss: 0.1923, step time: 1.5368\n",
      "19/388, train_loss: 0.1483, step time: 1.5350\n",
      "20/388, train_loss: 0.1773, step time: 1.5329\n",
      "21/388, train_loss: 0.1089, step time: 1.5448\n",
      "22/388, train_loss: 0.2725, step time: 1.5351\n",
      "23/388, train_loss: 0.1521, step time: 1.5349\n",
      "24/388, train_loss: 0.1355, step time: 1.5361\n",
      "25/388, train_loss: 0.1238, step time: 1.5417\n",
      "26/388, train_loss: 0.3777, step time: 1.5337\n",
      "27/388, train_loss: 0.0843, step time: 1.5359\n",
      "28/388, train_loss: 0.2788, step time: 1.5356\n",
      "29/388, train_loss: 0.1334, step time: 1.5425\n",
      "30/388, train_loss: 0.1583, step time: 1.5357\n",
      "31/388, train_loss: 0.2685, step time: 1.5338\n",
      "32/388, train_loss: 0.2684, step time: 1.5389\n",
      "33/388, train_loss: 0.2056, step time: 1.5502\n",
      "34/388, train_loss: 0.0707, step time: 1.5336\n",
      "35/388, train_loss: 0.1263, step time: 1.5319\n",
      "36/388, train_loss: 0.2344, step time: 1.5378\n",
      "37/388, train_loss: 0.1101, step time: 1.5493\n",
      "38/388, train_loss: 0.0898, step time: 1.5341\n",
      "39/388, train_loss: 0.2187, step time: 1.5333\n",
      "40/388, train_loss: 0.3224, step time: 1.5337\n",
      "41/388, train_loss: 0.1574, step time: 1.5476\n",
      "42/388, train_loss: 0.0914, step time: 1.5360\n",
      "43/388, train_loss: 0.3084, step time: 1.5320\n",
      "44/388, train_loss: 0.1422, step time: 1.5329\n",
      "45/388, train_loss: 0.1866, step time: 1.5423\n",
      "46/388, train_loss: 0.1506, step time: 1.5372\n",
      "47/388, train_loss: 0.1853, step time: 1.5352\n",
      "48/388, train_loss: 0.1796, step time: 1.5350\n",
      "49/388, train_loss: 0.2420, step time: 1.5431\n",
      "50/388, train_loss: 0.3510, step time: 1.5466\n",
      "51/388, train_loss: 0.0902, step time: 1.5354\n",
      "52/388, train_loss: 0.1146, step time: 1.5334\n",
      "53/388, train_loss: 0.2740, step time: 1.5443\n",
      "54/388, train_loss: 0.2697, step time: 1.5319\n",
      "55/388, train_loss: 0.0992, step time: 1.5350\n",
      "56/388, train_loss: 0.3188, step time: 1.5348\n",
      "57/388, train_loss: 0.2747, step time: 1.5490\n",
      "58/388, train_loss: 0.0930, step time: 1.5339\n",
      "59/388, train_loss: 0.3632, step time: 1.5355\n",
      "60/388, train_loss: 0.0943, step time: 1.5336\n",
      "61/388, train_loss: 0.0494, step time: 1.5471\n",
      "62/388, train_loss: 0.2808, step time: 1.5317\n",
      "63/388, train_loss: 0.1994, step time: 1.5396\n",
      "64/388, train_loss: 0.0936, step time: 1.5328\n",
      "65/388, train_loss: 0.1615, step time: 1.5497\n",
      "66/388, train_loss: 0.1642, step time: 1.5359\n",
      "67/388, train_loss: 0.1521, step time: 1.5342\n",
      "68/388, train_loss: 0.0682, step time: 1.5329\n",
      "69/388, train_loss: 0.1574, step time: 1.5383\n",
      "70/388, train_loss: 0.2113, step time: 1.5322\n",
      "71/388, train_loss: 0.1432, step time: 1.5357\n",
      "72/388, train_loss: 0.1097, step time: 1.5374\n",
      "73/388, train_loss: 0.3865, step time: 1.5484\n",
      "74/388, train_loss: 0.0680, step time: 1.5337\n",
      "75/388, train_loss: 0.1292, step time: 1.5351\n",
      "76/388, train_loss: 0.2525, step time: 1.5415\n",
      "77/388, train_loss: 0.1107, step time: 1.5495\n",
      "78/388, train_loss: 0.2424, step time: 1.5321\n",
      "79/388, train_loss: 0.2659, step time: 1.5348\n",
      "80/388, train_loss: 0.2189, step time: 1.5350\n",
      "81/388, train_loss: 0.1150, step time: 1.5505\n",
      "82/388, train_loss: 0.1276, step time: 1.5326\n",
      "83/388, train_loss: 0.0958, step time: 1.5343\n",
      "84/388, train_loss: 0.1297, step time: 1.5315\n",
      "85/388, train_loss: 0.1330, step time: 1.5463\n",
      "86/388, train_loss: 0.0656, step time: 1.5377\n",
      "87/388, train_loss: 0.0800, step time: 1.5344\n",
      "88/388, train_loss: 0.2148, step time: 1.5327\n",
      "89/388, train_loss: 0.1299, step time: 1.5396\n",
      "90/388, train_loss: 0.2416, step time: 1.5359\n",
      "91/388, train_loss: 0.2208, step time: 1.5362\n",
      "92/388, train_loss: 0.0990, step time: 1.5339\n",
      "93/388, train_loss: 0.1876, step time: 1.5422\n",
      "94/388, train_loss: 0.1538, step time: 1.5369\n",
      "95/388, train_loss: 0.0509, step time: 1.5404\n",
      "96/388, train_loss: 0.2087, step time: 1.5304\n",
      "97/388, train_loss: 0.4706, step time: 1.5438\n",
      "98/388, train_loss: 0.2818, step time: 1.5310\n",
      "99/388, train_loss: 0.0889, step time: 1.5370\n",
      "100/388, train_loss: 0.0424, step time: 1.5381\n",
      "101/388, train_loss: 0.0858, step time: 1.5485\n",
      "102/388, train_loss: 0.1537, step time: 1.5311\n",
      "103/388, train_loss: 0.1594, step time: 1.5324\n",
      "104/388, train_loss: 0.0530, step time: 1.5357\n",
      "105/388, train_loss: 0.0470, step time: 1.5460\n",
      "106/388, train_loss: 0.1420, step time: 1.5379\n",
      "107/388, train_loss: 0.2721, step time: 1.5321\n",
      "108/388, train_loss: 0.1089, step time: 1.5324\n",
      "109/388, train_loss: 0.2946, step time: 1.5391\n",
      "110/388, train_loss: 0.1333, step time: 1.5384\n",
      "111/388, train_loss: 0.1596, step time: 1.5330\n",
      "112/388, train_loss: 0.1642, step time: 1.5337\n",
      "113/388, train_loss: 0.2712, step time: 1.5442\n",
      "114/388, train_loss: 0.4615, step time: 1.5325\n",
      "115/388, train_loss: 0.2132, step time: 1.5348\n",
      "116/388, train_loss: 0.2214, step time: 1.5379\n",
      "117/388, train_loss: 0.1532, step time: 1.5438\n",
      "118/388, train_loss: 0.0783, step time: 1.5345\n",
      "119/388, train_loss: 0.1527, step time: 1.5337\n",
      "120/388, train_loss: 0.0969, step time: 1.5314\n",
      "121/388, train_loss: 0.0761, step time: 1.5482\n",
      "122/388, train_loss: 0.2354, step time: 1.5568\n",
      "123/388, train_loss: 0.5410, step time: 1.5618\n",
      "124/388, train_loss: 0.1150, step time: 1.5644\n",
      "125/388, train_loss: 0.1395, step time: 1.5450\n",
      "126/388, train_loss: 0.0522, step time: 1.5352\n",
      "127/388, train_loss: 0.1311, step time: 1.5360\n",
      "128/388, train_loss: 0.1051, step time: 1.5381\n",
      "129/388, train_loss: 0.1513, step time: 1.5481\n",
      "130/388, train_loss: 0.1716, step time: 1.5349\n",
      "131/388, train_loss: 0.0618, step time: 1.5336\n",
      "132/388, train_loss: 0.2251, step time: 1.5352\n",
      "133/388, train_loss: 0.1319, step time: 1.5418\n",
      "134/388, train_loss: 0.1310, step time: 1.5351\n",
      "135/388, train_loss: 0.1288, step time: 1.5320\n",
      "136/388, train_loss: 0.0736, step time: 1.5419\n",
      "137/388, train_loss: 0.4943, step time: 1.5468\n",
      "138/388, train_loss: 0.1696, step time: 1.5337\n",
      "139/388, train_loss: 0.1842, step time: 1.5343\n",
      "140/388, train_loss: 0.1426, step time: 1.5312\n",
      "141/388, train_loss: 0.0921, step time: 1.5472\n",
      "142/388, train_loss: 0.1856, step time: 1.5358\n",
      "143/388, train_loss: 0.1102, step time: 1.5323\n",
      "144/388, train_loss: 0.1319, step time: 1.5347\n",
      "145/388, train_loss: 0.1026, step time: 1.5435\n",
      "146/388, train_loss: 0.2687, step time: 1.5378\n",
      "147/388, train_loss: 0.3081, step time: 1.5356\n",
      "148/388, train_loss: 0.1564, step time: 1.5335\n",
      "149/388, train_loss: 0.0995, step time: 1.5427\n",
      "150/388, train_loss: 0.1434, step time: 1.5298\n",
      "151/388, train_loss: 0.0725, step time: 1.5347\n",
      "152/388, train_loss: 0.0547, step time: 1.5340\n",
      "153/388, train_loss: 0.4421, step time: 1.5451\n",
      "154/388, train_loss: 0.4220, step time: 1.5327\n",
      "155/388, train_loss: 0.1230, step time: 1.5299\n",
      "156/388, train_loss: 0.1890, step time: 1.5320\n",
      "157/388, train_loss: 0.1144, step time: 1.5362\n",
      "158/388, train_loss: 0.2117, step time: 1.5375\n",
      "159/388, train_loss: 0.1278, step time: 1.5361\n",
      "160/388, train_loss: 0.1036, step time: 1.5363\n",
      "161/388, train_loss: 0.6428, step time: 1.5465\n",
      "162/388, train_loss: 0.1298, step time: 1.5348\n",
      "163/388, train_loss: 0.2460, step time: 1.5514\n",
      "164/388, train_loss: 0.0575, step time: 1.5329\n",
      "165/388, train_loss: 0.2906, step time: 1.5330\n",
      "166/388, train_loss: 0.1284, step time: 1.5344\n",
      "167/388, train_loss: 0.1904, step time: 1.5353\n",
      "168/388, train_loss: 0.1516, step time: 1.5351\n",
      "169/388, train_loss: 0.2510, step time: 1.5384\n",
      "170/388, train_loss: 0.5300, step time: 1.5343\n",
      "171/388, train_loss: 0.0869, step time: 1.5314\n",
      "172/388, train_loss: 0.2930, step time: 1.5316\n",
      "173/388, train_loss: 0.0411, step time: 1.5361\n",
      "174/388, train_loss: 0.2239, step time: 1.5333\n",
      "175/388, train_loss: 0.0787, step time: 1.5342\n",
      "176/388, train_loss: 0.0828, step time: 1.5326\n",
      "177/388, train_loss: 0.1507, step time: 1.5337\n",
      "178/388, train_loss: 0.0983, step time: 1.5323\n",
      "179/388, train_loss: 0.1058, step time: 1.5356\n",
      "180/388, train_loss: 0.0681, step time: 1.5347\n",
      "181/388, train_loss: 0.1499, step time: 1.5347\n",
      "182/388, train_loss: 0.3616, step time: 1.5328\n",
      "183/388, train_loss: 0.2689, step time: 1.5343\n",
      "184/388, train_loss: 0.1844, step time: 1.5317\n",
      "185/388, train_loss: 0.0266, step time: 1.5634\n",
      "186/388, train_loss: 0.1507, step time: 1.5337\n",
      "187/388, train_loss: 0.3162, step time: 1.5356\n",
      "188/388, train_loss: 0.0563, step time: 1.5316\n",
      "189/388, train_loss: 0.1040, step time: 1.5318\n",
      "190/388, train_loss: 0.0949, step time: 1.5338\n",
      "191/388, train_loss: 0.1155, step time: 1.5343\n",
      "192/388, train_loss: 0.0648, step time: 1.5407\n",
      "193/388, train_loss: 0.1019, step time: 1.5317\n",
      "194/388, train_loss: 0.1291, step time: 1.5319\n",
      "195/388, train_loss: 0.2571, step time: 1.5323\n",
      "196/388, train_loss: 0.1298, step time: 1.5336\n",
      "197/388, train_loss: 0.3208, step time: 1.5309\n",
      "198/388, train_loss: 0.0322, step time: 1.5360\n",
      "199/388, train_loss: 0.1680, step time: 1.5467\n",
      "200/388, train_loss: 0.1667, step time: 1.5347\n",
      "201/388, train_loss: 0.1101, step time: 1.5317\n",
      "202/388, train_loss: 0.2210, step time: 1.5462\n",
      "203/388, train_loss: 0.1923, step time: 1.5361\n",
      "204/388, train_loss: 0.0606, step time: 1.5319\n",
      "205/388, train_loss: 0.0831, step time: 1.5334\n",
      "206/388, train_loss: 0.1542, step time: 1.5343\n",
      "207/388, train_loss: 0.5249, step time: 1.5354\n",
      "208/388, train_loss: 0.2770, step time: 1.5345\n",
      "209/388, train_loss: 0.3306, step time: 1.5373\n",
      "210/388, train_loss: 0.1385, step time: 1.5625\n",
      "211/388, train_loss: 0.0597, step time: 1.5481\n",
      "212/388, train_loss: 0.1519, step time: 1.5338\n",
      "213/388, train_loss: 0.2352, step time: 1.5404\n",
      "214/388, train_loss: 0.2737, step time: 1.5334\n",
      "215/388, train_loss: 0.4927, step time: 1.5477\n",
      "216/388, train_loss: 0.2665, step time: 1.5353\n",
      "217/388, train_loss: 0.5251, step time: 1.5306\n",
      "218/388, train_loss: 0.1398, step time: 1.5345\n",
      "219/388, train_loss: 0.1948, step time: 1.5551\n",
      "220/388, train_loss: 0.0899, step time: 1.5350\n",
      "221/388, train_loss: 0.3033, step time: 1.5348\n",
      "222/388, train_loss: 0.1337, step time: 1.5348\n",
      "223/388, train_loss: 0.1330, step time: 1.5553\n",
      "224/388, train_loss: 0.0912, step time: 1.5338\n",
      "225/388, train_loss: 0.1353, step time: 1.5553\n",
      "226/388, train_loss: 0.1009, step time: 1.5328\n",
      "227/388, train_loss: 0.2461, step time: 1.5520\n",
      "228/388, train_loss: 0.0986, step time: 1.5621\n",
      "229/388, train_loss: 0.1978, step time: 1.5315\n",
      "230/388, train_loss: 0.3594, step time: 1.5421\n",
      "231/388, train_loss: 0.0897, step time: 1.5503\n",
      "232/388, train_loss: 0.3851, step time: 1.5336\n",
      "233/388, train_loss: 0.1112, step time: 1.5337\n",
      "234/388, train_loss: 0.2789, step time: 1.5335\n",
      "235/388, train_loss: 0.2080, step time: 1.5521\n",
      "236/388, train_loss: 0.2241, step time: 1.5322\n",
      "237/388, train_loss: 0.3124, step time: 1.5337\n",
      "238/388, train_loss: 0.2182, step time: 1.5418\n",
      "239/388, train_loss: 0.1036, step time: 1.5525\n",
      "240/388, train_loss: 0.0681, step time: 1.5596\n",
      "241/388, train_loss: 0.0813, step time: 1.5353\n",
      "242/388, train_loss: 0.0685, step time: 1.5364\n",
      "243/388, train_loss: 0.2115, step time: 1.5434\n",
      "244/388, train_loss: 0.1349, step time: 1.5332\n",
      "245/388, train_loss: 0.2353, step time: 1.5297\n",
      "246/388, train_loss: 0.4061, step time: 1.5414\n",
      "247/388, train_loss: 0.1135, step time: 1.5520\n",
      "248/388, train_loss: 0.1327, step time: 1.5337\n",
      "249/388, train_loss: 0.1070, step time: 1.5368\n",
      "250/388, train_loss: 0.1187, step time: 1.5373\n",
      "251/388, train_loss: 0.1585, step time: 1.5524\n",
      "252/388, train_loss: 0.0872, step time: 1.5341\n",
      "253/388, train_loss: 0.2968, step time: 1.5630\n",
      "254/388, train_loss: 0.0648, step time: 1.5324\n",
      "255/388, train_loss: 0.1676, step time: 1.5578\n",
      "256/388, train_loss: 0.2279, step time: 1.5386\n",
      "257/388, train_loss: 0.0856, step time: 1.5354\n",
      "258/388, train_loss: 0.1690, step time: 1.5341\n",
      "259/388, train_loss: 0.1713, step time: 1.5432\n",
      "260/388, train_loss: 0.1587, step time: 1.5349\n",
      "261/388, train_loss: 0.2710, step time: 1.5308\n",
      "262/388, train_loss: 0.1047, step time: 1.5331\n",
      "263/388, train_loss: 0.1768, step time: 1.5480\n",
      "264/388, train_loss: 0.1076, step time: 1.5335\n",
      "265/388, train_loss: 0.0535, step time: 1.5306\n",
      "266/388, train_loss: 0.2001, step time: 1.5330\n",
      "267/388, train_loss: 0.1717, step time: 1.5526\n",
      "268/388, train_loss: 0.1006, step time: 1.5329\n",
      "269/388, train_loss: 0.1983, step time: 1.5345\n",
      "270/388, train_loss: 0.2241, step time: 1.5362\n",
      "271/388, train_loss: 0.1231, step time: 1.5405\n",
      "272/388, train_loss: 0.1719, step time: 1.5340\n",
      "273/388, train_loss: 0.0904, step time: 1.5311\n",
      "274/388, train_loss: 0.2698, step time: 1.5343\n",
      "275/388, train_loss: 0.0722, step time: 1.5524\n",
      "276/388, train_loss: 0.1431, step time: 1.5585\n",
      "277/388, train_loss: 0.2237, step time: 1.5345\n",
      "278/388, train_loss: 0.0988, step time: 1.5369\n",
      "279/388, train_loss: 0.1139, step time: 1.5543\n",
      "280/388, train_loss: 0.0877, step time: 1.5342\n",
      "281/388, train_loss: 0.1684, step time: 1.5347\n",
      "282/388, train_loss: 0.1030, step time: 1.5575\n",
      "283/388, train_loss: 0.2930, step time: 1.5562\n",
      "284/388, train_loss: 0.0986, step time: 1.5367\n",
      "285/388, train_loss: 0.1515, step time: 1.5331\n",
      "286/388, train_loss: 0.1843, step time: 1.5340\n",
      "287/388, train_loss: 0.1652, step time: 1.5454\n",
      "288/388, train_loss: 0.1131, step time: 1.5327\n",
      "289/388, train_loss: 0.1570, step time: 1.5327\n",
      "290/388, train_loss: 0.4338, step time: 1.5366\n",
      "291/388, train_loss: 0.3365, step time: 1.5543\n",
      "292/388, train_loss: 0.1837, step time: 1.5497\n",
      "293/388, train_loss: 0.0869, step time: 1.5337\n",
      "294/388, train_loss: 0.4017, step time: 1.5336\n",
      "295/388, train_loss: 0.1672, step time: 1.5526\n",
      "296/388, train_loss: 0.2165, step time: 1.5322\n",
      "297/388, train_loss: 0.2520, step time: 1.5340\n",
      "298/388, train_loss: 0.3978, step time: 1.5345\n",
      "299/388, train_loss: 0.2372, step time: 1.5563\n",
      "300/388, train_loss: 0.2293, step time: 1.5305\n",
      "301/388, train_loss: 0.1074, step time: 1.5428\n",
      "302/388, train_loss: 0.1195, step time: 1.5366\n",
      "303/388, train_loss: 0.3863, step time: 1.5547\n",
      "304/388, train_loss: 0.2029, step time: 1.5332\n",
      "305/388, train_loss: 0.2237, step time: 1.5363\n",
      "306/388, train_loss: 0.0804, step time: 1.5456\n",
      "307/388, train_loss: 0.2258, step time: 1.5486\n",
      "308/388, train_loss: 0.3802, step time: 1.5468\n",
      "309/388, train_loss: 0.1630, step time: 1.5357\n",
      "310/388, train_loss: 0.3918, step time: 1.5349\n",
      "311/388, train_loss: 0.1479, step time: 1.5383\n",
      "312/388, train_loss: 0.1156, step time: 1.5319\n",
      "313/388, train_loss: 0.0808, step time: 1.5330\n",
      "314/388, train_loss: 0.1550, step time: 1.5460\n",
      "315/388, train_loss: 0.0783, step time: 1.5454\n",
      "316/388, train_loss: 0.1746, step time: 1.5319\n",
      "317/388, train_loss: 0.1363, step time: 1.5316\n",
      "318/388, train_loss: 0.1558, step time: 1.5319\n",
      "319/388, train_loss: 0.0603, step time: 1.5475\n",
      "320/388, train_loss: 0.1227, step time: 1.5397\n",
      "321/388, train_loss: 0.3970, step time: 1.5337\n",
      "322/388, train_loss: 0.1094, step time: 1.5348\n",
      "323/388, train_loss: 0.2370, step time: 1.5542\n",
      "324/388, train_loss: 0.1647, step time: 1.5625\n",
      "325/388, train_loss: 0.5068, step time: 1.5319\n",
      "326/388, train_loss: 0.1389, step time: 1.5328\n",
      "327/388, train_loss: 0.0713, step time: 1.5470\n",
      "328/388, train_loss: 0.2839, step time: 1.5317\n",
      "329/388, train_loss: 0.0553, step time: 1.5325\n",
      "330/388, train_loss: 0.1844, step time: 1.5319\n",
      "331/388, train_loss: 0.0902, step time: 1.5489\n",
      "332/388, train_loss: 0.1805, step time: 1.5365\n",
      "333/388, train_loss: 0.0840, step time: 1.5335\n",
      "334/388, train_loss: 0.3917, step time: 1.5341\n",
      "335/388, train_loss: 0.2396, step time: 1.5519\n",
      "336/388, train_loss: 0.1752, step time: 1.5361\n",
      "337/388, train_loss: 0.2897, step time: 1.5352\n",
      "338/388, train_loss: 0.1069, step time: 1.5370\n",
      "339/388, train_loss: 0.0491, step time: 1.5387\n",
      "340/388, train_loss: 0.1661, step time: 1.5321\n",
      "341/388, train_loss: 0.1285, step time: 1.5352\n",
      "342/388, train_loss: 0.3203, step time: 1.5367\n",
      "343/388, train_loss: 0.1576, step time: 1.5513\n",
      "344/388, train_loss: 0.0913, step time: 1.5645\n",
      "345/388, train_loss: 0.0845, step time: 1.5350\n",
      "346/388, train_loss: 0.2179, step time: 1.5375\n",
      "347/388, train_loss: 0.2007, step time: 1.5468\n",
      "348/388, train_loss: 0.2887, step time: 1.5350\n",
      "349/388, train_loss: 0.1821, step time: 1.5336\n",
      "350/388, train_loss: 0.3359, step time: 1.5314\n",
      "351/388, train_loss: 0.2574, step time: 1.5582\n",
      "352/388, train_loss: 0.2324, step time: 1.5325\n",
      "353/388, train_loss: 0.2231, step time: 1.5330\n",
      "354/388, train_loss: 0.1211, step time: 1.5332\n",
      "355/388, train_loss: 0.1976, step time: 1.5512\n",
      "356/388, train_loss: 0.2786, step time: 1.5309\n",
      "357/388, train_loss: 0.0891, step time: 1.5321\n",
      "358/388, train_loss: 0.0885, step time: 1.5363\n",
      "359/388, train_loss: 0.0638, step time: 1.5458\n",
      "360/388, train_loss: 0.2218, step time: 1.5302\n",
      "361/388, train_loss: 0.2042, step time: 1.5347\n",
      "362/388, train_loss: 0.2157, step time: 1.5367\n",
      "363/388, train_loss: 0.1997, step time: 1.5570\n",
      "364/388, train_loss: 0.3574, step time: 1.5595\n",
      "365/388, train_loss: 0.1444, step time: 1.5733\n",
      "366/388, train_loss: 0.1037, step time: 1.5341\n",
      "367/388, train_loss: 0.2797, step time: 1.5431\n",
      "368/388, train_loss: 0.2308, step time: 1.5380\n",
      "369/388, train_loss: 0.1703, step time: 1.5282\n",
      "370/388, train_loss: 0.3021, step time: 1.5325\n",
      "371/388, train_loss: 0.1034, step time: 1.5400\n",
      "372/388, train_loss: 0.1689, step time: 1.5363\n",
      "373/388, train_loss: 0.1635, step time: 1.5469\n",
      "374/388, train_loss: 0.1000, step time: 1.5323\n",
      "375/388, train_loss: 0.1074, step time: 1.5535\n",
      "376/388, train_loss: 0.1692, step time: 1.5363\n",
      "377/388, train_loss: 0.5261, step time: 1.5441\n",
      "378/388, train_loss: 0.2434, step time: 1.5330\n",
      "379/388, train_loss: 0.1497, step time: 1.5525\n",
      "380/388, train_loss: 0.3749, step time: 1.5602\n",
      "381/388, train_loss: 0.0818, step time: 1.5323\n",
      "382/388, train_loss: 0.2826, step time: 1.5327\n",
      "383/388, train_loss: 0.3080, step time: 1.5450\n",
      "384/388, train_loss: 0.3221, step time: 1.5389\n",
      "385/388, train_loss: 0.2837, step time: 1.5358\n",
      "386/388, train_loss: 0.0814, step time: 1.5332\n",
      "387/388, train_loss: 0.1790, step time: 1.5433\n",
      "388/388, train_loss: 0.2066, step time: 1.5325\n",
      "epoch 48 average loss: 0.1849\n",
      "current epoch: 48 current mean dice: 0.7570 tc: 0.8016 wt: 0.9004 et: 0.5691\n",
      "best mean dice: 0.7640 at epoch: 36\n",
      "time consuming of epoch 48 is: 706.9599\n",
      "----------\n",
      "epoch 49/100\n",
      "1/388, train_loss: 0.1190, step time: 1.5493\n",
      "2/388, train_loss: 0.4873, step time: 1.5336\n",
      "3/388, train_loss: 0.1361, step time: 1.5370\n",
      "4/388, train_loss: 0.1442, step time: 1.5372\n",
      "5/388, train_loss: 0.1924, step time: 1.5322\n",
      "6/388, train_loss: 0.0850, step time: 1.5379\n",
      "7/388, train_loss: 0.0783, step time: 1.5395\n",
      "8/388, train_loss: 0.1056, step time: 1.5334\n",
      "9/388, train_loss: 0.0931, step time: 1.5352\n",
      "10/388, train_loss: 0.1822, step time: 1.5300\n",
      "11/388, train_loss: 0.1529, step time: 1.5328\n",
      "12/388, train_loss: 0.1027, step time: 1.5347\n",
      "13/388, train_loss: 0.3775, step time: 1.5319\n",
      "14/388, train_loss: 0.2412, step time: 1.5349\n",
      "15/388, train_loss: 0.1918, step time: 1.5334\n",
      "16/388, train_loss: 0.2609, step time: 1.5373\n",
      "17/388, train_loss: 0.1414, step time: 1.5345\n",
      "18/388, train_loss: 0.2786, step time: 1.5301\n",
      "19/388, train_loss: 0.0983, step time: 1.5349\n",
      "20/388, train_loss: 0.2220, step time: 1.5366\n",
      "21/388, train_loss: 0.1162, step time: 1.5333\n",
      "22/388, train_loss: 0.1024, step time: 1.5331\n",
      "23/388, train_loss: 0.2448, step time: 1.5337\n",
      "24/388, train_loss: 0.2060, step time: 1.5335\n",
      "25/388, train_loss: 0.1682, step time: 1.5353\n",
      "26/388, train_loss: 0.1103, step time: 1.5337\n",
      "27/388, train_loss: 0.1932, step time: 1.5353\n",
      "28/388, train_loss: 0.1243, step time: 1.5340\n",
      "29/388, train_loss: 0.3417, step time: 1.5347\n",
      "30/388, train_loss: 0.1525, step time: 1.5330\n",
      "31/388, train_loss: 0.1797, step time: 1.5362\n",
      "32/388, train_loss: 0.0938, step time: 1.5349\n",
      "33/388, train_loss: 0.3864, step time: 1.5361\n",
      "34/388, train_loss: 0.1329, step time: 1.5330\n",
      "35/388, train_loss: 0.0895, step time: 1.5329\n",
      "36/388, train_loss: 0.0853, step time: 1.5337\n",
      "37/388, train_loss: 0.0859, step time: 1.5307\n",
      "38/388, train_loss: 0.2196, step time: 1.5424\n",
      "39/388, train_loss: 0.3146, step time: 1.5347\n",
      "40/388, train_loss: 0.2958, step time: 1.5354\n",
      "41/388, train_loss: 0.2665, step time: 1.5353\n",
      "42/388, train_loss: 0.1112, step time: 1.5302\n",
      "43/388, train_loss: 0.1322, step time: 1.5353\n",
      "44/388, train_loss: 0.0972, step time: 1.5360\n",
      "45/388, train_loss: 0.2143, step time: 1.5397\n",
      "46/388, train_loss: 0.1383, step time: 1.5390\n",
      "47/388, train_loss: 0.2781, step time: 1.5355\n",
      "48/388, train_loss: 0.2811, step time: 1.5342\n",
      "49/388, train_loss: 0.1036, step time: 1.5337\n",
      "50/388, train_loss: 0.1654, step time: 1.5340\n",
      "51/388, train_loss: 0.1336, step time: 1.5393\n",
      "52/388, train_loss: 0.1349, step time: 1.5627\n",
      "53/388, train_loss: 0.4920, step time: 1.5312\n",
      "54/388, train_loss: 0.1890, step time: 1.5351\n",
      "55/388, train_loss: 0.2372, step time: 1.5395\n",
      "56/388, train_loss: 0.1083, step time: 1.5362\n",
      "57/388, train_loss: 0.2494, step time: 1.5397\n",
      "58/388, train_loss: 0.2866, step time: 1.5348\n",
      "59/388, train_loss: 0.3275, step time: 1.5342\n",
      "60/388, train_loss: 0.0886, step time: 1.5358\n",
      "61/388, train_loss: 0.1781, step time: 1.5332\n",
      "62/388, train_loss: 0.1293, step time: 1.5348\n",
      "63/388, train_loss: 0.1823, step time: 1.5337\n",
      "64/388, train_loss: 0.2164, step time: 1.5321\n",
      "65/388, train_loss: 0.2178, step time: 1.5340\n",
      "66/388, train_loss: 0.2913, step time: 1.5360\n",
      "67/388, train_loss: 0.0727, step time: 1.5393\n",
      "68/388, train_loss: 0.1251, step time: 1.5373\n",
      "69/388, train_loss: 0.1841, step time: 1.5359\n",
      "70/388, train_loss: 0.2165, step time: 1.5318\n",
      "71/388, train_loss: 0.0943, step time: 1.5331\n",
      "72/388, train_loss: 0.1184, step time: 1.5327\n",
      "73/388, train_loss: 0.4625, step time: 1.5375\n",
      "74/388, train_loss: 0.1808, step time: 1.5368\n",
      "75/388, train_loss: 0.3013, step time: 1.5323\n",
      "76/388, train_loss: 0.4857, step time: 1.5299\n",
      "77/388, train_loss: 0.4624, step time: 1.5314\n",
      "78/388, train_loss: 0.1311, step time: 1.5340\n",
      "79/388, train_loss: 0.2921, step time: 1.5338\n",
      "80/388, train_loss: 0.2646, step time: 1.5377\n",
      "81/388, train_loss: 0.1098, step time: 1.5368\n",
      "82/388, train_loss: 0.1358, step time: 1.5367\n",
      "83/388, train_loss: 0.1210, step time: 1.5368\n",
      "84/388, train_loss: 0.2916, step time: 1.5347\n",
      "85/388, train_loss: 0.1432, step time: 1.5331\n",
      "86/388, train_loss: 0.2277, step time: 1.5386\n",
      "87/388, train_loss: 0.1282, step time: 1.5350\n",
      "88/388, train_loss: 0.1128, step time: 1.5327\n",
      "89/388, train_loss: 0.1890, step time: 1.5341\n",
      "90/388, train_loss: 0.1822, step time: 1.5310\n",
      "91/388, train_loss: 0.0903, step time: 1.5328\n",
      "92/388, train_loss: 0.1413, step time: 1.5359\n",
      "93/388, train_loss: 0.0843, step time: 1.5347\n",
      "94/388, train_loss: 0.4358, step time: 1.5345\n",
      "95/388, train_loss: 0.1035, step time: 1.5296\n",
      "96/388, train_loss: 0.1175, step time: 1.5280\n",
      "97/388, train_loss: 0.0982, step time: 1.5315\n",
      "98/388, train_loss: 0.3135, step time: 1.5337\n",
      "99/388, train_loss: 0.1231, step time: 1.5371\n",
      "100/388, train_loss: 0.2138, step time: 1.5363\n",
      "101/388, train_loss: 0.0608, step time: 1.5322\n",
      "102/388, train_loss: 0.1103, step time: 1.5300\n",
      "103/388, train_loss: 0.1473, step time: 1.5325\n",
      "104/388, train_loss: 0.1124, step time: 1.5326\n",
      "105/388, train_loss: 0.0935, step time: 1.5322\n",
      "106/388, train_loss: 0.2644, step time: 1.5411\n",
      "107/388, train_loss: 0.1970, step time: 1.5361\n",
      "108/388, train_loss: 0.1788, step time: 1.5318\n",
      "109/388, train_loss: 0.1156, step time: 1.5341\n",
      "110/388, train_loss: 0.0505, step time: 1.5289\n",
      "111/388, train_loss: 0.0826, step time: 1.5310\n",
      "112/388, train_loss: 0.1447, step time: 1.5444\n",
      "113/388, train_loss: 0.1505, step time: 1.5395\n",
      "114/388, train_loss: 0.0523, step time: 1.5356\n",
      "115/388, train_loss: 0.0863, step time: 1.5335\n",
      "116/388, train_loss: 0.0671, step time: 1.5344\n",
      "117/388, train_loss: 0.0883, step time: 1.5300\n",
      "118/388, train_loss: 0.0725, step time: 1.5414\n",
      "119/388, train_loss: 0.1878, step time: 1.5351\n",
      "120/388, train_loss: 0.0946, step time: 1.5352\n",
      "121/388, train_loss: 0.0852, step time: 1.5373\n",
      "122/388, train_loss: 0.2236, step time: 1.5336\n",
      "123/388, train_loss: 0.2017, step time: 1.5309\n",
      "124/388, train_loss: 0.2625, step time: 1.5326\n",
      "125/388, train_loss: 0.3320, step time: 1.5368\n",
      "126/388, train_loss: 0.1209, step time: 1.5403\n",
      "127/388, train_loss: 0.3765, step time: 1.5376\n",
      "128/388, train_loss: 0.0796, step time: 1.5298\n",
      "129/388, train_loss: 0.2474, step time: 1.5346\n",
      "130/388, train_loss: 0.1487, step time: 1.5329\n",
      "131/388, train_loss: 0.1683, step time: 1.5355\n",
      "132/388, train_loss: 0.2747, step time: 1.5332\n",
      "133/388, train_loss: 0.1066, step time: 1.5364\n",
      "134/388, train_loss: 0.1071, step time: 1.5350\n",
      "135/388, train_loss: 0.0825, step time: 1.5337\n",
      "136/388, train_loss: 0.1868, step time: 1.5334\n",
      "137/388, train_loss: 0.2247, step time: 1.5345\n",
      "138/388, train_loss: 0.1606, step time: 1.5372\n",
      "139/388, train_loss: 0.3901, step time: 1.5360\n",
      "140/388, train_loss: 0.1874, step time: 1.5370\n",
      "141/388, train_loss: 0.0678, step time: 1.5327\n",
      "142/388, train_loss: 0.1102, step time: 1.5339\n",
      "143/388, train_loss: 0.1060, step time: 1.5337\n",
      "144/388, train_loss: 0.1114, step time: 1.5377\n",
      "145/388, train_loss: 0.5525, step time: 1.5364\n",
      "146/388, train_loss: 0.1105, step time: 1.5345\n",
      "147/388, train_loss: 0.2258, step time: 1.5335\n",
      "148/388, train_loss: 0.1048, step time: 1.5331\n",
      "149/388, train_loss: 0.0885, step time: 1.5334\n",
      "150/388, train_loss: 0.2109, step time: 1.5320\n",
      "151/388, train_loss: 0.1138, step time: 1.5325\n",
      "152/388, train_loss: 0.2809, step time: 1.5343\n",
      "153/388, train_loss: 0.0975, step time: 1.5438\n",
      "154/388, train_loss: 0.1809, step time: 1.5337\n",
      "155/388, train_loss: 0.2012, step time: 1.5314\n",
      "156/388, train_loss: 0.1483, step time: 1.5344\n",
      "157/388, train_loss: 0.1485, step time: 1.5466\n",
      "158/388, train_loss: 0.0679, step time: 1.5373\n",
      "159/388, train_loss: 0.1001, step time: 1.5348\n",
      "160/388, train_loss: 0.1614, step time: 1.5521\n",
      "161/388, train_loss: 0.0375, step time: 1.5438\n",
      "162/388, train_loss: 0.1282, step time: 1.5342\n",
      "163/388, train_loss: 0.1876, step time: 1.5333\n",
      "164/388, train_loss: 0.2600, step time: 1.5346\n",
      "165/388, train_loss: 0.0643, step time: 1.5339\n",
      "166/388, train_loss: 0.0856, step time: 1.5370\n",
      "167/388, train_loss: 0.0971, step time: 1.5357\n",
      "168/388, train_loss: 0.2684, step time: 1.5335\n",
      "169/388, train_loss: 0.2794, step time: 1.5316\n",
      "170/388, train_loss: 0.1558, step time: 1.5391\n",
      "171/388, train_loss: 0.2359, step time: 1.5370\n",
      "172/388, train_loss: 0.0458, step time: 1.5421\n",
      "173/388, train_loss: 0.1489, step time: 1.5334\n",
      "174/388, train_loss: 0.1320, step time: 1.5339\n",
      "175/388, train_loss: 0.4251, step time: 1.5353\n",
      "176/388, train_loss: 0.1184, step time: 1.5377\n",
      "177/388, train_loss: 0.1262, step time: 1.5322\n",
      "178/388, train_loss: 0.1324, step time: 1.5326\n",
      "179/388, train_loss: 0.1566, step time: 1.5330\n",
      "180/388, train_loss: 0.0933, step time: 1.5345\n",
      "181/388, train_loss: 0.2097, step time: 1.5368\n",
      "182/388, train_loss: 0.1753, step time: 1.5446\n",
      "183/388, train_loss: 0.0667, step time: 1.5324\n",
      "184/388, train_loss: 0.0650, step time: 1.5352\n",
      "185/388, train_loss: 0.1973, step time: 1.5317\n",
      "186/388, train_loss: 0.1872, step time: 1.5343\n",
      "187/388, train_loss: 0.0592, step time: 1.5371\n",
      "188/388, train_loss: 0.2162, step time: 1.5365\n",
      "189/388, train_loss: 0.2462, step time: 1.5305\n",
      "190/388, train_loss: 0.2027, step time: 1.5353\n",
      "191/388, train_loss: 0.3809, step time: 1.5321\n",
      "192/388, train_loss: 0.1037, step time: 1.5484\n",
      "193/388, train_loss: 0.2053, step time: 1.5358\n",
      "194/388, train_loss: 0.3734, step time: 1.5360\n",
      "195/388, train_loss: 0.1130, step time: 1.5341\n",
      "196/388, train_loss: 0.5380, step time: 1.5348\n",
      "197/388, train_loss: 0.1547, step time: 1.5366\n",
      "198/388, train_loss: 0.5212, step time: 1.5399\n",
      "199/388, train_loss: 0.0450, step time: 1.5351\n",
      "200/388, train_loss: 0.3755, step time: 1.5327\n",
      "201/388, train_loss: 0.0593, step time: 1.5500\n",
      "202/388, train_loss: 0.1928, step time: 1.5327\n",
      "203/388, train_loss: 0.2484, step time: 1.5345\n",
      "204/388, train_loss: 0.1801, step time: 1.5377\n",
      "205/388, train_loss: 0.2541, step time: 1.5374\n",
      "206/388, train_loss: 0.5570, step time: 1.5336\n",
      "207/388, train_loss: 0.2833, step time: 1.5325\n",
      "208/388, train_loss: 0.1354, step time: 1.5398\n",
      "209/388, train_loss: 0.2854, step time: 1.5360\n",
      "210/388, train_loss: 0.0962, step time: 1.5370\n",
      "211/388, train_loss: 0.4249, step time: 1.5399\n",
      "212/388, train_loss: 0.2345, step time: 1.5407\n",
      "213/388, train_loss: 0.2188, step time: 1.5310\n",
      "214/388, train_loss: 0.2376, step time: 1.5326\n",
      "215/388, train_loss: 0.2804, step time: 1.5405\n",
      "216/388, train_loss: 0.0593, step time: 1.5461\n",
      "217/388, train_loss: 0.0725, step time: 1.5328\n",
      "218/388, train_loss: 0.1116, step time: 1.5340\n",
      "219/388, train_loss: 0.2975, step time: 1.5331\n",
      "220/388, train_loss: 0.2591, step time: 1.5363\n",
      "221/388, train_loss: 0.1594, step time: 1.5361\n",
      "222/388, train_loss: 0.2053, step time: 1.5372\n",
      "223/388, train_loss: 0.0727, step time: 1.5322\n",
      "224/388, train_loss: 0.1449, step time: 1.5342\n",
      "225/388, train_loss: 0.2452, step time: 1.5369\n",
      "226/388, train_loss: 0.1428, step time: 1.5466\n",
      "227/388, train_loss: 0.1283, step time: 1.5351\n",
      "228/388, train_loss: 0.0835, step time: 1.5349\n",
      "229/388, train_loss: 0.1219, step time: 1.5351\n",
      "230/388, train_loss: 0.2219, step time: 1.5335\n",
      "231/388, train_loss: 0.1778, step time: 1.5371\n",
      "232/388, train_loss: 0.0989, step time: 1.5358\n",
      "233/388, train_loss: 0.0903, step time: 1.5327\n",
      "234/388, train_loss: 0.1674, step time: 1.5368\n",
      "235/388, train_loss: 0.0974, step time: 1.5390\n",
      "236/388, train_loss: 0.1962, step time: 1.5352\n",
      "237/388, train_loss: 0.5373, step time: 1.5309\n",
      "238/388, train_loss: 0.4007, step time: 1.5354\n",
      "239/388, train_loss: 0.1551, step time: 1.5339\n",
      "240/388, train_loss: 0.2581, step time: 1.5371\n",
      "241/388, train_loss: 0.0853, step time: 1.5377\n",
      "242/388, train_loss: 0.0859, step time: 1.5353\n",
      "243/388, train_loss: 0.3607, step time: 1.5342\n",
      "244/388, train_loss: 0.0957, step time: 1.5349\n",
      "245/388, train_loss: 0.2744, step time: 1.5324\n",
      "246/388, train_loss: 0.2515, step time: 1.5303\n",
      "247/388, train_loss: 0.2496, step time: 1.5344\n",
      "248/388, train_loss: 0.1487, step time: 1.5359\n",
      "249/388, train_loss: 0.4653, step time: 1.5347\n",
      "250/388, train_loss: 0.1639, step time: 1.5372\n",
      "251/388, train_loss: 0.2489, step time: 1.5318\n",
      "252/388, train_loss: 0.1639, step time: 1.5324\n",
      "253/388, train_loss: 0.2290, step time: 1.5330\n",
      "254/388, train_loss: 0.1923, step time: 1.5383\n",
      "255/388, train_loss: 0.4557, step time: 1.5383\n",
      "256/388, train_loss: 0.0863, step time: 1.5359\n",
      "257/388, train_loss: 0.0674, step time: 1.5346\n",
      "258/388, train_loss: 0.1694, step time: 1.5362\n",
      "259/388, train_loss: 0.2041, step time: 1.5364\n",
      "260/388, train_loss: 0.0859, step time: 1.5366\n",
      "261/388, train_loss: 0.3791, step time: 1.5326\n",
      "262/388, train_loss: 0.1550, step time: 1.5341\n",
      "263/388, train_loss: 0.2786, step time: 1.5345\n",
      "264/388, train_loss: 0.3128, step time: 1.5376\n",
      "265/388, train_loss: 0.0685, step time: 1.5394\n",
      "266/388, train_loss: 0.0880, step time: 1.5366\n",
      "267/388, train_loss: 0.1491, step time: 1.5329\n",
      "268/388, train_loss: 0.2442, step time: 1.5327\n",
      "269/388, train_loss: 0.3434, step time: 1.5336\n",
      "270/388, train_loss: 0.0992, step time: 1.5363\n",
      "271/388, train_loss: 0.1592, step time: 1.5336\n",
      "272/388, train_loss: 0.1000, step time: 1.5360\n",
      "273/388, train_loss: 0.2018, step time: 1.5357\n",
      "274/388, train_loss: 0.2177, step time: 1.5327\n",
      "275/388, train_loss: 0.2550, step time: 1.5333\n",
      "276/388, train_loss: 0.1210, step time: 1.5342\n",
      "277/388, train_loss: 0.1758, step time: 1.5373\n",
      "278/388, train_loss: 0.0620, step time: 1.5380\n",
      "279/388, train_loss: 0.0687, step time: 1.5301\n",
      "280/388, train_loss: 0.2434, step time: 1.5329\n",
      "281/388, train_loss: 0.1653, step time: 1.5339\n",
      "282/388, train_loss: 0.2039, step time: 1.5338\n",
      "283/388, train_loss: 0.1683, step time: 1.5346\n",
      "284/388, train_loss: 0.2734, step time: 1.5402\n",
      "285/388, train_loss: 0.1498, step time: 1.5332\n",
      "286/388, train_loss: 0.2966, step time: 1.5357\n",
      "287/388, train_loss: 0.3901, step time: 1.5354\n",
      "288/388, train_loss: 0.0966, step time: 1.5341\n",
      "289/388, train_loss: 0.1873, step time: 1.5355\n",
      "290/388, train_loss: 0.0855, step time: 1.5363\n",
      "291/388, train_loss: 0.0758, step time: 1.5342\n",
      "292/388, train_loss: 0.1020, step time: 1.5330\n",
      "293/388, train_loss: 0.4722, step time: 1.5305\n",
      "294/388, train_loss: 0.2400, step time: 1.5329\n",
      "295/388, train_loss: 0.0816, step time: 1.5362\n",
      "296/388, train_loss: 0.2049, step time: 1.5358\n",
      "297/388, train_loss: 0.1371, step time: 1.5309\n",
      "298/388, train_loss: 0.2864, step time: 1.5314\n",
      "299/388, train_loss: 0.1655, step time: 1.5344\n",
      "300/388, train_loss: 0.1620, step time: 1.5371\n",
      "301/388, train_loss: 0.1222, step time: 1.5486\n",
      "302/388, train_loss: 0.0937, step time: 1.5354\n",
      "303/388, train_loss: 0.1638, step time: 1.5368\n",
      "304/388, train_loss: 0.0863, step time: 1.5370\n",
      "305/388, train_loss: 0.1312, step time: 1.5345\n",
      "306/388, train_loss: 0.1102, step time: 1.5310\n",
      "307/388, train_loss: 0.2061, step time: 1.5343\n",
      "308/388, train_loss: 0.0927, step time: 1.5605\n",
      "309/388, train_loss: 0.2336, step time: 1.5326\n",
      "310/388, train_loss: 0.1645, step time: 1.5311\n",
      "311/388, train_loss: 0.1931, step time: 1.5327\n",
      "312/388, train_loss: 0.1098, step time: 1.5369\n",
      "313/388, train_loss: 0.1845, step time: 1.5362\n",
      "314/388, train_loss: 0.1210, step time: 1.5377\n",
      "315/388, train_loss: 0.1077, step time: 1.5352\n",
      "316/388, train_loss: 0.2139, step time: 1.5325\n",
      "317/388, train_loss: 0.2203, step time: 1.5357\n",
      "318/388, train_loss: 0.0706, step time: 1.5415\n",
      "319/388, train_loss: 0.2096, step time: 1.5360\n",
      "320/388, train_loss: 0.1366, step time: 1.5310\n",
      "321/388, train_loss: 0.1345, step time: 1.5368\n",
      "322/388, train_loss: 0.0877, step time: 1.5320\n",
      "323/388, train_loss: 0.1117, step time: 1.5326\n",
      "324/388, train_loss: 0.3595, step time: 1.5381\n",
      "325/388, train_loss: 0.1108, step time: 1.5377\n",
      "326/388, train_loss: 0.5451, step time: 1.5361\n",
      "327/388, train_loss: 0.1487, step time: 1.5342\n",
      "328/388, train_loss: 0.2500, step time: 1.5336\n",
      "329/388, train_loss: 0.1230, step time: 1.5526\n",
      "330/388, train_loss: 0.0938, step time: 1.5380\n",
      "331/388, train_loss: 0.2565, step time: 1.5361\n",
      "332/388, train_loss: 0.1031, step time: 1.5328\n",
      "333/388, train_loss: 0.0902, step time: 1.5368\n",
      "334/388, train_loss: 0.2351, step time: 1.5345\n",
      "335/388, train_loss: 0.2661, step time: 1.5385\n",
      "336/388, train_loss: 0.1376, step time: 1.5350\n",
      "337/388, train_loss: 0.0623, step time: 1.5348\n",
      "338/388, train_loss: 0.2336, step time: 1.5314\n",
      "339/388, train_loss: 0.1059, step time: 1.5328\n",
      "340/388, train_loss: 0.2585, step time: 1.5328\n",
      "341/388, train_loss: 0.1389, step time: 1.5361\n",
      "342/388, train_loss: 0.1035, step time: 1.5359\n",
      "343/388, train_loss: 0.0626, step time: 1.5355\n",
      "344/388, train_loss: 0.3842, step time: 1.5442\n",
      "345/388, train_loss: 0.3483, step time: 1.5567\n",
      "346/388, train_loss: 0.0572, step time: 1.5349\n",
      "347/388, train_loss: 0.1923, step time: 1.5364\n",
      "348/388, train_loss: 0.1522, step time: 1.5347\n",
      "349/388, train_loss: 0.0879, step time: 1.5295\n",
      "350/388, train_loss: 0.3020, step time: 1.5346\n",
      "351/388, train_loss: 0.0608, step time: 1.5466\n",
      "352/388, train_loss: 0.2368, step time: 1.5375\n",
      "353/388, train_loss: 0.6077, step time: 1.5362\n",
      "354/388, train_loss: 0.2520, step time: 1.5360\n",
      "355/388, train_loss: 0.1782, step time: 1.5453\n",
      "356/388, train_loss: 0.3366, step time: 1.5431\n",
      "357/388, train_loss: 0.1344, step time: 1.5366\n",
      "358/388, train_loss: 0.0283, step time: 1.5334\n",
      "359/388, train_loss: 0.0921, step time: 1.5384\n",
      "360/388, train_loss: 0.1507, step time: 1.5366\n",
      "361/388, train_loss: 0.1692, step time: 1.5335\n",
      "362/388, train_loss: 0.1913, step time: 1.5326\n",
      "363/388, train_loss: 0.1901, step time: 1.5349\n",
      "364/388, train_loss: 0.0342, step time: 1.5361\n",
      "365/388, train_loss: 0.0815, step time: 1.5361\n",
      "366/388, train_loss: 0.1742, step time: 1.5348\n",
      "367/388, train_loss: 0.3729, step time: 1.5352\n",
      "368/388, train_loss: 0.2028, step time: 1.5343\n",
      "369/388, train_loss: 0.0945, step time: 1.5382\n",
      "370/388, train_loss: 0.1930, step time: 1.5328\n",
      "371/388, train_loss: 0.1771, step time: 1.5328\n",
      "372/388, train_loss: 0.3838, step time: 1.5348\n",
      "373/388, train_loss: 0.1339, step time: 1.5356\n",
      "374/388, train_loss: 0.0651, step time: 1.5362\n",
      "375/388, train_loss: 0.1284, step time: 1.5334\n",
      "376/388, train_loss: 0.1608, step time: 1.5338\n",
      "377/388, train_loss: 0.1486, step time: 1.5295\n",
      "378/388, train_loss: 0.1261, step time: 1.5304\n",
      "379/388, train_loss: 0.2087, step time: 1.5587\n",
      "380/388, train_loss: 0.1547, step time: 1.5334\n",
      "381/388, train_loss: 0.2133, step time: 1.5318\n",
      "382/388, train_loss: 0.0477, step time: 1.5314\n",
      "383/388, train_loss: 0.2004, step time: 1.5345\n",
      "384/388, train_loss: 0.1711, step time: 1.5389\n",
      "385/388, train_loss: 0.1070, step time: 1.5360\n",
      "386/388, train_loss: 0.1213, step time: 1.5345\n",
      "387/388, train_loss: 0.1820, step time: 1.5302\n",
      "388/388, train_loss: 0.2444, step time: 1.5328\n",
      "epoch 49 average loss: 0.1847\n",
      "saved new best metric model\n",
      "current epoch: 49 current mean dice: 0.7692 tc: 0.8151 wt: 0.9024 et: 0.5900\n",
      "best mean dice: 0.7692 at epoch: 49\n",
      "time consuming of epoch 49 is: 702.8121\n",
      "----------\n",
      "epoch 50/100\n",
      "1/388, train_loss: 0.2911, step time: 1.5441\n",
      "2/388, train_loss: 0.1274, step time: 1.5371\n",
      "3/388, train_loss: 0.2148, step time: 1.5336\n",
      "4/388, train_loss: 0.1304, step time: 1.5367\n",
      "5/388, train_loss: 0.3504, step time: 1.5319\n",
      "6/388, train_loss: 0.2168, step time: 1.5343\n",
      "7/388, train_loss: 0.2406, step time: 1.5370\n",
      "8/388, train_loss: 0.0996, step time: 1.5346\n",
      "9/388, train_loss: 0.0853, step time: 1.5401\n",
      "10/388, train_loss: 0.0763, step time: 1.5350\n",
      "11/388, train_loss: 0.1717, step time: 1.5344\n",
      "12/388, train_loss: 0.0675, step time: 1.5319\n",
      "13/388, train_loss: 0.0747, step time: 1.5356\n",
      "14/388, train_loss: 0.0270, step time: 1.5368\n",
      "15/388, train_loss: 0.1043, step time: 1.5378\n",
      "16/388, train_loss: 0.2643, step time: 1.5386\n",
      "17/388, train_loss: 0.0761, step time: 1.5360\n",
      "18/388, train_loss: 0.3174, step time: 1.5336\n",
      "19/388, train_loss: 0.3494, step time: 1.5344\n",
      "20/388, train_loss: 0.1087, step time: 1.5355\n",
      "21/388, train_loss: 0.2410, step time: 1.5373\n",
      "22/388, train_loss: 0.1914, step time: 1.5369\n",
      "23/388, train_loss: 0.2955, step time: 1.5356\n",
      "24/388, train_loss: 0.1663, step time: 1.5343\n",
      "25/388, train_loss: 0.1358, step time: 1.5296\n",
      "26/388, train_loss: 0.1498, step time: 1.5379\n",
      "27/388, train_loss: 0.0824, step time: 1.5348\n",
      "28/388, train_loss: 0.1322, step time: 1.5369\n",
      "29/388, train_loss: 0.0974, step time: 1.5346\n",
      "30/388, train_loss: 0.1908, step time: 1.5343\n",
      "31/388, train_loss: 0.1410, step time: 1.5306\n",
      "32/388, train_loss: 0.0703, step time: 1.5325\n",
      "33/388, train_loss: 0.4861, step time: 1.5364\n",
      "34/388, train_loss: 0.2574, step time: 1.5356\n",
      "35/388, train_loss: 0.1140, step time: 1.5424\n",
      "36/388, train_loss: 0.1234, step time: 1.5327\n",
      "37/388, train_loss: 0.0893, step time: 1.5303\n",
      "38/388, train_loss: 0.1964, step time: 1.5366\n",
      "39/388, train_loss: 0.1140, step time: 1.5356\n",
      "40/388, train_loss: 0.1352, step time: 1.5374\n",
      "41/388, train_loss: 0.1232, step time: 1.5351\n",
      "42/388, train_loss: 0.0702, step time: 1.5338\n",
      "43/388, train_loss: 0.2106, step time: 1.5324\n",
      "44/388, train_loss: 0.0945, step time: 1.5325\n",
      "45/388, train_loss: 0.0689, step time: 1.5334\n",
      "46/388, train_loss: 0.0951, step time: 1.5413\n",
      "47/388, train_loss: 0.0427, step time: 1.5354\n",
      "48/388, train_loss: 0.0720, step time: 1.5372\n",
      "49/388, train_loss: 0.1408, step time: 1.5313\n",
      "50/388, train_loss: 0.2149, step time: 1.5340\n",
      "51/388, train_loss: 0.2362, step time: 1.5323\n",
      "52/388, train_loss: 0.0865, step time: 1.5377\n",
      "53/388, train_loss: 0.2045, step time: 1.5370\n",
      "54/388, train_loss: 0.2913, step time: 1.5312\n",
      "55/388, train_loss: 0.4984, step time: 1.5326\n",
      "56/388, train_loss: 0.1385, step time: 1.5303\n",
      "57/388, train_loss: 0.1293, step time: 1.5316\n",
      "58/388, train_loss: 0.3020, step time: 1.5355\n",
      "59/388, train_loss: 0.1963, step time: 1.5360\n",
      "60/388, train_loss: 0.0807, step time: 1.5381\n",
      "61/388, train_loss: 0.2773, step time: 1.5330\n",
      "62/388, train_loss: 0.2533, step time: 1.5363\n",
      "63/388, train_loss: 0.1728, step time: 1.5321\n",
      "64/388, train_loss: 0.1341, step time: 1.5334\n",
      "65/388, train_loss: 0.1208, step time: 1.5368\n",
      "66/388, train_loss: 0.3894, step time: 1.5377\n",
      "67/388, train_loss: 0.0949, step time: 1.5319\n",
      "68/388, train_loss: 0.1590, step time: 1.5322\n",
      "69/388, train_loss: 0.1269, step time: 1.5296\n",
      "70/388, train_loss: 0.2262, step time: 1.5370\n",
      "71/388, train_loss: 0.1091, step time: 1.5368\n",
      "72/388, train_loss: 0.1368, step time: 1.5337\n",
      "73/388, train_loss: 0.2869, step time: 1.5332\n",
      "74/388, train_loss: 0.0547, step time: 1.5328\n",
      "75/388, train_loss: 0.1987, step time: 1.5320\n",
      "76/388, train_loss: 0.1542, step time: 1.5337\n",
      "77/388, train_loss: 0.1563, step time: 1.5340\n",
      "78/388, train_loss: 0.1190, step time: 1.5307\n",
      "79/388, train_loss: 0.1399, step time: 1.5366\n",
      "80/388, train_loss: 0.1624, step time: 1.5348\n",
      "81/388, train_loss: 0.3305, step time: 1.5375\n",
      "82/388, train_loss: 0.1911, step time: 1.5372\n",
      "83/388, train_loss: 0.2483, step time: 1.5382\n",
      "84/388, train_loss: 0.1309, step time: 1.5378\n",
      "85/388, train_loss: 0.0924, step time: 1.5337\n",
      "86/388, train_loss: 0.0938, step time: 1.5378\n",
      "87/388, train_loss: 0.3324, step time: 1.5359\n",
      "88/388, train_loss: 0.3252, step time: 1.5330\n",
      "89/388, train_loss: 0.1131, step time: 1.5337\n",
      "90/388, train_loss: 0.1772, step time: 1.5330\n",
      "91/388, train_loss: 0.1702, step time: 1.5336\n",
      "92/388, train_loss: 0.1334, step time: 1.5375\n",
      "93/388, train_loss: 0.2974, step time: 1.5342\n",
      "94/388, train_loss: 0.3723, step time: 1.5358\n",
      "95/388, train_loss: 0.1058, step time: 1.5313\n",
      "96/388, train_loss: 0.1330, step time: 1.5336\n",
      "97/388, train_loss: 0.2210, step time: 1.5308\n",
      "98/388, train_loss: 0.0757, step time: 1.5334\n",
      "99/388, train_loss: 0.2552, step time: 1.5370\n",
      "100/388, train_loss: 0.1016, step time: 1.5353\n",
      "101/388, train_loss: 0.2112, step time: 1.5335\n",
      "102/388, train_loss: 0.1509, step time: 1.5343\n",
      "103/388, train_loss: 0.0595, step time: 1.5358\n",
      "104/388, train_loss: 0.2055, step time: 1.5342\n",
      "105/388, train_loss: 0.2194, step time: 1.5365\n",
      "106/388, train_loss: 0.1783, step time: 1.5364\n",
      "107/388, train_loss: 0.1167, step time: 1.5381\n",
      "108/388, train_loss: 0.1352, step time: 1.5340\n",
      "109/388, train_loss: 0.2324, step time: 1.5323\n",
      "110/388, train_loss: 0.2240, step time: 1.5327\n",
      "111/388, train_loss: 0.2059, step time: 1.5332\n",
      "112/388, train_loss: 0.2087, step time: 1.5348\n",
      "113/388, train_loss: 0.1831, step time: 1.5357\n",
      "114/388, train_loss: 0.1451, step time: 1.5380\n",
      "115/388, train_loss: 0.1538, step time: 1.5328\n",
      "116/388, train_loss: 0.1273, step time: 1.5345\n",
      "117/388, train_loss: 0.0851, step time: 1.5326\n",
      "118/388, train_loss: 0.1739, step time: 1.5369\n",
      "119/388, train_loss: 0.1364, step time: 1.5372\n",
      "120/388, train_loss: 0.4236, step time: 1.5346\n",
      "121/388, train_loss: 0.0751, step time: 1.5338\n",
      "122/388, train_loss: 0.0928, step time: 1.5325\n",
      "123/388, train_loss: 0.1326, step time: 1.5378\n",
      "124/388, train_loss: 0.1745, step time: 1.5360\n",
      "125/388, train_loss: 0.1542, step time: 1.5344\n",
      "126/388, train_loss: 0.2108, step time: 1.5334\n",
      "127/388, train_loss: 0.1562, step time: 1.5313\n",
      "128/388, train_loss: 0.2192, step time: 1.5328\n",
      "129/388, train_loss: 0.0539, step time: 1.5312\n",
      "130/388, train_loss: 0.2705, step time: 1.5372\n",
      "131/388, train_loss: 0.3487, step time: 1.5352\n",
      "132/388, train_loss: 0.0671, step time: 1.5339\n",
      "133/388, train_loss: 0.3355, step time: 1.5331\n",
      "134/388, train_loss: 0.1173, step time: 1.5325\n",
      "135/388, train_loss: 0.0853, step time: 1.5330\n",
      "136/388, train_loss: 0.1976, step time: 1.5356\n",
      "137/388, train_loss: 0.1721, step time: 1.5375\n",
      "138/388, train_loss: 0.3099, step time: 1.5351\n",
      "139/388, train_loss: 0.1370, step time: 1.5294\n",
      "140/388, train_loss: 0.1707, step time: 1.5319\n",
      "141/388, train_loss: 0.1233, step time: 1.5327\n",
      "142/388, train_loss: 0.1166, step time: 1.5431\n",
      "143/388, train_loss: 0.1358, step time: 1.5348\n",
      "144/388, train_loss: 0.4767, step time: 1.5347\n",
      "145/388, train_loss: 0.2337, step time: 1.5323\n",
      "146/388, train_loss: 0.2992, step time: 1.5329\n",
      "147/388, train_loss: 0.0912, step time: 1.5328\n",
      "148/388, train_loss: 0.1730, step time: 1.5382\n",
      "149/388, train_loss: 0.6034, step time: 1.5385\n",
      "150/388, train_loss: 0.1357, step time: 1.5377\n",
      "151/388, train_loss: 0.1925, step time: 1.5340\n",
      "152/388, train_loss: 0.2945, step time: 1.5292\n",
      "153/388, train_loss: 0.0905, step time: 1.5304\n",
      "154/388, train_loss: 0.1848, step time: 1.5354\n",
      "155/388, train_loss: 0.1461, step time: 1.5685\n",
      "156/388, train_loss: 0.0963, step time: 1.5345\n",
      "157/388, train_loss: 0.1068, step time: 1.5390\n",
      "158/388, train_loss: 0.1208, step time: 1.5339\n",
      "159/388, train_loss: 0.1632, step time: 1.5334\n",
      "160/388, train_loss: 0.1133, step time: 1.5329\n",
      "161/388, train_loss: 0.2683, step time: 1.5323\n",
      "162/388, train_loss: 0.0528, step time: 1.5369\n",
      "163/388, train_loss: 0.2280, step time: 1.5380\n",
      "164/388, train_loss: 0.0856, step time: 1.5372\n",
      "165/388, train_loss: 0.0358, step time: 1.5313\n",
      "166/388, train_loss: 0.2540, step time: 1.5334\n",
      "167/388, train_loss: 0.5208, step time: 1.5305\n",
      "168/388, train_loss: 0.0502, step time: 1.5289\n",
      "169/388, train_loss: 0.1047, step time: 1.5306\n",
      "170/388, train_loss: 0.1646, step time: 1.5379\n",
      "171/388, train_loss: 0.2706, step time: 1.5354\n",
      "172/388, train_loss: 0.4177, step time: 1.5319\n",
      "173/388, train_loss: 0.2180, step time: 1.5378\n",
      "174/388, train_loss: 0.2028, step time: 1.5411\n",
      "175/388, train_loss: 0.1822, step time: 1.5358\n",
      "176/388, train_loss: 0.3787, step time: 1.5344\n",
      "177/388, train_loss: 0.2232, step time: 1.5327\n",
      "178/388, train_loss: 0.0672, step time: 1.5386\n",
      "179/388, train_loss: 0.0590, step time: 1.5328\n",
      "180/388, train_loss: 0.1520, step time: 1.5406\n",
      "181/388, train_loss: 0.2809, step time: 1.5335\n",
      "182/388, train_loss: 0.4155, step time: 1.5372\n",
      "183/388, train_loss: 0.2869, step time: 1.5357\n",
      "184/388, train_loss: 0.1373, step time: 1.5333\n",
      "185/388, train_loss: 0.2250, step time: 1.5325\n",
      "186/388, train_loss: 0.1038, step time: 1.5342\n",
      "187/388, train_loss: 0.3693, step time: 1.5362\n",
      "188/388, train_loss: 0.0944, step time: 1.5377\n",
      "189/388, train_loss: 0.2828, step time: 1.5326\n",
      "190/388, train_loss: 0.1747, step time: 1.5335\n",
      "191/388, train_loss: 0.1462, step time: 1.5320\n",
      "192/388, train_loss: 0.4185, step time: 1.5379\n",
      "193/388, train_loss: 0.1179, step time: 1.5341\n",
      "194/388, train_loss: 0.2097, step time: 1.5360\n",
      "195/388, train_loss: 0.2143, step time: 1.5334\n",
      "196/388, train_loss: 0.2303, step time: 1.5364\n",
      "197/388, train_loss: 0.2803, step time: 1.5345\n",
      "198/388, train_loss: 0.0786, step time: 1.5330\n",
      "199/388, train_loss: 0.3401, step time: 1.5366\n",
      "200/388, train_loss: 0.1436, step time: 1.5327\n",
      "201/388, train_loss: 0.0921, step time: 1.5328\n",
      "202/388, train_loss: 0.1151, step time: 1.5308\n",
      "203/388, train_loss: 0.2371, step time: 1.5330\n",
      "204/388, train_loss: 0.1049, step time: 1.5355\n",
      "205/388, train_loss: 0.0794, step time: 1.5401\n",
      "206/388, train_loss: 0.1356, step time: 1.5318\n",
      "207/388, train_loss: 0.0884, step time: 1.5320\n",
      "208/388, train_loss: 0.1513, step time: 1.5318\n",
      "209/388, train_loss: 0.4960, step time: 1.5341\n",
      "210/388, train_loss: 0.1603, step time: 1.5359\n",
      "211/388, train_loss: 0.3138, step time: 1.5353\n",
      "212/388, train_loss: 0.1042, step time: 1.5359\n",
      "213/388, train_loss: 0.3210, step time: 1.5333\n",
      "214/388, train_loss: 0.3763, step time: 1.5315\n",
      "215/388, train_loss: 0.1488, step time: 1.5293\n",
      "216/388, train_loss: 0.0937, step time: 1.5323\n",
      "217/388, train_loss: 0.0674, step time: 1.5409\n",
      "218/388, train_loss: 0.2712, step time: 1.5362\n",
      "219/388, train_loss: 0.0985, step time: 1.5340\n",
      "220/388, train_loss: 0.2026, step time: 1.5303\n",
      "221/388, train_loss: 0.0743, step time: 1.5335\n",
      "222/388, train_loss: 0.2564, step time: 1.5344\n",
      "223/388, train_loss: 0.1909, step time: 1.5343\n",
      "224/388, train_loss: 0.0610, step time: 1.5370\n",
      "225/388, train_loss: 0.2008, step time: 1.5352\n",
      "226/388, train_loss: 0.0922, step time: 1.5341\n",
      "227/388, train_loss: 0.0572, step time: 1.5314\n",
      "228/388, train_loss: 0.2195, step time: 1.5351\n",
      "229/388, train_loss: 0.0908, step time: 1.5360\n",
      "230/388, train_loss: 0.0891, step time: 1.5370\n",
      "231/388, train_loss: 0.1703, step time: 1.5384\n",
      "232/388, train_loss: 0.1977, step time: 1.5339\n",
      "233/388, train_loss: 0.0871, step time: 1.5358\n",
      "234/388, train_loss: 0.2085, step time: 1.5352\n",
      "235/388, train_loss: 0.0609, step time: 1.5357\n",
      "236/388, train_loss: 0.1977, step time: 1.5362\n",
      "237/388, train_loss: 0.1159, step time: 1.5360\n",
      "238/388, train_loss: 0.1283, step time: 1.5331\n",
      "239/388, train_loss: 0.2813, step time: 1.5315\n",
      "240/388, train_loss: 0.0859, step time: 1.5354\n",
      "241/388, train_loss: 0.0863, step time: 1.5316\n",
      "242/388, train_loss: 0.1812, step time: 1.5351\n",
      "243/388, train_loss: 0.1147, step time: 1.5360\n",
      "244/388, train_loss: 0.0545, step time: 1.5368\n",
      "245/388, train_loss: 0.1235, step time: 1.5338\n",
      "246/388, train_loss: 0.3396, step time: 1.5328\n",
      "247/388, train_loss: 0.0667, step time: 1.5321\n",
      "248/388, train_loss: 0.2163, step time: 1.5521\n",
      "249/388, train_loss: 0.1755, step time: 1.5329\n",
      "250/388, train_loss: 0.0902, step time: 1.5338\n",
      "251/388, train_loss: 0.1870, step time: 1.5307\n",
      "252/388, train_loss: 0.1132, step time: 1.5393\n",
      "253/388, train_loss: 0.1916, step time: 1.5368\n",
      "254/388, train_loss: 0.2103, step time: 1.5328\n",
      "255/388, train_loss: 0.1207, step time: 1.5337\n",
      "256/388, train_loss: 0.0374, step time: 1.5319\n",
      "257/388, train_loss: 0.0956, step time: 1.5352\n",
      "258/388, train_loss: 0.2399, step time: 1.5378\n",
      "259/388, train_loss: 0.1572, step time: 1.5378\n",
      "260/388, train_loss: 0.2108, step time: 1.5357\n",
      "261/388, train_loss: 0.0902, step time: 1.5348\n",
      "262/388, train_loss: 0.0888, step time: 1.5359\n",
      "263/388, train_loss: 0.0924, step time: 1.5361\n",
      "264/388, train_loss: 0.1078, step time: 1.5380\n",
      "265/388, train_loss: 0.2646, step time: 1.5347\n",
      "266/388, train_loss: 0.5251, step time: 1.5311\n",
      "267/388, train_loss: 0.0978, step time: 1.5346\n",
      "268/388, train_loss: 0.2532, step time: 1.5337\n",
      "269/388, train_loss: 0.1104, step time: 1.5318\n",
      "270/388, train_loss: 0.0751, step time: 1.5354\n",
      "271/388, train_loss: 0.1067, step time: 1.5370\n",
      "272/388, train_loss: 0.1055, step time: 1.5358\n",
      "273/388, train_loss: 0.0538, step time: 1.5318\n",
      "274/388, train_loss: 0.2975, step time: 1.5353\n",
      "275/388, train_loss: 0.1269, step time: 1.5351\n",
      "276/388, train_loss: 0.1442, step time: 1.5347\n",
      "277/388, train_loss: 0.0687, step time: 1.5366\n",
      "278/388, train_loss: 0.3062, step time: 1.5342\n",
      "279/388, train_loss: 0.2661, step time: 1.5317\n",
      "280/388, train_loss: 0.2272, step time: 1.5313\n",
      "281/388, train_loss: 0.1768, step time: 1.5346\n",
      "282/388, train_loss: 0.1474, step time: 1.5317\n",
      "283/388, train_loss: 0.3537, step time: 1.5349\n",
      "284/388, train_loss: 0.2098, step time: 1.5355\n",
      "285/388, train_loss: 0.2688, step time: 1.5374\n",
      "286/388, train_loss: 0.2057, step time: 1.5388\n",
      "287/388, train_loss: 0.1416, step time: 1.5309\n",
      "288/388, train_loss: 0.2383, step time: 1.5345\n",
      "289/388, train_loss: 0.1660, step time: 1.5370\n",
      "290/388, train_loss: 0.3438, step time: 1.5337\n",
      "291/388, train_loss: 0.1492, step time: 1.5348\n",
      "292/388, train_loss: 0.2166, step time: 1.5317\n",
      "293/388, train_loss: 0.2098, step time: 1.5315\n",
      "294/388, train_loss: 0.2574, step time: 1.5323\n",
      "295/388, train_loss: 0.1590, step time: 1.5329\n",
      "296/388, train_loss: 0.2211, step time: 1.5340\n",
      "297/388, train_loss: 0.2091, step time: 1.5343\n",
      "298/388, train_loss: 0.1784, step time: 1.5339\n",
      "299/388, train_loss: 0.1104, step time: 1.5312\n",
      "300/388, train_loss: 0.2609, step time: 1.5473\n",
      "301/388, train_loss: 0.0669, step time: 1.5398\n",
      "302/388, train_loss: 0.3551, step time: 1.5379\n",
      "303/388, train_loss: 0.5636, step time: 1.5339\n",
      "304/388, train_loss: 0.2262, step time: 1.5338\n",
      "305/388, train_loss: 0.1337, step time: 1.5336\n",
      "306/388, train_loss: 0.2820, step time: 1.5365\n",
      "307/388, train_loss: 0.2714, step time: 1.5328\n",
      "308/388, train_loss: 0.1573, step time: 1.5347\n",
      "309/388, train_loss: 0.2037, step time: 1.5349\n",
      "310/388, train_loss: 0.1108, step time: 1.5325\n",
      "311/388, train_loss: 0.1686, step time: 1.5325\n",
      "312/388, train_loss: 0.4842, step time: 1.5346\n",
      "313/388, train_loss: 0.0547, step time: 1.5340\n",
      "314/388, train_loss: 0.1098, step time: 1.5358\n",
      "315/388, train_loss: 0.1265, step time: 1.5345\n",
      "316/388, train_loss: 0.6636, step time: 1.5353\n",
      "317/388, train_loss: 0.0977, step time: 1.5361\n",
      "318/388, train_loss: 0.0928, step time: 1.5359\n",
      "319/388, train_loss: 0.0543, step time: 1.5369\n",
      "320/388, train_loss: 0.1500, step time: 1.5416\n",
      "321/388, train_loss: 0.1013, step time: 1.5339\n",
      "322/388, train_loss: 0.2106, step time: 1.5344\n",
      "323/388, train_loss: 0.2303, step time: 1.5315\n",
      "324/388, train_loss: 0.2054, step time: 1.5425\n",
      "325/388, train_loss: 0.2131, step time: 1.5344\n",
      "326/388, train_loss: 0.2844, step time: 1.5396\n",
      "327/388, train_loss: 0.0356, step time: 1.5335\n",
      "328/388, train_loss: 0.2515, step time: 1.5337\n",
      "329/388, train_loss: 0.2101, step time: 1.5659\n",
      "330/388, train_loss: 0.5425, step time: 1.5358\n",
      "331/388, train_loss: 0.0810, step time: 1.5325\n",
      "332/388, train_loss: 0.3798, step time: 1.5358\n",
      "333/388, train_loss: 0.1781, step time: 1.5345\n",
      "334/388, train_loss: 0.2507, step time: 1.5360\n",
      "335/388, train_loss: 0.1723, step time: 1.5311\n",
      "336/388, train_loss: 0.1706, step time: 1.5336\n",
      "337/388, train_loss: 0.1214, step time: 1.5355\n",
      "338/388, train_loss: 0.0998, step time: 1.5393\n",
      "339/388, train_loss: 0.1454, step time: 1.5392\n",
      "340/388, train_loss: 0.1661, step time: 1.5452\n",
      "341/388, train_loss: 0.1394, step time: 1.5331\n",
      "342/388, train_loss: 0.1396, step time: 1.5355\n",
      "343/388, train_loss: 0.2763, step time: 1.5333\n",
      "344/388, train_loss: 0.2641, step time: 1.5401\n",
      "345/388, train_loss: 0.1459, step time: 1.5370\n",
      "346/388, train_loss: 0.0565, step time: 1.5348\n",
      "347/388, train_loss: 0.1373, step time: 1.5316\n",
      "348/388, train_loss: 0.1364, step time: 1.5330\n",
      "349/388, train_loss: 0.0938, step time: 1.5381\n",
      "350/388, train_loss: 0.1057, step time: 1.5379\n",
      "351/388, train_loss: 0.1729, step time: 1.5351\n",
      "352/388, train_loss: 0.3016, step time: 1.5352\n",
      "353/388, train_loss: 0.3074, step time: 1.5321\n",
      "354/388, train_loss: 0.1002, step time: 1.5352\n",
      "355/388, train_loss: 0.2784, step time: 1.5341\n",
      "356/388, train_loss: 0.3472, step time: 1.5361\n",
      "357/388, train_loss: 0.4242, step time: 1.5387\n",
      "358/388, train_loss: 0.0775, step time: 1.5335\n",
      "359/388, train_loss: 0.1049, step time: 1.5333\n",
      "360/388, train_loss: 0.2604, step time: 1.5353\n",
      "361/388, train_loss: 0.1208, step time: 1.5352\n",
      "362/388, train_loss: 0.2616, step time: 1.5371\n",
      "363/388, train_loss: 0.0565, step time: 1.5382\n",
      "364/388, train_loss: 0.2477, step time: 1.5320\n",
      "365/388, train_loss: 0.2254, step time: 1.5346\n",
      "366/388, train_loss: 0.1933, step time: 1.5337\n",
      "367/388, train_loss: 0.1017, step time: 1.5328\n",
      "368/388, train_loss: 0.1979, step time: 1.5362\n",
      "369/388, train_loss: 0.1515, step time: 1.5362\n",
      "370/388, train_loss: 0.2211, step time: 1.5366\n",
      "371/388, train_loss: 0.3053, step time: 1.5323\n",
      "372/388, train_loss: 0.1110, step time: 1.5330\n",
      "373/388, train_loss: 0.0991, step time: 1.5348\n",
      "374/388, train_loss: 0.0615, step time: 1.5374\n",
      "375/388, train_loss: 0.0697, step time: 1.5334\n",
      "376/388, train_loss: 0.1999, step time: 1.5360\n",
      "377/388, train_loss: 0.2849, step time: 1.5323\n",
      "378/388, train_loss: 0.0938, step time: 1.5313\n",
      "379/388, train_loss: 0.1187, step time: 1.5316\n",
      "380/388, train_loss: 0.1643, step time: 1.5341\n",
      "381/388, train_loss: 0.4458, step time: 1.5370\n",
      "382/388, train_loss: 0.1190, step time: 1.5343\n",
      "383/388, train_loss: 0.2253, step time: 1.5357\n",
      "384/388, train_loss: 0.5612, step time: 1.5325\n",
      "385/388, train_loss: 0.4664, step time: 1.5318\n",
      "386/388, train_loss: 0.1049, step time: 1.5338\n",
      "387/388, train_loss: 0.4716, step time: 1.5317\n",
      "388/388, train_loss: 0.0741, step time: 1.5378\n",
      "epoch 50 average loss: 0.1859\n",
      "current epoch: 50 current mean dice: 0.7660 tc: 0.8159 wt: 0.8980 et: 0.5841\n",
      "best mean dice: 0.7692 at epoch: 49\n",
      "time consuming of epoch 50 is: 703.1941\n",
      "----------\n",
      "epoch 51/100\n",
      "1/388, train_loss: 0.1255, step time: 1.5385\n",
      "2/388, train_loss: 0.1471, step time: 1.5377\n",
      "3/388, train_loss: 0.2844, step time: 1.5355\n",
      "4/388, train_loss: 0.2275, step time: 1.5349\n",
      "5/388, train_loss: 0.0982, step time: 1.5382\n",
      "6/388, train_loss: 0.1248, step time: 1.5345\n",
      "7/388, train_loss: 0.1541, step time: 1.5350\n",
      "8/388, train_loss: 0.4781, step time: 1.5381\n",
      "9/388, train_loss: 0.2347, step time: 1.5352\n",
      "10/388, train_loss: 0.2514, step time: 1.5426\n",
      "11/388, train_loss: 0.3773, step time: 1.5328\n",
      "12/388, train_loss: 0.0777, step time: 1.5345\n",
      "13/388, train_loss: 0.1943, step time: 1.5352\n",
      "14/388, train_loss: 0.0912, step time: 1.5340\n",
      "15/388, train_loss: 0.1207, step time: 1.5375\n",
      "16/388, train_loss: 0.1043, step time: 1.5351\n",
      "17/388, train_loss: 0.1111, step time: 1.5323\n",
      "18/388, train_loss: 0.3383, step time: 1.5313\n",
      "19/388, train_loss: 0.0834, step time: 1.5314\n",
      "20/388, train_loss: 0.2212, step time: 1.5348\n",
      "21/388, train_loss: 0.3715, step time: 1.5353\n",
      "22/388, train_loss: 0.1330, step time: 1.5357\n",
      "23/388, train_loss: 0.1813, step time: 1.5366\n",
      "24/388, train_loss: 0.0798, step time: 1.5341\n",
      "25/388, train_loss: 0.0940, step time: 1.5351\n",
      "26/388, train_loss: 0.4463, step time: 1.5368\n",
      "27/388, train_loss: 0.1321, step time: 1.5351\n",
      "28/388, train_loss: 0.1354, step time: 1.5332\n",
      "29/388, train_loss: 0.1146, step time: 1.5318\n",
      "30/388, train_loss: 0.4038, step time: 1.5361\n",
      "31/388, train_loss: 0.1127, step time: 1.5372\n",
      "32/388, train_loss: 0.2014, step time: 1.5358\n",
      "33/388, train_loss: 0.0987, step time: 1.5318\n",
      "34/388, train_loss: 0.1327, step time: 1.5316\n",
      "35/388, train_loss: 0.0861, step time: 1.5395\n",
      "36/388, train_loss: 0.1031, step time: 1.5342\n",
      "37/388, train_loss: 0.3962, step time: 1.5346\n",
      "38/388, train_loss: 0.1628, step time: 1.5353\n",
      "39/388, train_loss: 0.0923, step time: 1.5300\n",
      "40/388, train_loss: 0.0562, step time: 1.5361\n",
      "41/388, train_loss: 0.2040, step time: 1.5295\n",
      "42/388, train_loss: 0.1425, step time: 1.5333\n",
      "43/388, train_loss: 0.1983, step time: 1.5356\n",
      "44/388, train_loss: 0.2103, step time: 1.5380\n",
      "45/388, train_loss: 0.2901, step time: 1.5335\n",
      "46/388, train_loss: 0.1449, step time: 1.5325\n",
      "47/388, train_loss: 0.0501, step time: 1.5328\n",
      "48/388, train_loss: 0.1817, step time: 1.5616\n",
      "49/388, train_loss: 0.2205, step time: 1.5305\n",
      "50/388, train_loss: 0.2130, step time: 1.5311\n",
      "51/388, train_loss: 0.1644, step time: 1.5363\n",
      "52/388, train_loss: 0.1555, step time: 1.5369\n",
      "53/388, train_loss: 0.3351, step time: 1.5381\n",
      "54/388, train_loss: 0.0507, step time: 1.5346\n",
      "55/388, train_loss: 0.1170, step time: 1.5356\n",
      "56/388, train_loss: 0.1738, step time: 1.5365\n",
      "57/388, train_loss: 0.2717, step time: 1.5385\n",
      "58/388, train_loss: 0.4327, step time: 1.5401\n",
      "59/388, train_loss: 0.1839, step time: 1.5339\n",
      "60/388, train_loss: 0.2334, step time: 1.5318\n",
      "61/388, train_loss: 0.1712, step time: 1.5333\n",
      "62/388, train_loss: 0.0791, step time: 1.5364\n",
      "63/388, train_loss: 0.1629, step time: 1.5368\n",
      "64/388, train_loss: 0.1325, step time: 1.5330\n",
      "65/388, train_loss: 0.2287, step time: 1.5298\n",
      "66/388, train_loss: 0.1409, step time: 1.5314\n",
      "67/388, train_loss: 0.5426, step time: 1.5316\n",
      "68/388, train_loss: 0.1737, step time: 1.5300\n",
      "69/388, train_loss: 0.0676, step time: 1.5523\n",
      "70/388, train_loss: 0.1768, step time: 1.5357\n",
      "71/388, train_loss: 0.1997, step time: 1.5369\n",
      "72/388, train_loss: 0.3651, step time: 1.5353\n",
      "73/388, train_loss: 0.1336, step time: 1.5358\n",
      "74/388, train_loss: 0.1787, step time: 1.5335\n",
      "75/388, train_loss: 0.1173, step time: 1.5341\n",
      "76/388, train_loss: 0.1974, step time: 1.5377\n",
      "77/388, train_loss: 0.2577, step time: 1.5354\n",
      "78/388, train_loss: 0.0668, step time: 1.5339\n",
      "79/388, train_loss: 0.2319, step time: 1.5311\n",
      "80/388, train_loss: 0.0776, step time: 1.5323\n",
      "81/388, train_loss: 0.2344, step time: 1.5386\n",
      "82/388, train_loss: 0.1860, step time: 1.5347\n",
      "83/388, train_loss: 0.2823, step time: 1.5349\n",
      "84/388, train_loss: 0.2015, step time: 1.5317\n",
      "85/388, train_loss: 0.2113, step time: 1.5346\n",
      "86/388, train_loss: 0.0740, step time: 1.5330\n",
      "87/388, train_loss: 0.2456, step time: 1.5312\n",
      "88/388, train_loss: 0.1323, step time: 1.5371\n",
      "89/388, train_loss: 0.0433, step time: 1.5381\n",
      "90/388, train_loss: 0.1042, step time: 1.5402\n",
      "91/388, train_loss: 0.2311, step time: 1.5343\n",
      "92/388, train_loss: 0.1440, step time: 1.5341\n",
      "93/388, train_loss: 0.1068, step time: 1.5391\n",
      "94/388, train_loss: 0.4625, step time: 1.5380\n",
      "95/388, train_loss: 0.1931, step time: 1.5357\n",
      "96/388, train_loss: 0.1068, step time: 1.5353\n",
      "97/388, train_loss: 0.0994, step time: 1.5319\n",
      "98/388, train_loss: 0.0363, step time: 1.5368\n",
      "99/388, train_loss: 0.1967, step time: 1.5405\n",
      "100/388, train_loss: 0.0647, step time: 1.5352\n",
      "101/388, train_loss: 0.2024, step time: 1.5364\n",
      "102/388, train_loss: 0.0734, step time: 1.5372\n",
      "103/388, train_loss: 0.2766, step time: 1.5400\n",
      "104/388, train_loss: 0.0959, step time: 1.5364\n",
      "105/388, train_loss: 0.1075, step time: 1.5339\n",
      "106/388, train_loss: 0.1040, step time: 1.5335\n",
      "107/388, train_loss: 0.1059, step time: 1.5358\n",
      "108/388, train_loss: 0.1525, step time: 1.5342\n",
      "109/388, train_loss: 0.2617, step time: 1.5318\n",
      "110/388, train_loss: 0.1324, step time: 1.5350\n",
      "111/388, train_loss: 0.0597, step time: 1.5400\n",
      "112/388, train_loss: 0.1216, step time: 1.5376\n",
      "113/388, train_loss: 0.0897, step time: 1.5310\n",
      "114/388, train_loss: 0.1587, step time: 1.5369\n",
      "115/388, train_loss: 0.0922, step time: 1.5327\n",
      "116/388, train_loss: 0.1169, step time: 1.5368\n",
      "117/388, train_loss: 0.1009, step time: 1.5350\n",
      "118/388, train_loss: 0.2100, step time: 1.5338\n",
      "119/388, train_loss: 0.1250, step time: 1.5356\n",
      "120/388, train_loss: 0.1097, step time: 1.5384\n",
      "121/388, train_loss: 0.0653, step time: 1.5331\n",
      "122/388, train_loss: 0.1196, step time: 1.5467\n",
      "123/388, train_loss: 0.1097, step time: 1.5377\n",
      "124/388, train_loss: 0.2623, step time: 1.5386\n",
      "125/388, train_loss: 0.1143, step time: 1.5332\n",
      "126/388, train_loss: 0.1315, step time: 1.5395\n",
      "127/388, train_loss: 0.0405, step time: 1.5334\n",
      "128/388, train_loss: 0.0505, step time: 1.5388\n",
      "129/388, train_loss: 0.2369, step time: 1.5361\n",
      "130/388, train_loss: 0.1403, step time: 1.5362\n",
      "131/388, train_loss: 0.1688, step time: 1.5410\n",
      "132/388, train_loss: 0.0874, step time: 1.5492\n",
      "133/388, train_loss: 0.1144, step time: 1.5406\n",
      "134/388, train_loss: 0.1422, step time: 1.5356\n",
      "135/388, train_loss: 0.1691, step time: 1.5335\n",
      "136/388, train_loss: 0.0802, step time: 1.5323\n",
      "137/388, train_loss: 0.2338, step time: 1.5319\n",
      "138/388, train_loss: 0.1477, step time: 1.5346\n",
      "139/388, train_loss: 0.1266, step time: 1.5336\n",
      "140/388, train_loss: 0.1466, step time: 1.5352\n",
      "141/388, train_loss: 0.0962, step time: 1.5344\n",
      "142/388, train_loss: 0.1622, step time: 1.5329\n",
      "143/388, train_loss: 0.0312, step time: 1.5349\n",
      "144/388, train_loss: 0.0668, step time: 1.5343\n",
      "145/388, train_loss: 0.0938, step time: 1.5391\n",
      "146/388, train_loss: 0.0782, step time: 1.5366\n",
      "147/388, train_loss: 0.0719, step time: 1.5362\n",
      "148/388, train_loss: 0.2114, step time: 1.5351\n",
      "149/388, train_loss: 0.3127, step time: 1.5324\n",
      "150/388, train_loss: 0.2003, step time: 1.5369\n",
      "151/388, train_loss: 0.1922, step time: 1.5337\n",
      "152/388, train_loss: 0.2682, step time: 1.5373\n",
      "153/388, train_loss: 0.2261, step time: 1.5384\n",
      "154/388, train_loss: 0.1801, step time: 1.5337\n",
      "155/388, train_loss: 0.1610, step time: 1.5466\n",
      "156/388, train_loss: 0.1183, step time: 1.5359\n",
      "157/388, train_loss: 0.2247, step time: 1.5383\n",
      "158/388, train_loss: 0.2467, step time: 1.5335\n",
      "159/388, train_loss: 0.1480, step time: 1.5311\n",
      "160/388, train_loss: 0.1238, step time: 1.5320\n",
      "161/388, train_loss: 0.2989, step time: 1.5340\n",
      "162/388, train_loss: 0.1565, step time: 1.5384\n",
      "163/388, train_loss: 0.1060, step time: 1.5354\n",
      "164/388, train_loss: 0.1605, step time: 1.5343\n",
      "165/388, train_loss: 0.0482, step time: 1.5335\n",
      "166/388, train_loss: 0.1575, step time: 1.5338\n",
      "167/388, train_loss: 0.1925, step time: 1.5333\n",
      "168/388, train_loss: 0.4446, step time: 1.5372\n",
      "169/388, train_loss: 0.1743, step time: 1.5409\n",
      "170/388, train_loss: 0.0621, step time: 1.5331\n",
      "171/388, train_loss: 0.1501, step time: 1.5318\n",
      "172/388, train_loss: 0.1196, step time: 1.5297\n",
      "173/388, train_loss: 0.1835, step time: 1.5317\n",
      "174/388, train_loss: 0.2835, step time: 1.5306\n",
      "175/388, train_loss: 0.0599, step time: 1.5347\n",
      "176/388, train_loss: 0.1088, step time: 1.5337\n",
      "177/388, train_loss: 0.3646, step time: 1.5356\n",
      "178/388, train_loss: 0.1658, step time: 1.5323\n",
      "179/388, train_loss: 0.0786, step time: 1.5310\n",
      "180/388, train_loss: 0.1322, step time: 1.5312\n",
      "181/388, train_loss: 0.2566, step time: 1.5310\n",
      "182/388, train_loss: 0.0962, step time: 1.5339\n",
      "183/388, train_loss: 0.0914, step time: 1.5347\n",
      "184/388, train_loss: 0.2075, step time: 1.5374\n",
      "185/388, train_loss: 0.0459, step time: 1.5323\n",
      "186/388, train_loss: 0.0530, step time: 1.5309\n",
      "187/388, train_loss: 0.1583, step time: 1.5366\n",
      "188/388, train_loss: 0.1800, step time: 1.5329\n",
      "189/388, train_loss: 0.1381, step time: 1.5432\n",
      "190/388, train_loss: 0.2379, step time: 1.5343\n",
      "191/388, train_loss: 0.0957, step time: 1.5314\n",
      "192/388, train_loss: 0.3555, step time: 1.5334\n",
      "193/388, train_loss: 0.0404, step time: 1.5326\n",
      "194/388, train_loss: 0.0929, step time: 1.5429\n",
      "195/388, train_loss: 0.2461, step time: 1.5543\n",
      "196/388, train_loss: 0.0734, step time: 1.5316\n",
      "197/388, train_loss: 0.1921, step time: 1.5301\n",
      "198/388, train_loss: 0.2277, step time: 1.5340\n",
      "199/388, train_loss: 0.1985, step time: 1.5319\n",
      "200/388, train_loss: 0.3762, step time: 1.5345\n",
      "201/388, train_loss: 0.3855, step time: 1.5356\n",
      "202/388, train_loss: 0.1483, step time: 1.5318\n",
      "203/388, train_loss: 0.1959, step time: 1.5309\n",
      "204/388, train_loss: 0.2322, step time: 1.5339\n",
      "205/388, train_loss: 0.2194, step time: 1.5384\n",
      "206/388, train_loss: 0.1030, step time: 1.5384\n",
      "207/388, train_loss: 0.1740, step time: 1.5338\n",
      "208/388, train_loss: 0.0922, step time: 1.5321\n",
      "209/388, train_loss: 0.1287, step time: 1.5330\n",
      "210/388, train_loss: 0.4173, step time: 1.5377\n",
      "211/388, train_loss: 0.1121, step time: 1.5384\n",
      "212/388, train_loss: 0.1019, step time: 1.5337\n",
      "213/388, train_loss: 0.1485, step time: 1.5337\n",
      "214/388, train_loss: 0.1401, step time: 1.5346\n",
      "215/388, train_loss: 0.1509, step time: 1.5335\n",
      "216/388, train_loss: 0.3786, step time: 1.5343\n",
      "217/388, train_loss: 0.1601, step time: 1.5353\n",
      "218/388, train_loss: 0.5725, step time: 1.5324\n",
      "219/388, train_loss: 0.2179, step time: 1.5316\n",
      "220/388, train_loss: 0.0975, step time: 1.5337\n",
      "221/388, train_loss: 0.1672, step time: 1.5330\n",
      "222/388, train_loss: 0.2239, step time: 1.5358\n",
      "223/388, train_loss: 0.2907, step time: 1.5373\n",
      "224/388, train_loss: 0.0955, step time: 1.5352\n",
      "225/388, train_loss: 0.1102, step time: 1.5343\n",
      "226/388, train_loss: 0.0854, step time: 1.5318\n",
      "227/388, train_loss: 0.1196, step time: 1.5345\n",
      "228/388, train_loss: 0.2475, step time: 1.5385\n",
      "229/388, train_loss: 0.1253, step time: 1.5349\n",
      "230/388, train_loss: 0.2659, step time: 1.5490\n",
      "231/388, train_loss: 0.0662, step time: 1.5390\n",
      "232/388, train_loss: 0.3402, step time: 1.5465\n",
      "233/388, train_loss: 0.4736, step time: 1.5354\n",
      "234/388, train_loss: 0.2490, step time: 1.5348\n",
      "235/388, train_loss: 0.3487, step time: 1.5299\n",
      "236/388, train_loss: 0.0914, step time: 1.5333\n",
      "237/388, train_loss: 0.1991, step time: 1.5363\n",
      "238/388, train_loss: 0.0922, step time: 1.5356\n",
      "239/388, train_loss: 0.1658, step time: 1.5440\n",
      "240/388, train_loss: 0.0921, step time: 1.5358\n",
      "241/388, train_loss: 0.2011, step time: 1.5329\n",
      "242/388, train_loss: 0.0736, step time: 1.5374\n",
      "243/388, train_loss: 0.0662, step time: 1.5314\n",
      "244/388, train_loss: 0.1246, step time: 1.5345\n",
      "245/388, train_loss: 0.4612, step time: 1.5303\n",
      "246/388, train_loss: 0.2630, step time: 1.5320\n",
      "247/388, train_loss: 0.2049, step time: 1.5377\n",
      "248/388, train_loss: 0.1485, step time: 1.5389\n",
      "249/388, train_loss: 0.2140, step time: 1.5408\n",
      "250/388, train_loss: 0.0889, step time: 1.5308\n",
      "251/388, train_loss: 0.2128, step time: 1.5320\n",
      "252/388, train_loss: 0.2028, step time: 1.5314\n",
      "253/388, train_loss: 0.2306, step time: 1.5315\n",
      "254/388, train_loss: 0.2632, step time: 1.5399\n",
      "255/388, train_loss: 0.2264, step time: 1.5344\n",
      "256/388, train_loss: 0.0935, step time: 1.5460\n",
      "257/388, train_loss: 0.0860, step time: 1.5334\n",
      "258/388, train_loss: 0.1915, step time: 1.5344\n",
      "259/388, train_loss: 0.1512, step time: 1.5335\n",
      "260/388, train_loss: 0.3762, step time: 1.5371\n",
      "261/388, train_loss: 0.1616, step time: 1.5371\n",
      "262/388, train_loss: 0.0647, step time: 1.5365\n",
      "263/388, train_loss: 0.2482, step time: 1.5331\n",
      "264/388, train_loss: 0.2089, step time: 1.5314\n",
      "265/388, train_loss: 0.1048, step time: 1.5372\n",
      "266/388, train_loss: 0.1338, step time: 1.5342\n",
      "267/388, train_loss: 0.0913, step time: 1.5333\n",
      "268/388, train_loss: 0.3843, step time: 1.5360\n",
      "269/388, train_loss: 0.0882, step time: 1.5334\n",
      "270/388, train_loss: 0.1635, step time: 1.5360\n",
      "271/388, train_loss: 0.0564, step time: 1.5364\n",
      "272/388, train_loss: 0.1264, step time: 1.5326\n",
      "273/388, train_loss: 0.1062, step time: 1.5310\n",
      "274/388, train_loss: 0.2387, step time: 1.5355\n",
      "275/388, train_loss: 0.2047, step time: 1.5399\n",
      "276/388, train_loss: 0.5494, step time: 1.5360\n",
      "277/388, train_loss: 0.1028, step time: 1.5368\n",
      "278/388, train_loss: 0.5477, step time: 1.5352\n",
      "279/388, train_loss: 0.0899, step time: 1.5360\n",
      "280/388, train_loss: 0.2483, step time: 1.5326\n",
      "281/388, train_loss: 0.2708, step time: 1.5325\n",
      "282/388, train_loss: 0.1466, step time: 1.5341\n",
      "283/388, train_loss: 0.2346, step time: 1.5370\n",
      "284/388, train_loss: 0.2983, step time: 1.5365\n",
      "285/388, train_loss: 0.1216, step time: 1.5350\n",
      "286/388, train_loss: 0.1172, step time: 1.5321\n",
      "287/388, train_loss: 0.2796, step time: 1.5394\n",
      "288/388, train_loss: 0.1360, step time: 1.5451\n",
      "289/388, train_loss: 0.1167, step time: 1.5345\n",
      "290/388, train_loss: 0.1790, step time: 1.5373\n",
      "291/388, train_loss: 0.2464, step time: 1.5346\n",
      "292/388, train_loss: 0.1539, step time: 1.5441\n",
      "293/388, train_loss: 0.1746, step time: 1.5334\n",
      "294/388, train_loss: 0.2283, step time: 1.5404\n",
      "295/388, train_loss: 0.1842, step time: 1.5375\n",
      "296/388, train_loss: 0.1446, step time: 1.5432\n",
      "297/388, train_loss: 0.1887, step time: 1.5312\n",
      "298/388, train_loss: 0.1557, step time: 1.5329\n",
      "299/388, train_loss: 0.2176, step time: 1.5340\n",
      "300/388, train_loss: 0.2066, step time: 1.5468\n",
      "301/388, train_loss: 0.4444, step time: 1.5355\n",
      "302/388, train_loss: 0.4429, step time: 1.5326\n",
      "303/388, train_loss: 0.4457, step time: 1.5355\n",
      "304/388, train_loss: 0.1057, step time: 1.5486\n",
      "305/388, train_loss: 0.0790, step time: 1.5331\n",
      "306/388, train_loss: 0.1353, step time: 1.5330\n",
      "307/388, train_loss: 0.2281, step time: 1.5326\n",
      "308/388, train_loss: 0.1155, step time: 1.5410\n",
      "309/388, train_loss: 0.1443, step time: 1.5361\n",
      "310/388, train_loss: 0.2831, step time: 1.5389\n",
      "311/388, train_loss: 0.2077, step time: 1.5349\n",
      "312/388, train_loss: 0.1536, step time: 1.5436\n",
      "313/388, train_loss: 0.0790, step time: 1.5363\n",
      "314/388, train_loss: 0.1701, step time: 1.5380\n",
      "315/388, train_loss: 0.0309, step time: 1.5366\n",
      "316/388, train_loss: 0.1057, step time: 1.5408\n",
      "317/388, train_loss: 0.1341, step time: 1.5329\n",
      "318/388, train_loss: 0.0850, step time: 1.5408\n",
      "319/388, train_loss: 0.2242, step time: 1.5352\n",
      "320/388, train_loss: 0.2058, step time: 1.5479\n",
      "321/388, train_loss: 0.1532, step time: 1.5344\n",
      "322/388, train_loss: 0.0706, step time: 1.5393\n",
      "323/388, train_loss: 0.1747, step time: 1.5340\n",
      "324/388, train_loss: 0.2270, step time: 1.5387\n",
      "325/388, train_loss: 0.1054, step time: 1.5334\n",
      "326/388, train_loss: 0.1006, step time: 1.5562\n",
      "327/388, train_loss: 0.0844, step time: 1.5378\n",
      "328/388, train_loss: 0.2969, step time: 1.5385\n",
      "329/388, train_loss: 0.3073, step time: 1.5334\n",
      "330/388, train_loss: 0.0662, step time: 1.5410\n",
      "331/388, train_loss: 0.2111, step time: 1.5382\n",
      "332/388, train_loss: 0.3267, step time: 1.5538\n",
      "333/388, train_loss: 0.0981, step time: 1.5337\n",
      "334/388, train_loss: 0.3461, step time: 1.5343\n",
      "335/388, train_loss: 0.1629, step time: 1.5341\n",
      "336/388, train_loss: 0.1986, step time: 1.5374\n",
      "337/388, train_loss: 0.1996, step time: 1.5387\n",
      "338/388, train_loss: 0.0657, step time: 1.5344\n",
      "339/388, train_loss: 0.2853, step time: 1.5328\n",
      "340/388, train_loss: 0.0863, step time: 1.5312\n",
      "341/388, train_loss: 0.0893, step time: 1.5327\n",
      "342/388, train_loss: 0.2508, step time: 1.5381\n",
      "343/388, train_loss: 0.0797, step time: 1.5346\n",
      "344/388, train_loss: 0.3559, step time: 1.5333\n",
      "345/388, train_loss: 0.1803, step time: 1.5328\n",
      "346/388, train_loss: 0.1482, step time: 1.5310\n",
      "347/388, train_loss: 0.2304, step time: 1.5315\n",
      "348/388, train_loss: 0.0862, step time: 1.5345\n",
      "349/388, train_loss: 0.2359, step time: 1.5382\n",
      "350/388, train_loss: 0.2302, step time: 1.5391\n",
      "351/388, train_loss: 0.2385, step time: 1.5366\n",
      "352/388, train_loss: 0.2050, step time: 1.5343\n",
      "353/388, train_loss: 0.4174, step time: 1.5324\n",
      "354/388, train_loss: 0.1447, step time: 1.5384\n",
      "355/388, train_loss: 0.2220, step time: 1.5340\n",
      "356/388, train_loss: 0.4916, step time: 1.5322\n",
      "357/388, train_loss: 0.2854, step time: 1.5334\n",
      "358/388, train_loss: 0.1749, step time: 1.5309\n",
      "359/388, train_loss: 0.0783, step time: 1.5327\n",
      "360/388, train_loss: 0.1749, step time: 1.5362\n",
      "361/388, train_loss: 0.2043, step time: 1.5367\n",
      "362/388, train_loss: 0.0957, step time: 1.5340\n",
      "363/388, train_loss: 0.1574, step time: 1.5329\n",
      "364/388, train_loss: 0.4994, step time: 1.5321\n",
      "365/388, train_loss: 0.2348, step time: 1.5341\n",
      "366/388, train_loss: 0.1320, step time: 1.5375\n",
      "367/388, train_loss: 0.0974, step time: 1.5364\n",
      "368/388, train_loss: 0.1118, step time: 1.5361\n",
      "369/388, train_loss: 0.2503, step time: 1.5340\n",
      "370/388, train_loss: 0.3062, step time: 1.5315\n",
      "371/388, train_loss: 0.3011, step time: 1.5331\n",
      "372/388, train_loss: 0.1555, step time: 1.5354\n",
      "373/388, train_loss: 0.0512, step time: 1.5361\n",
      "374/388, train_loss: 0.4846, step time: 1.5346\n",
      "375/388, train_loss: 0.1348, step time: 1.5329\n",
      "376/388, train_loss: 0.2313, step time: 1.5456\n",
      "377/388, train_loss: 0.0786, step time: 1.5613\n",
      "378/388, train_loss: 0.1587, step time: 1.5378\n",
      "379/388, train_loss: 0.1496, step time: 1.5326\n",
      "380/388, train_loss: 0.2232, step time: 1.5325\n",
      "381/388, train_loss: 0.2262, step time: 1.5339\n",
      "382/388, train_loss: 0.1474, step time: 1.5346\n",
      "383/388, train_loss: 0.2357, step time: 1.5369\n",
      "384/388, train_loss: 0.0873, step time: 1.5433\n",
      "385/388, train_loss: 0.1308, step time: 1.5397\n",
      "386/388, train_loss: 0.1000, step time: 1.5306\n",
      "387/388, train_loss: 0.4150, step time: 1.5351\n",
      "388/388, train_loss: 0.1934, step time: 1.5345\n",
      "epoch 51 average loss: 0.1823\n",
      "current epoch: 51 current mean dice: 0.7670 tc: 0.8183 wt: 0.8999 et: 0.5827\n",
      "best mean dice: 0.7692 at epoch: 49\n",
      "time consuming of epoch 51 is: 703.7961\n",
      "----------\n",
      "epoch 52/100\n",
      "1/388, train_loss: 0.1483, step time: 1.5419\n",
      "2/388, train_loss: 0.2620, step time: 1.5360\n",
      "3/388, train_loss: 0.1593, step time: 1.5326\n",
      "4/388, train_loss: 0.2194, step time: 1.5342\n",
      "5/388, train_loss: 0.1769, step time: 1.5313\n",
      "6/388, train_loss: 0.0846, step time: 1.5303\n",
      "7/388, train_loss: 0.1008, step time: 1.5301\n",
      "8/388, train_loss: 0.3369, step time: 1.5284\n",
      "9/388, train_loss: 0.2823, step time: 1.5329\n",
      "10/388, train_loss: 0.1131, step time: 1.5468\n",
      "11/388, train_loss: 0.1653, step time: 1.5466\n",
      "12/388, train_loss: 0.2795, step time: 1.5452\n",
      "13/388, train_loss: 0.1105, step time: 1.5475\n",
      "14/388, train_loss: 0.0922, step time: 1.5356\n",
      "15/388, train_loss: 0.2744, step time: 1.5316\n",
      "16/388, train_loss: 0.0709, step time: 1.5315\n",
      "17/388, train_loss: 0.2148, step time: 1.5320\n",
      "18/388, train_loss: 0.1021, step time: 1.5297\n",
      "19/388, train_loss: 0.2399, step time: 1.5362\n",
      "20/388, train_loss: 0.0548, step time: 1.5359\n",
      "21/388, train_loss: 0.1546, step time: 1.5323\n",
      "22/388, train_loss: 0.2037, step time: 1.5327\n",
      "23/388, train_loss: 0.4513, step time: 1.5329\n",
      "24/388, train_loss: 0.0431, step time: 1.5289\n",
      "25/388, train_loss: 0.2360, step time: 1.5294\n",
      "26/388, train_loss: 0.1524, step time: 1.5352\n",
      "27/388, train_loss: 0.1809, step time: 1.5336\n",
      "28/388, train_loss: 0.3044, step time: 1.5309\n",
      "29/388, train_loss: 0.0629, step time: 1.5300\n",
      "30/388, train_loss: 0.0876, step time: 1.5321\n",
      "31/388, train_loss: 0.2425, step time: 1.5338\n",
      "32/388, train_loss: 0.2652, step time: 1.5306\n",
      "33/388, train_loss: 0.0743, step time: 1.5310\n",
      "34/388, train_loss: 0.1647, step time: 1.5360\n",
      "35/388, train_loss: 0.1051, step time: 1.5342\n",
      "36/388, train_loss: 0.0733, step time: 1.5327\n",
      "37/388, train_loss: 0.0688, step time: 1.5302\n",
      "38/388, train_loss: 0.1899, step time: 1.5295\n",
      "39/388, train_loss: 0.1334, step time: 1.5337\n",
      "40/388, train_loss: 0.1511, step time: 1.5344\n",
      "41/388, train_loss: 0.0717, step time: 1.5344\n",
      "42/388, train_loss: 0.2874, step time: 1.5329\n",
      "43/388, train_loss: 0.2157, step time: 1.5322\n",
      "44/388, train_loss: 0.1070, step time: 1.5295\n",
      "45/388, train_loss: 0.1316, step time: 1.5315\n",
      "46/388, train_loss: 0.1207, step time: 1.5346\n",
      "47/388, train_loss: 0.0797, step time: 1.5339\n",
      "48/388, train_loss: 0.1040, step time: 1.5331\n",
      "49/388, train_loss: 0.2156, step time: 1.5310\n",
      "50/388, train_loss: 0.1363, step time: 1.5284\n",
      "51/388, train_loss: 0.3354, step time: 1.5331\n",
      "52/388, train_loss: 0.0936, step time: 1.5287\n",
      "53/388, train_loss: 0.1097, step time: 1.5332\n",
      "54/388, train_loss: 0.2724, step time: 1.5376\n",
      "55/388, train_loss: 0.2173, step time: 1.5339\n",
      "56/388, train_loss: 0.2055, step time: 1.5366\n",
      "57/388, train_loss: 0.5308, step time: 1.5333\n",
      "58/388, train_loss: 0.2165, step time: 1.5325\n",
      "59/388, train_loss: 0.0989, step time: 1.5294\n",
      "60/388, train_loss: 0.0608, step time: 1.5315\n",
      "61/388, train_loss: 0.1435, step time: 1.5487\n",
      "62/388, train_loss: 0.2280, step time: 1.5564\n",
      "63/388, train_loss: 0.1162, step time: 1.5338\n",
      "64/388, train_loss: 0.3359, step time: 1.5321\n",
      "65/388, train_loss: 0.1234, step time: 1.5319\n",
      "66/388, train_loss: 0.2345, step time: 1.5389\n",
      "67/388, train_loss: 0.1981, step time: 1.5348\n",
      "68/388, train_loss: 0.0619, step time: 1.5330\n",
      "69/388, train_loss: 0.0311, step time: 1.5311\n",
      "70/388, train_loss: 0.2697, step time: 1.5311\n",
      "71/388, train_loss: 0.1662, step time: 1.5317\n",
      "72/388, train_loss: 0.2410, step time: 1.5315\n",
      "73/388, train_loss: 0.2431, step time: 1.5347\n",
      "74/388, train_loss: 0.2817, step time: 1.5345\n",
      "75/388, train_loss: 0.1770, step time: 1.5365\n",
      "76/388, train_loss: 0.1270, step time: 1.5328\n",
      "77/388, train_loss: 0.3272, step time: 1.5338\n",
      "78/388, train_loss: 0.1640, step time: 1.5353\n",
      "79/388, train_loss: 0.1740, step time: 1.5345\n",
      "80/388, train_loss: 0.4761, step time: 1.5422\n",
      "81/388, train_loss: 0.1659, step time: 1.5354\n",
      "82/388, train_loss: 0.1681, step time: 1.5333\n",
      "83/388, train_loss: 0.1981, step time: 1.5311\n",
      "84/388, train_loss: 0.0947, step time: 1.5308\n",
      "85/388, train_loss: 0.4247, step time: 1.5320\n",
      "86/388, train_loss: 0.2040, step time: 1.5350\n",
      "87/388, train_loss: 0.1627, step time: 1.5328\n",
      "88/388, train_loss: 0.1277, step time: 1.5321\n",
      "89/388, train_loss: 0.1183, step time: 1.5367\n",
      "90/388, train_loss: 0.0614, step time: 1.5328\n",
      "91/388, train_loss: 0.1732, step time: 1.5320\n",
      "92/388, train_loss: 0.2414, step time: 1.5348\n",
      "93/388, train_loss: 0.1883, step time: 1.5346\n",
      "94/388, train_loss: 0.1436, step time: 1.5340\n",
      "95/388, train_loss: 0.1234, step time: 1.5523\n",
      "96/388, train_loss: 0.2638, step time: 1.5367\n",
      "97/388, train_loss: 0.1282, step time: 1.5360\n",
      "98/388, train_loss: 0.0959, step time: 1.5358\n",
      "99/388, train_loss: 0.2005, step time: 1.5342\n",
      "100/388, train_loss: 0.1856, step time: 1.5302\n",
      "101/388, train_loss: 0.2160, step time: 1.5326\n",
      "102/388, train_loss: 0.2614, step time: 1.5302\n",
      "103/388, train_loss: 0.1632, step time: 1.5314\n",
      "104/388, train_loss: 0.2120, step time: 1.5345\n",
      "105/388, train_loss: 0.0594, step time: 1.5331\n",
      "106/388, train_loss: 0.2064, step time: 1.5340\n",
      "107/388, train_loss: 0.2729, step time: 1.5292\n",
      "108/388, train_loss: 0.0995, step time: 1.5318\n",
      "109/388, train_loss: 0.0665, step time: 1.5320\n",
      "110/388, train_loss: 0.0981, step time: 1.5319\n",
      "111/388, train_loss: 0.2337, step time: 1.5341\n",
      "112/388, train_loss: 0.1625, step time: 1.5350\n",
      "113/388, train_loss: 0.1985, step time: 1.5328\n",
      "114/388, train_loss: 0.1984, step time: 1.5340\n",
      "115/388, train_loss: 0.0975, step time: 1.5305\n",
      "116/388, train_loss: 0.2622, step time: 1.5325\n",
      "117/388, train_loss: 0.2498, step time: 1.5310\n",
      "118/388, train_loss: 0.2302, step time: 1.5323\n",
      "119/388, train_loss: 0.3350, step time: 1.5323\n",
      "120/388, train_loss: 0.1463, step time: 1.5347\n",
      "121/388, train_loss: 0.0874, step time: 1.5312\n",
      "122/388, train_loss: 0.2288, step time: 1.5298\n",
      "123/388, train_loss: 0.0700, step time: 1.5301\n",
      "124/388, train_loss: 0.1729, step time: 1.5337\n",
      "125/388, train_loss: 0.3210, step time: 1.5390\n",
      "126/388, train_loss: 0.1031, step time: 1.5353\n",
      "127/388, train_loss: 0.1518, step time: 1.5347\n",
      "128/388, train_loss: 0.0563, step time: 1.5396\n",
      "129/388, train_loss: 0.0795, step time: 1.5344\n",
      "130/388, train_loss: 0.5359, step time: 1.5323\n",
      "131/388, train_loss: 0.2393, step time: 1.5367\n",
      "132/388, train_loss: 0.1623, step time: 1.5332\n",
      "133/388, train_loss: 0.1412, step time: 1.5333\n",
      "134/388, train_loss: 0.0941, step time: 1.5341\n",
      "135/388, train_loss: 0.1212, step time: 1.5326\n",
      "136/388, train_loss: 0.1465, step time: 1.5299\n",
      "137/388, train_loss: 0.2541, step time: 1.5307\n",
      "138/388, train_loss: 0.2343, step time: 1.5337\n",
      "139/388, train_loss: 0.2178, step time: 1.5312\n",
      "140/388, train_loss: 0.1070, step time: 1.5319\n",
      "141/388, train_loss: 0.1843, step time: 1.5333\n",
      "142/388, train_loss: 0.1485, step time: 1.5326\n",
      "143/388, train_loss: 0.2732, step time: 1.5303\n",
      "144/388, train_loss: 0.2271, step time: 1.5329\n",
      "145/388, train_loss: 0.3303, step time: 1.5317\n",
      "146/388, train_loss: 0.2057, step time: 1.5309\n",
      "147/388, train_loss: 0.1045, step time: 1.5329\n",
      "148/388, train_loss: 0.1891, step time: 1.5349\n",
      "149/388, train_loss: 0.4325, step time: 1.5356\n",
      "150/388, train_loss: 0.1049, step time: 1.5343\n",
      "151/388, train_loss: 0.1592, step time: 1.5343\n",
      "152/388, train_loss: 0.2419, step time: 1.5342\n",
      "153/388, train_loss: 0.1698, step time: 1.5370\n",
      "154/388, train_loss: 0.1485, step time: 1.5349\n",
      "155/388, train_loss: 0.3225, step time: 1.5315\n",
      "156/388, train_loss: 0.2045, step time: 1.5301\n",
      "157/388, train_loss: 0.4802, step time: 1.5366\n",
      "158/388, train_loss: 0.1161, step time: 1.5334\n",
      "159/388, train_loss: 0.1012, step time: 1.5362\n",
      "160/388, train_loss: 0.1459, step time: 1.5332\n",
      "161/388, train_loss: 0.1464, step time: 1.5327\n",
      "162/388, train_loss: 0.2452, step time: 1.5338\n",
      "163/388, train_loss: 0.1875, step time: 1.5367\n",
      "164/388, train_loss: 0.1449, step time: 1.5319\n",
      "165/388, train_loss: 0.2796, step time: 1.5370\n",
      "166/388, train_loss: 0.3240, step time: 1.5376\n",
      "167/388, train_loss: 0.3403, step time: 1.5347\n",
      "168/388, train_loss: 0.1467, step time: 1.5306\n",
      "169/388, train_loss: 0.1054, step time: 1.5310\n",
      "170/388, train_loss: 0.1879, step time: 1.5298\n",
      "171/388, train_loss: 0.4113, step time: 1.5342\n",
      "172/388, train_loss: 0.1438, step time: 1.5353\n",
      "173/388, train_loss: 0.0753, step time: 1.5341\n",
      "174/388, train_loss: 0.1551, step time: 1.5289\n",
      "175/388, train_loss: 0.1450, step time: 1.5306\n",
      "176/388, train_loss: 0.0349, step time: 1.5340\n",
      "177/388, train_loss: 0.3761, step time: 1.5295\n",
      "178/388, train_loss: 0.2271, step time: 1.5323\n",
      "179/388, train_loss: 0.2495, step time: 1.5321\n",
      "180/388, train_loss: 0.2794, step time: 1.5374\n",
      "181/388, train_loss: 0.1041, step time: 1.5356\n",
      "182/388, train_loss: 0.2134, step time: 1.5317\n",
      "183/388, train_loss: 0.0785, step time: 1.5321\n",
      "184/388, train_loss: 0.2213, step time: 1.5321\n",
      "185/388, train_loss: 0.2151, step time: 1.5356\n",
      "186/388, train_loss: 0.1850, step time: 1.5350\n",
      "187/388, train_loss: 0.0843, step time: 1.5358\n",
      "188/388, train_loss: 0.2348, step time: 1.5341\n",
      "189/388, train_loss: 0.3327, step time: 1.5293\n",
      "190/388, train_loss: 0.5549, step time: 1.5347\n",
      "191/388, train_loss: 0.0977, step time: 1.5350\n",
      "192/388, train_loss: 0.3336, step time: 1.5607\n",
      "193/388, train_loss: 0.0307, step time: 1.5291\n",
      "194/388, train_loss: 0.1596, step time: 1.5328\n",
      "195/388, train_loss: 0.1888, step time: 1.5332\n",
      "196/388, train_loss: 0.1377, step time: 1.5301\n",
      "197/388, train_loss: 0.1585, step time: 1.5327\n",
      "198/388, train_loss: 0.1167, step time: 1.5310\n",
      "199/388, train_loss: 0.0879, step time: 1.5334\n",
      "200/388, train_loss: 0.0872, step time: 1.5351\n",
      "201/388, train_loss: 0.0879, step time: 1.5406\n",
      "202/388, train_loss: 0.1252, step time: 1.5311\n",
      "203/388, train_loss: 0.2487, step time: 1.5298\n",
      "204/388, train_loss: 0.1030, step time: 1.5304\n",
      "205/388, train_loss: 0.2537, step time: 1.5343\n",
      "206/388, train_loss: 0.1663, step time: 1.5332\n",
      "207/388, train_loss: 0.0597, step time: 1.5365\n",
      "208/388, train_loss: 0.1996, step time: 1.5349\n",
      "209/388, train_loss: 0.0923, step time: 1.5341\n",
      "210/388, train_loss: 0.1604, step time: 1.5321\n",
      "211/388, train_loss: 0.1126, step time: 1.5301\n",
      "212/388, train_loss: 0.3586, step time: 1.5313\n",
      "213/388, train_loss: 0.2533, step time: 1.5348\n",
      "214/388, train_loss: 0.2481, step time: 1.5368\n",
      "215/388, train_loss: 0.2078, step time: 1.5492\n",
      "216/388, train_loss: 0.2121, step time: 1.5461\n",
      "217/388, train_loss: 0.1602, step time: 1.5345\n",
      "218/388, train_loss: 0.1913, step time: 1.5350\n",
      "219/388, train_loss: 0.0740, step time: 1.5368\n",
      "220/388, train_loss: 0.4480, step time: 1.5408\n",
      "221/388, train_loss: 0.0839, step time: 1.5293\n",
      "222/388, train_loss: 0.0658, step time: 1.5320\n",
      "223/388, train_loss: 0.1380, step time: 1.5310\n",
      "224/388, train_loss: 0.6874, step time: 1.5380\n",
      "225/388, train_loss: 0.4931, step time: 1.5351\n",
      "226/388, train_loss: 0.0834, step time: 1.5330\n",
      "227/388, train_loss: 0.1109, step time: 1.5335\n",
      "228/388, train_loss: 0.0466, step time: 1.5338\n",
      "229/388, train_loss: 0.0878, step time: 1.5335\n",
      "230/388, train_loss: 0.0896, step time: 1.5474\n",
      "231/388, train_loss: 0.0899, step time: 1.5358\n",
      "232/388, train_loss: 0.1496, step time: 1.5336\n",
      "233/388, train_loss: 0.2466, step time: 1.5314\n",
      "234/388, train_loss: 0.0944, step time: 1.5578\n",
      "235/388, train_loss: 0.1473, step time: 1.5350\n",
      "236/388, train_loss: 0.1531, step time: 1.5322\n",
      "237/388, train_loss: 0.1555, step time: 1.5305\n",
      "238/388, train_loss: 0.1147, step time: 1.5293\n",
      "239/388, train_loss: 0.2050, step time: 1.5301\n",
      "240/388, train_loss: 0.2312, step time: 1.5358\n",
      "241/388, train_loss: 0.2778, step time: 1.5338\n",
      "242/388, train_loss: 0.1437, step time: 1.5334\n",
      "243/388, train_loss: 0.4832, step time: 1.5326\n",
      "244/388, train_loss: 0.2348, step time: 1.5315\n",
      "245/388, train_loss: 0.1974, step time: 1.5308\n",
      "246/388, train_loss: 0.2447, step time: 1.5349\n",
      "247/388, train_loss: 0.1277, step time: 1.5313\n",
      "248/388, train_loss: 0.1345, step time: 1.5344\n",
      "249/388, train_loss: 0.1680, step time: 1.5316\n",
      "250/388, train_loss: 0.1405, step time: 1.5306\n",
      "251/388, train_loss: 0.1589, step time: 1.5286\n",
      "252/388, train_loss: 0.1010, step time: 1.5335\n",
      "253/388, train_loss: 0.0517, step time: 1.5410\n",
      "254/388, train_loss: 0.0617, step time: 1.5324\n",
      "255/388, train_loss: 0.1480, step time: 1.5363\n",
      "256/388, train_loss: 0.1473, step time: 1.5310\n",
      "257/388, train_loss: 0.2169, step time: 1.5306\n",
      "258/388, train_loss: 0.0962, step time: 1.5302\n",
      "259/388, train_loss: 0.2902, step time: 1.5300\n",
      "260/388, train_loss: 0.0478, step time: 1.5316\n",
      "261/388, train_loss: 0.2858, step time: 1.5435\n",
      "262/388, train_loss: 0.1152, step time: 1.5331\n",
      "263/388, train_loss: 0.1482, step time: 1.5427\n",
      "264/388, train_loss: 0.2720, step time: 1.5323\n",
      "265/388, train_loss: 0.1981, step time: 1.5292\n",
      "266/388, train_loss: 0.1169, step time: 1.5479\n",
      "267/388, train_loss: 0.5444, step time: 1.5320\n",
      "268/388, train_loss: 0.0885, step time: 1.5304\n",
      "269/388, train_loss: 0.1713, step time: 1.5352\n",
      "270/388, train_loss: 0.4215, step time: 1.5329\n",
      "271/388, train_loss: 0.1566, step time: 1.5335\n",
      "272/388, train_loss: 0.1024, step time: 1.5344\n",
      "273/388, train_loss: 0.1393, step time: 1.5361\n",
      "274/388, train_loss: 0.4054, step time: 1.5320\n",
      "275/388, train_loss: 0.1803, step time: 1.5291\n",
      "276/388, train_loss: 0.2606, step time: 1.5325\n",
      "277/388, train_loss: 0.0825, step time: 1.5317\n",
      "278/388, train_loss: 0.2754, step time: 1.5332\n",
      "279/388, train_loss: 0.1889, step time: 1.5364\n",
      "280/388, train_loss: 0.0943, step time: 1.5357\n",
      "281/388, train_loss: 0.0429, step time: 1.5318\n",
      "282/388, train_loss: 0.1517, step time: 1.5351\n",
      "283/388, train_loss: 0.3788, step time: 1.5315\n",
      "284/388, train_loss: 0.1261, step time: 1.5296\n",
      "285/388, train_loss: 0.1694, step time: 1.5305\n",
      "286/388, train_loss: 0.2231, step time: 1.5347\n",
      "287/388, train_loss: 0.1215, step time: 1.5362\n",
      "288/388, train_loss: 0.1281, step time: 1.5311\n",
      "289/388, train_loss: 0.0792, step time: 1.5346\n",
      "290/388, train_loss: 0.0942, step time: 1.5303\n",
      "291/388, train_loss: 0.0885, step time: 1.5315\n",
      "292/388, train_loss: 0.2338, step time: 1.5345\n",
      "293/388, train_loss: 0.1624, step time: 1.5360\n",
      "294/388, train_loss: 0.0997, step time: 1.5361\n",
      "295/388, train_loss: 0.1495, step time: 1.5347\n",
      "296/388, train_loss: 0.0713, step time: 1.5423\n",
      "297/388, train_loss: 0.1446, step time: 1.5340\n",
      "298/388, train_loss: 0.0928, step time: 1.5355\n",
      "299/388, train_loss: 0.3996, step time: 1.5305\n",
      "300/388, train_loss: 0.1567, step time: 1.5328\n",
      "301/388, train_loss: 0.1951, step time: 1.5318\n",
      "302/388, train_loss: 0.5089, step time: 1.5324\n",
      "303/388, train_loss: 0.0970, step time: 1.5339\n",
      "304/388, train_loss: 0.0742, step time: 1.5350\n",
      "305/388, train_loss: 0.1032, step time: 1.5370\n",
      "306/388, train_loss: 0.0587, step time: 1.5331\n",
      "307/388, train_loss: 0.0979, step time: 1.5303\n",
      "308/388, train_loss: 0.2535, step time: 1.5321\n",
      "309/388, train_loss: 0.0883, step time: 1.5311\n",
      "310/388, train_loss: 0.1156, step time: 1.5342\n",
      "311/388, train_loss: 0.1363, step time: 1.5329\n",
      "312/388, train_loss: 0.1294, step time: 1.5365\n",
      "313/388, train_loss: 0.2488, step time: 1.5327\n",
      "314/388, train_loss: 0.3077, step time: 1.5320\n",
      "315/388, train_loss: 0.1867, step time: 1.5300\n",
      "316/388, train_loss: 0.1306, step time: 1.5333\n",
      "317/388, train_loss: 0.1710, step time: 1.5328\n",
      "318/388, train_loss: 0.1024, step time: 1.5315\n",
      "319/388, train_loss: 0.0823, step time: 1.5343\n",
      "320/388, train_loss: 0.0932, step time: 1.5363\n",
      "321/388, train_loss: 0.2768, step time: 1.5353\n",
      "322/388, train_loss: 0.1062, step time: 1.5357\n",
      "323/388, train_loss: 0.1482, step time: 1.5299\n",
      "324/388, train_loss: 0.1913, step time: 1.5312\n",
      "325/388, train_loss: 0.1228, step time: 1.5317\n",
      "326/388, train_loss: 0.2446, step time: 1.5315\n",
      "327/388, train_loss: 0.3069, step time: 1.5359\n",
      "328/388, train_loss: 0.0979, step time: 1.5385\n",
      "329/388, train_loss: 0.0678, step time: 1.5304\n",
      "330/388, train_loss: 0.2910, step time: 1.5288\n",
      "331/388, train_loss: 0.1211, step time: 1.5340\n",
      "332/388, train_loss: 0.1265, step time: 1.5343\n",
      "333/388, train_loss: 0.1219, step time: 1.5340\n",
      "334/388, train_loss: 0.0545, step time: 1.5358\n",
      "335/388, train_loss: 0.1159, step time: 1.5325\n",
      "336/388, train_loss: 0.3893, step time: 1.5358\n",
      "337/388, train_loss: 0.2069, step time: 1.5301\n",
      "338/388, train_loss: 0.1768, step time: 1.5330\n",
      "339/388, train_loss: 0.0915, step time: 1.5319\n",
      "340/388, train_loss: 0.0877, step time: 1.5316\n",
      "341/388, train_loss: 0.1011, step time: 1.5414\n",
      "342/388, train_loss: 0.1415, step time: 1.5383\n",
      "343/388, train_loss: 0.2585, step time: 1.5293\n",
      "344/388, train_loss: 0.1461, step time: 1.5267\n",
      "345/388, train_loss: 0.0864, step time: 1.5330\n",
      "346/388, train_loss: 0.2875, step time: 1.5342\n",
      "347/388, train_loss: 0.4998, step time: 1.5352\n",
      "348/388, train_loss: 0.0730, step time: 1.5347\n",
      "349/388, train_loss: 0.1320, step time: 1.5351\n",
      "350/388, train_loss: 0.1125, step time: 1.5315\n",
      "351/388, train_loss: 0.0614, step time: 1.5289\n",
      "352/388, train_loss: 0.1016, step time: 1.5316\n",
      "353/388, train_loss: 0.0595, step time: 1.5437\n",
      "354/388, train_loss: 0.0864, step time: 1.5337\n",
      "355/388, train_loss: 0.1925, step time: 1.5329\n",
      "356/388, train_loss: 0.0477, step time: 1.5353\n",
      "357/388, train_loss: 0.0410, step time: 1.5528\n",
      "358/388, train_loss: 0.0970, step time: 1.5317\n",
      "359/388, train_loss: 0.2556, step time: 1.5320\n",
      "360/388, train_loss: 0.1734, step time: 1.5305\n",
      "361/388, train_loss: 0.1730, step time: 1.5344\n",
      "362/388, train_loss: 0.3279, step time: 1.5302\n",
      "363/388, train_loss: 0.0853, step time: 1.5355\n",
      "364/388, train_loss: 0.1876, step time: 1.5354\n",
      "365/388, train_loss: 0.2939, step time: 1.5317\n",
      "366/388, train_loss: 0.3857, step time: 1.5310\n",
      "367/388, train_loss: 0.2884, step time: 1.5287\n",
      "368/388, train_loss: 0.1264, step time: 1.5303\n",
      "369/388, train_loss: 0.2637, step time: 1.5295\n",
      "370/388, train_loss: 0.1476, step time: 1.5351\n",
      "371/388, train_loss: 0.2249, step time: 1.5315\n",
      "372/388, train_loss: 0.1974, step time: 1.5382\n",
      "373/388, train_loss: 0.1566, step time: 1.5551\n",
      "374/388, train_loss: 0.1100, step time: 1.5316\n",
      "375/388, train_loss: 0.1727, step time: 1.5344\n",
      "376/388, train_loss: 0.1534, step time: 1.5332\n",
      "377/388, train_loss: 0.1029, step time: 1.5339\n",
      "378/388, train_loss: 0.1666, step time: 1.5355\n",
      "379/388, train_loss: 0.1113, step time: 1.5333\n",
      "380/388, train_loss: 0.0707, step time: 1.5331\n",
      "381/388, train_loss: 0.0476, step time: 1.5334\n",
      "382/388, train_loss: 0.0776, step time: 1.5396\n",
      "383/388, train_loss: 0.2625, step time: 1.5351\n",
      "384/388, train_loss: 0.2112, step time: 1.5441\n",
      "385/388, train_loss: 0.1446, step time: 1.5360\n",
      "386/388, train_loss: 0.1393, step time: 1.5327\n",
      "387/388, train_loss: 0.1230, step time: 1.5318\n",
      "388/388, train_loss: 0.1384, step time: 1.5330\n",
      "epoch 52 average loss: 0.1801\n",
      "current epoch: 52 current mean dice: 0.7550 tc: 0.7989 wt: 0.8979 et: 0.5682\n",
      "best mean dice: 0.7692 at epoch: 49\n",
      "time consuming of epoch 52 is: 701.8783\n",
      "----------\n",
      "epoch 53/100\n",
      "1/388, train_loss: 0.2205, step time: 1.5519\n",
      "2/388, train_loss: 0.2460, step time: 1.5313\n",
      "3/388, train_loss: 0.2123, step time: 1.5332\n",
      "4/388, train_loss: 0.0876, step time: 1.5310\n",
      "5/388, train_loss: 0.0716, step time: 1.5365\n",
      "6/388, train_loss: 0.2368, step time: 1.5344\n",
      "7/388, train_loss: 0.2340, step time: 1.5361\n",
      "8/388, train_loss: 0.1667, step time: 1.5363\n",
      "9/388, train_loss: 0.1261, step time: 1.5298\n",
      "10/388, train_loss: 0.2450, step time: 1.5355\n",
      "11/388, train_loss: 0.1603, step time: 1.5309\n",
      "12/388, train_loss: 0.1906, step time: 1.5359\n",
      "13/388, train_loss: 0.1201, step time: 1.5318\n",
      "14/388, train_loss: 0.0933, step time: 1.5375\n",
      "15/388, train_loss: 0.0859, step time: 1.5384\n",
      "16/388, train_loss: 0.0468, step time: 1.5324\n",
      "17/388, train_loss: 0.1032, step time: 1.5330\n",
      "18/388, train_loss: 0.3828, step time: 1.5355\n",
      "19/388, train_loss: 0.0782, step time: 1.5298\n",
      "20/388, train_loss: 0.0865, step time: 1.5322\n",
      "21/388, train_loss: 0.1189, step time: 1.5332\n",
      "22/388, train_loss: 0.1378, step time: 1.5376\n",
      "23/388, train_loss: 0.1409, step time: 1.5338\n",
      "24/388, train_loss: 0.0832, step time: 1.5325\n",
      "25/388, train_loss: 0.3683, step time: 1.5359\n",
      "26/388, train_loss: 0.2748, step time: 1.5297\n",
      "27/388, train_loss: 0.1514, step time: 1.5303\n",
      "28/388, train_loss: 0.2036, step time: 1.5372\n",
      "29/388, train_loss: 0.1161, step time: 1.5297\n",
      "30/388, train_loss: 0.0937, step time: 1.5413\n",
      "31/388, train_loss: 0.1342, step time: 1.5327\n",
      "32/388, train_loss: 0.1289, step time: 1.5315\n",
      "33/388, train_loss: 0.5153, step time: 1.5348\n",
      "34/388, train_loss: 0.1686, step time: 1.5376\n",
      "35/388, train_loss: 0.2120, step time: 1.5352\n",
      "36/388, train_loss: 0.0333, step time: 1.5308\n",
      "37/388, train_loss: 0.1356, step time: 1.5306\n",
      "38/388, train_loss: 0.1230, step time: 1.5306\n",
      "39/388, train_loss: 0.2007, step time: 1.5338\n",
      "40/388, train_loss: 0.2613, step time: 1.5367\n",
      "41/388, train_loss: 0.2024, step time: 1.5357\n",
      "42/388, train_loss: 0.1522, step time: 1.5341\n",
      "43/388, train_loss: 0.4035, step time: 1.5304\n",
      "44/388, train_loss: 0.3260, step time: 1.5341\n",
      "45/388, train_loss: 0.1118, step time: 1.5433\n",
      "46/388, train_loss: 0.2448, step time: 1.5349\n",
      "47/388, train_loss: 0.1198, step time: 1.5324\n",
      "48/388, train_loss: 0.1040, step time: 1.5331\n",
      "49/388, train_loss: 0.2781, step time: 1.5317\n",
      "50/388, train_loss: 0.1653, step time: 1.5370\n",
      "51/388, train_loss: 0.1209, step time: 1.5376\n",
      "52/388, train_loss: 0.4954, step time: 1.5342\n",
      "53/388, train_loss: 0.2073, step time: 1.5349\n",
      "54/388, train_loss: 0.1134, step time: 1.5314\n",
      "55/388, train_loss: 0.1265, step time: 1.5316\n",
      "56/388, train_loss: 0.1803, step time: 1.5349\n",
      "57/388, train_loss: 0.2137, step time: 1.5585\n",
      "58/388, train_loss: 0.3290, step time: 1.5317\n",
      "59/388, train_loss: 0.1636, step time: 1.5317\n",
      "60/388, train_loss: 0.2818, step time: 1.5445\n",
      "61/388, train_loss: 0.1024, step time: 1.5334\n",
      "62/388, train_loss: 0.3429, step time: 1.5339\n",
      "63/388, train_loss: 0.5460, step time: 1.5339\n",
      "64/388, train_loss: 0.1878, step time: 1.5344\n",
      "65/388, train_loss: 0.2464, step time: 1.5392\n",
      "66/388, train_loss: 0.2537, step time: 1.5310\n",
      "67/388, train_loss: 0.0810, step time: 1.5304\n",
      "68/388, train_loss: 0.1077, step time: 1.5299\n",
      "69/388, train_loss: 0.1827, step time: 1.5321\n",
      "70/388, train_loss: 0.0976, step time: 1.5351\n",
      "71/388, train_loss: 0.1542, step time: 1.5346\n",
      "72/388, train_loss: 0.2821, step time: 1.5341\n",
      "73/388, train_loss: 0.1059, step time: 1.5298\n",
      "74/388, train_loss: 0.4509, step time: 1.5286\n",
      "75/388, train_loss: 0.0754, step time: 1.5296\n",
      "76/388, train_loss: 0.1049, step time: 1.5292\n",
      "77/388, train_loss: 0.1935, step time: 1.5308\n",
      "78/388, train_loss: 0.1746, step time: 1.5368\n",
      "79/388, train_loss: 0.1150, step time: 1.5368\n",
      "80/388, train_loss: 0.4644, step time: 1.5332\n",
      "81/388, train_loss: 0.0973, step time: 1.5322\n",
      "82/388, train_loss: 0.2022, step time: 1.5330\n",
      "83/388, train_loss: 0.2833, step time: 1.5306\n",
      "84/388, train_loss: 0.0617, step time: 1.5328\n",
      "85/388, train_loss: 0.1959, step time: 1.5340\n",
      "86/388, train_loss: 0.3888, step time: 1.5328\n",
      "87/388, train_loss: 0.2193, step time: 1.5324\n",
      "88/388, train_loss: 0.0915, step time: 1.5367\n",
      "89/388, train_loss: 0.1847, step time: 1.5370\n",
      "90/388, train_loss: 0.1336, step time: 1.5313\n",
      "91/388, train_loss: 0.1301, step time: 1.5306\n",
      "92/388, train_loss: 0.1687, step time: 1.5320\n",
      "93/388, train_loss: 0.3627, step time: 1.5349\n",
      "94/388, train_loss: 0.1787, step time: 1.5343\n",
      "95/388, train_loss: 0.1110, step time: 1.5354\n",
      "96/388, train_loss: 0.0831, step time: 1.5324\n",
      "97/388, train_loss: 0.1405, step time: 1.5326\n",
      "98/388, train_loss: 0.1372, step time: 1.5330\n",
      "99/388, train_loss: 0.2283, step time: 1.5337\n",
      "100/388, train_loss: 0.1533, step time: 1.5361\n",
      "101/388, train_loss: 0.1943, step time: 1.5334\n",
      "102/388, train_loss: 0.2031, step time: 1.5332\n",
      "103/388, train_loss: 0.0623, step time: 1.5317\n",
      "104/388, train_loss: 0.1706, step time: 1.5308\n",
      "105/388, train_loss: 0.3466, step time: 1.5321\n",
      "106/388, train_loss: 0.1613, step time: 1.5333\n",
      "107/388, train_loss: 0.1002, step time: 1.5331\n",
      "108/388, train_loss: 0.1077, step time: 1.5318\n",
      "109/388, train_loss: 0.2317, step time: 1.5365\n",
      "110/388, train_loss: 0.2490, step time: 1.5318\n",
      "111/388, train_loss: 0.3647, step time: 1.5352\n",
      "112/388, train_loss: 0.3849, step time: 1.5356\n",
      "113/388, train_loss: 0.2000, step time: 1.5341\n",
      "114/388, train_loss: 0.1386, step time: 1.5317\n",
      "115/388, train_loss: 0.1419, step time: 1.5299\n",
      "116/388, train_loss: 0.3003, step time: 1.5325\n",
      "117/388, train_loss: 0.1397, step time: 1.5340\n",
      "118/388, train_loss: 0.1104, step time: 1.5393\n",
      "119/388, train_loss: 0.1509, step time: 1.5358\n",
      "120/388, train_loss: 0.1176, step time: 1.5357\n",
      "121/388, train_loss: 0.1075, step time: 1.5305\n",
      "122/388, train_loss: 0.1990, step time: 1.5314\n",
      "123/388, train_loss: 0.4144, step time: 1.5313\n",
      "124/388, train_loss: 0.0887, step time: 1.5309\n",
      "125/388, train_loss: 0.1605, step time: 1.5345\n",
      "126/388, train_loss: 0.1414, step time: 1.5332\n",
      "127/388, train_loss: 0.1140, step time: 1.5350\n",
      "128/388, train_loss: 0.2109, step time: 1.5354\n",
      "129/388, train_loss: 0.1062, step time: 1.5293\n",
      "130/388, train_loss: 0.3961, step time: 1.5298\n",
      "131/388, train_loss: 0.1784, step time: 1.5316\n",
      "132/388, train_loss: 0.2368, step time: 1.5352\n",
      "133/388, train_loss: 0.1930, step time: 1.5333\n",
      "134/388, train_loss: 0.2366, step time: 1.5358\n",
      "135/388, train_loss: 0.4809, step time: 1.5357\n",
      "136/388, train_loss: 0.1786, step time: 1.5339\n",
      "137/388, train_loss: 0.5506, step time: 1.5308\n",
      "138/388, train_loss: 0.0653, step time: 1.5370\n",
      "139/388, train_loss: 0.0871, step time: 1.5358\n",
      "140/388, train_loss: 0.1796, step time: 1.5343\n",
      "141/388, train_loss: 0.1407, step time: 1.5330\n",
      "142/388, train_loss: 0.1631, step time: 1.5308\n",
      "143/388, train_loss: 0.1096, step time: 1.5310\n",
      "144/388, train_loss: 0.1080, step time: 1.5303\n",
      "145/388, train_loss: 0.2038, step time: 1.5319\n",
      "146/388, train_loss: 0.1516, step time: 1.5332\n",
      "147/388, train_loss: 0.1352, step time: 1.5347\n",
      "148/388, train_loss: 0.0882, step time: 1.5319\n",
      "149/388, train_loss: 0.2526, step time: 1.5339\n",
      "150/388, train_loss: 0.1204, step time: 1.5326\n",
      "151/388, train_loss: 0.2717, step time: 1.5328\n",
      "152/388, train_loss: 0.0624, step time: 1.5292\n",
      "153/388, train_loss: 0.2736, step time: 1.5311\n",
      "154/388, train_loss: 0.1204, step time: 1.5319\n",
      "155/388, train_loss: 0.1175, step time: 1.5352\n",
      "156/388, train_loss: 0.3630, step time: 1.5290\n",
      "157/388, train_loss: 0.2723, step time: 1.5346\n",
      "158/388, train_loss: 0.1395, step time: 1.5303\n",
      "159/388, train_loss: 0.1091, step time: 1.5442\n",
      "160/388, train_loss: 0.3084, step time: 1.5342\n",
      "161/388, train_loss: 0.1388, step time: 1.5360\n",
      "162/388, train_loss: 0.4737, step time: 1.5319\n",
      "163/388, train_loss: 0.2401, step time: 1.5311\n",
      "164/388, train_loss: 0.5493, step time: 1.5283\n",
      "165/388, train_loss: 0.1310, step time: 1.5314\n",
      "166/388, train_loss: 0.1815, step time: 1.5316\n",
      "167/388, train_loss: 0.1458, step time: 1.5363\n",
      "168/388, train_loss: 0.1487, step time: 1.5355\n",
      "169/388, train_loss: 0.2146, step time: 1.5365\n",
      "170/388, train_loss: 0.2750, step time: 1.5327\n",
      "171/388, train_loss: 0.1010, step time: 1.5299\n",
      "172/388, train_loss: 0.0776, step time: 1.5304\n",
      "173/388, train_loss: 0.0957, step time: 1.5324\n",
      "174/388, train_loss: 0.2027, step time: 1.5342\n",
      "175/388, train_loss: 0.1479, step time: 1.5349\n",
      "176/388, train_loss: 0.2793, step time: 1.5347\n",
      "177/388, train_loss: 0.1595, step time: 1.5329\n",
      "178/388, train_loss: 0.1796, step time: 1.5314\n",
      "179/388, train_loss: 0.0638, step time: 1.5312\n",
      "180/388, train_loss: 0.0856, step time: 1.5319\n",
      "181/388, train_loss: 0.1045, step time: 1.5305\n",
      "182/388, train_loss: 0.1136, step time: 1.5313\n",
      "183/388, train_loss: 0.2473, step time: 1.5345\n",
      "184/388, train_loss: 0.0469, step time: 1.5348\n",
      "185/388, train_loss: 0.3557, step time: 1.5347\n",
      "186/388, train_loss: 0.1176, step time: 1.5340\n",
      "187/388, train_loss: 0.0880, step time: 1.5299\n",
      "188/388, train_loss: 0.1530, step time: 1.5308\n",
      "189/388, train_loss: 0.1277, step time: 1.5294\n",
      "190/388, train_loss: 0.1247, step time: 1.5371\n",
      "191/388, train_loss: 0.1903, step time: 1.5364\n",
      "192/388, train_loss: 0.1520, step time: 1.5351\n",
      "193/388, train_loss: 0.0464, step time: 1.5344\n",
      "194/388, train_loss: 0.1569, step time: 1.5341\n",
      "195/388, train_loss: 0.0970, step time: 1.5321\n",
      "196/388, train_loss: 0.1105, step time: 1.5327\n",
      "197/388, train_loss: 0.0879, step time: 1.5314\n",
      "198/388, train_loss: 0.1025, step time: 1.5367\n",
      "199/388, train_loss: 0.2252, step time: 1.5331\n",
      "200/388, train_loss: 0.0594, step time: 1.5352\n",
      "201/388, train_loss: 0.2549, step time: 1.5325\n",
      "202/388, train_loss: 0.0962, step time: 1.5347\n",
      "203/388, train_loss: 0.2480, step time: 1.5335\n",
      "204/388, train_loss: 0.1770, step time: 1.5343\n",
      "205/388, train_loss: 0.1008, step time: 1.5337\n",
      "206/388, train_loss: 0.4684, step time: 1.5341\n",
      "207/388, train_loss: 0.0556, step time: 1.5321\n",
      "208/388, train_loss: 0.0739, step time: 1.5314\n",
      "209/388, train_loss: 0.0989, step time: 1.5353\n",
      "210/388, train_loss: 0.3019, step time: 1.5361\n",
      "211/388, train_loss: 0.0854, step time: 1.5356\n",
      "212/388, train_loss: 0.3347, step time: 1.5305\n",
      "213/388, train_loss: 0.1205, step time: 1.5306\n",
      "214/388, train_loss: 0.2509, step time: 1.5317\n",
      "215/388, train_loss: 0.0385, step time: 1.5331\n",
      "216/388, train_loss: 0.2606, step time: 1.5344\n",
      "217/388, train_loss: 0.2097, step time: 1.5351\n",
      "218/388, train_loss: 0.0257, step time: 1.5343\n",
      "219/388, train_loss: 0.0837, step time: 1.5340\n",
      "220/388, train_loss: 0.1527, step time: 1.5330\n",
      "221/388, train_loss: 0.0972, step time: 1.5317\n",
      "222/388, train_loss: 0.2288, step time: 1.5310\n",
      "223/388, train_loss: 0.2091, step time: 1.5342\n",
      "224/388, train_loss: 0.1018, step time: 1.5375\n",
      "225/388, train_loss: 0.1972, step time: 1.5356\n",
      "226/388, train_loss: 0.2266, step time: 1.5306\n",
      "227/388, train_loss: 0.2179, step time: 1.5292\n",
      "228/388, train_loss: 0.2076, step time: 1.5371\n",
      "229/388, train_loss: 0.3971, step time: 1.5351\n",
      "230/388, train_loss: 0.1177, step time: 1.5356\n",
      "231/388, train_loss: 0.3987, step time: 1.5350\n",
      "232/388, train_loss: 0.0705, step time: 1.5340\n",
      "233/388, train_loss: 0.2843, step time: 1.5340\n",
      "234/388, train_loss: 0.2635, step time: 1.5314\n",
      "235/388, train_loss: 0.1079, step time: 1.5321\n",
      "236/388, train_loss: 0.2796, step time: 1.5323\n",
      "237/388, train_loss: 0.1188, step time: 1.5353\n",
      "238/388, train_loss: 0.1452, step time: 1.5358\n",
      "239/388, train_loss: 0.2123, step time: 1.5302\n",
      "240/388, train_loss: 0.2360, step time: 1.5341\n",
      "241/388, train_loss: 0.2180, step time: 1.5309\n",
      "242/388, train_loss: 0.1746, step time: 1.5391\n",
      "243/388, train_loss: 0.1293, step time: 1.5365\n",
      "244/388, train_loss: 0.4481, step time: 1.5384\n",
      "245/388, train_loss: 0.1776, step time: 1.5330\n",
      "246/388, train_loss: 0.1603, step time: 1.5321\n",
      "247/388, train_loss: 0.0813, step time: 1.5297\n",
      "248/388, train_loss: 0.1338, step time: 1.5340\n",
      "249/388, train_loss: 0.1065, step time: 1.5314\n",
      "250/388, train_loss: 0.0650, step time: 1.5354\n",
      "251/388, train_loss: 0.4263, step time: 1.5340\n",
      "252/388, train_loss: 0.0916, step time: 1.5328\n",
      "253/388, train_loss: 0.0655, step time: 1.5307\n",
      "254/388, train_loss: 0.2735, step time: 1.5334\n",
      "255/388, train_loss: 0.2815, step time: 1.5339\n",
      "256/388, train_loss: 0.2586, step time: 1.5330\n",
      "257/388, train_loss: 0.1123, step time: 1.5342\n",
      "258/388, train_loss: 0.2395, step time: 1.5335\n",
      "259/388, train_loss: 0.1243, step time: 1.5347\n",
      "260/388, train_loss: 0.0964, step time: 1.5289\n",
      "261/388, train_loss: 0.1336, step time: 1.5302\n",
      "262/388, train_loss: 0.2153, step time: 1.5304\n",
      "263/388, train_loss: 0.0734, step time: 1.5321\n",
      "264/388, train_loss: 0.5237, step time: 1.5295\n",
      "265/388, train_loss: 0.1469, step time: 1.5382\n",
      "266/388, train_loss: 0.0998, step time: 1.5359\n",
      "267/388, train_loss: 0.2084, step time: 1.5367\n",
      "268/388, train_loss: 0.0890, step time: 1.5314\n",
      "269/388, train_loss: 0.0825, step time: 1.5384\n",
      "270/388, train_loss: 0.0945, step time: 1.5308\n",
      "271/388, train_loss: 0.1794, step time: 1.5327\n",
      "272/388, train_loss: 0.0646, step time: 1.5419\n",
      "273/388, train_loss: 0.2853, step time: 1.5332\n",
      "274/388, train_loss: 0.0891, step time: 1.5347\n",
      "275/388, train_loss: 0.2433, step time: 1.5310\n",
      "276/388, train_loss: 0.1116, step time: 1.5307\n",
      "277/388, train_loss: 0.0769, step time: 1.5337\n",
      "278/388, train_loss: 0.4366, step time: 1.5312\n",
      "279/388, train_loss: 0.0998, step time: 1.5343\n",
      "280/388, train_loss: 0.0719, step time: 1.5333\n",
      "281/388, train_loss: 0.2114, step time: 1.5334\n",
      "282/388, train_loss: 0.2849, step time: 1.5336\n",
      "283/388, train_loss: 0.0825, step time: 1.5293\n",
      "284/388, train_loss: 0.0627, step time: 1.5293\n",
      "285/388, train_loss: 0.0476, step time: 1.5353\n",
      "286/388, train_loss: 0.0906, step time: 1.5343\n",
      "287/388, train_loss: 0.0938, step time: 1.5314\n",
      "288/388, train_loss: 0.1467, step time: 1.5322\n",
      "289/388, train_loss: 0.1297, step time: 1.5299\n",
      "290/388, train_loss: 0.0498, step time: 1.5312\n",
      "291/388, train_loss: 0.0466, step time: 1.5320\n",
      "292/388, train_loss: 0.1830, step time: 1.5318\n",
      "293/388, train_loss: 0.1052, step time: 1.5305\n",
      "294/388, train_loss: 0.0676, step time: 1.5329\n",
      "295/388, train_loss: 0.1443, step time: 1.5342\n",
      "296/388, train_loss: 0.2644, step time: 1.5356\n",
      "297/388, train_loss: 0.2778, step time: 1.5315\n",
      "298/388, train_loss: 0.0437, step time: 1.5308\n",
      "299/388, train_loss: 0.0920, step time: 1.5308\n",
      "300/388, train_loss: 0.2145, step time: 1.5314\n",
      "301/388, train_loss: 0.2213, step time: 1.5339\n",
      "302/388, train_loss: 0.1487, step time: 1.5321\n",
      "303/388, train_loss: 0.1992, step time: 1.5298\n",
      "304/388, train_loss: 0.1168, step time: 1.5304\n",
      "305/388, train_loss: 0.3485, step time: 1.5320\n",
      "306/388, train_loss: 0.1328, step time: 1.5316\n",
      "307/388, train_loss: 0.2256, step time: 1.5339\n",
      "308/388, train_loss: 0.5051, step time: 1.5324\n",
      "309/388, train_loss: 0.2308, step time: 1.5344\n",
      "310/388, train_loss: 0.2937, step time: 1.5291\n",
      "311/388, train_loss: 0.2153, step time: 1.5346\n",
      "312/388, train_loss: 0.2933, step time: 1.5334\n",
      "313/388, train_loss: 0.1713, step time: 1.5347\n",
      "314/388, train_loss: 0.2240, step time: 1.5356\n",
      "315/388, train_loss: 0.0864, step time: 1.5362\n",
      "316/388, train_loss: 0.1571, step time: 1.5313\n",
      "317/388, train_loss: 0.1423, step time: 1.5329\n",
      "318/388, train_loss: 0.1758, step time: 1.5326\n",
      "319/388, train_loss: 0.1077, step time: 1.5305\n",
      "320/388, train_loss: 0.1863, step time: 1.5301\n",
      "321/388, train_loss: 0.1981, step time: 1.5361\n",
      "322/388, train_loss: 0.2666, step time: 1.5544\n",
      "323/388, train_loss: 0.2692, step time: 1.5321\n",
      "324/388, train_loss: 0.2368, step time: 1.5280\n",
      "325/388, train_loss: 0.1803, step time: 1.5300\n",
      "326/388, train_loss: 0.4581, step time: 1.5328\n",
      "327/388, train_loss: 0.2722, step time: 1.5374\n",
      "328/388, train_loss: 0.0744, step time: 1.5352\n",
      "329/388, train_loss: 0.1345, step time: 1.5307\n",
      "330/388, train_loss: 0.1698, step time: 1.5320\n",
      "331/388, train_loss: 0.0971, step time: 1.5324\n",
      "332/388, train_loss: 0.1913, step time: 1.5398\n",
      "333/388, train_loss: 0.2689, step time: 1.5417\n",
      "334/388, train_loss: 0.3087, step time: 1.5365\n",
      "335/388, train_loss: 0.3037, step time: 1.5299\n",
      "336/388, train_loss: 0.1615, step time: 1.5346\n",
      "337/388, train_loss: 0.2489, step time: 1.5293\n",
      "338/388, train_loss: 0.1200, step time: 1.5313\n",
      "339/388, train_loss: 0.0943, step time: 1.5355\n",
      "340/388, train_loss: 0.3824, step time: 1.5373\n",
      "341/388, train_loss: 0.1125, step time: 1.5353\n",
      "342/388, train_loss: 0.2230, step time: 1.5336\n",
      "343/388, train_loss: 0.1490, step time: 1.5328\n",
      "344/388, train_loss: 0.0847, step time: 1.5318\n",
      "345/388, train_loss: 0.0750, step time: 1.5340\n",
      "346/388, train_loss: 0.0932, step time: 1.5343\n",
      "347/388, train_loss: 0.2952, step time: 1.5370\n",
      "348/388, train_loss: 0.1924, step time: 1.5329\n",
      "349/388, train_loss: 0.0968, step time: 1.5324\n",
      "350/388, train_loss: 0.1833, step time: 1.5354\n",
      "351/388, train_loss: 0.1335, step time: 1.5299\n",
      "352/388, train_loss: 0.1089, step time: 1.5567\n",
      "353/388, train_loss: 0.0634, step time: 1.5342\n",
      "354/388, train_loss: 0.0994, step time: 1.5353\n",
      "355/388, train_loss: 0.2590, step time: 1.5365\n",
      "356/388, train_loss: 0.0674, step time: 1.5340\n",
      "357/388, train_loss: 0.1238, step time: 1.5332\n",
      "358/388, train_loss: 0.2224, step time: 1.5321\n",
      "359/388, train_loss: 0.0453, step time: 1.5307\n",
      "360/388, train_loss: 0.2473, step time: 1.5351\n",
      "361/388, train_loss: 0.1651, step time: 1.5372\n",
      "362/388, train_loss: 0.3882, step time: 1.5310\n",
      "363/388, train_loss: 0.1565, step time: 1.5283\n",
      "364/388, train_loss: 0.0961, step time: 1.5316\n",
      "365/388, train_loss: 0.3218, step time: 1.5308\n",
      "366/388, train_loss: 0.1535, step time: 1.5337\n",
      "367/388, train_loss: 0.1425, step time: 1.5370\n",
      "368/388, train_loss: 0.1415, step time: 1.5339\n",
      "369/388, train_loss: 0.2976, step time: 1.5325\n",
      "370/388, train_loss: 0.0717, step time: 1.5302\n",
      "371/388, train_loss: 0.2077, step time: 1.5308\n",
      "372/388, train_loss: 0.0545, step time: 1.5326\n",
      "373/388, train_loss: 0.0823, step time: 1.5339\n",
      "374/388, train_loss: 0.2090, step time: 1.5381\n",
      "375/388, train_loss: 0.0831, step time: 1.5379\n",
      "376/388, train_loss: 0.2131, step time: 1.5334\n",
      "377/388, train_loss: 0.0891, step time: 1.5327\n",
      "378/388, train_loss: 0.2062, step time: 1.5313\n",
      "379/388, train_loss: 0.1470, step time: 1.5293\n",
      "380/388, train_loss: 0.1499, step time: 1.5354\n",
      "381/388, train_loss: 0.0716, step time: 1.5351\n",
      "382/388, train_loss: 0.1930, step time: 1.5316\n",
      "383/388, train_loss: 0.2724, step time: 1.5335\n",
      "384/388, train_loss: 0.2064, step time: 1.5312\n",
      "385/388, train_loss: 0.1184, step time: 1.5335\n",
      "386/388, train_loss: 0.1465, step time: 1.5298\n",
      "387/388, train_loss: 0.0548, step time: 1.5280\n",
      "388/388, train_loss: 0.1214, step time: 1.5337\n",
      "epoch 53 average loss: 0.1817\n",
      "current epoch: 53 current mean dice: 0.7551 tc: 0.7931 wt: 0.9048 et: 0.5674\n",
      "best mean dice: 0.7692 at epoch: 49\n",
      "time consuming of epoch 53 is: 701.4781\n",
      "----------\n",
      "epoch 54/100\n",
      "1/388, train_loss: 0.2072, step time: 1.5436\n",
      "2/388, train_loss: 0.2558, step time: 1.5349\n",
      "3/388, train_loss: 0.2199, step time: 1.5363\n",
      "4/388, train_loss: 0.0947, step time: 1.5330\n",
      "5/388, train_loss: 0.1867, step time: 1.5344\n",
      "6/388, train_loss: 0.1137, step time: 1.5360\n",
      "7/388, train_loss: 0.0731, step time: 1.5339\n",
      "8/388, train_loss: 0.0918, step time: 1.5366\n",
      "9/388, train_loss: 0.1766, step time: 1.5330\n",
      "10/388, train_loss: 0.2016, step time: 1.5360\n",
      "11/388, train_loss: 0.0909, step time: 1.5372\n",
      "12/388, train_loss: 0.1292, step time: 1.5315\n",
      "13/388, train_loss: 0.1285, step time: 1.5318\n",
      "14/388, train_loss: 0.3327, step time: 1.5340\n",
      "15/388, train_loss: 0.1598, step time: 1.5385\n",
      "16/388, train_loss: 0.0650, step time: 1.5353\n",
      "17/388, train_loss: 0.1663, step time: 1.5342\n",
      "18/388, train_loss: 0.1077, step time: 1.5329\n",
      "19/388, train_loss: 0.0966, step time: 1.5340\n",
      "20/388, train_loss: 0.2802, step time: 1.5323\n",
      "21/388, train_loss: 0.5881, step time: 1.5321\n",
      "22/388, train_loss: 0.1079, step time: 1.5371\n",
      "23/388, train_loss: 0.1815, step time: 1.5317\n",
      "24/388, train_loss: 0.1015, step time: 1.5302\n",
      "25/388, train_loss: 0.1309, step time: 1.5300\n",
      "26/388, train_loss: 0.2130, step time: 1.5338\n",
      "27/388, train_loss: 0.1374, step time: 1.5395\n",
      "28/388, train_loss: 0.1683, step time: 1.5386\n",
      "29/388, train_loss: 0.0749, step time: 1.5377\n",
      "30/388, train_loss: 0.0988, step time: 1.5298\n",
      "31/388, train_loss: 0.0999, step time: 1.5332\n",
      "32/388, train_loss: 0.1501, step time: 1.5375\n",
      "33/388, train_loss: 0.1298, step time: 1.5348\n",
      "34/388, train_loss: 0.2769, step time: 1.5335\n",
      "35/388, train_loss: 0.1068, step time: 1.5316\n",
      "36/388, train_loss: 0.3067, step time: 1.5323\n",
      "37/388, train_loss: 0.1732, step time: 1.5333\n",
      "38/388, train_loss: 0.1164, step time: 1.5339\n",
      "39/388, train_loss: 0.0851, step time: 1.5298\n",
      "40/388, train_loss: 0.1351, step time: 1.5323\n",
      "41/388, train_loss: 0.1808, step time: 1.5333\n",
      "42/388, train_loss: 0.1150, step time: 1.5375\n",
      "43/388, train_loss: 0.0856, step time: 1.5348\n",
      "44/388, train_loss: 0.0958, step time: 1.5359\n",
      "45/388, train_loss: 0.3855, step time: 1.5330\n",
      "46/388, train_loss: 0.1041, step time: 1.5350\n",
      "47/388, train_loss: 0.4522, step time: 1.5319\n",
      "48/388, train_loss: 0.1217, step time: 1.5303\n",
      "49/388, train_loss: 0.0840, step time: 1.5309\n",
      "50/388, train_loss: 0.2173, step time: 1.5339\n",
      "51/388, train_loss: 0.2110, step time: 1.5424\n",
      "52/388, train_loss: 0.3775, step time: 1.5372\n",
      "53/388, train_loss: 0.1431, step time: 1.5319\n",
      "54/388, train_loss: 0.1631, step time: 1.5344\n",
      "55/388, train_loss: 0.1418, step time: 1.5303\n",
      "56/388, train_loss: 0.1456, step time: 1.5337\n",
      "57/388, train_loss: 0.2437, step time: 1.5336\n",
      "58/388, train_loss: 0.0487, step time: 1.5355\n",
      "59/388, train_loss: 0.1684, step time: 1.5358\n",
      "60/388, train_loss: 0.1721, step time: 1.5342\n",
      "61/388, train_loss: 0.3516, step time: 1.5296\n",
      "62/388, train_loss: 0.2106, step time: 1.5311\n",
      "63/388, train_loss: 0.2123, step time: 1.5400\n",
      "64/388, train_loss: 0.0609, step time: 1.5440\n",
      "65/388, train_loss: 0.2307, step time: 1.5314\n",
      "66/388, train_loss: 0.3795, step time: 1.5322\n",
      "67/388, train_loss: 0.1945, step time: 1.5417\n",
      "68/388, train_loss: 0.2210, step time: 1.5337\n",
      "69/388, train_loss: 0.0940, step time: 1.5326\n",
      "70/388, train_loss: 0.4811, step time: 1.5335\n",
      "71/388, train_loss: 0.3587, step time: 1.5289\n",
      "72/388, train_loss: 0.3005, step time: 1.5301\n",
      "73/388, train_loss: 0.1678, step time: 1.5324\n",
      "74/388, train_loss: 0.2698, step time: 1.5295\n",
      "75/388, train_loss: 0.1218, step time: 1.5290\n",
      "76/388, train_loss: 0.3684, step time: 1.5321\n",
      "77/388, train_loss: 0.0569, step time: 1.5325\n",
      "78/388, train_loss: 0.1970, step time: 1.5415\n",
      "79/388, train_loss: 0.1036, step time: 1.5306\n",
      "80/388, train_loss: 0.1709, step time: 1.5307\n",
      "81/388, train_loss: 0.1544, step time: 1.5306\n",
      "82/388, train_loss: 0.2711, step time: 1.5301\n",
      "83/388, train_loss: 0.0577, step time: 1.5394\n",
      "84/388, train_loss: 0.1488, step time: 1.5314\n",
      "85/388, train_loss: 0.1039, step time: 1.5325\n",
      "86/388, train_loss: 0.2206, step time: 1.5302\n",
      "87/388, train_loss: 0.2470, step time: 1.5325\n",
      "88/388, train_loss: 0.3283, step time: 1.5364\n",
      "89/388, train_loss: 0.1457, step time: 1.5362\n",
      "90/388, train_loss: 0.1617, step time: 1.5314\n",
      "91/388, train_loss: 0.2884, step time: 1.5326\n",
      "92/388, train_loss: 0.3918, step time: 1.5307\n",
      "93/388, train_loss: 0.0499, step time: 1.5303\n",
      "94/388, train_loss: 0.2026, step time: 1.5352\n",
      "95/388, train_loss: 0.1116, step time: 1.5310\n",
      "96/388, train_loss: 0.2053, step time: 1.5314\n",
      "97/388, train_loss: 0.1393, step time: 1.5329\n",
      "98/388, train_loss: 0.2415, step time: 1.5311\n",
      "99/388, train_loss: 0.1258, step time: 1.5319\n",
      "100/388, train_loss: 0.1176, step time: 1.5278\n",
      "101/388, train_loss: 0.0993, step time: 1.5308\n",
      "102/388, train_loss: 0.0908, step time: 1.5414\n",
      "103/388, train_loss: 0.1514, step time: 1.5354\n",
      "104/388, train_loss: 0.0994, step time: 1.5352\n",
      "105/388, train_loss: 0.0935, step time: 1.5357\n",
      "106/388, train_loss: 0.1771, step time: 1.5292\n",
      "107/388, train_loss: 0.0964, step time: 1.5342\n",
      "108/388, train_loss: 0.0827, step time: 1.5343\n",
      "109/388, train_loss: 0.1324, step time: 1.5327\n",
      "110/388, train_loss: 0.2841, step time: 1.5363\n",
      "111/388, train_loss: 0.1702, step time: 1.5306\n",
      "112/388, train_loss: 0.0826, step time: 1.5308\n",
      "113/388, train_loss: 0.1113, step time: 1.5363\n",
      "114/388, train_loss: 0.3523, step time: 1.5338\n",
      "115/388, train_loss: 0.1756, step time: 1.5361\n",
      "116/388, train_loss: 0.1605, step time: 1.5356\n",
      "117/388, train_loss: 0.2723, step time: 1.5341\n",
      "118/388, train_loss: 0.1800, step time: 1.5326\n",
      "119/388, train_loss: 0.1659, step time: 1.5310\n",
      "120/388, train_loss: 0.2833, step time: 1.5288\n",
      "121/388, train_loss: 0.1084, step time: 1.5324\n",
      "122/388, train_loss: 0.0804, step time: 1.5323\n",
      "123/388, train_loss: 0.1990, step time: 1.5357\n",
      "124/388, train_loss: 0.1108, step time: 1.5347\n",
      "125/388, train_loss: 0.0903, step time: 1.5374\n",
      "126/388, train_loss: 0.2240, step time: 1.5304\n",
      "127/388, train_loss: 0.0843, step time: 1.5292\n",
      "128/388, train_loss: 0.1482, step time: 1.5290\n",
      "129/388, train_loss: 0.0860, step time: 1.5344\n",
      "130/388, train_loss: 0.1597, step time: 1.5442\n",
      "131/388, train_loss: 0.1207, step time: 1.5548\n",
      "132/388, train_loss: 0.2284, step time: 1.5301\n",
      "133/388, train_loss: 0.1531, step time: 1.5349\n",
      "134/388, train_loss: 0.1071, step time: 1.5336\n",
      "135/388, train_loss: 0.1549, step time: 1.5319\n",
      "136/388, train_loss: 0.0447, step time: 1.5344\n",
      "137/388, train_loss: 0.2417, step time: 1.5318\n",
      "138/388, train_loss: 0.1594, step time: 1.5299\n",
      "139/388, train_loss: 0.0980, step time: 1.5299\n",
      "140/388, train_loss: 0.3644, step time: 1.5300\n",
      "141/388, train_loss: 0.0932, step time: 1.5337\n",
      "142/388, train_loss: 0.2669, step time: 1.5336\n",
      "143/388, train_loss: 0.1206, step time: 1.5355\n",
      "144/388, train_loss: 0.1324, step time: 1.5342\n",
      "145/388, train_loss: 0.3463, step time: 1.5311\n",
      "146/388, train_loss: 0.1079, step time: 1.5356\n",
      "147/388, train_loss: 0.1332, step time: 1.5339\n",
      "148/388, train_loss: 0.3891, step time: 1.5301\n",
      "149/388, train_loss: 0.1504, step time: 1.5317\n",
      "150/388, train_loss: 0.4091, step time: 1.5317\n",
      "151/388, train_loss: 0.0498, step time: 1.5397\n",
      "152/388, train_loss: 0.3520, step time: 1.5375\n",
      "153/388, train_loss: 0.1262, step time: 1.5318\n",
      "154/388, train_loss: 0.3547, step time: 1.5291\n",
      "155/388, train_loss: 0.0998, step time: 1.5323\n",
      "156/388, train_loss: 0.1448, step time: 1.5329\n",
      "157/388, train_loss: 0.0551, step time: 1.5328\n",
      "158/388, train_loss: 0.1140, step time: 1.5378\n",
      "159/388, train_loss: 0.2348, step time: 1.5367\n",
      "160/388, train_loss: 0.2641, step time: 1.5370\n",
      "161/388, train_loss: 0.1808, step time: 1.5299\n",
      "162/388, train_loss: 0.1060, step time: 1.5334\n",
      "163/388, train_loss: 0.1220, step time: 1.5329\n",
      "164/388, train_loss: 0.3008, step time: 1.5298\n",
      "165/388, train_loss: 0.0464, step time: 1.5404\n",
      "166/388, train_loss: 0.1426, step time: 1.5367\n",
      "167/388, train_loss: 0.2996, step time: 1.5345\n",
      "168/388, train_loss: 0.4304, step time: 1.5324\n",
      "169/388, train_loss: 0.0557, step time: 1.5326\n",
      "170/388, train_loss: 0.3057, step time: 1.5290\n",
      "171/388, train_loss: 0.1180, step time: 1.5339\n",
      "172/388, train_loss: 0.1149, step time: 1.5370\n",
      "173/388, train_loss: 0.1345, step time: 1.5339\n",
      "174/388, train_loss: 0.0311, step time: 1.5309\n",
      "175/388, train_loss: 0.2316, step time: 1.5318\n",
      "176/388, train_loss: 0.1998, step time: 1.5316\n",
      "177/388, train_loss: 0.1603, step time: 1.5290\n",
      "178/388, train_loss: 0.0989, step time: 1.5278\n",
      "179/388, train_loss: 0.0879, step time: 1.5379\n",
      "180/388, train_loss: 0.1396, step time: 1.5332\n",
      "181/388, train_loss: 0.2650, step time: 1.5375\n",
      "182/388, train_loss: 0.1580, step time: 1.5336\n",
      "183/388, train_loss: 0.2682, step time: 1.5312\n",
      "184/388, train_loss: 0.0806, step time: 1.5387\n",
      "185/388, train_loss: 0.1567, step time: 1.5340\n",
      "186/388, train_loss: 0.0546, step time: 1.5327\n",
      "187/388, train_loss: 0.2172, step time: 1.5311\n",
      "188/388, train_loss: 0.1107, step time: 1.5323\n",
      "189/388, train_loss: 0.1149, step time: 1.5302\n",
      "190/388, train_loss: 0.1841, step time: 1.5348\n",
      "191/388, train_loss: 0.2696, step time: 1.5365\n",
      "192/388, train_loss: 0.1979, step time: 1.5328\n",
      "193/388, train_loss: 0.3281, step time: 1.5322\n",
      "194/388, train_loss: 0.4324, step time: 1.5298\n",
      "195/388, train_loss: 0.1846, step time: 1.5281\n",
      "196/388, train_loss: 0.2576, step time: 1.5309\n",
      "197/388, train_loss: 0.2049, step time: 1.5338\n",
      "198/388, train_loss: 0.2385, step time: 1.5322\n",
      "199/388, train_loss: 0.0988, step time: 1.5331\n",
      "200/388, train_loss: 0.1369, step time: 1.5343\n",
      "201/388, train_loss: 0.0783, step time: 1.5328\n",
      "202/388, train_loss: 0.1321, step time: 1.5339\n",
      "203/388, train_loss: 0.0714, step time: 1.5320\n",
      "204/388, train_loss: 0.1488, step time: 1.5320\n",
      "205/388, train_loss: 0.1321, step time: 1.5296\n",
      "206/388, train_loss: 0.1235, step time: 1.5379\n",
      "207/388, train_loss: 0.1210, step time: 1.5608\n",
      "208/388, train_loss: 0.2108, step time: 1.5313\n",
      "209/388, train_loss: 0.2442, step time: 1.5333\n",
      "210/388, train_loss: 0.0846, step time: 1.5339\n",
      "211/388, train_loss: 0.1201, step time: 1.5387\n",
      "212/388, train_loss: 0.0304, step time: 1.5324\n",
      "213/388, train_loss: 0.1091, step time: 1.5305\n",
      "214/388, train_loss: 0.2013, step time: 1.5296\n",
      "215/388, train_loss: 0.1181, step time: 1.5313\n",
      "216/388, train_loss: 0.1721, step time: 1.5338\n",
      "217/388, train_loss: 0.1636, step time: 1.5347\n",
      "218/388, train_loss: 0.0631, step time: 1.5330\n",
      "219/388, train_loss: 0.2788, step time: 1.5311\n",
      "220/388, train_loss: 0.2174, step time: 1.5472\n",
      "221/388, train_loss: 0.0641, step time: 1.5299\n",
      "222/388, train_loss: 0.0578, step time: 1.5370\n",
      "223/388, train_loss: 0.3146, step time: 1.5370\n",
      "224/388, train_loss: 0.0534, step time: 1.5381\n",
      "225/388, train_loss: 0.0722, step time: 1.5339\n",
      "226/388, train_loss: 0.1372, step time: 1.5337\n",
      "227/388, train_loss: 0.1825, step time: 1.5358\n",
      "228/388, train_loss: 0.4185, step time: 1.5363\n",
      "229/388, train_loss: 0.1228, step time: 1.5340\n",
      "230/388, train_loss: 0.1026, step time: 1.5300\n",
      "231/388, train_loss: 0.4330, step time: 1.5293\n",
      "232/388, train_loss: 0.2330, step time: 1.5328\n",
      "233/388, train_loss: 0.0887, step time: 1.5330\n",
      "234/388, train_loss: 0.1737, step time: 1.5315\n",
      "235/388, train_loss: 0.3117, step time: 1.5373\n",
      "236/388, train_loss: 0.1692, step time: 1.5308\n",
      "237/388, train_loss: 0.1622, step time: 1.5350\n",
      "238/388, train_loss: 0.2448, step time: 1.5340\n",
      "239/388, train_loss: 0.2119, step time: 1.5306\n",
      "240/388, train_loss: 0.3210, step time: 1.5326\n",
      "241/388, train_loss: 0.3470, step time: 1.5289\n",
      "242/388, train_loss: 0.0973, step time: 1.5322\n",
      "243/388, train_loss: 0.0628, step time: 1.5333\n",
      "244/388, train_loss: 0.2306, step time: 1.5359\n",
      "245/388, train_loss: 0.4468, step time: 1.5359\n",
      "246/388, train_loss: 0.1368, step time: 1.5307\n",
      "247/388, train_loss: 0.1739, step time: 1.5314\n",
      "248/388, train_loss: 0.1798, step time: 1.5350\n",
      "249/388, train_loss: 0.2310, step time: 1.5273\n",
      "250/388, train_loss: 0.0770, step time: 1.5295\n",
      "251/388, train_loss: 0.1065, step time: 1.5314\n",
      "252/388, train_loss: 0.2081, step time: 1.5344\n",
      "253/388, train_loss: 0.1036, step time: 1.5327\n",
      "254/388, train_loss: 0.4303, step time: 1.5367\n",
      "255/388, train_loss: 0.5070, step time: 1.5306\n",
      "256/388, train_loss: 0.2401, step time: 1.5312\n",
      "257/388, train_loss: 0.1661, step time: 1.5312\n",
      "258/388, train_loss: 0.2548, step time: 1.5374\n",
      "259/388, train_loss: 0.0619, step time: 1.5389\n",
      "260/388, train_loss: 0.1735, step time: 1.5350\n",
      "261/388, train_loss: 0.0382, step time: 1.5305\n",
      "262/388, train_loss: 0.0582, step time: 1.5339\n",
      "263/388, train_loss: 0.1940, step time: 1.5310\n",
      "264/388, train_loss: 0.1305, step time: 1.5321\n",
      "265/388, train_loss: 0.0950, step time: 1.5300\n",
      "266/388, train_loss: 0.1468, step time: 1.5340\n",
      "267/388, train_loss: 0.1879, step time: 1.5340\n",
      "268/388, train_loss: 0.1016, step time: 1.5353\n",
      "269/388, train_loss: 0.2188, step time: 1.5333\n",
      "270/388, train_loss: 0.0916, step time: 1.5330\n",
      "271/388, train_loss: 0.0865, step time: 1.5278\n",
      "272/388, train_loss: 0.1486, step time: 1.5337\n",
      "273/388, train_loss: 0.1275, step time: 1.5330\n",
      "274/388, train_loss: 0.1955, step time: 1.5321\n",
      "275/388, train_loss: 0.1639, step time: 1.5334\n",
      "276/388, train_loss: 0.2352, step time: 1.5344\n",
      "277/388, train_loss: 0.0864, step time: 1.5302\n",
      "278/388, train_loss: 0.0936, step time: 1.5338\n",
      "279/388, train_loss: 0.1601, step time: 1.5387\n",
      "280/388, train_loss: 0.0563, step time: 1.5362\n",
      "281/388, train_loss: 0.1351, step time: 1.5322\n",
      "282/388, train_loss: 0.2896, step time: 1.5338\n",
      "283/388, train_loss: 0.2048, step time: 1.5308\n",
      "284/388, train_loss: 0.5110, step time: 1.5305\n",
      "285/388, train_loss: 0.0912, step time: 1.5362\n",
      "286/388, train_loss: 0.0560, step time: 1.5331\n",
      "287/388, train_loss: 0.2481, step time: 1.5352\n",
      "288/388, train_loss: 0.1493, step time: 1.5358\n",
      "289/388, train_loss: 0.1304, step time: 1.5306\n",
      "290/388, train_loss: 0.3683, step time: 1.5322\n",
      "291/388, train_loss: 0.2036, step time: 1.5369\n",
      "292/388, train_loss: 0.0772, step time: 1.5535\n",
      "293/388, train_loss: 0.1743, step time: 1.5332\n",
      "294/388, train_loss: 0.2138, step time: 1.5322\n",
      "295/388, train_loss: 0.2070, step time: 1.5384\n",
      "296/388, train_loss: 0.2695, step time: 1.5291\n",
      "297/388, train_loss: 0.1775, step time: 1.5299\n",
      "298/388, train_loss: 0.1024, step time: 1.5342\n",
      "299/388, train_loss: 0.1678, step time: 1.5334\n",
      "300/388, train_loss: 0.2896, step time: 1.5347\n",
      "301/388, train_loss: 0.1097, step time: 1.5349\n",
      "302/388, train_loss: 0.2101, step time: 1.5283\n",
      "303/388, train_loss: 0.1796, step time: 1.5286\n",
      "304/388, train_loss: 0.2064, step time: 1.5406\n",
      "305/388, train_loss: 0.0854, step time: 1.5381\n",
      "306/388, train_loss: 0.1044, step time: 1.5334\n",
      "307/388, train_loss: 0.1002, step time: 1.5326\n",
      "308/388, train_loss: 0.1418, step time: 1.5315\n",
      "309/388, train_loss: 0.0740, step time: 1.5321\n",
      "310/388, train_loss: 0.1569, step time: 1.5330\n",
      "311/388, train_loss: 0.2042, step time: 1.5354\n",
      "312/388, train_loss: 0.2178, step time: 1.5333\n",
      "313/388, train_loss: 0.6075, step time: 1.5330\n",
      "314/388, train_loss: 0.2076, step time: 1.5310\n",
      "315/388, train_loss: 0.1218, step time: 1.5300\n",
      "316/388, train_loss: 0.1884, step time: 1.5314\n",
      "317/388, train_loss: 0.0347, step time: 1.5317\n",
      "318/388, train_loss: 0.3748, step time: 1.5404\n",
      "319/388, train_loss: 0.1308, step time: 1.5357\n",
      "320/388, train_loss: 0.1033, step time: 1.5300\n",
      "321/388, train_loss: 0.3540, step time: 1.5323\n",
      "322/388, train_loss: 0.2103, step time: 1.5340\n",
      "323/388, train_loss: 0.0852, step time: 1.5306\n",
      "324/388, train_loss: 0.1244, step time: 1.5327\n",
      "325/388, train_loss: 0.2029, step time: 1.5304\n",
      "326/388, train_loss: 0.2726, step time: 1.5332\n",
      "327/388, train_loss: 0.2796, step time: 1.5358\n",
      "328/388, train_loss: 0.3292, step time: 1.5339\n",
      "329/388, train_loss: 0.2288, step time: 1.5331\n",
      "330/388, train_loss: 0.2449, step time: 1.5287\n",
      "331/388, train_loss: 0.1003, step time: 1.5298\n",
      "332/388, train_loss: 0.1446, step time: 1.5310\n",
      "333/388, train_loss: 0.1710, step time: 1.5307\n",
      "334/388, train_loss: 0.2660, step time: 1.5325\n",
      "335/388, train_loss: 0.3011, step time: 1.5334\n",
      "336/388, train_loss: 0.1981, step time: 1.5345\n",
      "337/388, train_loss: 0.1372, step time: 1.5301\n",
      "338/388, train_loss: 0.1364, step time: 1.5286\n",
      "339/388, train_loss: 0.1746, step time: 1.5337\n",
      "340/388, train_loss: 0.3007, step time: 1.5541\n",
      "341/388, train_loss: 0.0684, step time: 1.5311\n",
      "342/388, train_loss: 0.2707, step time: 1.5288\n",
      "343/388, train_loss: 0.2065, step time: 1.5326\n",
      "344/388, train_loss: 0.2257, step time: 1.5282\n",
      "345/388, train_loss: 0.1168, step time: 1.5337\n",
      "346/388, train_loss: 0.2532, step time: 1.5333\n",
      "347/388, train_loss: 0.2133, step time: 1.5355\n",
      "348/388, train_loss: 0.0836, step time: 1.5356\n",
      "349/388, train_loss: 0.2468, step time: 1.5262\n",
      "350/388, train_loss: 0.2313, step time: 1.5334\n",
      "351/388, train_loss: 0.2338, step time: 1.5323\n",
      "352/388, train_loss: 0.3369, step time: 1.5333\n",
      "353/388, train_loss: 0.1109, step time: 1.5352\n",
      "354/388, train_loss: 0.1504, step time: 1.5350\n",
      "355/388, train_loss: 0.1668, step time: 1.5324\n",
      "356/388, train_loss: 0.1238, step time: 1.5301\n",
      "357/388, train_loss: 0.1245, step time: 1.5307\n",
      "358/388, train_loss: 0.2344, step time: 1.5302\n",
      "359/388, train_loss: 0.0846, step time: 1.5330\n",
      "360/388, train_loss: 0.1006, step time: 1.5364\n",
      "361/388, train_loss: 0.2601, step time: 1.5347\n",
      "362/388, train_loss: 0.2331, step time: 1.5306\n",
      "363/388, train_loss: 0.0791, step time: 1.5315\n",
      "364/388, train_loss: 0.0954, step time: 1.5293\n",
      "365/388, train_loss: 0.0655, step time: 1.5345\n",
      "366/388, train_loss: 0.1257, step time: 1.5340\n",
      "367/388, train_loss: 0.0780, step time: 1.5313\n",
      "368/388, train_loss: 0.5202, step time: 1.5337\n",
      "369/388, train_loss: 0.1359, step time: 1.5330\n",
      "370/388, train_loss: 0.0803, step time: 1.5279\n",
      "371/388, train_loss: 0.2816, step time: 1.5396\n",
      "372/388, train_loss: 0.0843, step time: 1.5340\n",
      "373/388, train_loss: 0.1710, step time: 1.5342\n",
      "374/388, train_loss: 0.4670, step time: 1.5450\n",
      "375/388, train_loss: 0.1196, step time: 1.5315\n",
      "376/388, train_loss: 0.0899, step time: 1.5282\n",
      "377/388, train_loss: 0.0980, step time: 1.5312\n",
      "378/388, train_loss: 0.0482, step time: 1.5312\n",
      "379/388, train_loss: 0.1142, step time: 1.5335\n",
      "380/388, train_loss: 0.2384, step time: 1.5393\n",
      "381/388, train_loss: 0.1437, step time: 1.5373\n",
      "382/388, train_loss: 0.1344, step time: 1.5393\n",
      "383/388, train_loss: 0.0947, step time: 1.5313\n",
      "384/388, train_loss: 0.2586, step time: 1.5328\n",
      "385/388, train_loss: 0.2696, step time: 1.5296\n",
      "386/388, train_loss: 0.5102, step time: 1.5268\n",
      "387/388, train_loss: 0.0865, step time: 1.5301\n",
      "388/388, train_loss: 0.1754, step time: 1.5363\n",
      "epoch 54 average loss: 0.1804\n",
      "current epoch: 54 current mean dice: 0.7680 tc: 0.8168 wt: 0.9007 et: 0.5866\n",
      "best mean dice: 0.7692 at epoch: 49\n",
      "time consuming of epoch 54 is: 702.6343\n",
      "----------\n",
      "epoch 55/100\n",
      "1/388, train_loss: 0.0620, step time: 1.5460\n",
      "2/388, train_loss: 0.1136, step time: 1.5397\n",
      "3/388, train_loss: 0.3253, step time: 1.5315\n",
      "4/388, train_loss: 0.1366, step time: 1.5350\n",
      "5/388, train_loss: 0.1403, step time: 1.5353\n",
      "6/388, train_loss: 0.1429, step time: 1.5315\n",
      "7/388, train_loss: 0.0816, step time: 1.5354\n",
      "8/388, train_loss: 0.0798, step time: 1.5388\n",
      "9/388, train_loss: 0.1213, step time: 1.5373\n",
      "10/388, train_loss: 0.2769, step time: 1.5362\n",
      "11/388, train_loss: 0.2092, step time: 1.5355\n",
      "12/388, train_loss: 0.0944, step time: 1.5339\n",
      "13/388, train_loss: 0.1455, step time: 1.5338\n",
      "14/388, train_loss: 0.2097, step time: 1.5356\n",
      "15/388, train_loss: 0.2334, step time: 1.5349\n",
      "16/388, train_loss: 0.3529, step time: 1.5368\n",
      "17/388, train_loss: 0.1235, step time: 1.5334\n",
      "18/388, train_loss: 0.1911, step time: 1.5331\n",
      "19/388, train_loss: 0.1176, step time: 1.5366\n",
      "20/388, train_loss: 0.0769, step time: 1.5400\n",
      "21/388, train_loss: 0.1610, step time: 1.5371\n",
      "22/388, train_loss: 0.1141, step time: 1.5367\n",
      "23/388, train_loss: 0.4888, step time: 1.5364\n",
      "24/388, train_loss: 0.0992, step time: 1.5342\n",
      "25/388, train_loss: 0.1481, step time: 1.5360\n",
      "26/388, train_loss: 0.1536, step time: 1.5338\n",
      "27/388, train_loss: 0.1262, step time: 1.5309\n",
      "28/388, train_loss: 0.0865, step time: 1.5341\n",
      "29/388, train_loss: 0.1978, step time: 1.5311\n",
      "30/388, train_loss: 0.1169, step time: 1.5357\n",
      "31/388, train_loss: 0.3195, step time: 1.5368\n",
      "32/388, train_loss: 0.1808, step time: 1.5351\n",
      "33/388, train_loss: 0.1847, step time: 1.5351\n",
      "34/388, train_loss: 0.1721, step time: 1.5338\n",
      "35/388, train_loss: 0.2126, step time: 1.5349\n",
      "36/388, train_loss: 0.1038, step time: 1.5385\n",
      "37/388, train_loss: 0.0604, step time: 1.5342\n",
      "38/388, train_loss: 0.1620, step time: 1.5349\n",
      "39/388, train_loss: 0.0951, step time: 1.5324\n",
      "40/388, train_loss: 0.1884, step time: 1.5329\n",
      "41/388, train_loss: 0.0321, step time: 1.5418\n",
      "42/388, train_loss: 0.0916, step time: 1.5343\n",
      "43/388, train_loss: 0.0554, step time: 1.5349\n",
      "44/388, train_loss: 0.2361, step time: 1.5329\n",
      "45/388, train_loss: 0.1153, step time: 1.5343\n",
      "46/388, train_loss: 0.1443, step time: 1.5349\n",
      "47/388, train_loss: 0.2539, step time: 1.5337\n",
      "48/388, train_loss: 0.0884, step time: 1.5359\n",
      "49/388, train_loss: 0.0859, step time: 1.5366\n",
      "50/388, train_loss: 0.1147, step time: 1.5325\n",
      "51/388, train_loss: 0.3375, step time: 1.5323\n",
      "52/388, train_loss: 0.1359, step time: 1.5328\n",
      "53/388, train_loss: 0.4280, step time: 1.5372\n",
      "54/388, train_loss: 0.2038, step time: 1.5351\n",
      "55/388, train_loss: 0.0688, step time: 1.5337\n",
      "56/388, train_loss: 0.2301, step time: 1.5356\n",
      "57/388, train_loss: 0.1999, step time: 1.5307\n",
      "58/388, train_loss: 0.1882, step time: 1.5303\n",
      "59/388, train_loss: 0.1499, step time: 1.5365\n",
      "60/388, train_loss: 0.1798, step time: 1.5387\n",
      "61/388, train_loss: 0.1084, step time: 1.5383\n",
      "62/388, train_loss: 0.1062, step time: 1.5353\n",
      "63/388, train_loss: 0.1595, step time: 1.5355\n",
      "64/388, train_loss: 0.4646, step time: 1.5374\n",
      "65/388, train_loss: 0.3762, step time: 1.5342\n",
      "66/388, train_loss: 0.1390, step time: 1.5317\n",
      "67/388, train_loss: 0.1435, step time: 1.5310\n",
      "68/388, train_loss: 0.3168, step time: 1.5339\n",
      "69/388, train_loss: 0.1300, step time: 1.5317\n",
      "70/388, train_loss: 0.0818, step time: 1.5388\n",
      "71/388, train_loss: 0.1672, step time: 1.5344\n",
      "72/388, train_loss: 0.0927, step time: 1.5379\n",
      "73/388, train_loss: 0.1086, step time: 1.5322\n",
      "74/388, train_loss: 0.1303, step time: 1.5334\n",
      "75/388, train_loss: 0.1708, step time: 1.5318\n",
      "76/388, train_loss: 0.5000, step time: 1.5365\n",
      "77/388, train_loss: 0.0459, step time: 1.5391\n",
      "78/388, train_loss: 0.1771, step time: 1.5363\n",
      "79/388, train_loss: 0.1170, step time: 1.5342\n",
      "80/388, train_loss: 0.1071, step time: 1.5303\n",
      "81/388, train_loss: 0.2955, step time: 1.5358\n",
      "82/388, train_loss: 0.0874, step time: 1.5361\n",
      "83/388, train_loss: 0.1011, step time: 1.5358\n",
      "84/388, train_loss: 0.2377, step time: 1.5336\n",
      "85/388, train_loss: 0.1121, step time: 1.5303\n",
      "86/388, train_loss: 0.2051, step time: 1.5327\n",
      "87/388, train_loss: 0.1056, step time: 1.5371\n",
      "88/388, train_loss: 0.1944, step time: 1.5403\n",
      "89/388, train_loss: 0.2470, step time: 1.5413\n",
      "90/388, train_loss: 0.0826, step time: 1.5330\n",
      "91/388, train_loss: 0.1765, step time: 1.5342\n",
      "92/388, train_loss: 0.2144, step time: 1.5378\n",
      "93/388, train_loss: 0.0410, step time: 1.5371\n",
      "94/388, train_loss: 0.3366, step time: 1.5336\n",
      "95/388, train_loss: 0.1812, step time: 1.5324\n",
      "96/388, train_loss: 0.3603, step time: 1.5310\n",
      "97/388, train_loss: 0.1049, step time: 1.5348\n",
      "98/388, train_loss: 0.3959, step time: 1.5410\n",
      "99/388, train_loss: 0.0609, step time: 1.5333\n",
      "100/388, train_loss: 0.1280, step time: 1.5334\n",
      "101/388, train_loss: 0.1510, step time: 1.5626\n",
      "102/388, train_loss: 0.5787, step time: 1.5389\n",
      "103/388, train_loss: 0.1629, step time: 1.5346\n",
      "104/388, train_loss: 0.0710, step time: 1.5344\n",
      "105/388, train_loss: 0.2970, step time: 1.5301\n",
      "106/388, train_loss: 0.1270, step time: 1.5365\n",
      "107/388, train_loss: 0.1137, step time: 1.5371\n",
      "108/388, train_loss: 0.2808, step time: 1.5473\n",
      "109/388, train_loss: 0.3563, step time: 1.5343\n",
      "110/388, train_loss: 0.2440, step time: 1.5362\n",
      "111/388, train_loss: 0.1316, step time: 1.5355\n",
      "112/388, train_loss: 0.0914, step time: 1.5340\n",
      "113/388, train_loss: 0.3397, step time: 1.5327\n",
      "114/388, train_loss: 0.2510, step time: 1.5323\n",
      "115/388, train_loss: 0.1713, step time: 1.5361\n",
      "116/388, train_loss: 0.0918, step time: 1.5345\n",
      "117/388, train_loss: 0.2926, step time: 1.5338\n",
      "118/388, train_loss: 0.0594, step time: 1.5347\n",
      "119/388, train_loss: 0.2801, step time: 1.5388\n",
      "120/388, train_loss: 0.2133, step time: 1.5343\n",
      "121/388, train_loss: 0.0990, step time: 1.5346\n",
      "122/388, train_loss: 0.1204, step time: 1.5380\n",
      "123/388, train_loss: 0.1011, step time: 1.5345\n",
      "124/388, train_loss: 0.1784, step time: 1.5329\n",
      "125/388, train_loss: 0.1330, step time: 1.5309\n",
      "126/388, train_loss: 0.0805, step time: 1.5320\n",
      "127/388, train_loss: 0.1731, step time: 1.5362\n",
      "128/388, train_loss: 0.1621, step time: 1.5362\n",
      "129/388, train_loss: 0.1124, step time: 1.5363\n",
      "130/388, train_loss: 0.2189, step time: 1.5344\n",
      "131/388, train_loss: 0.1364, step time: 1.5310\n",
      "132/388, train_loss: 0.0972, step time: 1.5357\n",
      "133/388, train_loss: 0.1450, step time: 1.5352\n",
      "134/388, train_loss: 0.0745, step time: 1.5384\n",
      "135/388, train_loss: 0.1386, step time: 1.5339\n",
      "136/388, train_loss: 0.1078, step time: 1.5328\n",
      "137/388, train_loss: 0.3230, step time: 1.5317\n",
      "138/388, train_loss: 0.2523, step time: 1.5312\n",
      "139/388, train_loss: 0.1136, step time: 1.5342\n",
      "140/388, train_loss: 0.1196, step time: 1.5362\n",
      "141/388, train_loss: 0.1918, step time: 1.5384\n",
      "142/388, train_loss: 0.2026, step time: 1.5339\n",
      "143/388, train_loss: 0.2467, step time: 1.5338\n",
      "144/388, train_loss: 0.0954, step time: 1.5330\n",
      "145/388, train_loss: 0.2049, step time: 1.5337\n",
      "146/388, train_loss: 0.2836, step time: 1.5389\n",
      "147/388, train_loss: 0.1127, step time: 1.5375\n",
      "148/388, train_loss: 0.2870, step time: 1.5354\n",
      "149/388, train_loss: 0.0455, step time: 1.5313\n",
      "150/388, train_loss: 0.2075, step time: 1.5319\n",
      "151/388, train_loss: 0.4716, step time: 1.5315\n",
      "152/388, train_loss: 0.2241, step time: 1.5340\n",
      "153/388, train_loss: 0.2290, step time: 1.5372\n",
      "154/388, train_loss: 0.0528, step time: 1.5368\n",
      "155/388, train_loss: 0.2161, step time: 1.5333\n",
      "156/388, train_loss: 0.5044, step time: 1.5364\n",
      "157/388, train_loss: 0.2688, step time: 1.5349\n",
      "158/388, train_loss: 0.1976, step time: 1.5418\n",
      "159/388, train_loss: 0.2795, step time: 1.5352\n",
      "160/388, train_loss: 0.2082, step time: 1.5344\n",
      "161/388, train_loss: 0.2359, step time: 1.5345\n",
      "162/388, train_loss: 0.1686, step time: 1.5411\n",
      "163/388, train_loss: 0.1220, step time: 1.5321\n",
      "164/388, train_loss: 0.1011, step time: 1.5424\n",
      "165/388, train_loss: 0.2609, step time: 1.5329\n",
      "166/388, train_loss: 0.0923, step time: 1.5357\n",
      "167/388, train_loss: 0.0777, step time: 1.5388\n",
      "168/388, train_loss: 0.1927, step time: 1.5384\n",
      "169/388, train_loss: 0.2936, step time: 1.5348\n",
      "170/388, train_loss: 0.1925, step time: 1.5323\n",
      "171/388, train_loss: 0.1904, step time: 1.5357\n",
      "172/388, train_loss: 0.1016, step time: 1.5340\n",
      "173/388, train_loss: 0.1207, step time: 1.5373\n",
      "174/388, train_loss: 0.2682, step time: 1.5353\n",
      "175/388, train_loss: 0.2115, step time: 1.5328\n",
      "176/388, train_loss: 0.2494, step time: 1.5351\n",
      "177/388, train_loss: 0.0970, step time: 1.5354\n",
      "178/388, train_loss: 0.1007, step time: 1.5384\n",
      "179/388, train_loss: 0.1020, step time: 1.5336\n",
      "180/388, train_loss: 0.0551, step time: 1.5355\n",
      "181/388, train_loss: 0.0837, step time: 1.5320\n",
      "182/388, train_loss: 0.0820, step time: 1.5325\n",
      "183/388, train_loss: 0.1182, step time: 1.5373\n",
      "184/388, train_loss: 0.2494, step time: 1.5381\n",
      "185/388, train_loss: 0.0990, step time: 1.5426\n",
      "186/388, train_loss: 0.2342, step time: 1.5324\n",
      "187/388, train_loss: 0.0925, step time: 1.5328\n",
      "188/388, train_loss: 0.0760, step time: 1.5351\n",
      "189/388, train_loss: 0.1170, step time: 1.5361\n",
      "190/388, train_loss: 0.1099, step time: 1.5356\n",
      "191/388, train_loss: 0.0891, step time: 1.5337\n",
      "192/388, train_loss: 0.0914, step time: 1.5331\n",
      "193/388, train_loss: 0.2781, step time: 1.5349\n",
      "194/388, train_loss: 0.0522, step time: 1.5382\n",
      "195/388, train_loss: 0.3080, step time: 1.5354\n",
      "196/388, train_loss: 0.2909, step time: 1.5393\n",
      "197/388, train_loss: 0.1398, step time: 1.5362\n",
      "198/388, train_loss: 0.0495, step time: 1.5348\n",
      "199/388, train_loss: 0.3232, step time: 1.5329\n",
      "200/388, train_loss: 0.1481, step time: 1.5332\n",
      "201/388, train_loss: 0.1870, step time: 1.5320\n",
      "202/388, train_loss: 0.1278, step time: 1.5441\n",
      "203/388, train_loss: 0.1093, step time: 1.5306\n",
      "204/388, train_loss: 0.1650, step time: 1.5346\n",
      "205/388, train_loss: 0.0931, step time: 1.5341\n",
      "206/388, train_loss: 0.0710, step time: 1.5365\n",
      "207/388, train_loss: 0.2361, step time: 1.5354\n",
      "208/388, train_loss: 0.1121, step time: 1.5377\n",
      "209/388, train_loss: 0.1054, step time: 1.5331\n",
      "210/388, train_loss: 0.4465, step time: 1.5313\n",
      "211/388, train_loss: 0.1003, step time: 1.5321\n",
      "212/388, train_loss: 0.1142, step time: 1.5365\n",
      "213/388, train_loss: 0.1826, step time: 1.5348\n",
      "214/388, train_loss: 0.0994, step time: 1.5329\n",
      "215/388, train_loss: 0.4303, step time: 1.5336\n",
      "216/388, train_loss: 0.1349, step time: 1.5383\n",
      "217/388, train_loss: 0.1773, step time: 1.5364\n",
      "218/388, train_loss: 0.1256, step time: 1.5322\n",
      "219/388, train_loss: 0.2184, step time: 1.5333\n",
      "220/388, train_loss: 0.1516, step time: 1.5330\n",
      "221/388, train_loss: 0.0884, step time: 1.5358\n",
      "222/388, train_loss: 0.1658, step time: 1.5376\n",
      "223/388, train_loss: 0.4991, step time: 1.5355\n",
      "224/388, train_loss: 0.2580, step time: 1.5326\n",
      "225/388, train_loss: 0.2761, step time: 1.5320\n",
      "226/388, train_loss: 0.2030, step time: 1.5442\n",
      "227/388, train_loss: 0.2092, step time: 1.5361\n",
      "228/388, train_loss: 0.2504, step time: 1.5331\n",
      "229/388, train_loss: 0.3322, step time: 1.5314\n",
      "230/388, train_loss: 0.2760, step time: 1.5327\n",
      "231/388, train_loss: 0.2086, step time: 1.5393\n",
      "232/388, train_loss: 0.1873, step time: 1.5332\n",
      "233/388, train_loss: 0.2200, step time: 1.5331\n",
      "234/388, train_loss: 0.4995, step time: 1.5347\n",
      "235/388, train_loss: 0.1807, step time: 1.5327\n",
      "236/388, train_loss: 0.1777, step time: 1.5365\n",
      "237/388, train_loss: 0.1489, step time: 1.5376\n",
      "238/388, train_loss: 0.2709, step time: 1.5339\n",
      "239/388, train_loss: 0.1745, step time: 1.5326\n",
      "240/388, train_loss: 0.1273, step time: 1.5327\n",
      "241/388, train_loss: 0.0741, step time: 1.5348\n",
      "242/388, train_loss: 0.0938, step time: 1.5342\n",
      "243/388, train_loss: 0.2191, step time: 1.5350\n",
      "244/388, train_loss: 0.2183, step time: 1.5390\n",
      "245/388, train_loss: 0.3310, step time: 1.5362\n",
      "246/388, train_loss: 0.2652, step time: 1.5620\n",
      "247/388, train_loss: 0.0655, step time: 1.5317\n",
      "248/388, train_loss: 0.1774, step time: 1.5331\n",
      "249/388, train_loss: 0.2070, step time: 1.5352\n",
      "250/388, train_loss: 0.1075, step time: 1.5348\n",
      "251/388, train_loss: 0.2183, step time: 1.5333\n",
      "252/388, train_loss: 0.2020, step time: 1.5366\n",
      "253/388, train_loss: 0.1430, step time: 1.5337\n",
      "254/388, train_loss: 0.1393, step time: 1.5355\n",
      "255/388, train_loss: 0.1610, step time: 1.5380\n",
      "256/388, train_loss: 0.0948, step time: 1.5479\n",
      "257/388, train_loss: 0.1869, step time: 1.5340\n",
      "258/388, train_loss: 0.0678, step time: 1.5493\n",
      "259/388, train_loss: 0.1521, step time: 1.5331\n",
      "260/388, train_loss: 0.1080, step time: 1.5374\n",
      "261/388, train_loss: 0.1391, step time: 1.5349\n",
      "262/388, train_loss: 0.1273, step time: 1.5335\n",
      "263/388, train_loss: 0.2097, step time: 1.5309\n",
      "264/388, train_loss: 0.3304, step time: 1.5369\n",
      "265/388, train_loss: 0.1646, step time: 1.5342\n",
      "266/388, train_loss: 0.2290, step time: 1.5351\n",
      "267/388, train_loss: 0.0932, step time: 1.5318\n",
      "268/388, train_loss: 0.0287, step time: 1.5321\n",
      "269/388, train_loss: 0.2809, step time: 1.5314\n",
      "270/388, train_loss: 0.4938, step time: 1.5355\n",
      "271/388, train_loss: 0.0631, step time: 1.5363\n",
      "272/388, train_loss: 0.2751, step time: 1.5376\n",
      "273/388, train_loss: 0.1101, step time: 1.5314\n",
      "274/388, train_loss: 0.2070, step time: 1.5314\n",
      "275/388, train_loss: 0.3079, step time: 1.5384\n",
      "276/388, train_loss: 0.0465, step time: 1.5341\n",
      "277/388, train_loss: 0.2003, step time: 1.5343\n",
      "278/388, train_loss: 0.1709, step time: 1.5320\n",
      "279/388, train_loss: 0.0783, step time: 1.5326\n",
      "280/388, train_loss: 0.4160, step time: 1.5344\n",
      "281/388, train_loss: 0.1804, step time: 1.5483\n",
      "282/388, train_loss: 0.2029, step time: 1.5378\n",
      "283/388, train_loss: 0.1160, step time: 1.5329\n",
      "284/388, train_loss: 0.1156, step time: 1.5331\n",
      "285/388, train_loss: 0.0993, step time: 1.5325\n",
      "286/388, train_loss: 0.0832, step time: 1.5374\n",
      "287/388, train_loss: 0.2940, step time: 1.5372\n",
      "288/388, train_loss: 0.0957, step time: 1.5346\n",
      "289/388, train_loss: 0.1501, step time: 1.5339\n",
      "290/388, train_loss: 0.2530, step time: 1.5334\n",
      "291/388, train_loss: 0.1497, step time: 1.5319\n",
      "292/388, train_loss: 0.1574, step time: 1.5328\n",
      "293/388, train_loss: 0.1656, step time: 1.5354\n",
      "294/388, train_loss: 0.1180, step time: 1.5368\n",
      "295/388, train_loss: 0.2057, step time: 1.5341\n",
      "296/388, train_loss: 0.1585, step time: 1.5318\n",
      "297/388, train_loss: 0.2123, step time: 1.5329\n",
      "298/388, train_loss: 0.1136, step time: 1.5418\n",
      "299/388, train_loss: 0.2212, step time: 1.5315\n",
      "300/388, train_loss: 0.3780, step time: 1.5328\n",
      "301/388, train_loss: 0.2349, step time: 1.5334\n",
      "302/388, train_loss: 0.2337, step time: 1.5343\n",
      "303/388, train_loss: 0.2078, step time: 1.5365\n",
      "304/388, train_loss: 0.2056, step time: 1.5359\n",
      "305/388, train_loss: 0.1534, step time: 1.5350\n",
      "306/388, train_loss: 0.0812, step time: 1.5344\n",
      "307/388, train_loss: 0.2034, step time: 1.5314\n",
      "308/388, train_loss: 0.1371, step time: 1.5348\n",
      "309/388, train_loss: 0.3490, step time: 1.5363\n",
      "310/388, train_loss: 0.2048, step time: 1.5363\n",
      "311/388, train_loss: 0.4631, step time: 1.5359\n",
      "312/388, train_loss: 0.2154, step time: 1.5322\n",
      "313/388, train_loss: 0.1700, step time: 1.5298\n",
      "314/388, train_loss: 0.1501, step time: 1.5308\n",
      "315/388, train_loss: 0.3299, step time: 1.5377\n",
      "316/388, train_loss: 0.2232, step time: 1.5380\n",
      "317/388, train_loss: 0.3234, step time: 1.5331\n",
      "318/388, train_loss: 0.2189, step time: 1.5328\n",
      "319/388, train_loss: 0.2517, step time: 1.5304\n",
      "320/388, train_loss: 0.1565, step time: 1.5348\n",
      "321/388, train_loss: 0.0973, step time: 1.5362\n",
      "322/388, train_loss: 0.2752, step time: 1.5376\n",
      "323/388, train_loss: 0.3006, step time: 1.5352\n",
      "324/388, train_loss: 0.0739, step time: 1.5344\n",
      "325/388, train_loss: 0.0993, step time: 1.5337\n",
      "326/388, train_loss: 0.0935, step time: 1.5343\n",
      "327/388, train_loss: 0.1555, step time: 1.5394\n",
      "328/388, train_loss: 0.1745, step time: 1.5345\n",
      "329/388, train_loss: 0.1726, step time: 1.5365\n",
      "330/388, train_loss: 0.0987, step time: 1.5334\n",
      "331/388, train_loss: 0.2436, step time: 1.5340\n",
      "332/388, train_loss: 0.1228, step time: 1.5348\n",
      "333/388, train_loss: 0.2242, step time: 1.5368\n",
      "334/388, train_loss: 0.0766, step time: 1.5416\n",
      "335/388, train_loss: 0.1282, step time: 1.5356\n",
      "336/388, train_loss: 0.1411, step time: 1.5337\n",
      "337/388, train_loss: 0.2232, step time: 1.5377\n",
      "338/388, train_loss: 0.2862, step time: 1.5360\n",
      "339/388, train_loss: 0.0874, step time: 1.5360\n",
      "340/388, train_loss: 0.1205, step time: 1.5348\n",
      "341/388, train_loss: 0.1867, step time: 1.5325\n",
      "342/388, train_loss: 0.1746, step time: 1.5335\n",
      "343/388, train_loss: 0.4016, step time: 1.5350\n",
      "344/388, train_loss: 0.1530, step time: 1.5432\n",
      "345/388, train_loss: 0.4376, step time: 1.5322\n",
      "346/388, train_loss: 0.0372, step time: 1.5303\n",
      "347/388, train_loss: 0.2259, step time: 1.5343\n",
      "348/388, train_loss: 0.1495, step time: 1.5337\n",
      "349/388, train_loss: 0.0820, step time: 1.5361\n",
      "350/388, train_loss: 0.2066, step time: 1.5348\n",
      "351/388, train_loss: 0.1341, step time: 1.5334\n",
      "352/388, train_loss: 0.1044, step time: 1.5320\n",
      "353/388, train_loss: 0.2843, step time: 1.5363\n",
      "354/388, train_loss: 0.1776, step time: 1.5353\n",
      "355/388, train_loss: 0.0685, step time: 1.5349\n",
      "356/388, train_loss: 0.1535, step time: 1.5352\n",
      "357/388, train_loss: 0.0601, step time: 1.5322\n",
      "358/388, train_loss: 0.1187, step time: 1.5345\n",
      "359/388, train_loss: 0.0743, step time: 1.5459\n",
      "360/388, train_loss: 0.1204, step time: 1.5397\n",
      "361/388, train_loss: 0.2564, step time: 1.5341\n",
      "362/388, train_loss: 0.1905, step time: 1.5323\n",
      "363/388, train_loss: 0.1121, step time: 1.5323\n",
      "364/388, train_loss: 0.1403, step time: 1.5358\n",
      "365/388, train_loss: 0.0783, step time: 1.5377\n",
      "366/388, train_loss: 0.0869, step time: 1.5371\n",
      "367/388, train_loss: 0.2430, step time: 1.5342\n",
      "368/388, train_loss: 0.2785, step time: 1.5335\n",
      "369/388, train_loss: 0.0980, step time: 1.5366\n",
      "370/388, train_loss: 0.2047, step time: 1.5438\n",
      "371/388, train_loss: 0.0998, step time: 1.5383\n",
      "372/388, train_loss: 0.1487, step time: 1.5370\n",
      "373/388, train_loss: 0.2652, step time: 1.5333\n",
      "374/388, train_loss: 0.1861, step time: 1.5327\n",
      "375/388, train_loss: 0.1152, step time: 1.5367\n",
      "376/388, train_loss: 0.0577, step time: 1.5344\n",
      "377/388, train_loss: 0.1107, step time: 1.5338\n",
      "378/388, train_loss: 0.0649, step time: 1.5300\n",
      "379/388, train_loss: 0.1708, step time: 1.5309\n",
      "380/388, train_loss: 0.1884, step time: 1.5367\n",
      "381/388, train_loss: 0.3808, step time: 1.5338\n",
      "382/388, train_loss: 0.0684, step time: 1.5340\n",
      "383/388, train_loss: 0.0922, step time: 1.5342\n",
      "384/388, train_loss: 0.4523, step time: 1.5327\n",
      "385/388, train_loss: 0.0964, step time: 1.5361\n",
      "386/388, train_loss: 0.1059, step time: 1.5382\n",
      "387/388, train_loss: 0.1997, step time: 1.5573\n",
      "388/388, train_loss: 0.0885, step time: 1.5342\n",
      "epoch 55 average loss: 0.1796\n",
      "current epoch: 55 current mean dice: 0.7600 tc: 0.8026 wt: 0.9012 et: 0.5762\n",
      "best mean dice: 0.7692 at epoch: 49\n",
      "time consuming of epoch 55 is: 704.8909\n",
      "----------\n",
      "epoch 56/100\n",
      "1/388, train_loss: 0.0973, step time: 1.5525\n",
      "2/388, train_loss: 0.2304, step time: 1.5316\n",
      "3/388, train_loss: 0.1025, step time: 1.5393\n",
      "4/388, train_loss: 0.0934, step time: 1.5386\n",
      "5/388, train_loss: 0.1649, step time: 1.5321\n",
      "6/388, train_loss: 0.1737, step time: 1.5356\n",
      "7/388, train_loss: 0.1036, step time: 1.5376\n",
      "8/388, train_loss: 0.2245, step time: 1.5363\n",
      "9/388, train_loss: 0.0920, step time: 1.5341\n",
      "10/388, train_loss: 0.2389, step time: 1.5349\n",
      "11/388, train_loss: 0.2239, step time: 1.5380\n",
      "12/388, train_loss: 0.0376, step time: 1.5337\n",
      "13/388, train_loss: 0.0986, step time: 1.5335\n",
      "14/388, train_loss: 0.0916, step time: 1.5341\n",
      "15/388, train_loss: 0.1259, step time: 1.5356\n",
      "16/388, train_loss: 0.1843, step time: 1.5346\n",
      "17/388, train_loss: 0.2442, step time: 1.5385\n",
      "18/388, train_loss: 0.2815, step time: 1.5354\n",
      "19/388, train_loss: 0.0865, step time: 1.5318\n",
      "20/388, train_loss: 0.1640, step time: 1.5327\n",
      "21/388, train_loss: 0.1648, step time: 1.5345\n",
      "22/388, train_loss: 0.2725, step time: 1.5370\n",
      "23/388, train_loss: 0.0584, step time: 1.5354\n",
      "24/388, train_loss: 0.2091, step time: 1.5333\n",
      "25/388, train_loss: 0.1810, step time: 1.5324\n",
      "26/388, train_loss: 0.2264, step time: 1.5361\n",
      "27/388, train_loss: 0.1330, step time: 1.5366\n",
      "28/388, train_loss: 0.0415, step time: 1.5380\n",
      "29/388, train_loss: 0.2743, step time: 1.5364\n",
      "30/388, train_loss: 0.2081, step time: 1.5396\n",
      "31/388, train_loss: 0.1207, step time: 1.5442\n",
      "32/388, train_loss: 0.1208, step time: 1.5383\n",
      "33/388, train_loss: 0.1403, step time: 1.5350\n",
      "34/388, train_loss: 0.0835, step time: 1.5349\n",
      "35/388, train_loss: 0.1474, step time: 1.5332\n",
      "36/388, train_loss: 0.1328, step time: 1.5395\n",
      "37/388, train_loss: 0.1405, step time: 1.5383\n",
      "38/388, train_loss: 0.1657, step time: 1.5389\n",
      "39/388, train_loss: 0.0791, step time: 1.5373\n",
      "40/388, train_loss: 0.2893, step time: 1.5360\n",
      "41/388, train_loss: 0.2367, step time: 1.5334\n",
      "42/388, train_loss: 0.0692, step time: 1.5372\n",
      "43/388, train_loss: 0.3039, step time: 1.5368\n",
      "44/388, train_loss: 0.3343, step time: 1.5355\n",
      "45/388, train_loss: 0.1849, step time: 1.5338\n",
      "46/388, train_loss: 0.0829, step time: 1.5325\n",
      "47/388, train_loss: 0.1476, step time: 1.5366\n",
      "48/388, train_loss: 0.1144, step time: 1.5357\n",
      "49/388, train_loss: 0.1031, step time: 1.5362\n",
      "50/388, train_loss: 0.1302, step time: 1.5348\n",
      "51/388, train_loss: 0.0670, step time: 1.5327\n",
      "52/388, train_loss: 0.2789, step time: 1.5352\n",
      "53/388, train_loss: 0.2625, step time: 1.5372\n",
      "54/388, train_loss: 0.2128, step time: 1.5409\n",
      "55/388, train_loss: 0.1134, step time: 1.5331\n",
      "56/388, train_loss: 0.1036, step time: 1.5319\n",
      "57/388, train_loss: 0.2000, step time: 1.5338\n",
      "58/388, train_loss: 0.2426, step time: 1.5381\n",
      "59/388, train_loss: 0.1715, step time: 1.5313\n",
      "60/388, train_loss: 0.2604, step time: 1.5358\n",
      "61/388, train_loss: 0.1751, step time: 1.5368\n",
      "62/388, train_loss: 0.1787, step time: 1.5419\n",
      "63/388, train_loss: 0.2337, step time: 1.5357\n",
      "64/388, train_loss: 0.1071, step time: 1.5353\n",
      "65/388, train_loss: 0.3897, step time: 1.5338\n",
      "66/388, train_loss: 0.2159, step time: 1.5375\n",
      "67/388, train_loss: 0.2353, step time: 1.5394\n",
      "68/388, train_loss: 0.3396, step time: 1.5351\n",
      "69/388, train_loss: 0.4552, step time: 1.5322\n",
      "70/388, train_loss: 0.2168, step time: 1.5356\n",
      "71/388, train_loss: 0.1268, step time: 1.5375\n",
      "72/388, train_loss: 0.1051, step time: 1.5338\n",
      "73/388, train_loss: 0.1284, step time: 1.5336\n",
      "74/388, train_loss: 0.0786, step time: 1.5321\n",
      "75/388, train_loss: 0.0467, step time: 1.5335\n",
      "76/388, train_loss: 0.0744, step time: 1.5395\n",
      "77/388, train_loss: 0.0929, step time: 1.5357\n",
      "78/388, train_loss: 0.1137, step time: 1.5352\n",
      "79/388, train_loss: 0.2104, step time: 1.5310\n",
      "80/388, train_loss: 0.0748, step time: 1.5312\n",
      "81/388, train_loss: 0.1592, step time: 1.5337\n",
      "82/388, train_loss: 0.2066, step time: 1.5413\n",
      "83/388, train_loss: 0.1068, step time: 1.5382\n",
      "84/388, train_loss: 0.5174, step time: 1.5379\n",
      "85/388, train_loss: 0.1235, step time: 1.5347\n",
      "86/388, train_loss: 0.1169, step time: 1.5347\n",
      "87/388, train_loss: 0.3572, step time: 1.5397\n",
      "88/388, train_loss: 0.1157, step time: 1.5390\n",
      "89/388, train_loss: 0.2605, step time: 1.5353\n",
      "90/388, train_loss: 0.2567, step time: 1.5335\n",
      "91/388, train_loss: 0.2176, step time: 1.5357\n",
      "92/388, train_loss: 0.2425, step time: 1.5359\n",
      "93/388, train_loss: 0.4371, step time: 1.5356\n",
      "94/388, train_loss: 0.1455, step time: 1.5340\n",
      "95/388, train_loss: 0.1340, step time: 1.5344\n",
      "96/388, train_loss: 0.0910, step time: 1.5386\n",
      "97/388, train_loss: 0.3566, step time: 1.5413\n",
      "98/388, train_loss: 0.1555, step time: 1.5369\n",
      "99/388, train_loss: 0.1433, step time: 1.5341\n",
      "100/388, train_loss: 0.1240, step time: 1.5351\n",
      "101/388, train_loss: 0.1310, step time: 1.5327\n",
      "102/388, train_loss: 0.0638, step time: 1.5375\n",
      "103/388, train_loss: 0.1963, step time: 1.5356\n",
      "104/388, train_loss: 0.0631, step time: 1.5382\n",
      "105/388, train_loss: 0.1762, step time: 1.5319\n",
      "106/388, train_loss: 0.3130, step time: 1.5335\n",
      "107/388, train_loss: 0.0668, step time: 1.5358\n",
      "108/388, train_loss: 0.1953, step time: 1.5327\n",
      "109/388, train_loss: 0.1903, step time: 1.5358\n",
      "110/388, train_loss: 0.0840, step time: 1.5341\n",
      "111/388, train_loss: 0.0590, step time: 1.5373\n",
      "112/388, train_loss: 0.2736, step time: 1.5406\n",
      "113/388, train_loss: 0.2229, step time: 1.5353\n",
      "114/388, train_loss: 0.1919, step time: 1.5341\n",
      "115/388, train_loss: 0.2051, step time: 1.5355\n",
      "116/388, train_loss: 0.1395, step time: 1.5412\n",
      "117/388, train_loss: 0.2410, step time: 1.5383\n",
      "118/388, train_loss: 0.2240, step time: 1.5370\n",
      "119/388, train_loss: 0.1757, step time: 1.5348\n",
      "120/388, train_loss: 0.1689, step time: 1.5356\n",
      "121/388, train_loss: 0.2412, step time: 1.5357\n",
      "122/388, train_loss: 0.1225, step time: 1.5349\n",
      "123/388, train_loss: 0.1964, step time: 1.5352\n",
      "124/388, train_loss: 0.1642, step time: 1.5369\n",
      "125/388, train_loss: 0.1338, step time: 1.5381\n",
      "126/388, train_loss: 0.1990, step time: 1.5339\n",
      "127/388, train_loss: 0.2342, step time: 1.5311\n",
      "128/388, train_loss: 0.0926, step time: 1.5343\n",
      "129/388, train_loss: 0.1353, step time: 1.5340\n",
      "130/388, train_loss: 0.0945, step time: 1.5363\n",
      "131/388, train_loss: 0.1772, step time: 1.5334\n",
      "132/388, train_loss: 0.0746, step time: 1.5355\n",
      "133/388, train_loss: 0.1624, step time: 1.5336\n",
      "134/388, train_loss: 0.0575, step time: 1.5372\n",
      "135/388, train_loss: 0.3378, step time: 1.5352\n",
      "136/388, train_loss: 0.2648, step time: 1.5342\n",
      "137/388, train_loss: 0.2615, step time: 1.5351\n",
      "138/388, train_loss: 0.1858, step time: 1.5365\n",
      "139/388, train_loss: 0.0970, step time: 1.5402\n",
      "140/388, train_loss: 0.2062, step time: 1.5406\n",
      "141/388, train_loss: 0.1012, step time: 1.5370\n",
      "142/388, train_loss: 0.1927, step time: 1.5334\n",
      "143/388, train_loss: 0.0822, step time: 1.5340\n",
      "144/388, train_loss: 0.1201, step time: 1.5343\n",
      "145/388, train_loss: 0.4613, step time: 1.5359\n",
      "146/388, train_loss: 0.1884, step time: 1.5356\n",
      "147/388, train_loss: 0.0908, step time: 1.5347\n",
      "148/388, train_loss: 0.2703, step time: 1.5337\n",
      "149/388, train_loss: 0.0891, step time: 1.5340\n",
      "150/388, train_loss: 0.1270, step time: 1.5323\n",
      "151/388, train_loss: 0.2578, step time: 1.5327\n",
      "152/388, train_loss: 0.3057, step time: 1.5398\n",
      "153/388, train_loss: 0.1350, step time: 1.5372\n",
      "154/388, train_loss: 0.0934, step time: 1.5320\n",
      "155/388, train_loss: 0.1102, step time: 1.5335\n",
      "156/388, train_loss: 0.1460, step time: 1.5343\n",
      "157/388, train_loss: 0.1759, step time: 1.5379\n",
      "158/388, train_loss: 0.1180, step time: 1.5368\n",
      "159/388, train_loss: 0.4320, step time: 1.5341\n",
      "160/388, train_loss: 0.4475, step time: 1.5364\n",
      "161/388, train_loss: 0.2719, step time: 1.5373\n",
      "162/388, train_loss: 0.1968, step time: 1.5377\n",
      "163/388, train_loss: 0.0846, step time: 1.5385\n",
      "164/388, train_loss: 0.2137, step time: 1.5325\n",
      "165/388, train_loss: 0.0374, step time: 1.5328\n",
      "166/388, train_loss: 0.1515, step time: 1.5336\n",
      "167/388, train_loss: 0.1883, step time: 1.5336\n",
      "168/388, train_loss: 0.5617, step time: 1.5371\n",
      "169/388, train_loss: 0.0669, step time: 1.5322\n",
      "170/388, train_loss: 0.0923, step time: 1.5346\n",
      "171/388, train_loss: 0.1171, step time: 1.5321\n",
      "172/388, train_loss: 0.1191, step time: 1.5460\n",
      "173/388, train_loss: 0.1470, step time: 1.5348\n",
      "174/388, train_loss: 0.0632, step time: 1.5342\n",
      "175/388, train_loss: 0.2751, step time: 1.5338\n",
      "176/388, train_loss: 0.0930, step time: 1.5374\n",
      "177/388, train_loss: 0.0985, step time: 1.5372\n",
      "178/388, train_loss: 0.1779, step time: 1.5344\n",
      "179/388, train_loss: 0.1227, step time: 1.5306\n",
      "180/388, train_loss: 0.1394, step time: 1.5332\n",
      "181/388, train_loss: 0.2361, step time: 1.5338\n",
      "182/388, train_loss: 0.2040, step time: 1.5411\n",
      "183/388, train_loss: 0.4238, step time: 1.5326\n",
      "184/388, train_loss: 0.1657, step time: 1.5383\n",
      "185/388, train_loss: 0.1791, step time: 1.5358\n",
      "186/388, train_loss: 0.1684, step time: 1.5348\n",
      "187/388, train_loss: 0.1872, step time: 1.5376\n",
      "188/388, train_loss: 0.2697, step time: 1.5325\n",
      "189/388, train_loss: 0.1329, step time: 1.5335\n",
      "190/388, train_loss: 0.1671, step time: 1.5356\n",
      "191/388, train_loss: 0.1511, step time: 1.5336\n",
      "192/388, train_loss: 0.0885, step time: 1.5328\n",
      "193/388, train_loss: 0.1420, step time: 1.5313\n",
      "194/388, train_loss: 0.1842, step time: 1.5332\n",
      "195/388, train_loss: 0.2057, step time: 1.5391\n",
      "196/388, train_loss: 0.5286, step time: 1.5355\n",
      "197/388, train_loss: 0.2757, step time: 1.5327\n",
      "198/388, train_loss: 0.1266, step time: 1.5342\n",
      "199/388, train_loss: 0.0658, step time: 1.5368\n",
      "200/388, train_loss: 0.1486, step time: 1.5347\n",
      "201/388, train_loss: 0.2051, step time: 1.5312\n",
      "202/388, train_loss: 0.1672, step time: 1.5356\n",
      "203/388, train_loss: 0.0701, step time: 1.5590\n",
      "204/388, train_loss: 0.1032, step time: 1.5338\n",
      "205/388, train_loss: 0.1382, step time: 1.5307\n",
      "206/388, train_loss: 0.0688, step time: 1.5331\n",
      "207/388, train_loss: 0.0327, step time: 1.5344\n",
      "208/388, train_loss: 0.3725, step time: 1.5496\n",
      "209/388, train_loss: 0.5150, step time: 1.5357\n",
      "210/388, train_loss: 0.1436, step time: 1.5335\n",
      "211/388, train_loss: 0.1966, step time: 1.5335\n",
      "212/388, train_loss: 0.1148, step time: 1.5341\n",
      "213/388, train_loss: 0.0739, step time: 1.5396\n",
      "214/388, train_loss: 0.1266, step time: 1.5380\n",
      "215/388, train_loss: 0.2679, step time: 1.5355\n",
      "216/388, train_loss: 0.1470, step time: 1.5334\n",
      "217/388, train_loss: 0.2612, step time: 1.5307\n",
      "218/388, train_loss: 0.1295, step time: 1.5345\n",
      "219/388, train_loss: 0.0976, step time: 1.5346\n",
      "220/388, train_loss: 0.1150, step time: 1.5375\n",
      "221/388, train_loss: 0.0475, step time: 1.5363\n",
      "222/388, train_loss: 0.0923, step time: 1.5337\n",
      "223/388, train_loss: 0.2882, step time: 1.5337\n",
      "224/388, train_loss: 0.1917, step time: 1.5353\n",
      "225/388, train_loss: 0.1416, step time: 1.5365\n",
      "226/388, train_loss: 0.1910, step time: 1.5355\n",
      "227/388, train_loss: 0.1684, step time: 1.5332\n",
      "228/388, train_loss: 0.5336, step time: 1.5340\n",
      "229/388, train_loss: 0.1149, step time: 1.5434\n",
      "230/388, train_loss: 0.1904, step time: 1.5331\n",
      "231/388, train_loss: 0.2806, step time: 1.5303\n",
      "232/388, train_loss: 0.1567, step time: 1.5309\n",
      "233/388, train_loss: 0.0662, step time: 1.5368\n",
      "234/388, train_loss: 0.4169, step time: 1.5362\n",
      "235/388, train_loss: 0.4289, step time: 1.5367\n",
      "236/388, train_loss: 0.1032, step time: 1.5344\n",
      "237/388, train_loss: 0.1592, step time: 1.5332\n",
      "238/388, train_loss: 0.1589, step time: 1.5333\n",
      "239/388, train_loss: 0.1231, step time: 1.5370\n",
      "240/388, train_loss: 0.1344, step time: 1.5397\n",
      "241/388, train_loss: 0.1190, step time: 1.5331\n",
      "242/388, train_loss: 0.1414, step time: 1.5334\n",
      "243/388, train_loss: 0.3574, step time: 1.5348\n",
      "244/388, train_loss: 0.3510, step time: 1.5373\n",
      "245/388, train_loss: 0.2309, step time: 1.5386\n",
      "246/388, train_loss: 0.2631, step time: 1.5343\n",
      "247/388, train_loss: 0.3837, step time: 1.5349\n",
      "248/388, train_loss: 0.0501, step time: 1.5342\n",
      "249/388, train_loss: 0.3707, step time: 1.5367\n",
      "250/388, train_loss: 0.1883, step time: 1.5385\n",
      "251/388, train_loss: 0.0657, step time: 1.5323\n",
      "252/388, train_loss: 0.0756, step time: 1.5351\n",
      "253/388, train_loss: 0.2298, step time: 1.5324\n",
      "254/388, train_loss: 0.3104, step time: 1.5401\n",
      "255/388, train_loss: 0.0836, step time: 1.5356\n",
      "256/388, train_loss: 0.1990, step time: 1.5301\n",
      "257/388, train_loss: 0.2158, step time: 1.5294\n",
      "258/388, train_loss: 0.1192, step time: 1.5345\n",
      "259/388, train_loss: 0.1755, step time: 1.5373\n",
      "260/388, train_loss: 0.0894, step time: 1.5374\n",
      "261/388, train_loss: 0.1441, step time: 1.5314\n",
      "262/388, train_loss: 0.0833, step time: 1.5358\n",
      "263/388, train_loss: 0.0982, step time: 1.5467\n",
      "264/388, train_loss: 0.0944, step time: 1.5345\n",
      "265/388, train_loss: 0.1756, step time: 1.5335\n",
      "266/388, train_loss: 0.0931, step time: 1.5331\n",
      "267/388, train_loss: 0.2036, step time: 1.5323\n",
      "268/388, train_loss: 0.3091, step time: 1.5357\n",
      "269/388, train_loss: 0.0364, step time: 1.5317\n",
      "270/388, train_loss: 0.1967, step time: 1.5328\n",
      "271/388, train_loss: 0.1172, step time: 1.5360\n",
      "272/388, train_loss: 0.1321, step time: 1.5384\n",
      "273/388, train_loss: 0.0858, step time: 1.5402\n",
      "274/388, train_loss: 0.1291, step time: 1.5368\n",
      "275/388, train_loss: 0.3129, step time: 1.5343\n",
      "276/388, train_loss: 0.0969, step time: 1.5349\n",
      "277/388, train_loss: 0.1603, step time: 1.5349\n",
      "278/388, train_loss: 0.0808, step time: 1.5408\n",
      "279/388, train_loss: 0.0951, step time: 1.5341\n",
      "280/388, train_loss: 0.3495, step time: 1.5341\n",
      "281/388, train_loss: 0.1009, step time: 1.5322\n",
      "282/388, train_loss: 0.2380, step time: 1.5367\n",
      "283/388, train_loss: 0.0673, step time: 1.5388\n",
      "284/388, train_loss: 0.2537, step time: 1.5334\n",
      "285/388, train_loss: 0.0733, step time: 1.5337\n",
      "286/388, train_loss: 0.1512, step time: 1.5326\n",
      "287/388, train_loss: 0.3199, step time: 1.5359\n",
      "288/388, train_loss: 0.0848, step time: 1.5379\n",
      "289/388, train_loss: 0.0644, step time: 1.5348\n",
      "290/388, train_loss: 0.2259, step time: 1.5330\n",
      "291/388, train_loss: 0.1417, step time: 1.5312\n",
      "292/388, train_loss: 0.2129, step time: 1.5355\n",
      "293/388, train_loss: 0.0719, step time: 1.5381\n",
      "294/388, train_loss: 0.0490, step time: 1.5371\n",
      "295/388, train_loss: 0.1205, step time: 1.5323\n",
      "296/388, train_loss: 0.1764, step time: 1.5358\n",
      "297/388, train_loss: 0.1760, step time: 1.5355\n",
      "298/388, train_loss: 0.1020, step time: 1.5367\n",
      "299/388, train_loss: 0.1191, step time: 1.5319\n",
      "300/388, train_loss: 0.1014, step time: 1.5336\n",
      "301/388, train_loss: 0.1461, step time: 1.5360\n",
      "302/388, train_loss: 0.1278, step time: 1.5335\n",
      "303/388, train_loss: 0.5202, step time: 1.5336\n",
      "304/388, train_loss: 0.1531, step time: 1.5330\n",
      "305/388, train_loss: 0.0830, step time: 1.5396\n",
      "306/388, train_loss: 0.0444, step time: 1.5369\n",
      "307/388, train_loss: 0.2253, step time: 1.5363\n",
      "308/388, train_loss: 0.0818, step time: 1.5337\n",
      "309/388, train_loss: 0.0944, step time: 1.5329\n",
      "310/388, train_loss: 0.0555, step time: 1.5361\n",
      "311/388, train_loss: 0.1060, step time: 1.5347\n",
      "312/388, train_loss: 0.1248, step time: 1.5378\n",
      "313/388, train_loss: 0.1580, step time: 1.5361\n",
      "314/388, train_loss: 0.1282, step time: 1.5311\n",
      "315/388, train_loss: 0.3280, step time: 1.5369\n",
      "316/388, train_loss: 0.2947, step time: 1.5376\n",
      "317/388, train_loss: 0.0791, step time: 1.5421\n",
      "318/388, train_loss: 0.1049, step time: 1.5341\n",
      "319/388, train_loss: 0.0830, step time: 1.5330\n",
      "320/388, train_loss: 0.5761, step time: 1.5387\n",
      "321/388, train_loss: 0.2237, step time: 1.5349\n",
      "322/388, train_loss: 0.0817, step time: 1.5322\n",
      "323/388, train_loss: 0.5028, step time: 1.5336\n",
      "324/388, train_loss: 0.2474, step time: 1.5331\n",
      "325/388, train_loss: 0.0912, step time: 1.5360\n",
      "326/388, train_loss: 0.0842, step time: 1.5349\n",
      "327/388, train_loss: 0.1414, step time: 1.5353\n",
      "328/388, train_loss: 0.4114, step time: 1.5331\n",
      "329/388, train_loss: 0.1120, step time: 1.5318\n",
      "330/388, train_loss: 0.0837, step time: 1.5330\n",
      "331/388, train_loss: 0.2507, step time: 1.5358\n",
      "332/388, train_loss: 0.2691, step time: 1.5382\n",
      "333/388, train_loss: 0.0674, step time: 1.5346\n",
      "334/388, train_loss: 0.2349, step time: 1.5365\n",
      "335/388, train_loss: 0.2207, step time: 1.5345\n",
      "336/388, train_loss: 0.1857, step time: 1.5370\n",
      "337/388, train_loss: 0.1760, step time: 1.5381\n",
      "338/388, train_loss: 0.1325, step time: 1.5318\n",
      "339/388, train_loss: 0.0889, step time: 1.5349\n",
      "340/388, train_loss: 0.1213, step time: 1.5321\n",
      "341/388, train_loss: 0.1888, step time: 1.5416\n",
      "342/388, train_loss: 0.2702, step time: 1.5368\n",
      "343/388, train_loss: 0.0623, step time: 1.5335\n",
      "344/388, train_loss: 0.3641, step time: 1.5326\n",
      "345/388, train_loss: 0.0761, step time: 1.5323\n",
      "346/388, train_loss: 0.1016, step time: 1.5344\n",
      "347/388, train_loss: 0.2941, step time: 1.5368\n",
      "348/388, train_loss: 0.2693, step time: 1.5365\n",
      "349/388, train_loss: 0.2067, step time: 1.5356\n",
      "350/388, train_loss: 0.0726, step time: 1.5321\n",
      "351/388, train_loss: 0.2312, step time: 1.5336\n",
      "352/388, train_loss: 0.0980, step time: 1.5351\n",
      "353/388, train_loss: 0.1018, step time: 1.5387\n",
      "354/388, train_loss: 0.1763, step time: 1.5394\n",
      "355/388, train_loss: 0.1085, step time: 1.5362\n",
      "356/388, train_loss: 0.0906, step time: 1.5347\n",
      "357/388, train_loss: 0.0903, step time: 1.5374\n",
      "358/388, train_loss: 0.1343, step time: 1.5385\n",
      "359/388, train_loss: 0.2143, step time: 1.5353\n",
      "360/388, train_loss: 0.2206, step time: 1.5303\n",
      "361/388, train_loss: 0.1315, step time: 1.5364\n",
      "362/388, train_loss: 0.3647, step time: 1.5352\n",
      "363/388, train_loss: 0.2323, step time: 1.5335\n",
      "364/388, train_loss: 0.1444, step time: 1.5328\n",
      "365/388, train_loss: 0.2520, step time: 1.5317\n",
      "366/388, train_loss: 0.2286, step time: 1.5311\n",
      "367/388, train_loss: 0.1607, step time: 1.5328\n",
      "368/388, train_loss: 0.1317, step time: 1.5354\n",
      "369/388, train_loss: 0.5106, step time: 1.5359\n",
      "370/388, train_loss: 0.1879, step time: 1.5360\n",
      "371/388, train_loss: 0.0640, step time: 1.5440\n",
      "372/388, train_loss: 0.2398, step time: 1.5355\n",
      "373/388, train_loss: 0.2421, step time: 1.5376\n",
      "374/388, train_loss: 0.1629, step time: 1.5358\n",
      "375/388, train_loss: 0.2830, step time: 1.5365\n",
      "376/388, train_loss: 0.0857, step time: 1.5320\n",
      "377/388, train_loss: 0.3369, step time: 1.5369\n",
      "378/388, train_loss: 0.0996, step time: 1.5340\n",
      "379/388, train_loss: 0.2483, step time: 1.5320\n",
      "380/388, train_loss: 0.1735, step time: 1.5353\n",
      "381/388, train_loss: 0.1023, step time: 1.5380\n",
      "382/388, train_loss: 0.1918, step time: 1.5422\n",
      "383/388, train_loss: 0.1047, step time: 1.5348\n",
      "384/388, train_loss: 0.2005, step time: 1.5340\n",
      "385/388, train_loss: 0.1571, step time: 1.5358\n",
      "386/388, train_loss: 0.1269, step time: 1.5381\n",
      "387/388, train_loss: 0.1751, step time: 1.5353\n",
      "388/388, train_loss: 0.0537, step time: 1.5377\n",
      "epoch 56 average loss: 0.1778\n",
      "current epoch: 56 current mean dice: 0.7542 tc: 0.7971 wt: 0.8889 et: 0.5767\n",
      "best mean dice: 0.7692 at epoch: 49\n",
      "time consuming of epoch 56 is: 706.3083\n",
      "----------\n",
      "epoch 57/100\n",
      "1/388, train_loss: 0.1085, step time: 1.5505\n",
      "2/388, train_loss: 0.2099, step time: 1.5399\n",
      "3/388, train_loss: 0.1903, step time: 1.5321\n",
      "4/388, train_loss: 0.5009, step time: 1.5376\n",
      "5/388, train_loss: 0.1207, step time: 1.5358\n",
      "6/388, train_loss: 0.0897, step time: 1.5350\n",
      "7/388, train_loss: 0.1513, step time: 1.5344\n",
      "8/388, train_loss: 0.3182, step time: 1.5378\n",
      "9/388, train_loss: 0.2121, step time: 1.5328\n",
      "10/388, train_loss: 0.1696, step time: 1.5341\n",
      "11/388, train_loss: 0.1791, step time: 1.5315\n",
      "12/388, train_loss: 0.1619, step time: 1.5330\n",
      "13/388, train_loss: 0.0999, step time: 1.5383\n",
      "14/388, train_loss: 0.2126, step time: 1.5363\n",
      "15/388, train_loss: 0.1949, step time: 1.5341\n",
      "16/388, train_loss: 0.0723, step time: 1.5326\n",
      "17/388, train_loss: 0.0978, step time: 1.5343\n",
      "18/388, train_loss: 0.2170, step time: 1.5337\n",
      "19/388, train_loss: 0.0865, step time: 1.5382\n",
      "20/388, train_loss: 0.1398, step time: 1.5350\n",
      "21/388, train_loss: 0.2369, step time: 1.5327\n",
      "22/388, train_loss: 0.2745, step time: 1.5337\n",
      "23/388, train_loss: 0.0703, step time: 1.5361\n",
      "24/388, train_loss: 0.2935, step time: 1.5352\n",
      "25/388, train_loss: 0.1036, step time: 1.5316\n",
      "26/388, train_loss: 0.1080, step time: 1.5346\n",
      "27/388, train_loss: 0.0879, step time: 1.5312\n",
      "28/388, train_loss: 0.0993, step time: 1.5347\n",
      "29/388, train_loss: 0.0758, step time: 1.5363\n",
      "30/388, train_loss: 0.1659, step time: 1.5379\n",
      "31/388, train_loss: 0.2172, step time: 1.5332\n",
      "32/388, train_loss: 0.1680, step time: 1.5323\n",
      "33/388, train_loss: 0.1587, step time: 1.5314\n",
      "34/388, train_loss: 0.3229, step time: 1.5447\n",
      "35/388, train_loss: 0.3343, step time: 1.5334\n",
      "36/388, train_loss: 0.1481, step time: 1.5331\n",
      "37/388, train_loss: 0.0667, step time: 1.5314\n",
      "38/388, train_loss: 0.2173, step time: 1.5344\n",
      "39/388, train_loss: 0.3053, step time: 1.5328\n",
      "40/388, train_loss: 0.0604, step time: 1.5401\n",
      "41/388, train_loss: 0.0995, step time: 1.5349\n",
      "42/388, train_loss: 0.1525, step time: 1.5336\n",
      "43/388, train_loss: 0.1045, step time: 1.5319\n",
      "44/388, train_loss: 0.0296, step time: 1.5332\n",
      "45/388, train_loss: 0.3285, step time: 1.5379\n",
      "46/388, train_loss: 0.0488, step time: 1.5375\n",
      "47/388, train_loss: 0.6280, step time: 1.5329\n",
      "48/388, train_loss: 0.2634, step time: 1.5354\n",
      "49/388, train_loss: 0.0561, step time: 1.5323\n",
      "50/388, train_loss: 0.1325, step time: 1.5342\n",
      "51/388, train_loss: 0.2409, step time: 1.5365\n",
      "52/388, train_loss: 0.2751, step time: 1.5372\n",
      "53/388, train_loss: 0.1969, step time: 1.5312\n",
      "54/388, train_loss: 0.1228, step time: 1.5321\n",
      "55/388, train_loss: 0.2238, step time: 1.5330\n",
      "56/388, train_loss: 0.1839, step time: 1.5376\n",
      "57/388, train_loss: 0.0986, step time: 1.5343\n",
      "58/388, train_loss: 0.1154, step time: 1.5395\n",
      "59/388, train_loss: 0.1270, step time: 1.5319\n",
      "60/388, train_loss: 0.1456, step time: 1.5327\n",
      "61/388, train_loss: 0.0769, step time: 1.5370\n",
      "62/388, train_loss: 0.1668, step time: 1.5356\n",
      "63/388, train_loss: 0.0939, step time: 1.5337\n",
      "64/388, train_loss: 0.1998, step time: 1.5296\n",
      "65/388, train_loss: 0.1986, step time: 1.5306\n",
      "66/388, train_loss: 0.1488, step time: 1.5359\n",
      "67/388, train_loss: 0.0760, step time: 1.5369\n",
      "68/388, train_loss: 0.2920, step time: 1.5319\n",
      "69/388, train_loss: 0.1896, step time: 1.5337\n",
      "70/388, train_loss: 0.0985, step time: 1.5442\n",
      "71/388, train_loss: 0.3322, step time: 1.5346\n",
      "72/388, train_loss: 0.1545, step time: 1.5415\n",
      "73/388, train_loss: 0.0801, step time: 1.5345\n",
      "74/388, train_loss: 0.0338, step time: 1.5335\n",
      "75/388, train_loss: 0.0938, step time: 1.5334\n",
      "76/388, train_loss: 0.2848, step time: 1.5335\n",
      "77/388, train_loss: 0.2277, step time: 1.5337\n",
      "78/388, train_loss: 0.2223, step time: 1.5348\n",
      "79/388, train_loss: 0.1030, step time: 1.5360\n",
      "80/388, train_loss: 0.1865, step time: 1.5376\n",
      "81/388, train_loss: 0.1447, step time: 1.5296\n",
      "82/388, train_loss: 0.2502, step time: 1.5344\n",
      "83/388, train_loss: 0.2456, step time: 1.5323\n",
      "84/388, train_loss: 0.0627, step time: 1.5356\n",
      "85/388, train_loss: 0.1650, step time: 1.5373\n",
      "86/388, train_loss: 0.1937, step time: 1.5341\n",
      "87/388, train_loss: 0.1130, step time: 1.5319\n",
      "88/388, train_loss: 0.1557, step time: 1.5325\n",
      "89/388, train_loss: 0.1249, step time: 1.5396\n",
      "90/388, train_loss: 0.0834, step time: 1.5371\n",
      "91/388, train_loss: 0.0804, step time: 1.5349\n",
      "92/388, train_loss: 0.1940, step time: 1.5349\n",
      "93/388, train_loss: 0.0962, step time: 1.5334\n",
      "94/388, train_loss: 0.2855, step time: 1.5351\n",
      "95/388, train_loss: 0.1068, step time: 1.5341\n",
      "96/388, train_loss: 0.2683, step time: 1.5368\n",
      "97/388, train_loss: 0.2661, step time: 1.5348\n",
      "98/388, train_loss: 0.2025, step time: 1.5342\n",
      "99/388, train_loss: 0.0814, step time: 1.5319\n",
      "100/388, train_loss: 0.0957, step time: 1.5347\n",
      "101/388, train_loss: 0.1393, step time: 1.5367\n",
      "102/388, train_loss: 0.2631, step time: 1.5333\n",
      "103/388, train_loss: 0.3838, step time: 1.5323\n",
      "104/388, train_loss: 0.1425, step time: 1.5374\n",
      "105/388, train_loss: 0.2449, step time: 1.5339\n",
      "106/388, train_loss: 0.2560, step time: 1.5354\n",
      "107/388, train_loss: 0.3729, step time: 1.5322\n",
      "108/388, train_loss: 0.1188, step time: 1.5324\n",
      "109/388, train_loss: 0.1024, step time: 1.5343\n",
      "110/388, train_loss: 0.1020, step time: 1.5369\n",
      "111/388, train_loss: 0.0979, step time: 1.5345\n",
      "112/388, train_loss: 0.2479, step time: 1.5340\n",
      "113/388, train_loss: 0.2474, step time: 1.5337\n",
      "114/388, train_loss: 0.0920, step time: 1.5322\n",
      "115/388, train_loss: 0.1345, step time: 1.5363\n",
      "116/388, train_loss: 0.4975, step time: 1.5380\n",
      "117/388, train_loss: 0.2668, step time: 1.5355\n",
      "118/388, train_loss: 0.1678, step time: 1.5334\n",
      "119/388, train_loss: 0.4238, step time: 1.5309\n",
      "120/388, train_loss: 0.2416, step time: 1.5323\n",
      "121/388, train_loss: 0.0807, step time: 1.5331\n",
      "122/388, train_loss: 0.2154, step time: 1.5346\n",
      "123/388, train_loss: 0.1460, step time: 1.5380\n",
      "124/388, train_loss: 0.2536, step time: 1.5328\n",
      "125/388, train_loss: 0.1936, step time: 1.5342\n",
      "126/388, train_loss: 0.1100, step time: 1.5339\n",
      "127/388, train_loss: 0.0659, step time: 1.5366\n",
      "128/388, train_loss: 0.1884, step time: 1.5363\n",
      "129/388, train_loss: 0.3498, step time: 1.5354\n",
      "130/388, train_loss: 0.0804, step time: 1.5338\n",
      "131/388, train_loss: 0.1000, step time: 1.5305\n",
      "132/388, train_loss: 0.2214, step time: 1.5348\n",
      "133/388, train_loss: 0.1156, step time: 1.5343\n",
      "134/388, train_loss: 0.1727, step time: 1.5410\n",
      "135/388, train_loss: 0.1885, step time: 1.5383\n",
      "136/388, train_loss: 0.2760, step time: 1.5331\n",
      "137/388, train_loss: 0.0943, step time: 1.5327\n",
      "138/388, train_loss: 0.1293, step time: 1.5344\n",
      "139/388, train_loss: 0.1854, step time: 1.5373\n",
      "140/388, train_loss: 0.2272, step time: 1.5363\n",
      "141/388, train_loss: 0.2513, step time: 1.5341\n",
      "142/388, train_loss: 0.1544, step time: 1.5688\n",
      "143/388, train_loss: 0.0960, step time: 1.5374\n",
      "144/388, train_loss: 0.3208, step time: 1.5362\n",
      "145/388, train_loss: 0.3093, step time: 1.5340\n",
      "146/388, train_loss: 0.1493, step time: 1.5317\n",
      "147/388, train_loss: 0.0849, step time: 1.5338\n",
      "148/388, train_loss: 0.2402, step time: 1.5498\n",
      "149/388, train_loss: 0.0945, step time: 1.5382\n",
      "150/388, train_loss: 0.1443, step time: 1.5353\n",
      "151/388, train_loss: 0.0320, step time: 1.5335\n",
      "152/388, train_loss: 0.1416, step time: 1.5322\n",
      "153/388, train_loss: 0.1356, step time: 1.5332\n",
      "154/388, train_loss: 0.3396, step time: 1.5326\n",
      "155/388, train_loss: 0.0528, step time: 1.5401\n",
      "156/388, train_loss: 0.2008, step time: 1.5520\n",
      "157/388, train_loss: 0.0738, step time: 1.5326\n",
      "158/388, train_loss: 0.5419, step time: 1.5308\n",
      "159/388, train_loss: 0.1389, step time: 1.5321\n",
      "160/388, train_loss: 0.0967, step time: 1.5378\n",
      "161/388, train_loss: 0.1159, step time: 1.5357\n",
      "162/388, train_loss: 0.2074, step time: 1.5334\n",
      "163/388, train_loss: 0.3346, step time: 1.5321\n",
      "164/388, train_loss: 0.2109, step time: 1.5328\n",
      "165/388, train_loss: 0.2188, step time: 1.5335\n",
      "166/388, train_loss: 0.1531, step time: 1.5579\n",
      "167/388, train_loss: 0.1072, step time: 1.5490\n",
      "168/388, train_loss: 0.1288, step time: 1.5327\n",
      "169/388, train_loss: 0.1034, step time: 1.5336\n",
      "170/388, train_loss: 0.0418, step time: 1.5364\n",
      "171/388, train_loss: 0.2329, step time: 1.5372\n",
      "172/388, train_loss: 0.1373, step time: 1.5326\n",
      "173/388, train_loss: 0.0890, step time: 1.5337\n",
      "174/388, train_loss: 0.1360, step time: 1.5392\n",
      "175/388, train_loss: 0.0863, step time: 1.5379\n",
      "176/388, train_loss: 0.1474, step time: 1.5365\n",
      "177/388, train_loss: 0.0917, step time: 1.5331\n",
      "178/388, train_loss: 0.1854, step time: 1.5331\n",
      "179/388, train_loss: 0.1718, step time: 1.5334\n",
      "180/388, train_loss: 0.1075, step time: 1.5376\n",
      "181/388, train_loss: 0.1927, step time: 1.5379\n",
      "182/388, train_loss: 0.1284, step time: 1.5346\n",
      "183/388, train_loss: 0.3175, step time: 1.5329\n",
      "184/388, train_loss: 0.1147, step time: 1.5340\n",
      "185/388, train_loss: 0.1548, step time: 1.5345\n",
      "186/388, train_loss: 0.0971, step time: 1.5367\n",
      "187/388, train_loss: 0.2796, step time: 1.5363\n",
      "188/388, train_loss: 0.1057, step time: 1.5350\n",
      "189/388, train_loss: 0.2270, step time: 1.5347\n",
      "190/388, train_loss: 0.2610, step time: 1.5347\n",
      "191/388, train_loss: 0.1467, step time: 1.5400\n",
      "192/388, train_loss: 0.1919, step time: 1.5364\n",
      "193/388, train_loss: 0.2674, step time: 1.5349\n",
      "194/388, train_loss: 0.2639, step time: 1.5346\n",
      "195/388, train_loss: 0.1214, step time: 1.5338\n",
      "196/388, train_loss: 0.0530, step time: 1.5370\n",
      "197/388, train_loss: 0.3801, step time: 1.5341\n",
      "198/388, train_loss: 0.1599, step time: 1.5371\n",
      "199/388, train_loss: 0.2009, step time: 1.5322\n",
      "200/388, train_loss: 0.0905, step time: 1.5293\n",
      "201/388, train_loss: 0.4407, step time: 1.5329\n",
      "202/388, train_loss: 0.2210, step time: 1.5362\n",
      "203/388, train_loss: 0.1052, step time: 1.5368\n",
      "204/388, train_loss: 0.1914, step time: 1.5378\n",
      "205/388, train_loss: 0.3750, step time: 1.5320\n",
      "206/388, train_loss: 0.2020, step time: 1.5329\n",
      "207/388, train_loss: 0.1045, step time: 1.5381\n",
      "208/388, train_loss: 0.1597, step time: 1.5389\n",
      "209/388, train_loss: 0.1263, step time: 1.5359\n",
      "210/388, train_loss: 0.0936, step time: 1.5346\n",
      "211/388, train_loss: 0.0642, step time: 1.5336\n",
      "212/388, train_loss: 0.1776, step time: 1.5323\n",
      "213/388, train_loss: 0.1761, step time: 1.5390\n",
      "214/388, train_loss: 0.3892, step time: 1.5343\n",
      "215/388, train_loss: 0.2703, step time: 1.5321\n",
      "216/388, train_loss: 0.0644, step time: 1.5332\n",
      "217/388, train_loss: 0.0662, step time: 1.5336\n",
      "218/388, train_loss: 0.1199, step time: 1.5382\n",
      "219/388, train_loss: 0.1423, step time: 1.5341\n",
      "220/388, train_loss: 0.0666, step time: 1.5341\n",
      "221/388, train_loss: 0.2007, step time: 1.5350\n",
      "222/388, train_loss: 0.0806, step time: 1.5322\n",
      "223/388, train_loss: 0.1856, step time: 1.5361\n",
      "224/388, train_loss: 0.2148, step time: 1.5359\n",
      "225/388, train_loss: 0.2331, step time: 1.5342\n",
      "226/388, train_loss: 0.2201, step time: 1.5327\n",
      "227/388, train_loss: 0.1184, step time: 1.5337\n",
      "228/388, train_loss: 0.1786, step time: 1.5341\n",
      "229/388, train_loss: 0.2186, step time: 1.5377\n",
      "230/388, train_loss: 0.1180, step time: 1.5377\n",
      "231/388, train_loss: 0.2718, step time: 1.5361\n",
      "232/388, train_loss: 0.0832, step time: 1.5327\n",
      "233/388, train_loss: 0.1356, step time: 1.5325\n",
      "234/388, train_loss: 0.0880, step time: 1.5333\n",
      "235/388, train_loss: 0.2282, step time: 1.5375\n",
      "236/388, train_loss: 0.0853, step time: 1.5384\n",
      "237/388, train_loss: 0.1099, step time: 1.5348\n",
      "238/388, train_loss: 0.3648, step time: 1.5333\n",
      "239/388, train_loss: 0.3053, step time: 1.5318\n",
      "240/388, train_loss: 0.2897, step time: 1.5346\n",
      "241/388, train_loss: 0.1363, step time: 1.5328\n",
      "242/388, train_loss: 0.1168, step time: 1.5347\n",
      "243/388, train_loss: 0.1889, step time: 1.5357\n",
      "244/388, train_loss: 0.2657, step time: 1.5360\n",
      "245/388, train_loss: 0.2609, step time: 1.5326\n",
      "246/388, train_loss: 0.0960, step time: 1.5319\n",
      "247/388, train_loss: 0.1049, step time: 1.5343\n",
      "248/388, train_loss: 0.1687, step time: 1.5375\n",
      "249/388, train_loss: 0.0545, step time: 1.5357\n",
      "250/388, train_loss: 0.1742, step time: 1.5375\n",
      "251/388, train_loss: 0.0956, step time: 1.5328\n",
      "252/388, train_loss: 0.1068, step time: 1.5332\n",
      "253/388, train_loss: 0.0989, step time: 1.5375\n",
      "254/388, train_loss: 0.1634, step time: 1.5371\n",
      "255/388, train_loss: 0.1479, step time: 1.5359\n",
      "256/388, train_loss: 0.5310, step time: 1.5332\n",
      "257/388, train_loss: 0.1770, step time: 1.5341\n",
      "258/388, train_loss: 0.0883, step time: 1.5336\n",
      "259/388, train_loss: 0.4188, step time: 1.5333\n",
      "260/388, train_loss: 0.0757, step time: 1.5346\n",
      "261/388, train_loss: 0.1401, step time: 1.5338\n",
      "262/388, train_loss: 0.0511, step time: 1.5379\n",
      "263/388, train_loss: 0.0995, step time: 1.5336\n",
      "264/388, train_loss: 0.0744, step time: 1.5324\n",
      "265/388, train_loss: 0.1648, step time: 1.5317\n",
      "266/388, train_loss: 0.0750, step time: 1.5328\n",
      "267/388, train_loss: 0.0694, step time: 1.5359\n",
      "268/388, train_loss: 0.1047, step time: 1.5360\n",
      "269/388, train_loss: 0.0634, step time: 1.5321\n",
      "270/388, train_loss: 0.1976, step time: 1.5317\n",
      "271/388, train_loss: 0.2930, step time: 1.5339\n",
      "272/388, train_loss: 0.2009, step time: 1.5349\n",
      "273/388, train_loss: 0.1056, step time: 1.5358\n",
      "274/388, train_loss: 0.2422, step time: 1.5352\n",
      "275/388, train_loss: 0.0932, step time: 1.5342\n",
      "276/388, train_loss: 0.1366, step time: 1.5343\n",
      "277/388, train_loss: 0.1394, step time: 1.5352\n",
      "278/388, train_loss: 0.0944, step time: 1.5362\n",
      "279/388, train_loss: 0.0875, step time: 1.5337\n",
      "280/388, train_loss: 0.1842, step time: 1.5329\n",
      "281/388, train_loss: 0.0955, step time: 1.5329\n",
      "282/388, train_loss: 0.1218, step time: 1.5356\n",
      "283/388, train_loss: 0.1423, step time: 1.5414\n",
      "284/388, train_loss: 0.2927, step time: 1.5340\n",
      "285/388, train_loss: 0.1989, step time: 1.5328\n",
      "286/388, train_loss: 0.3569, step time: 1.5329\n",
      "287/388, train_loss: 0.4490, step time: 1.5353\n",
      "288/388, train_loss: 0.0847, step time: 1.5401\n",
      "289/388, train_loss: 0.1512, step time: 1.5340\n",
      "290/388, train_loss: 0.0806, step time: 1.5339\n",
      "291/388, train_loss: 0.2775, step time: 1.5343\n",
      "292/388, train_loss: 0.0708, step time: 1.5363\n",
      "293/388, train_loss: 0.2933, step time: 1.5380\n",
      "294/388, train_loss: 0.1810, step time: 1.5364\n",
      "295/388, train_loss: 0.1352, step time: 1.5342\n",
      "296/388, train_loss: 0.0800, step time: 1.5373\n",
      "297/388, train_loss: 0.0471, step time: 1.5355\n",
      "298/388, train_loss: 0.1189, step time: 1.5353\n",
      "299/388, train_loss: 0.1054, step time: 1.5344\n",
      "300/388, train_loss: 0.1281, step time: 1.5370\n",
      "301/388, train_loss: 0.0992, step time: 1.5304\n",
      "302/388, train_loss: 0.1442, step time: 1.5297\n",
      "303/388, train_loss: 0.1868, step time: 1.5372\n",
      "304/388, train_loss: 0.3715, step time: 1.5385\n",
      "305/388, train_loss: 0.2017, step time: 1.5387\n",
      "306/388, train_loss: 0.1036, step time: 1.5345\n",
      "307/388, train_loss: 0.1062, step time: 1.5340\n",
      "308/388, train_loss: 0.1758, step time: 1.5376\n",
      "309/388, train_loss: 0.2374, step time: 1.5377\n",
      "310/388, train_loss: 0.1884, step time: 1.5350\n",
      "311/388, train_loss: 0.4459, step time: 1.5335\n",
      "312/388, train_loss: 0.2344, step time: 1.5308\n",
      "313/388, train_loss: 0.0821, step time: 1.5350\n",
      "314/388, train_loss: 0.2037, step time: 1.5361\n",
      "315/388, train_loss: 0.2528, step time: 1.5366\n",
      "316/388, train_loss: 0.1595, step time: 1.5376\n",
      "317/388, train_loss: 0.0682, step time: 1.5319\n",
      "318/388, train_loss: 0.3050, step time: 1.5303\n",
      "319/388, train_loss: 0.3268, step time: 1.5319\n",
      "320/388, train_loss: 0.2161, step time: 1.5346\n",
      "321/388, train_loss: 0.3750, step time: 1.5337\n",
      "322/388, train_loss: 0.1240, step time: 1.5365\n",
      "323/388, train_loss: 0.0963, step time: 1.5356\n",
      "324/388, train_loss: 0.0972, step time: 1.5316\n",
      "325/388, train_loss: 0.1253, step time: 1.5335\n",
      "326/388, train_loss: 0.1468, step time: 1.5351\n",
      "327/388, train_loss: 0.1950, step time: 1.5619\n",
      "328/388, train_loss: 0.2500, step time: 1.5329\n",
      "329/388, train_loss: 0.1812, step time: 1.5339\n",
      "330/388, train_loss: 0.3784, step time: 1.5455\n",
      "331/388, train_loss: 0.0706, step time: 1.5344\n",
      "332/388, train_loss: 0.2526, step time: 1.5340\n",
      "333/388, train_loss: 0.0724, step time: 1.5330\n",
      "334/388, train_loss: 0.1112, step time: 1.5347\n",
      "335/388, train_loss: 0.2001, step time: 1.5317\n",
      "336/388, train_loss: 0.0942, step time: 1.5337\n",
      "337/388, train_loss: 0.2545, step time: 1.5354\n",
      "338/388, train_loss: 0.3559, step time: 1.5321\n",
      "339/388, train_loss: 0.3793, step time: 1.5326\n",
      "340/388, train_loss: 0.0884, step time: 1.5322\n",
      "341/388, train_loss: 0.1556, step time: 1.5334\n",
      "342/388, train_loss: 0.2782, step time: 1.5425\n",
      "343/388, train_loss: 0.1712, step time: 1.5387\n",
      "344/388, train_loss: 0.1420, step time: 1.5349\n",
      "345/388, train_loss: 0.1371, step time: 1.5334\n",
      "346/388, train_loss: 0.3738, step time: 1.5341\n",
      "347/388, train_loss: 0.1363, step time: 1.5402\n",
      "348/388, train_loss: 0.0428, step time: 1.5345\n",
      "349/388, train_loss: 0.1078, step time: 1.5341\n",
      "350/388, train_loss: 0.2523, step time: 1.5371\n",
      "351/388, train_loss: 0.0655, step time: 1.5352\n",
      "352/388, train_loss: 0.1539, step time: 1.5378\n",
      "353/388, train_loss: 0.2245, step time: 1.5315\n",
      "354/388, train_loss: 0.2228, step time: 1.5332\n",
      "355/388, train_loss: 0.1389, step time: 1.5416\n",
      "356/388, train_loss: 0.2328, step time: 1.5393\n",
      "357/388, train_loss: 0.0928, step time: 1.5367\n",
      "358/388, train_loss: 0.4048, step time: 1.5352\n",
      "359/388, train_loss: 0.1899, step time: 1.5349\n",
      "360/388, train_loss: 0.4407, step time: 1.5373\n",
      "361/388, train_loss: 0.1570, step time: 1.5443\n",
      "362/388, train_loss: 0.0528, step time: 1.5385\n",
      "363/388, train_loss: 0.1956, step time: 1.5335\n",
      "364/388, train_loss: 0.1424, step time: 1.5385\n",
      "365/388, train_loss: 0.2569, step time: 1.5396\n",
      "366/388, train_loss: 0.2447, step time: 1.5344\n",
      "367/388, train_loss: 0.1141, step time: 1.5358\n",
      "368/388, train_loss: 0.2940, step time: 1.5340\n",
      "369/388, train_loss: 0.1499, step time: 1.5363\n",
      "370/388, train_loss: 0.3234, step time: 1.5397\n",
      "371/388, train_loss: 0.1496, step time: 1.5418\n",
      "372/388, train_loss: 0.0695, step time: 1.5408\n",
      "373/388, train_loss: 0.0495, step time: 1.5368\n",
      "374/388, train_loss: 0.1073, step time: 1.5400\n",
      "375/388, train_loss: 0.1103, step time: 1.5351\n",
      "376/388, train_loss: 0.1406, step time: 1.5355\n",
      "377/388, train_loss: 0.5041, step time: 1.5325\n",
      "378/388, train_loss: 0.1686, step time: 1.5370\n",
      "379/388, train_loss: 0.1080, step time: 1.5383\n",
      "380/388, train_loss: 0.0839, step time: 1.5458\n",
      "381/388, train_loss: 0.1843, step time: 1.5333\n",
      "382/388, train_loss: 0.1844, step time: 1.5331\n",
      "383/388, train_loss: 0.1049, step time: 1.5515\n",
      "384/388, train_loss: 0.1463, step time: 1.5359\n",
      "385/388, train_loss: 0.1175, step time: 1.5326\n",
      "386/388, train_loss: 0.1515, step time: 1.5328\n",
      "387/388, train_loss: 0.1969, step time: 1.5320\n",
      "388/388, train_loss: 0.0479, step time: 1.5359\n",
      "epoch 57 average loss: 0.1769\n",
      "saved new best metric model\n",
      "current epoch: 57 current mean dice: 0.7699 tc: 0.8175 wt: 0.9032 et: 0.5891\n",
      "best mean dice: 0.7699 at epoch: 57\n",
      "time consuming of epoch 57 is: 703.8725\n",
      "----------\n",
      "epoch 58/100\n",
      "1/388, train_loss: 0.1351, step time: 1.5529\n",
      "2/388, train_loss: 0.1535, step time: 1.5365\n",
      "3/388, train_loss: 0.0953, step time: 1.5352\n",
      "4/388, train_loss: 0.1768, step time: 1.5340\n",
      "5/388, train_loss: 0.1626, step time: 1.5375\n",
      "6/388, train_loss: 0.2590, step time: 1.5410\n",
      "7/388, train_loss: 0.1606, step time: 1.5352\n",
      "8/388, train_loss: 0.1529, step time: 1.5348\n",
      "9/388, train_loss: 0.1986, step time: 1.5438\n",
      "10/388, train_loss: 0.1040, step time: 1.5339\n",
      "11/388, train_loss: 0.3426, step time: 1.5324\n",
      "12/388, train_loss: 0.1622, step time: 1.5354\n",
      "13/388, train_loss: 0.1739, step time: 1.5309\n",
      "14/388, train_loss: 0.1089, step time: 1.5370\n",
      "15/388, train_loss: 0.1070, step time: 1.5476\n",
      "16/388, train_loss: 0.3697, step time: 1.5346\n",
      "17/388, train_loss: 0.1097, step time: 1.5361\n",
      "18/388, train_loss: 0.1150, step time: 1.5353\n",
      "19/388, train_loss: 0.1619, step time: 1.5374\n",
      "20/388, train_loss: 0.1067, step time: 1.5382\n",
      "21/388, train_loss: 0.1969, step time: 1.5366\n",
      "22/388, train_loss: 0.0722, step time: 1.5332\n",
      "23/388, train_loss: 0.0856, step time: 1.5339\n",
      "24/388, train_loss: 0.2569, step time: 1.5339\n",
      "25/388, train_loss: 0.1274, step time: 1.5365\n",
      "26/388, train_loss: 0.2091, step time: 1.5377\n",
      "27/388, train_loss: 0.1183, step time: 1.5348\n",
      "28/388, train_loss: 0.2233, step time: 1.5356\n",
      "29/388, train_loss: 0.0643, step time: 1.5370\n",
      "30/388, train_loss: 0.2211, step time: 1.5426\n",
      "31/388, train_loss: 0.2568, step time: 1.5388\n",
      "32/388, train_loss: 0.0937, step time: 1.5361\n",
      "33/388, train_loss: 0.0663, step time: 1.5321\n",
      "34/388, train_loss: 0.2671, step time: 1.5320\n",
      "35/388, train_loss: 0.1036, step time: 1.5351\n",
      "36/388, train_loss: 0.0839, step time: 1.5386\n",
      "37/388, train_loss: 0.2382, step time: 1.5348\n",
      "38/388, train_loss: 0.1786, step time: 1.5364\n",
      "39/388, train_loss: 0.2715, step time: 1.5341\n",
      "40/388, train_loss: 0.0853, step time: 1.5338\n",
      "41/388, train_loss: 0.1482, step time: 1.5326\n",
      "42/388, train_loss: 0.1107, step time: 1.5419\n",
      "43/388, train_loss: 0.2027, step time: 1.5338\n",
      "44/388, train_loss: 0.1125, step time: 1.5366\n",
      "45/388, train_loss: 0.2248, step time: 1.5349\n",
      "46/388, train_loss: 0.1062, step time: 1.5316\n",
      "47/388, train_loss: 0.1819, step time: 1.5363\n",
      "48/388, train_loss: 0.1658, step time: 1.5444\n",
      "49/388, train_loss: 0.1940, step time: 1.5352\n",
      "50/388, train_loss: 0.1437, step time: 1.5327\n",
      "51/388, train_loss: 0.0897, step time: 1.5330\n",
      "52/388, train_loss: 0.1883, step time: 1.5384\n",
      "53/388, train_loss: 0.1986, step time: 1.5379\n",
      "54/388, train_loss: 0.2421, step time: 1.5423\n",
      "55/388, train_loss: 0.2134, step time: 1.5371\n",
      "56/388, train_loss: 0.0664, step time: 1.5330\n",
      "57/388, train_loss: 0.0668, step time: 1.5345\n",
      "58/388, train_loss: 0.4645, step time: 1.5464\n",
      "59/388, train_loss: 0.1572, step time: 1.5342\n",
      "60/388, train_loss: 0.0803, step time: 1.5324\n",
      "61/388, train_loss: 0.3450, step time: 1.5333\n",
      "62/388, train_loss: 0.2411, step time: 1.5374\n",
      "63/388, train_loss: 0.2328, step time: 1.5370\n",
      "64/388, train_loss: 0.1054, step time: 1.5370\n",
      "65/388, train_loss: 0.1022, step time: 1.5369\n",
      "66/388, train_loss: 0.2486, step time: 1.5353\n",
      "67/388, train_loss: 0.2037, step time: 1.5385\n",
      "68/388, train_loss: 0.3395, step time: 1.5382\n",
      "69/388, train_loss: 0.1418, step time: 1.5371\n",
      "70/388, train_loss: 0.1142, step time: 1.5382\n",
      "71/388, train_loss: 0.1680, step time: 1.5328\n",
      "72/388, train_loss: 0.1141, step time: 1.5356\n",
      "73/388, train_loss: 0.3522, step time: 1.5334\n",
      "74/388, train_loss: 0.0816, step time: 1.5404\n",
      "75/388, train_loss: 0.1499, step time: 1.5553\n",
      "76/388, train_loss: 0.1952, step time: 1.5337\n",
      "77/388, train_loss: 0.4575, step time: 1.5345\n",
      "78/388, train_loss: 0.1023, step time: 1.5344\n",
      "79/388, train_loss: 0.1162, step time: 1.5354\n",
      "80/388, train_loss: 0.2479, step time: 1.5329\n",
      "81/388, train_loss: 0.2125, step time: 1.5321\n",
      "82/388, train_loss: 0.1845, step time: 1.5298\n",
      "83/388, train_loss: 0.1680, step time: 1.5313\n",
      "84/388, train_loss: 0.0715, step time: 1.5373\n",
      "85/388, train_loss: 0.2866, step time: 1.5398\n",
      "86/388, train_loss: 0.1250, step time: 1.5343\n",
      "87/388, train_loss: 0.2395, step time: 1.5325\n",
      "88/388, train_loss: 0.1207, step time: 1.5365\n",
      "89/388, train_loss: 0.0913, step time: 1.5353\n",
      "90/388, train_loss: 0.0760, step time: 1.5385\n",
      "91/388, train_loss: 0.2119, step time: 1.5338\n",
      "92/388, train_loss: 0.2252, step time: 1.5312\n",
      "93/388, train_loss: 0.1120, step time: 1.5356\n",
      "94/388, train_loss: 0.2162, step time: 1.5379\n",
      "95/388, train_loss: 0.0997, step time: 1.5369\n",
      "96/388, train_loss: 0.0676, step time: 1.5360\n",
      "97/388, train_loss: 0.4092, step time: 1.5325\n",
      "98/388, train_loss: 0.1379, step time: 1.5348\n",
      "99/388, train_loss: 0.0556, step time: 1.5389\n",
      "100/388, train_loss: 0.0867, step time: 1.5430\n",
      "101/388, train_loss: 0.3349, step time: 1.5622\n",
      "102/388, train_loss: 0.1530, step time: 1.5385\n",
      "103/388, train_loss: 0.1365, step time: 1.5374\n",
      "104/388, train_loss: 0.1158, step time: 1.5352\n",
      "105/388, train_loss: 0.1279, step time: 1.5343\n",
      "106/388, train_loss: 0.2499, step time: 1.5338\n",
      "107/388, train_loss: 0.0752, step time: 1.5319\n",
      "108/388, train_loss: 0.1190, step time: 1.5380\n",
      "109/388, train_loss: 0.1496, step time: 1.5437\n",
      "110/388, train_loss: 0.1757, step time: 1.5328\n",
      "111/388, train_loss: 0.1407, step time: 1.5338\n",
      "112/388, train_loss: 0.1019, step time: 1.5342\n",
      "113/388, train_loss: 0.1214, step time: 1.5363\n",
      "114/388, train_loss: 0.1505, step time: 1.5356\n",
      "115/388, train_loss: 0.2112, step time: 1.5458\n",
      "116/388, train_loss: 0.2238, step time: 1.5400\n",
      "117/388, train_loss: 0.3338, step time: 1.5448\n",
      "118/388, train_loss: 0.0850, step time: 1.5326\n",
      "119/388, train_loss: 0.3319, step time: 1.5341\n",
      "120/388, train_loss: 0.1368, step time: 1.5395\n",
      "121/388, train_loss: 0.2341, step time: 1.5301\n",
      "122/388, train_loss: 0.1037, step time: 1.5355\n",
      "123/388, train_loss: 0.3958, step time: 1.5349\n",
      "124/388, train_loss: 0.5542, step time: 1.5359\n",
      "125/388, train_loss: 0.2756, step time: 1.5331\n",
      "126/388, train_loss: 0.1943, step time: 1.5353\n",
      "127/388, train_loss: 0.1200, step time: 1.5321\n",
      "128/388, train_loss: 0.2502, step time: 1.5332\n",
      "129/388, train_loss: 0.2212, step time: 1.5376\n",
      "130/388, train_loss: 0.1902, step time: 1.5422\n",
      "131/388, train_loss: 0.1592, step time: 1.5412\n",
      "132/388, train_loss: 0.3422, step time: 1.5333\n",
      "133/388, train_loss: 0.1720, step time: 1.5496\n",
      "134/388, train_loss: 0.0809, step time: 1.5398\n",
      "135/388, train_loss: 0.0685, step time: 1.5359\n",
      "136/388, train_loss: 0.4974, step time: 1.5321\n",
      "137/388, train_loss: 0.2089, step time: 1.5336\n",
      "138/388, train_loss: 0.2673, step time: 1.5352\n",
      "139/388, train_loss: 0.2707, step time: 1.5370\n",
      "140/388, train_loss: 0.1001, step time: 1.5387\n",
      "141/388, train_loss: 0.2305, step time: 1.5317\n",
      "142/388, train_loss: 0.0347, step time: 1.5326\n",
      "143/388, train_loss: 0.1024, step time: 1.5317\n",
      "144/388, train_loss: 0.1445, step time: 1.5478\n",
      "145/388, train_loss: 0.1711, step time: 1.5347\n",
      "146/388, train_loss: 0.4898, step time: 1.5369\n",
      "147/388, train_loss: 0.1859, step time: 1.5321\n",
      "148/388, train_loss: 0.0835, step time: 1.5340\n",
      "149/388, train_loss: 0.2710, step time: 1.5386\n",
      "150/388, train_loss: 0.1522, step time: 1.5372\n",
      "151/388, train_loss: 0.0705, step time: 1.5332\n",
      "152/388, train_loss: 0.1363, step time: 1.5342\n",
      "153/388, train_loss: 0.0838, step time: 1.5329\n",
      "154/388, train_loss: 0.1148, step time: 1.5360\n",
      "155/388, train_loss: 0.1573, step time: 1.5380\n",
      "156/388, train_loss: 0.1224, step time: 1.5314\n",
      "157/388, train_loss: 0.0851, step time: 1.5311\n",
      "158/388, train_loss: 0.2215, step time: 1.5338\n",
      "159/388, train_loss: 0.1321, step time: 1.5380\n",
      "160/388, train_loss: 0.0906, step time: 1.5355\n",
      "161/388, train_loss: 0.0941, step time: 1.5355\n",
      "162/388, train_loss: 0.3007, step time: 1.5330\n",
      "163/388, train_loss: 0.0855, step time: 1.5344\n",
      "164/388, train_loss: 0.1136, step time: 1.5378\n",
      "165/388, train_loss: 0.1039, step time: 1.5366\n",
      "166/388, train_loss: 0.0926, step time: 1.5370\n",
      "167/388, train_loss: 0.1725, step time: 1.5334\n",
      "168/388, train_loss: 0.2015, step time: 1.5302\n",
      "169/388, train_loss: 0.2024, step time: 1.5338\n",
      "170/388, train_loss: 0.3000, step time: 1.5378\n",
      "171/388, train_loss: 0.1822, step time: 1.5394\n",
      "172/388, train_loss: 0.1389, step time: 1.5327\n",
      "173/388, train_loss: 0.2248, step time: 1.5338\n",
      "174/388, train_loss: 0.2587, step time: 1.5374\n",
      "175/388, train_loss: 0.0606, step time: 1.5358\n",
      "176/388, train_loss: 0.0351, step time: 1.5475\n",
      "177/388, train_loss: 0.2110, step time: 1.5327\n",
      "178/388, train_loss: 0.1379, step time: 1.5327\n",
      "179/388, train_loss: 0.0769, step time: 1.5365\n",
      "180/388, train_loss: 0.1003, step time: 1.5408\n",
      "181/388, train_loss: 0.3494, step time: 1.5371\n",
      "182/388, train_loss: 0.3404, step time: 1.5350\n",
      "183/388, train_loss: 0.1272, step time: 1.5632\n",
      "184/388, train_loss: 0.5808, step time: 1.5342\n",
      "185/388, train_loss: 0.1643, step time: 1.5430\n",
      "186/388, train_loss: 0.1388, step time: 1.5317\n",
      "187/388, train_loss: 0.3486, step time: 1.5333\n",
      "188/388, train_loss: 0.1853, step time: 1.5375\n",
      "189/388, train_loss: 0.1356, step time: 1.5385\n",
      "190/388, train_loss: 0.2049, step time: 1.5321\n",
      "191/388, train_loss: 0.0965, step time: 1.5340\n",
      "192/388, train_loss: 0.0948, step time: 1.5326\n",
      "193/388, train_loss: 0.2065, step time: 1.5347\n",
      "194/388, train_loss: 0.0940, step time: 1.5401\n",
      "195/388, train_loss: 0.1018, step time: 1.5380\n",
      "196/388, train_loss: 0.1277, step time: 1.5353\n",
      "197/388, train_loss: 0.2197, step time: 1.5331\n",
      "198/388, train_loss: 0.1867, step time: 1.5386\n",
      "199/388, train_loss: 0.1054, step time: 1.5378\n",
      "200/388, train_loss: 0.0617, step time: 1.5340\n",
      "201/388, train_loss: 0.1299, step time: 1.5319\n",
      "202/388, train_loss: 0.0636, step time: 1.5317\n",
      "203/388, train_loss: 0.1287, step time: 1.5355\n",
      "204/388, train_loss: 0.1976, step time: 1.5337\n",
      "205/388, train_loss: 0.4347, step time: 1.5375\n",
      "206/388, train_loss: 0.1459, step time: 1.5370\n",
      "207/388, train_loss: 0.0780, step time: 1.5338\n",
      "208/388, train_loss: 0.0553, step time: 1.5355\n",
      "209/388, train_loss: 0.2465, step time: 1.5372\n",
      "210/388, train_loss: 0.0740, step time: 1.5364\n",
      "211/388, train_loss: 0.1017, step time: 1.5351\n",
      "212/388, train_loss: 0.1687, step time: 1.5350\n",
      "213/388, train_loss: 0.0535, step time: 1.5335\n",
      "214/388, train_loss: 0.0825, step time: 1.5300\n",
      "215/388, train_loss: 0.1048, step time: 1.5338\n",
      "216/388, train_loss: 0.3616, step time: 1.5376\n",
      "217/388, train_loss: 0.0844, step time: 1.5391\n",
      "218/388, train_loss: 0.1095, step time: 1.5355\n",
      "219/388, train_loss: 0.2450, step time: 1.5329\n",
      "220/388, train_loss: 0.1682, step time: 1.5351\n",
      "221/388, train_loss: 0.2317, step time: 1.5371\n",
      "222/388, train_loss: 0.0611, step time: 1.5373\n",
      "223/388, train_loss: 0.1155, step time: 1.5344\n",
      "224/388, train_loss: 0.1658, step time: 1.5328\n",
      "225/388, train_loss: 0.0820, step time: 1.5349\n",
      "226/388, train_loss: 0.3791, step time: 1.5323\n",
      "227/388, train_loss: 0.1331, step time: 1.5371\n",
      "228/388, train_loss: 0.1562, step time: 1.5359\n",
      "229/388, train_loss: 0.0607, step time: 1.5328\n",
      "230/388, train_loss: 0.0478, step time: 1.5329\n",
      "231/388, train_loss: 0.1761, step time: 1.5331\n",
      "232/388, train_loss: 0.0895, step time: 1.5388\n",
      "233/388, train_loss: 0.0794, step time: 1.5357\n",
      "234/388, train_loss: 0.3563, step time: 1.5368\n",
      "235/388, train_loss: 0.0902, step time: 1.5351\n",
      "236/388, train_loss: 0.1874, step time: 1.5336\n",
      "237/388, train_loss: 0.1841, step time: 1.5340\n",
      "238/388, train_loss: 0.2515, step time: 1.5410\n",
      "239/388, train_loss: 0.0964, step time: 1.5365\n",
      "240/388, train_loss: 0.1638, step time: 1.5358\n",
      "241/388, train_loss: 0.1745, step time: 1.5333\n",
      "242/388, train_loss: 0.0855, step time: 1.5374\n",
      "243/388, train_loss: 0.3181, step time: 1.5367\n",
      "244/388, train_loss: 0.0992, step time: 1.5326\n",
      "245/388, train_loss: 0.0405, step time: 1.5326\n",
      "246/388, train_loss: 0.1857, step time: 1.5371\n",
      "247/388, train_loss: 0.5285, step time: 1.5336\n",
      "248/388, train_loss: 0.1532, step time: 1.5331\n",
      "249/388, train_loss: 0.1674, step time: 1.5321\n",
      "250/388, train_loss: 0.3173, step time: 1.5323\n",
      "251/388, train_loss: 0.2226, step time: 1.5297\n",
      "252/388, train_loss: 0.0652, step time: 1.5339\n",
      "253/388, train_loss: 0.1498, step time: 1.5353\n",
      "254/388, train_loss: 0.2179, step time: 1.5372\n",
      "255/388, train_loss: 0.1799, step time: 1.5453\n",
      "256/388, train_loss: 0.2023, step time: 1.5337\n",
      "257/388, train_loss: 0.1595, step time: 1.5372\n",
      "258/388, train_loss: 0.1991, step time: 1.5377\n",
      "259/388, train_loss: 0.2379, step time: 1.5381\n",
      "260/388, train_loss: 0.0993, step time: 1.5360\n",
      "261/388, train_loss: 0.1683, step time: 1.5339\n",
      "262/388, train_loss: 0.1767, step time: 1.5340\n",
      "263/388, train_loss: 0.1670, step time: 1.5323\n",
      "264/388, train_loss: 0.3022, step time: 1.5343\n",
      "265/388, train_loss: 0.1367, step time: 1.5385\n",
      "266/388, train_loss: 0.1391, step time: 1.5363\n",
      "267/388, train_loss: 0.2271, step time: 1.5317\n",
      "268/388, train_loss: 0.2752, step time: 1.5378\n",
      "269/388, train_loss: 0.2162, step time: 1.5410\n",
      "270/388, train_loss: 0.1941, step time: 1.5370\n",
      "271/388, train_loss: 0.2498, step time: 1.5373\n",
      "272/388, train_loss: 0.2179, step time: 1.5362\n",
      "273/388, train_loss: 0.0902, step time: 1.5341\n",
      "274/388, train_loss: 0.1717, step time: 1.5352\n",
      "275/388, train_loss: 0.1746, step time: 1.5346\n",
      "276/388, train_loss: 0.1882, step time: 1.5390\n",
      "277/388, train_loss: 0.0927, step time: 1.5352\n",
      "278/388, train_loss: 0.0923, step time: 1.5325\n",
      "279/388, train_loss: 0.1233, step time: 1.5340\n",
      "280/388, train_loss: 0.2619, step time: 1.5373\n",
      "281/388, train_loss: 0.1721, step time: 1.5384\n",
      "282/388, train_loss: 0.2567, step time: 1.5368\n",
      "283/388, train_loss: 0.1412, step time: 1.5445\n",
      "284/388, train_loss: 0.2088, step time: 1.5389\n",
      "285/388, train_loss: 0.2041, step time: 1.5453\n",
      "286/388, train_loss: 0.3428, step time: 1.5340\n",
      "287/388, train_loss: 0.0611, step time: 1.5324\n",
      "288/388, train_loss: 0.1630, step time: 1.5346\n",
      "289/388, train_loss: 0.0489, step time: 1.5365\n",
      "290/388, train_loss: 0.1142, step time: 1.5349\n",
      "291/388, train_loss: 0.1391, step time: 1.5337\n",
      "292/388, train_loss: 0.2689, step time: 1.5300\n",
      "293/388, train_loss: 0.5102, step time: 1.5317\n",
      "294/388, train_loss: 0.1932, step time: 1.5312\n",
      "295/388, train_loss: 0.1494, step time: 1.5360\n",
      "296/388, train_loss: 0.1008, step time: 1.5374\n",
      "297/388, train_loss: 0.1792, step time: 1.5348\n",
      "298/388, train_loss: 0.0941, step time: 1.5643\n",
      "299/388, train_loss: 0.1722, step time: 1.5368\n",
      "300/388, train_loss: 0.2131, step time: 1.5331\n",
      "301/388, train_loss: 0.1790, step time: 1.5314\n",
      "302/388, train_loss: 0.1632, step time: 1.5336\n",
      "303/388, train_loss: 0.3960, step time: 1.5352\n",
      "304/388, train_loss: 0.2757, step time: 1.5363\n",
      "305/388, train_loss: 0.2012, step time: 1.5354\n",
      "306/388, train_loss: 0.1899, step time: 1.5329\n",
      "307/388, train_loss: 0.1060, step time: 1.5316\n",
      "308/388, train_loss: 0.1774, step time: 1.5324\n",
      "309/388, train_loss: 0.4241, step time: 1.5306\n",
      "310/388, train_loss: 0.3217, step time: 1.5368\n",
      "311/388, train_loss: 0.1969, step time: 1.5394\n",
      "312/388, train_loss: 0.0273, step time: 1.5351\n",
      "313/388, train_loss: 0.0643, step time: 1.5325\n",
      "314/388, train_loss: 0.1442, step time: 1.5322\n",
      "315/388, train_loss: 0.1071, step time: 1.5335\n",
      "316/388, train_loss: 0.1366, step time: 1.5386\n",
      "317/388, train_loss: 0.0811, step time: 1.5355\n",
      "318/388, train_loss: 0.2957, step time: 1.5372\n",
      "319/388, train_loss: 0.1427, step time: 1.5338\n",
      "320/388, train_loss: 0.3550, step time: 1.5304\n",
      "321/388, train_loss: 0.2530, step time: 1.5401\n",
      "322/388, train_loss: 0.1787, step time: 1.5351\n",
      "323/388, train_loss: 0.1584, step time: 1.5335\n",
      "324/388, train_loss: 0.2471, step time: 1.5324\n",
      "325/388, train_loss: 0.0827, step time: 1.5334\n",
      "326/388, train_loss: 0.5549, step time: 1.5352\n",
      "327/388, train_loss: 0.3063, step time: 1.5349\n",
      "328/388, train_loss: 0.2919, step time: 1.5319\n",
      "329/388, train_loss: 0.1319, step time: 1.5329\n",
      "330/388, train_loss: 0.1553, step time: 1.5355\n",
      "331/388, train_loss: 0.0726, step time: 1.5408\n",
      "332/388, train_loss: 0.2133, step time: 1.5392\n",
      "333/388, train_loss: 0.1123, step time: 1.5371\n",
      "334/388, train_loss: 0.0770, step time: 1.5342\n",
      "335/388, train_loss: 0.0502, step time: 1.5337\n",
      "336/388, train_loss: 0.2256, step time: 1.5330\n",
      "337/388, train_loss: 0.1362, step time: 1.5458\n",
      "338/388, train_loss: 0.2350, step time: 1.5363\n",
      "339/388, train_loss: 0.0935, step time: 1.5343\n",
      "340/388, train_loss: 0.2980, step time: 1.5350\n",
      "341/388, train_loss: 0.2039, step time: 1.5333\n",
      "342/388, train_loss: 0.0974, step time: 1.5378\n",
      "343/388, train_loss: 0.1860, step time: 1.5387\n",
      "344/388, train_loss: 0.0844, step time: 1.5307\n",
      "345/388, train_loss: 0.0491, step time: 1.5326\n",
      "346/388, train_loss: 0.1018, step time: 1.5306\n",
      "347/388, train_loss: 0.4228, step time: 1.5340\n",
      "348/388, train_loss: 0.0919, step time: 1.5379\n",
      "349/388, train_loss: 0.1783, step time: 1.5352\n",
      "350/388, train_loss: 0.1055, step time: 1.5339\n",
      "351/388, train_loss: 0.1079, step time: 1.5364\n",
      "352/388, train_loss: 0.0848, step time: 1.5374\n",
      "353/388, train_loss: 0.2589, step time: 1.5364\n",
      "354/388, train_loss: 0.0707, step time: 1.5388\n",
      "355/388, train_loss: 0.1953, step time: 1.5338\n",
      "356/388, train_loss: 0.0869, step time: 1.5334\n",
      "357/388, train_loss: 0.1538, step time: 1.5342\n",
      "358/388, train_loss: 0.0775, step time: 1.5377\n",
      "359/388, train_loss: 0.1264, step time: 1.5363\n",
      "360/388, train_loss: 0.1862, step time: 1.5350\n",
      "361/388, train_loss: 0.2006, step time: 1.5300\n",
      "362/388, train_loss: 0.1654, step time: 1.5299\n",
      "363/388, train_loss: 0.0877, step time: 1.5353\n",
      "364/388, train_loss: 0.2648, step time: 1.5370\n",
      "365/388, train_loss: 0.4788, step time: 1.5365\n",
      "366/388, train_loss: 0.0531, step time: 1.5332\n",
      "367/388, train_loss: 0.1873, step time: 1.5325\n",
      "368/388, train_loss: 0.1976, step time: 1.5313\n",
      "369/388, train_loss: 0.0849, step time: 1.5333\n",
      "370/388, train_loss: 0.1331, step time: 1.5426\n",
      "371/388, train_loss: 0.1263, step time: 1.5358\n",
      "372/388, train_loss: 0.1047, step time: 1.5328\n",
      "373/388, train_loss: 0.1052, step time: 1.5347\n",
      "374/388, train_loss: 0.0559, step time: 1.5359\n",
      "375/388, train_loss: 0.2281, step time: 1.5367\n",
      "376/388, train_loss: 0.2657, step time: 1.5349\n",
      "377/388, train_loss: 0.1641, step time: 1.5387\n",
      "378/388, train_loss: 0.0902, step time: 1.5348\n",
      "379/388, train_loss: 0.4482, step time: 1.5299\n",
      "380/388, train_loss: 0.1723, step time: 1.5334\n",
      "381/388, train_loss: 0.2844, step time: 1.5328\n",
      "382/388, train_loss: 0.2018, step time: 1.5392\n",
      "383/388, train_loss: 0.0376, step time: 1.5340\n",
      "384/388, train_loss: 0.2324, step time: 1.5347\n",
      "385/388, train_loss: 0.1182, step time: 1.5291\n",
      "386/388, train_loss: 0.1304, step time: 1.5320\n",
      "387/388, train_loss: 0.2137, step time: 1.5303\n",
      "388/388, train_loss: 0.1951, step time: 1.5332\n",
      "epoch 58 average loss: 0.1766\n",
      "saved new best metric model\n",
      "current epoch: 58 current mean dice: 0.7722 tc: 0.8191 wt: 0.9033 et: 0.5943\n",
      "best mean dice: 0.7722 at epoch: 58\n",
      "time consuming of epoch 58 is: 703.9342\n",
      "----------\n",
      "epoch 59/100\n",
      "1/388, train_loss: 0.2246, step time: 1.5521\n",
      "2/388, train_loss: 0.2362, step time: 1.5353\n",
      "3/388, train_loss: 0.1557, step time: 1.5388\n",
      "4/388, train_loss: 0.1612, step time: 1.5338\n",
      "5/388, train_loss: 0.4042, step time: 1.5344\n",
      "6/388, train_loss: 0.1613, step time: 1.5364\n",
      "7/388, train_loss: 0.1353, step time: 1.5350\n",
      "8/388, train_loss: 0.0647, step time: 1.5344\n",
      "9/388, train_loss: 0.0980, step time: 1.5361\n",
      "10/388, train_loss: 0.2344, step time: 1.5363\n",
      "11/388, train_loss: 0.0998, step time: 1.5346\n",
      "12/388, train_loss: 0.0413, step time: 1.5336\n",
      "13/388, train_loss: 0.1761, step time: 1.5318\n",
      "14/388, train_loss: 0.1044, step time: 1.5391\n",
      "15/388, train_loss: 0.1338, step time: 1.5368\n",
      "16/388, train_loss: 0.1041, step time: 1.5345\n",
      "17/388, train_loss: 0.0585, step time: 1.5354\n",
      "18/388, train_loss: 0.2061, step time: 1.5307\n",
      "19/388, train_loss: 0.1183, step time: 1.5323\n",
      "20/388, train_loss: 0.2035, step time: 1.5362\n",
      "21/388, train_loss: 0.1401, step time: 1.5372\n",
      "22/388, train_loss: 0.1202, step time: 1.5347\n",
      "23/388, train_loss: 0.0453, step time: 1.5339\n",
      "24/388, train_loss: 0.2651, step time: 1.5368\n",
      "25/388, train_loss: 0.3175, step time: 1.5365\n",
      "26/388, train_loss: 0.2886, step time: 1.5360\n",
      "27/388, train_loss: 0.3006, step time: 1.5340\n",
      "28/388, train_loss: 0.2041, step time: 1.5323\n",
      "29/388, train_loss: 0.4190, step time: 1.5374\n",
      "30/388, train_loss: 0.2450, step time: 1.5474\n",
      "31/388, train_loss: 0.2566, step time: 1.5352\n",
      "32/388, train_loss: 0.2781, step time: 1.5340\n",
      "33/388, train_loss: 0.1826, step time: 1.5387\n",
      "34/388, train_loss: 0.3106, step time: 1.5353\n",
      "35/388, train_loss: 0.0836, step time: 1.5326\n",
      "36/388, train_loss: 0.1426, step time: 1.5342\n",
      "37/388, train_loss: 0.6233, step time: 1.5340\n",
      "38/388, train_loss: 0.1526, step time: 1.5363\n",
      "39/388, train_loss: 0.3571, step time: 1.5392\n",
      "40/388, train_loss: 0.0794, step time: 1.5372\n",
      "41/388, train_loss: 0.4126, step time: 1.5426\n",
      "42/388, train_loss: 0.2853, step time: 1.5316\n",
      "43/388, train_loss: 0.1645, step time: 1.5421\n",
      "44/388, train_loss: 0.2012, step time: 1.5347\n",
      "45/388, train_loss: 0.2813, step time: 1.5353\n",
      "46/388, train_loss: 0.1015, step time: 1.5326\n",
      "47/388, train_loss: 0.2489, step time: 1.5324\n",
      "48/388, train_loss: 0.1956, step time: 1.5347\n",
      "49/388, train_loss: 0.0914, step time: 1.5373\n",
      "50/388, train_loss: 0.2092, step time: 1.5359\n",
      "51/388, train_loss: 0.2104, step time: 1.5333\n",
      "52/388, train_loss: 0.2334, step time: 1.5329\n",
      "53/388, train_loss: 0.0991, step time: 1.5313\n",
      "54/388, train_loss: 0.2616, step time: 1.5370\n",
      "55/388, train_loss: 0.0997, step time: 1.5581\n",
      "56/388, train_loss: 0.0577, step time: 1.5320\n",
      "57/388, train_loss: 0.2321, step time: 1.5343\n",
      "58/388, train_loss: 0.0678, step time: 1.5384\n",
      "59/388, train_loss: 0.0710, step time: 1.5360\n",
      "60/388, train_loss: 0.4485, step time: 1.5309\n",
      "61/388, train_loss: 0.1053, step time: 1.5349\n",
      "62/388, train_loss: 0.1940, step time: 1.5318\n",
      "63/388, train_loss: 0.1861, step time: 1.5406\n",
      "64/388, train_loss: 0.2564, step time: 1.5356\n",
      "65/388, train_loss: 0.0979, step time: 1.5339\n",
      "66/388, train_loss: 0.2005, step time: 1.5344\n",
      "67/388, train_loss: 0.2684, step time: 1.5295\n",
      "68/388, train_loss: 0.0866, step time: 1.5311\n",
      "69/388, train_loss: 0.1147, step time: 1.5366\n",
      "70/388, train_loss: 0.1773, step time: 1.5375\n",
      "71/388, train_loss: 0.1914, step time: 1.5357\n",
      "72/388, train_loss: 0.2476, step time: 1.5319\n",
      "73/388, train_loss: 0.1702, step time: 1.5304\n",
      "74/388, train_loss: 0.0678, step time: 1.5372\n",
      "75/388, train_loss: 0.1354, step time: 1.5339\n",
      "76/388, train_loss: 0.0465, step time: 1.5355\n",
      "77/388, train_loss: 0.2135, step time: 1.5342\n",
      "78/388, train_loss: 0.0960, step time: 1.5325\n",
      "79/388, train_loss: 0.0429, step time: 1.5323\n",
      "80/388, train_loss: 0.2966, step time: 1.5391\n",
      "81/388, train_loss: 0.2247, step time: 1.5357\n",
      "82/388, train_loss: 0.1580, step time: 1.5372\n",
      "83/388, train_loss: 0.1912, step time: 1.5332\n",
      "84/388, train_loss: 0.0807, step time: 1.5322\n",
      "85/388, train_loss: 0.1238, step time: 1.5316\n",
      "86/388, train_loss: 0.1730, step time: 1.5353\n",
      "87/388, train_loss: 0.0324, step time: 1.5377\n",
      "88/388, train_loss: 0.0882, step time: 1.5381\n",
      "89/388, train_loss: 0.0491, step time: 1.5344\n",
      "90/388, train_loss: 0.1096, step time: 1.5360\n",
      "91/388, train_loss: 0.2909, step time: 1.5354\n",
      "92/388, train_loss: 0.3485, step time: 1.5383\n",
      "93/388, train_loss: 0.4545, step time: 1.5369\n",
      "94/388, train_loss: 0.1803, step time: 1.5335\n",
      "95/388, train_loss: 0.3011, step time: 1.5346\n",
      "96/388, train_loss: 0.0818, step time: 1.5351\n",
      "97/388, train_loss: 0.2231, step time: 1.5348\n",
      "98/388, train_loss: 0.1435, step time: 1.5362\n",
      "99/388, train_loss: 0.2309, step time: 1.5374\n",
      "100/388, train_loss: 0.0802, step time: 1.5337\n",
      "101/388, train_loss: 0.1567, step time: 1.5346\n",
      "102/388, train_loss: 0.1414, step time: 1.5337\n",
      "103/388, train_loss: 0.1490, step time: 1.5347\n",
      "104/388, train_loss: 0.1580, step time: 1.5380\n",
      "105/388, train_loss: 0.1051, step time: 1.5360\n",
      "106/388, train_loss: 0.1152, step time: 1.5313\n",
      "107/388, train_loss: 0.1614, step time: 1.5324\n",
      "108/388, train_loss: 0.2696, step time: 1.5335\n",
      "109/388, train_loss: 0.1245, step time: 1.5406\n",
      "110/388, train_loss: 0.2024, step time: 1.5367\n",
      "111/388, train_loss: 0.0717, step time: 1.5355\n",
      "112/388, train_loss: 0.1779, step time: 1.5349\n",
      "113/388, train_loss: 0.1134, step time: 1.5345\n",
      "114/388, train_loss: 0.2082, step time: 1.5383\n",
      "115/388, train_loss: 0.2958, step time: 1.5362\n",
      "116/388, train_loss: 0.1556, step time: 1.5329\n",
      "117/388, train_loss: 0.3879, step time: 1.5319\n",
      "118/388, train_loss: 0.1227, step time: 1.5330\n",
      "119/388, train_loss: 0.2672, step time: 1.5360\n",
      "120/388, train_loss: 0.2104, step time: 1.5379\n",
      "121/388, train_loss: 0.1777, step time: 1.5372\n",
      "122/388, train_loss: 0.0779, step time: 1.5314\n",
      "123/388, train_loss: 0.0844, step time: 1.5328\n",
      "124/388, train_loss: 0.1460, step time: 1.5317\n",
      "125/388, train_loss: 0.1085, step time: 1.5314\n",
      "126/388, train_loss: 0.1304, step time: 1.5381\n",
      "127/388, train_loss: 0.1261, step time: 1.5355\n",
      "128/388, train_loss: 0.0604, step time: 1.5332\n",
      "129/388, train_loss: 0.1466, step time: 1.5327\n",
      "130/388, train_loss: 0.2029, step time: 1.5341\n",
      "131/388, train_loss: 0.2207, step time: 1.5339\n",
      "132/388, train_loss: 0.0951, step time: 1.5361\n",
      "133/388, train_loss: 0.0865, step time: 1.5402\n",
      "134/388, train_loss: 0.1498, step time: 1.5358\n",
      "135/388, train_loss: 0.1603, step time: 1.5346\n",
      "136/388, train_loss: 0.0995, step time: 1.5346\n",
      "137/388, train_loss: 0.1072, step time: 1.5324\n",
      "138/388, train_loss: 0.1895, step time: 1.5348\n",
      "139/388, train_loss: 0.1869, step time: 1.5344\n",
      "140/388, train_loss: 0.0824, step time: 1.5399\n",
      "141/388, train_loss: 0.1236, step time: 1.5407\n",
      "142/388, train_loss: 0.2624, step time: 1.5366\n",
      "143/388, train_loss: 0.0951, step time: 1.5361\n",
      "144/388, train_loss: 0.0748, step time: 1.5367\n",
      "145/388, train_loss: 0.1348, step time: 1.5351\n",
      "146/388, train_loss: 0.2770, step time: 1.5335\n",
      "147/388, train_loss: 0.0984, step time: 1.5320\n",
      "148/388, train_loss: 0.1548, step time: 1.5404\n",
      "149/388, train_loss: 0.1180, step time: 1.5370\n",
      "150/388, train_loss: 0.2637, step time: 1.5379\n",
      "151/388, train_loss: 0.3275, step time: 1.5352\n",
      "152/388, train_loss: 0.0649, step time: 1.5331\n",
      "153/388, train_loss: 0.0888, step time: 1.5348\n",
      "154/388, train_loss: 0.0883, step time: 1.5353\n",
      "155/388, train_loss: 0.1416, step time: 1.5333\n",
      "156/388, train_loss: 0.2041, step time: 1.5363\n",
      "157/388, train_loss: 0.0633, step time: 1.5371\n",
      "158/388, train_loss: 0.2758, step time: 1.5347\n",
      "159/388, train_loss: 0.0936, step time: 1.5330\n",
      "160/388, train_loss: 0.4459, step time: 1.5331\n",
      "161/388, train_loss: 0.1770, step time: 1.5405\n",
      "162/388, train_loss: 0.1712, step time: 1.5401\n",
      "163/388, train_loss: 0.0521, step time: 1.5391\n",
      "164/388, train_loss: 0.0853, step time: 1.5327\n",
      "165/388, train_loss: 0.2060, step time: 1.5308\n",
      "166/388, train_loss: 0.2563, step time: 1.5373\n",
      "167/388, train_loss: 0.1596, step time: 1.5347\n",
      "168/388, train_loss: 0.0876, step time: 1.5374\n",
      "169/388, train_loss: 0.1899, step time: 1.5314\n",
      "170/388, train_loss: 0.2027, step time: 1.5354\n",
      "171/388, train_loss: 0.1541, step time: 1.5316\n",
      "172/388, train_loss: 0.1558, step time: 1.5355\n",
      "173/388, train_loss: 0.1655, step time: 1.5393\n",
      "174/388, train_loss: 0.1982, step time: 1.5361\n",
      "175/388, train_loss: 0.1994, step time: 1.5337\n",
      "176/388, train_loss: 0.2436, step time: 1.5339\n",
      "177/388, train_loss: 0.1589, step time: 1.5407\n",
      "178/388, train_loss: 0.1635, step time: 1.5349\n",
      "179/388, train_loss: 0.0710, step time: 1.5378\n",
      "180/388, train_loss: 0.1761, step time: 1.5339\n",
      "181/388, train_loss: 0.0329, step time: 1.5333\n",
      "182/388, train_loss: 0.1078, step time: 1.5402\n",
      "183/388, train_loss: 0.2458, step time: 1.5636\n",
      "184/388, train_loss: 0.1590, step time: 1.5468\n",
      "185/388, train_loss: 0.3333, step time: 1.5328\n",
      "186/388, train_loss: 0.2312, step time: 1.5332\n",
      "187/388, train_loss: 0.0764, step time: 1.5326\n",
      "188/388, train_loss: 0.1329, step time: 1.5348\n",
      "189/388, train_loss: 0.3967, step time: 1.5392\n",
      "190/388, train_loss: 0.1211, step time: 1.5393\n",
      "191/388, train_loss: 0.2912, step time: 1.5329\n",
      "192/388, train_loss: 0.0818, step time: 1.5358\n",
      "193/388, train_loss: 0.1476, step time: 1.5327\n",
      "194/388, train_loss: 0.1187, step time: 1.5359\n",
      "195/388, train_loss: 0.1712, step time: 1.5395\n",
      "196/388, train_loss: 0.2500, step time: 1.5345\n",
      "197/388, train_loss: 0.1458, step time: 1.5301\n",
      "198/388, train_loss: 0.4647, step time: 1.5329\n",
      "199/388, train_loss: 0.0727, step time: 1.5358\n",
      "200/388, train_loss: 0.2382, step time: 1.5392\n",
      "201/388, train_loss: 0.1474, step time: 1.5368\n",
      "202/388, train_loss: 0.1155, step time: 1.5380\n",
      "203/388, train_loss: 0.1391, step time: 1.5363\n",
      "204/388, train_loss: 0.1536, step time: 1.5343\n",
      "205/388, train_loss: 0.1707, step time: 1.5367\n",
      "206/388, train_loss: 0.3877, step time: 1.5358\n",
      "207/388, train_loss: 0.1975, step time: 1.5377\n",
      "208/388, train_loss: 0.1806, step time: 1.5304\n",
      "209/388, train_loss: 0.3183, step time: 1.5318\n",
      "210/388, train_loss: 0.0745, step time: 1.5362\n",
      "211/388, train_loss: 0.1380, step time: 1.5396\n",
      "212/388, train_loss: 0.0637, step time: 1.5397\n",
      "213/388, train_loss: 0.1300, step time: 1.5346\n",
      "214/388, train_loss: 0.0725, step time: 1.5324\n",
      "215/388, train_loss: 0.1561, step time: 1.5369\n",
      "216/388, train_loss: 0.0908, step time: 1.5375\n",
      "217/388, train_loss: 0.1406, step time: 1.5353\n",
      "218/388, train_loss: 0.2108, step time: 1.5345\n",
      "219/388, train_loss: 0.1999, step time: 1.5344\n",
      "220/388, train_loss: 0.1121, step time: 1.5360\n",
      "221/388, train_loss: 0.0867, step time: 1.5327\n",
      "222/388, train_loss: 0.0965, step time: 1.5391\n",
      "223/388, train_loss: 0.0690, step time: 1.5372\n",
      "224/388, train_loss: 0.2864, step time: 1.5333\n",
      "225/388, train_loss: 0.1324, step time: 1.5327\n",
      "226/388, train_loss: 0.1346, step time: 1.5335\n",
      "227/388, train_loss: 0.2433, step time: 1.5360\n",
      "228/388, train_loss: 0.1002, step time: 1.5349\n",
      "229/388, train_loss: 0.0974, step time: 1.5344\n",
      "230/388, train_loss: 0.2988, step time: 1.5325\n",
      "231/388, train_loss: 0.0812, step time: 1.5321\n",
      "232/388, train_loss: 0.1567, step time: 1.5309\n",
      "233/388, train_loss: 0.0653, step time: 1.5345\n",
      "234/388, train_loss: 0.0869, step time: 1.5376\n",
      "235/388, train_loss: 0.0459, step time: 1.5342\n",
      "236/388, train_loss: 0.4644, step time: 1.5317\n",
      "237/388, train_loss: 0.2682, step time: 1.5404\n",
      "238/388, train_loss: 0.2627, step time: 1.5427\n",
      "239/388, train_loss: 0.0759, step time: 1.5363\n",
      "240/388, train_loss: 0.2621, step time: 1.5323\n",
      "241/388, train_loss: 0.2178, step time: 1.5334\n",
      "242/388, train_loss: 0.1005, step time: 1.5410\n",
      "243/388, train_loss: 0.1953, step time: 1.5362\n",
      "244/388, train_loss: 0.2549, step time: 1.5353\n",
      "245/388, train_loss: 0.0967, step time: 1.5324\n",
      "246/388, train_loss: 0.3485, step time: 1.5317\n",
      "247/388, train_loss: 0.1664, step time: 1.5338\n",
      "248/388, train_loss: 0.2660, step time: 1.5336\n",
      "249/388, train_loss: 0.1016, step time: 1.5351\n",
      "250/388, train_loss: 0.1164, step time: 1.5320\n",
      "251/388, train_loss: 0.0954, step time: 1.5316\n",
      "252/388, train_loss: 0.0932, step time: 1.5344\n",
      "253/388, train_loss: 0.1569, step time: 1.5318\n",
      "254/388, train_loss: 0.2713, step time: 1.5342\n",
      "255/388, train_loss: 0.1946, step time: 1.5377\n",
      "256/388, train_loss: 0.1450, step time: 1.5391\n",
      "257/388, train_loss: 0.2632, step time: 1.5335\n",
      "258/388, train_loss: 0.0961, step time: 1.5312\n",
      "259/388, train_loss: 0.0690, step time: 1.5305\n",
      "260/388, train_loss: 0.3803, step time: 1.5362\n",
      "261/388, train_loss: 0.0753, step time: 1.5344\n",
      "262/388, train_loss: 0.1370, step time: 1.5356\n",
      "263/388, train_loss: 0.0915, step time: 1.5363\n",
      "264/388, train_loss: 0.2055, step time: 1.5325\n",
      "265/388, train_loss: 0.0953, step time: 1.5345\n",
      "266/388, train_loss: 0.0888, step time: 1.5345\n",
      "267/388, train_loss: 0.1327, step time: 1.5352\n",
      "268/388, train_loss: 0.1648, step time: 1.5336\n",
      "269/388, train_loss: 0.4997, step time: 1.5357\n",
      "270/388, train_loss: 0.1914, step time: 1.5312\n",
      "271/388, train_loss: 0.0907, step time: 1.5331\n",
      "272/388, train_loss: 0.1526, step time: 1.5366\n",
      "273/388, train_loss: 0.1089, step time: 1.5365\n",
      "274/388, train_loss: 0.1460, step time: 1.5332\n",
      "275/388, train_loss: 0.2114, step time: 1.5335\n",
      "276/388, train_loss: 0.1337, step time: 1.5387\n",
      "277/388, train_loss: 0.2027, step time: 1.5573\n",
      "278/388, train_loss: 0.1214, step time: 1.5382\n",
      "279/388, train_loss: 0.0919, step time: 1.5378\n",
      "280/388, train_loss: 0.1010, step time: 1.5331\n",
      "281/388, train_loss: 0.1258, step time: 1.5318\n",
      "282/388, train_loss: 0.2671, step time: 1.5309\n",
      "283/388, train_loss: 0.0721, step time: 1.5378\n",
      "284/388, train_loss: 0.1759, step time: 1.5346\n",
      "285/388, train_loss: 0.1598, step time: 1.5317\n",
      "286/388, train_loss: 0.1095, step time: 1.5349\n",
      "287/388, train_loss: 0.1721, step time: 1.5316\n",
      "288/388, train_loss: 0.1942, step time: 1.5343\n",
      "289/388, train_loss: 0.0388, step time: 1.5375\n",
      "290/388, train_loss: 0.2621, step time: 1.5411\n",
      "291/388, train_loss: 0.1970, step time: 1.5394\n",
      "292/388, train_loss: 0.2187, step time: 1.5342\n",
      "293/388, train_loss: 0.3514, step time: 1.5377\n",
      "294/388, train_loss: 0.2103, step time: 1.5390\n",
      "295/388, train_loss: 0.2240, step time: 1.5363\n",
      "296/388, train_loss: 0.2944, step time: 1.5330\n",
      "297/388, train_loss: 0.1122, step time: 1.5339\n",
      "298/388, train_loss: 0.1345, step time: 1.5383\n",
      "299/388, train_loss: 0.0925, step time: 1.5377\n",
      "300/388, train_loss: 0.1080, step time: 1.5351\n",
      "301/388, train_loss: 0.5706, step time: 1.5330\n",
      "302/388, train_loss: 0.2102, step time: 1.5396\n",
      "303/388, train_loss: 0.2703, step time: 1.5384\n",
      "304/388, train_loss: 0.1531, step time: 1.5426\n",
      "305/388, train_loss: 0.1212, step time: 1.5320\n",
      "306/388, train_loss: 0.2076, step time: 1.5335\n",
      "307/388, train_loss: 0.1814, step time: 1.5356\n",
      "308/388, train_loss: 0.3812, step time: 1.5360\n",
      "309/388, train_loss: 0.1768, step time: 1.5357\n",
      "310/388, train_loss: 0.1945, step time: 1.5333\n",
      "311/388, train_loss: 0.1554, step time: 1.5347\n",
      "312/388, train_loss: 0.1053, step time: 1.5380\n",
      "313/388, train_loss: 0.1021, step time: 1.5362\n",
      "314/388, train_loss: 0.2133, step time: 1.5352\n",
      "315/388, train_loss: 0.3998, step time: 1.5319\n",
      "316/388, train_loss: 0.1005, step time: 1.5336\n",
      "317/388, train_loss: 0.1275, step time: 1.5319\n",
      "318/388, train_loss: 0.3427, step time: 1.5404\n",
      "319/388, train_loss: 0.0803, step time: 1.5375\n",
      "320/388, train_loss: 0.0805, step time: 1.5324\n",
      "321/388, train_loss: 0.1091, step time: 1.5551\n",
      "322/388, train_loss: 0.0964, step time: 1.5367\n",
      "323/388, train_loss: 0.1017, step time: 1.5353\n",
      "324/388, train_loss: 0.2178, step time: 1.5306\n",
      "325/388, train_loss: 0.0497, step time: 1.5322\n",
      "326/388, train_loss: 0.1302, step time: 1.5366\n",
      "327/388, train_loss: 0.0659, step time: 1.5349\n",
      "328/388, train_loss: 0.2289, step time: 1.5340\n",
      "329/388, train_loss: 0.1215, step time: 1.5353\n",
      "330/388, train_loss: 0.2840, step time: 1.5343\n",
      "331/388, train_loss: 0.2928, step time: 1.5335\n",
      "332/388, train_loss: 0.0266, step time: 1.5333\n",
      "333/388, train_loss: 0.1259, step time: 1.5362\n",
      "334/388, train_loss: 0.1812, step time: 1.5334\n",
      "335/388, train_loss: 0.1324, step time: 1.5360\n",
      "336/388, train_loss: 0.1252, step time: 1.5340\n",
      "337/388, train_loss: 0.2015, step time: 1.5322\n",
      "338/388, train_loss: 0.0826, step time: 1.5605\n",
      "339/388, train_loss: 0.3757, step time: 1.5313\n",
      "340/388, train_loss: 0.1726, step time: 1.5339\n",
      "341/388, train_loss: 0.1038, step time: 1.5282\n",
      "342/388, train_loss: 0.2345, step time: 1.5303\n",
      "343/388, train_loss: 0.1353, step time: 1.5444\n",
      "344/388, train_loss: 0.1271, step time: 1.5491\n",
      "345/388, train_loss: 0.5219, step time: 1.5417\n",
      "346/388, train_loss: 0.1842, step time: 1.5356\n",
      "347/388, train_loss: 0.1706, step time: 1.5380\n",
      "348/388, train_loss: 0.0588, step time: 1.5323\n",
      "349/388, train_loss: 0.0876, step time: 1.5310\n",
      "350/388, train_loss: 0.0648, step time: 1.5311\n",
      "351/388, train_loss: 0.1315, step time: 1.5313\n",
      "352/388, train_loss: 0.3689, step time: 1.5364\n",
      "353/388, train_loss: 0.0845, step time: 1.5341\n",
      "354/388, train_loss: 0.3272, step time: 1.5340\n",
      "355/388, train_loss: 0.2028, step time: 1.5306\n",
      "356/388, train_loss: 0.1858, step time: 1.5303\n",
      "357/388, train_loss: 0.2785, step time: 1.5334\n",
      "358/388, train_loss: 0.1185, step time: 1.5343\n",
      "359/388, train_loss: 0.2250, step time: 1.5335\n",
      "360/388, train_loss: 0.3145, step time: 1.5374\n",
      "361/388, train_loss: 0.2449, step time: 1.5452\n",
      "362/388, train_loss: 0.2220, step time: 1.5308\n",
      "363/388, train_loss: 0.2328, step time: 1.5293\n",
      "364/388, train_loss: 0.2635, step time: 1.5367\n",
      "365/388, train_loss: 0.1469, step time: 1.5334\n",
      "366/388, train_loss: 0.0686, step time: 1.5317\n",
      "367/388, train_loss: 0.0990, step time: 1.5304\n",
      "368/388, train_loss: 0.1259, step time: 1.5325\n",
      "369/388, train_loss: 0.1745, step time: 1.5318\n",
      "370/388, train_loss: 0.2180, step time: 1.5301\n",
      "371/388, train_loss: 0.1207, step time: 1.5337\n",
      "372/388, train_loss: 0.2506, step time: 1.5339\n",
      "373/388, train_loss: 0.0898, step time: 1.5364\n",
      "374/388, train_loss: 0.0946, step time: 1.5300\n",
      "375/388, train_loss: 0.1209, step time: 1.5314\n",
      "376/388, train_loss: 0.1514, step time: 1.5305\n",
      "377/388, train_loss: 0.0687, step time: 1.5322\n",
      "378/388, train_loss: 0.1550, step time: 1.5344\n",
      "379/388, train_loss: 0.3345, step time: 1.5356\n",
      "380/388, train_loss: 0.0670, step time: 1.5325\n",
      "381/388, train_loss: 0.0998, step time: 1.5302\n",
      "382/388, train_loss: 0.0960, step time: 1.5326\n",
      "383/388, train_loss: 0.0927, step time: 1.5322\n",
      "384/388, train_loss: 0.1932, step time: 1.5378\n",
      "385/388, train_loss: 0.1429, step time: 1.5368\n",
      "386/388, train_loss: 0.0811, step time: 1.5347\n",
      "387/388, train_loss: 0.2885, step time: 1.5311\n",
      "388/388, train_loss: 0.2311, step time: 1.5310\n",
      "epoch 59 average loss: 0.1747\n",
      "current epoch: 59 current mean dice: 0.7675 tc: 0.8116 wt: 0.9050 et: 0.5860\n",
      "best mean dice: 0.7722 at epoch: 58\n",
      "time consuming of epoch 59 is: 703.8541\n",
      "----------\n",
      "epoch 60/100\n",
      "1/388, train_loss: 0.1509, step time: 1.5460\n",
      "2/388, train_loss: 0.0300, step time: 1.5356\n",
      "3/388, train_loss: 0.1640, step time: 1.5322\n",
      "4/388, train_loss: 0.0819, step time: 1.5359\n",
      "5/388, train_loss: 0.0717, step time: 1.5372\n",
      "6/388, train_loss: 0.0908, step time: 1.5366\n",
      "7/388, train_loss: 0.1595, step time: 1.5310\n",
      "8/388, train_loss: 0.1258, step time: 1.5318\n",
      "9/388, train_loss: 0.2058, step time: 1.5340\n",
      "10/388, train_loss: 0.1537, step time: 1.5323\n",
      "11/388, train_loss: 0.1301, step time: 1.5314\n",
      "12/388, train_loss: 0.1359, step time: 1.5279\n",
      "13/388, train_loss: 0.1380, step time: 1.5305\n",
      "14/388, train_loss: 0.1254, step time: 1.5317\n",
      "15/388, train_loss: 0.0977, step time: 1.5384\n",
      "16/388, train_loss: 0.0858, step time: 1.5336\n",
      "17/388, train_loss: 0.2419, step time: 1.5444\n",
      "18/388, train_loss: 0.0714, step time: 1.5322\n",
      "19/388, train_loss: 0.2016, step time: 1.5341\n",
      "20/388, train_loss: 0.1331, step time: 1.5340\n",
      "21/388, train_loss: 0.0645, step time: 1.5369\n",
      "22/388, train_loss: 0.1005, step time: 1.5354\n",
      "23/388, train_loss: 0.1414, step time: 1.5324\n",
      "24/388, train_loss: 0.1239, step time: 1.5352\n",
      "25/388, train_loss: 0.2035, step time: 1.5318\n",
      "26/388, train_loss: 0.1118, step time: 1.5369\n",
      "27/388, train_loss: 0.1740, step time: 1.5393\n",
      "28/388, train_loss: 0.2143, step time: 1.5330\n",
      "29/388, train_loss: 0.3783, step time: 1.5310\n",
      "30/388, train_loss: 0.3752, step time: 1.5318\n",
      "31/388, train_loss: 0.0554, step time: 1.5304\n",
      "32/388, train_loss: 0.1232, step time: 1.5350\n",
      "33/388, train_loss: 0.1557, step time: 1.5331\n",
      "34/388, train_loss: 0.0855, step time: 1.5341\n",
      "35/388, train_loss: 0.1072, step time: 1.5366\n",
      "36/388, train_loss: 0.1883, step time: 1.5329\n",
      "37/388, train_loss: 0.2137, step time: 1.5352\n",
      "38/388, train_loss: 0.0873, step time: 1.5358\n",
      "39/388, train_loss: 0.1456, step time: 1.5361\n",
      "40/388, train_loss: 0.0426, step time: 1.5318\n",
      "41/388, train_loss: 0.3061, step time: 1.5318\n",
      "42/388, train_loss: 0.1825, step time: 1.5356\n",
      "43/388, train_loss: 0.2785, step time: 1.5343\n",
      "44/388, train_loss: 0.1217, step time: 1.5359\n",
      "45/388, train_loss: 0.1329, step time: 1.5286\n",
      "46/388, train_loss: 0.0930, step time: 1.5304\n",
      "47/388, train_loss: 0.1268, step time: 1.5334\n",
      "48/388, train_loss: 0.1009, step time: 1.5339\n",
      "49/388, train_loss: 0.1226, step time: 1.5364\n",
      "50/388, train_loss: 0.0976, step time: 1.5320\n",
      "51/388, train_loss: 0.2502, step time: 1.5313\n",
      "52/388, train_loss: 0.0998, step time: 1.5306\n",
      "53/388, train_loss: 0.1404, step time: 1.5304\n",
      "54/388, train_loss: 0.0926, step time: 1.5375\n",
      "55/388, train_loss: 0.0611, step time: 1.5336\n",
      "56/388, train_loss: 0.2165, step time: 1.5319\n",
      "57/388, train_loss: 0.0882, step time: 1.5328\n",
      "58/388, train_loss: 0.3394, step time: 1.5311\n",
      "59/388, train_loss: 0.1942, step time: 1.5346\n",
      "60/388, train_loss: 0.0903, step time: 1.5366\n",
      "61/388, train_loss: 0.1892, step time: 1.5319\n",
      "62/388, train_loss: 0.4209, step time: 1.5312\n",
      "63/388, train_loss: 0.1212, step time: 1.5308\n",
      "64/388, train_loss: 0.1569, step time: 1.5314\n",
      "65/388, train_loss: 0.0694, step time: 1.5351\n",
      "66/388, train_loss: 0.2101, step time: 1.5365\n",
      "67/388, train_loss: 0.1878, step time: 1.5318\n",
      "68/388, train_loss: 0.2432, step time: 1.5315\n",
      "69/388, train_loss: 0.1735, step time: 1.5308\n",
      "70/388, train_loss: 0.2785, step time: 1.5341\n",
      "71/388, train_loss: 0.1595, step time: 1.5341\n",
      "72/388, train_loss: 0.1390, step time: 1.5335\n",
      "73/388, train_loss: 0.3583, step time: 1.5467\n",
      "74/388, train_loss: 0.1621, step time: 1.5369\n",
      "75/388, train_loss: 0.1682, step time: 1.5361\n",
      "76/388, train_loss: 0.3749, step time: 1.5353\n",
      "77/388, train_loss: 0.2937, step time: 1.5368\n",
      "78/388, train_loss: 0.1078, step time: 1.5327\n",
      "79/388, train_loss: 0.0951, step time: 1.5308\n",
      "80/388, train_loss: 0.0795, step time: 1.5331\n",
      "81/388, train_loss: 0.1581, step time: 1.5339\n",
      "82/388, train_loss: 0.2156, step time: 1.5378\n",
      "83/388, train_loss: 0.2393, step time: 1.5338\n",
      "84/388, train_loss: 0.3942, step time: 1.5553\n",
      "85/388, train_loss: 0.2036, step time: 1.5353\n",
      "86/388, train_loss: 0.1965, step time: 1.5313\n",
      "87/388, train_loss: 0.1409, step time: 1.5332\n",
      "88/388, train_loss: 0.0852, step time: 1.5330\n",
      "89/388, train_loss: 0.1746, step time: 1.5300\n",
      "90/388, train_loss: 0.2554, step time: 1.5303\n",
      "91/388, train_loss: 0.3763, step time: 1.5292\n",
      "92/388, train_loss: 0.0600, step time: 1.5328\n",
      "93/388, train_loss: 0.1056, step time: 1.5287\n",
      "94/388, train_loss: 0.1230, step time: 1.5329\n",
      "95/388, train_loss: 0.1197, step time: 1.5310\n",
      "96/388, train_loss: 0.1890, step time: 1.5340\n",
      "97/388, train_loss: 0.2638, step time: 1.5351\n",
      "98/388, train_loss: 0.2104, step time: 1.5418\n",
      "99/388, train_loss: 0.1385, step time: 1.5327\n",
      "100/388, train_loss: 0.2740, step time: 1.5320\n",
      "101/388, train_loss: 0.0971, step time: 1.5329\n",
      "102/388, train_loss: 0.2632, step time: 1.5326\n",
      "103/388, train_loss: 0.1381, step time: 1.5339\n",
      "104/388, train_loss: 0.2381, step time: 1.5348\n",
      "105/388, train_loss: 0.0407, step time: 1.5311\n",
      "106/388, train_loss: 0.2190, step time: 1.5321\n",
      "107/388, train_loss: 0.0923, step time: 1.5368\n",
      "108/388, train_loss: 0.4701, step time: 1.5365\n",
      "109/388, train_loss: 0.0899, step time: 1.5337\n",
      "110/388, train_loss: 0.0461, step time: 1.5324\n",
      "111/388, train_loss: 0.2885, step time: 1.5285\n",
      "112/388, train_loss: 0.2841, step time: 1.5329\n",
      "113/388, train_loss: 0.0405, step time: 1.5343\n",
      "114/388, train_loss: 0.2367, step time: 1.5314\n",
      "115/388, train_loss: 0.0884, step time: 1.5342\n",
      "116/388, train_loss: 0.1981, step time: 1.5339\n",
      "117/388, train_loss: 0.1019, step time: 1.5317\n",
      "118/388, train_loss: 0.1501, step time: 1.5327\n",
      "119/388, train_loss: 0.0849, step time: 1.5347\n",
      "120/388, train_loss: 0.1769, step time: 1.5309\n",
      "121/388, train_loss: 0.1224, step time: 1.5285\n",
      "122/388, train_loss: 0.1145, step time: 1.5377\n",
      "123/388, train_loss: 0.2838, step time: 1.5355\n",
      "124/388, train_loss: 0.4299, step time: 1.5372\n",
      "125/388, train_loss: 0.1171, step time: 1.5333\n",
      "126/388, train_loss: 0.0693, step time: 1.5317\n",
      "127/388, train_loss: 0.1415, step time: 1.5350\n",
      "128/388, train_loss: 0.1630, step time: 1.5320\n",
      "129/388, train_loss: 0.0956, step time: 1.5497\n",
      "130/388, train_loss: 0.1165, step time: 1.5334\n",
      "131/388, train_loss: 0.1967, step time: 1.5308\n",
      "132/388, train_loss: 0.0944, step time: 1.5291\n",
      "133/388, train_loss: 0.0690, step time: 1.5312\n",
      "134/388, train_loss: 0.0854, step time: 1.5335\n",
      "135/388, train_loss: 0.4424, step time: 1.5342\n",
      "136/388, train_loss: 0.0979, step time: 1.5326\n",
      "137/388, train_loss: 0.0965, step time: 1.5337\n",
      "138/388, train_loss: 0.1751, step time: 1.5356\n",
      "139/388, train_loss: 0.2547, step time: 1.5359\n",
      "140/388, train_loss: 0.1073, step time: 1.5347\n",
      "141/388, train_loss: 0.2568, step time: 1.5368\n",
      "142/388, train_loss: 0.1245, step time: 1.5287\n",
      "143/388, train_loss: 0.3234, step time: 1.5319\n",
      "144/388, train_loss: 0.1548, step time: 1.5314\n",
      "145/388, train_loss: 0.2217, step time: 1.5324\n",
      "146/388, train_loss: 0.1008, step time: 1.5345\n",
      "147/388, train_loss: 0.1282, step time: 1.5323\n",
      "148/388, train_loss: 0.1730, step time: 1.5626\n",
      "149/388, train_loss: 0.1830, step time: 1.5348\n",
      "150/388, train_loss: 0.0820, step time: 1.5332\n",
      "151/388, train_loss: 0.2744, step time: 1.5375\n",
      "152/388, train_loss: 0.1756, step time: 1.5361\n",
      "153/388, train_loss: 0.1583, step time: 1.5303\n",
      "154/388, train_loss: 0.1045, step time: 1.5316\n",
      "155/388, train_loss: 0.1996, step time: 1.5315\n",
      "156/388, train_loss: 0.2507, step time: 1.5354\n",
      "157/388, train_loss: 0.1219, step time: 1.5351\n",
      "158/388, train_loss: 0.1356, step time: 1.5335\n",
      "159/388, train_loss: 0.1398, step time: 1.5316\n",
      "160/388, train_loss: 0.0638, step time: 1.5347\n",
      "161/388, train_loss: 0.0850, step time: 1.5322\n",
      "162/388, train_loss: 0.2740, step time: 1.5337\n",
      "163/388, train_loss: 0.1430, step time: 1.5325\n",
      "164/388, train_loss: 0.2041, step time: 1.5368\n",
      "165/388, train_loss: 0.1933, step time: 1.5326\n",
      "166/388, train_loss: 0.3105, step time: 1.5323\n",
      "167/388, train_loss: 0.4358, step time: 1.5292\n",
      "168/388, train_loss: 0.1966, step time: 1.5387\n",
      "169/388, train_loss: 0.0966, step time: 1.5362\n",
      "170/388, train_loss: 0.1221, step time: 1.5292\n",
      "171/388, train_loss: 0.1037, step time: 1.5305\n",
      "172/388, train_loss: 0.2135, step time: 1.5374\n",
      "173/388, train_loss: 0.0736, step time: 1.5360\n",
      "174/388, train_loss: 0.2723, step time: 1.5324\n",
      "175/388, train_loss: 0.0851, step time: 1.5349\n",
      "176/388, train_loss: 0.1514, step time: 1.5292\n",
      "177/388, train_loss: 0.0659, step time: 1.5325\n",
      "178/388, train_loss: 0.1592, step time: 1.5358\n",
      "179/388, train_loss: 0.1559, step time: 1.5333\n",
      "180/388, train_loss: 0.3651, step time: 1.5343\n",
      "181/388, train_loss: 0.1479, step time: 1.5324\n",
      "182/388, train_loss: 0.0832, step time: 1.5331\n",
      "183/388, train_loss: 0.0822, step time: 1.5309\n",
      "184/388, train_loss: 0.1919, step time: 1.5310\n",
      "185/388, train_loss: 0.1360, step time: 1.5432\n",
      "186/388, train_loss: 0.1584, step time: 1.5380\n",
      "187/388, train_loss: 0.1450, step time: 1.5341\n",
      "188/388, train_loss: 0.1145, step time: 1.5300\n",
      "189/388, train_loss: 0.0807, step time: 1.5355\n",
      "190/388, train_loss: 0.3232, step time: 1.5317\n",
      "191/388, train_loss: 0.0999, step time: 1.5327\n",
      "192/388, train_loss: 0.0875, step time: 1.5335\n",
      "193/388, train_loss: 0.1226, step time: 1.5324\n",
      "194/388, train_loss: 0.2149, step time: 1.5353\n",
      "195/388, train_loss: 0.0969, step time: 1.5311\n",
      "196/388, train_loss: 0.0925, step time: 1.5302\n",
      "197/388, train_loss: 0.0543, step time: 1.5333\n",
      "198/388, train_loss: 0.1502, step time: 1.5315\n",
      "199/388, train_loss: 0.2319, step time: 1.5376\n",
      "200/388, train_loss: 0.3329, step time: 1.5337\n",
      "201/388, train_loss: 0.1105, step time: 1.5325\n",
      "202/388, train_loss: 0.3436, step time: 1.5319\n",
      "203/388, train_loss: 0.1723, step time: 1.5316\n",
      "204/388, train_loss: 0.1433, step time: 1.5345\n",
      "205/388, train_loss: 0.1275, step time: 1.5332\n",
      "206/388, train_loss: 0.2389, step time: 1.5338\n",
      "207/388, train_loss: 0.1284, step time: 1.5315\n",
      "208/388, train_loss: 0.1138, step time: 1.5343\n",
      "209/388, train_loss: 0.2080, step time: 1.5308\n",
      "210/388, train_loss: 0.1164, step time: 1.5343\n",
      "211/388, train_loss: 0.2149, step time: 1.5398\n",
      "212/388, train_loss: 0.0806, step time: 1.5323\n",
      "213/388, train_loss: 0.2831, step time: 1.5348\n",
      "214/388, train_loss: 0.1082, step time: 1.5318\n",
      "215/388, train_loss: 0.1167, step time: 1.5306\n",
      "216/388, train_loss: 0.2519, step time: 1.5288\n",
      "217/388, train_loss: 0.1037, step time: 1.5358\n",
      "218/388, train_loss: 0.1026, step time: 1.5374\n",
      "219/388, train_loss: 0.0996, step time: 1.5350\n",
      "220/388, train_loss: 0.0890, step time: 1.5301\n",
      "221/388, train_loss: 0.1195, step time: 1.5308\n",
      "222/388, train_loss: 0.1678, step time: 1.5300\n",
      "223/388, train_loss: 0.2122, step time: 1.5318\n",
      "224/388, train_loss: 0.1752, step time: 1.5350\n",
      "225/388, train_loss: 0.2857, step time: 1.5363\n",
      "226/388, train_loss: 0.2257, step time: 1.5324\n",
      "227/388, train_loss: 0.1148, step time: 1.5325\n",
      "228/388, train_loss: 0.0667, step time: 1.5324\n",
      "229/388, train_loss: 0.1046, step time: 1.5314\n",
      "230/388, train_loss: 0.1762, step time: 1.5341\n",
      "231/388, train_loss: 0.1825, step time: 1.5352\n",
      "232/388, train_loss: 0.2687, step time: 1.5334\n",
      "233/388, train_loss: 0.1903, step time: 1.5342\n",
      "234/388, train_loss: 0.0646, step time: 1.5289\n",
      "235/388, train_loss: 0.2596, step time: 1.5296\n",
      "236/388, train_loss: 0.2964, step time: 1.5333\n",
      "237/388, train_loss: 0.0472, step time: 1.5296\n",
      "238/388, train_loss: 0.0964, step time: 1.5372\n",
      "239/388, train_loss: 0.1011, step time: 1.5324\n",
      "240/388, train_loss: 0.1261, step time: 1.5338\n",
      "241/388, train_loss: 0.2102, step time: 1.5328\n",
      "242/388, train_loss: 0.6246, step time: 1.5336\n",
      "243/388, train_loss: 0.2069, step time: 1.5318\n",
      "244/388, train_loss: 0.1969, step time: 1.5320\n",
      "245/388, train_loss: 0.3139, step time: 1.5345\n",
      "246/388, train_loss: 0.1427, step time: 1.5358\n",
      "247/388, train_loss: 0.0651, step time: 1.5311\n",
      "248/388, train_loss: 0.1541, step time: 1.5313\n",
      "249/388, train_loss: 0.2321, step time: 1.5277\n",
      "250/388, train_loss: 0.1370, step time: 1.5289\n",
      "251/388, train_loss: 0.1719, step time: 1.5323\n",
      "252/388, train_loss: 0.0927, step time: 1.5360\n",
      "253/388, train_loss: 0.1984, step time: 1.5348\n",
      "254/388, train_loss: 0.2041, step time: 1.5359\n",
      "255/388, train_loss: 0.3238, step time: 1.5339\n",
      "256/388, train_loss: 0.2829, step time: 1.5316\n",
      "257/388, train_loss: 0.0307, step time: 1.5341\n",
      "258/388, train_loss: 0.1570, step time: 1.5400\n",
      "259/388, train_loss: 0.1051, step time: 1.5341\n",
      "260/388, train_loss: 0.2056, step time: 1.5304\n",
      "261/388, train_loss: 0.2073, step time: 1.5317\n",
      "262/388, train_loss: 0.3052, step time: 1.5307\n",
      "263/388, train_loss: 0.0899, step time: 1.5347\n",
      "264/388, train_loss: 0.0820, step time: 1.5365\n",
      "265/388, train_loss: 0.4002, step time: 1.5391\n",
      "266/388, train_loss: 0.1480, step time: 1.5323\n",
      "267/388, train_loss: 0.1871, step time: 1.5306\n",
      "268/388, train_loss: 0.1531, step time: 1.5352\n",
      "269/388, train_loss: 0.5341, step time: 1.5296\n",
      "270/388, train_loss: 0.2519, step time: 1.5385\n",
      "271/388, train_loss: 0.1561, step time: 1.5355\n",
      "272/388, train_loss: 0.0627, step time: 1.5337\n",
      "273/388, train_loss: 0.0672, step time: 1.5332\n",
      "274/388, train_loss: 0.2184, step time: 1.5311\n",
      "275/388, train_loss: 0.1694, step time: 1.5339\n",
      "276/388, train_loss: 0.1721, step time: 1.5323\n",
      "277/388, train_loss: 0.2680, step time: 1.5373\n",
      "278/388, train_loss: 0.2276, step time: 1.5324\n",
      "279/388, train_loss: 0.1177, step time: 1.5301\n",
      "280/388, train_loss: 0.1943, step time: 1.5337\n",
      "281/388, train_loss: 0.4819, step time: 1.5313\n",
      "282/388, train_loss: 0.1335, step time: 1.5329\n",
      "283/388, train_loss: 0.0895, step time: 1.5319\n",
      "284/388, train_loss: 0.0317, step time: 1.5341\n",
      "285/388, train_loss: 0.0669, step time: 1.5342\n",
      "286/388, train_loss: 0.1340, step time: 1.5335\n",
      "287/388, train_loss: 0.3210, step time: 1.5296\n",
      "288/388, train_loss: 0.1052, step time: 1.5280\n",
      "289/388, train_loss: 0.2473, step time: 1.5310\n",
      "290/388, train_loss: 0.2060, step time: 1.5331\n",
      "291/388, train_loss: 0.1034, step time: 1.5335\n",
      "292/388, train_loss: 0.4126, step time: 1.5352\n",
      "293/388, train_loss: 0.2949, step time: 1.5364\n",
      "294/388, train_loss: 0.1528, step time: 1.5329\n",
      "295/388, train_loss: 0.1517, step time: 1.5330\n",
      "296/388, train_loss: 0.2159, step time: 1.5300\n",
      "297/388, train_loss: 0.1059, step time: 1.5398\n",
      "298/388, train_loss: 0.1204, step time: 1.5343\n",
      "299/388, train_loss: 0.1146, step time: 1.5306\n",
      "300/388, train_loss: 0.2160, step time: 1.5326\n",
      "301/388, train_loss: 0.1439, step time: 1.5376\n",
      "302/388, train_loss: 0.1255, step time: 1.5358\n",
      "303/388, train_loss: 0.1373, step time: 1.5292\n",
      "304/388, train_loss: 0.2039, step time: 1.5336\n",
      "305/388, train_loss: 0.0966, step time: 1.5292\n",
      "306/388, train_loss: 0.1854, step time: 1.5312\n",
      "307/388, train_loss: 0.1612, step time: 1.5354\n",
      "308/388, train_loss: 0.4596, step time: 1.5364\n",
      "309/388, train_loss: 0.2761, step time: 1.5311\n",
      "310/388, train_loss: 0.1005, step time: 1.5349\n",
      "311/388, train_loss: 0.1043, step time: 1.5310\n",
      "312/388, train_loss: 0.0765, step time: 1.5327\n",
      "313/388, train_loss: 0.5046, step time: 1.5366\n",
      "314/388, train_loss: 0.0856, step time: 1.5356\n",
      "315/388, train_loss: 0.2413, step time: 1.5314\n",
      "316/388, train_loss: 0.1433, step time: 1.5302\n",
      "317/388, train_loss: 0.1799, step time: 1.5355\n",
      "318/388, train_loss: 0.2143, step time: 1.5336\n",
      "319/388, train_loss: 0.3562, step time: 1.5375\n",
      "320/388, train_loss: 0.1448, step time: 1.5337\n",
      "321/388, train_loss: 0.2879, step time: 1.5337\n",
      "322/388, train_loss: 0.1547, step time: 1.5295\n",
      "323/388, train_loss: 0.1186, step time: 1.5305\n",
      "324/388, train_loss: 0.2823, step time: 1.5326\n",
      "325/388, train_loss: 0.3037, step time: 1.5308\n",
      "326/388, train_loss: 0.0818, step time: 1.5359\n",
      "327/388, train_loss: 0.0983, step time: 1.5335\n",
      "328/388, train_loss: 0.2291, step time: 1.5368\n",
      "329/388, train_loss: 0.0490, step time: 1.5314\n",
      "330/388, train_loss: 0.0939, step time: 1.5297\n",
      "331/388, train_loss: 0.2113, step time: 1.5319\n",
      "332/388, train_loss: 0.1003, step time: 1.5346\n",
      "333/388, train_loss: 0.1065, step time: 1.5366\n",
      "334/388, train_loss: 0.1704, step time: 1.5324\n",
      "335/388, train_loss: 0.0773, step time: 1.5310\n",
      "336/388, train_loss: 0.1036, step time: 1.5315\n",
      "337/388, train_loss: 0.0934, step time: 1.5305\n",
      "338/388, train_loss: 0.1079, step time: 1.5307\n",
      "339/388, train_loss: 0.0517, step time: 1.5513\n",
      "340/388, train_loss: 0.0778, step time: 1.5348\n",
      "341/388, train_loss: 0.2536, step time: 1.5312\n",
      "342/388, train_loss: 0.0838, step time: 1.5402\n",
      "343/388, train_loss: 0.1215, step time: 1.5353\n",
      "344/388, train_loss: 0.1231, step time: 1.5365\n",
      "345/388, train_loss: 0.1961, step time: 1.5369\n",
      "346/388, train_loss: 0.0656, step time: 1.5388\n",
      "347/388, train_loss: 0.1015, step time: 1.5330\n",
      "348/388, train_loss: 0.1952, step time: 1.5299\n",
      "349/388, train_loss: 0.2081, step time: 1.5337\n",
      "350/388, train_loss: 0.1814, step time: 1.5339\n",
      "351/388, train_loss: 0.2706, step time: 1.5334\n",
      "352/388, train_loss: 0.2303, step time: 1.5371\n",
      "353/388, train_loss: 0.1626, step time: 1.5335\n",
      "354/388, train_loss: 0.2469, step time: 1.5322\n",
      "355/388, train_loss: 0.3017, step time: 1.5358\n",
      "356/388, train_loss: 0.2609, step time: 1.5331\n",
      "357/388, train_loss: 0.3313, step time: 1.5319\n",
      "358/388, train_loss: 0.1527, step time: 1.5319\n",
      "359/388, train_loss: 0.1020, step time: 1.5295\n",
      "360/388, train_loss: 0.2810, step time: 1.5321\n",
      "361/388, train_loss: 0.0853, step time: 1.5325\n",
      "362/388, train_loss: 0.0816, step time: 1.5355\n",
      "363/388, train_loss: 0.1267, step time: 1.5379\n",
      "364/388, train_loss: 0.1461, step time: 1.5356\n",
      "365/388, train_loss: 0.1887, step time: 1.5316\n",
      "366/388, train_loss: 0.3762, step time: 1.5307\n",
      "367/388, train_loss: 0.3523, step time: 1.5342\n",
      "368/388, train_loss: 0.0627, step time: 1.5370\n",
      "369/388, train_loss: 0.2041, step time: 1.5342\n",
      "370/388, train_loss: 0.1587, step time: 1.5326\n",
      "371/388, train_loss: 0.0446, step time: 1.5302\n",
      "372/388, train_loss: 0.2453, step time: 1.5306\n",
      "373/388, train_loss: 0.1778, step time: 1.5356\n",
      "374/388, train_loss: 0.3422, step time: 1.5414\n",
      "375/388, train_loss: 0.4351, step time: 1.5315\n",
      "376/388, train_loss: 0.1737, step time: 1.5345\n",
      "377/388, train_loss: 0.3586, step time: 1.5312\n",
      "378/388, train_loss: 0.1823, step time: 1.5335\n",
      "379/388, train_loss: 0.0692, step time: 1.5353\n",
      "380/388, train_loss: 0.2710, step time: 1.5342\n",
      "381/388, train_loss: 0.1793, step time: 1.5342\n",
      "382/388, train_loss: 0.2307, step time: 1.5341\n",
      "383/388, train_loss: 0.1429, step time: 1.5284\n",
      "384/388, train_loss: 0.1142, step time: 1.5308\n",
      "385/388, train_loss: 0.1334, step time: 1.5313\n",
      "386/388, train_loss: 0.2017, step time: 1.5323\n",
      "387/388, train_loss: 0.2406, step time: 1.5325\n",
      "388/388, train_loss: 0.2950, step time: 1.5320\n",
      "epoch 60 average loss: 0.1748\n",
      "current epoch: 60 current mean dice: 0.7645 tc: 0.8125 wt: 0.8990 et: 0.5818\n",
      "best mean dice: 0.7722 at epoch: 58\n",
      "time consuming of epoch 60 is: 703.6700\n",
      "----------\n",
      "epoch 61/100\n",
      "1/388, train_loss: 0.3133, step time: 1.5496\n",
      "2/388, train_loss: 0.0717, step time: 1.5326\n",
      "3/388, train_loss: 0.0940, step time: 1.5327\n",
      "4/388, train_loss: 0.2631, step time: 1.5342\n",
      "5/388, train_loss: 0.0922, step time: 1.5354\n",
      "6/388, train_loss: 0.3950, step time: 1.5315\n",
      "7/388, train_loss: 0.1234, step time: 1.5318\n",
      "8/388, train_loss: 0.1934, step time: 1.5379\n",
      "9/388, train_loss: 0.1977, step time: 1.5318\n",
      "10/388, train_loss: 0.1357, step time: 1.5312\n",
      "11/388, train_loss: 0.2414, step time: 1.5296\n",
      "12/388, train_loss: 0.0955, step time: 1.5341\n",
      "13/388, train_loss: 0.2104, step time: 1.5341\n",
      "14/388, train_loss: 0.1485, step time: 1.5328\n",
      "15/388, train_loss: 0.2758, step time: 1.5348\n",
      "16/388, train_loss: 0.0664, step time: 1.5326\n",
      "17/388, train_loss: 0.0955, step time: 1.5299\n",
      "18/388, train_loss: 0.0838, step time: 1.5285\n",
      "19/388, train_loss: 0.0855, step time: 1.5326\n",
      "20/388, train_loss: 0.2520, step time: 1.5347\n",
      "21/388, train_loss: 0.1551, step time: 1.5349\n",
      "22/388, train_loss: 0.1524, step time: 1.5377\n",
      "23/388, train_loss: 0.2890, step time: 1.5307\n",
      "24/388, train_loss: 0.1390, step time: 1.5321\n",
      "25/388, train_loss: 0.4112, step time: 1.5309\n",
      "26/388, train_loss: 0.1075, step time: 1.5307\n",
      "27/388, train_loss: 0.1446, step time: 1.5355\n",
      "28/388, train_loss: 0.1133, step time: 1.5371\n",
      "29/388, train_loss: 0.2148, step time: 1.5334\n",
      "30/388, train_loss: 0.1605, step time: 1.5332\n",
      "31/388, train_loss: 0.5463, step time: 1.5325\n",
      "32/388, train_loss: 0.1309, step time: 1.5339\n",
      "33/388, train_loss: 0.0841, step time: 1.5343\n",
      "34/388, train_loss: 0.1134, step time: 1.5364\n",
      "35/388, train_loss: 0.0914, step time: 1.5392\n",
      "36/388, train_loss: 0.1764, step time: 1.5375\n",
      "37/388, train_loss: 0.2533, step time: 1.5352\n",
      "38/388, train_loss: 0.0972, step time: 1.5330\n",
      "39/388, train_loss: 0.0937, step time: 1.5374\n",
      "40/388, train_loss: 0.1282, step time: 1.5366\n",
      "41/388, train_loss: 0.0567, step time: 1.5333\n",
      "42/388, train_loss: 0.1511, step time: 1.5316\n",
      "43/388, train_loss: 0.0728, step time: 1.5519\n",
      "44/388, train_loss: 0.2613, step time: 1.5398\n",
      "45/388, train_loss: 0.1922, step time: 1.5352\n",
      "46/388, train_loss: 0.4054, step time: 1.5398\n",
      "47/388, train_loss: 0.2077, step time: 1.5376\n",
      "48/388, train_loss: 0.1639, step time: 1.5317\n",
      "49/388, train_loss: 0.1408, step time: 1.5334\n",
      "50/388, train_loss: 0.4255, step time: 1.5316\n",
      "51/388, train_loss: 0.0798, step time: 1.5320\n",
      "52/388, train_loss: 0.2643, step time: 1.5381\n",
      "53/388, train_loss: 0.0761, step time: 1.5331\n",
      "54/388, train_loss: 0.2550, step time: 1.5349\n",
      "55/388, train_loss: 0.1061, step time: 1.5332\n",
      "56/388, train_loss: 0.1496, step time: 1.5492\n",
      "57/388, train_loss: 0.0405, step time: 1.5314\n",
      "58/388, train_loss: 0.1100, step time: 1.5344\n",
      "59/388, train_loss: 0.1756, step time: 1.5366\n",
      "60/388, train_loss: 0.7582, step time: 1.5476\n",
      "61/388, train_loss: 0.3280, step time: 1.5323\n",
      "62/388, train_loss: 0.1063, step time: 1.5342\n",
      "63/388, train_loss: 0.2058, step time: 1.5363\n",
      "64/388, train_loss: 0.2259, step time: 1.5360\n",
      "65/388, train_loss: 0.2682, step time: 1.5325\n",
      "66/388, train_loss: 0.1237, step time: 1.5327\n",
      "67/388, train_loss: 0.0851, step time: 1.5297\n",
      "68/388, train_loss: 0.1239, step time: 1.5340\n",
      "69/388, train_loss: 0.1977, step time: 1.5364\n",
      "70/388, train_loss: 0.0841, step time: 1.5530\n",
      "71/388, train_loss: 0.0975, step time: 1.5313\n",
      "72/388, train_loss: 0.1640, step time: 1.5331\n",
      "73/388, train_loss: 0.0980, step time: 1.5350\n",
      "74/388, train_loss: 0.1451, step time: 1.5326\n",
      "75/388, train_loss: 0.1259, step time: 1.5317\n",
      "76/388, train_loss: 0.1671, step time: 1.5322\n",
      "77/388, train_loss: 0.0979, step time: 1.5336\n",
      "78/388, train_loss: 0.1584, step time: 1.5324\n",
      "79/388, train_loss: 0.2281, step time: 1.5342\n",
      "80/388, train_loss: 0.0643, step time: 1.5292\n",
      "81/388, train_loss: 0.1591, step time: 1.5357\n",
      "82/388, train_loss: 0.1894, step time: 1.5406\n",
      "83/388, train_loss: 0.0945, step time: 1.5488\n",
      "84/388, train_loss: 0.1879, step time: 1.5331\n",
      "85/388, train_loss: 0.2967, step time: 1.5289\n",
      "86/388, train_loss: 0.0537, step time: 1.5354\n",
      "87/388, train_loss: 0.0923, step time: 1.5343\n",
      "88/388, train_loss: 0.1004, step time: 1.5349\n",
      "89/388, train_loss: 0.1429, step time: 1.5343\n",
      "90/388, train_loss: 0.1804, step time: 1.5346\n",
      "91/388, train_loss: 0.1797, step time: 1.5323\n",
      "92/388, train_loss: 0.1234, step time: 1.5314\n",
      "93/388, train_loss: 0.1155, step time: 1.5326\n",
      "94/388, train_loss: 0.2751, step time: 1.5321\n",
      "95/388, train_loss: 0.1026, step time: 1.5310\n",
      "96/388, train_loss: 0.2186, step time: 1.5326\n",
      "97/388, train_loss: 0.1938, step time: 1.5316\n",
      "98/388, train_loss: 0.1453, step time: 1.5296\n",
      "99/388, train_loss: 0.2035, step time: 1.5337\n",
      "100/388, train_loss: 0.0845, step time: 1.5341\n",
      "101/388, train_loss: 0.0586, step time: 1.5345\n",
      "102/388, train_loss: 0.2620, step time: 1.5327\n",
      "103/388, train_loss: 0.0425, step time: 1.5280\n",
      "104/388, train_loss: 0.0841, step time: 1.5298\n",
      "105/388, train_loss: 0.3148, step time: 1.5328\n",
      "106/388, train_loss: 0.2663, step time: 1.5305\n",
      "107/388, train_loss: 0.0337, step time: 1.5367\n",
      "108/388, train_loss: 0.1859, step time: 1.5327\n",
      "109/388, train_loss: 0.2584, step time: 1.5322\n",
      "110/388, train_loss: 0.1702, step time: 1.5272\n",
      "111/388, train_loss: 0.1354, step time: 1.5311\n",
      "112/388, train_loss: 0.2153, step time: 1.5367\n",
      "113/388, train_loss: 0.0976, step time: 1.5335\n",
      "114/388, train_loss: 0.2573, step time: 1.5301\n",
      "115/388, train_loss: 0.0938, step time: 1.5316\n",
      "116/388, train_loss: 0.4164, step time: 1.5298\n",
      "117/388, train_loss: 0.0917, step time: 1.5346\n",
      "118/388, train_loss: 0.3589, step time: 1.5375\n",
      "119/388, train_loss: 0.0449, step time: 1.5326\n",
      "120/388, train_loss: 0.2919, step time: 1.5306\n",
      "121/388, train_loss: 0.2805, step time: 1.5306\n",
      "122/388, train_loss: 0.1738, step time: 1.5349\n",
      "123/388, train_loss: 0.2163, step time: 1.5334\n",
      "124/388, train_loss: 0.1584, step time: 1.5314\n",
      "125/388, train_loss: 0.1073, step time: 1.5337\n",
      "126/388, train_loss: 0.0629, step time: 1.5317\n",
      "127/388, train_loss: 0.3547, step time: 1.5288\n",
      "128/388, train_loss: 0.0856, step time: 1.5317\n",
      "129/388, train_loss: 0.1340, step time: 1.5285\n",
      "130/388, train_loss: 0.4659, step time: 1.5332\n",
      "131/388, train_loss: 0.0679, step time: 1.5358\n",
      "132/388, train_loss: 0.0822, step time: 1.5351\n",
      "133/388, train_loss: 0.2065, step time: 1.5314\n",
      "134/388, train_loss: 0.2734, step time: 1.5305\n",
      "135/388, train_loss: 0.0970, step time: 1.5301\n",
      "136/388, train_loss: 0.1394, step time: 1.5310\n",
      "137/388, train_loss: 0.1034, step time: 1.5316\n",
      "138/388, train_loss: 0.1341, step time: 1.5325\n",
      "139/388, train_loss: 0.0640, step time: 1.5443\n",
      "140/388, train_loss: 0.1845, step time: 1.5390\n",
      "141/388, train_loss: 0.1584, step time: 1.5306\n",
      "142/388, train_loss: 0.1504, step time: 1.5292\n",
      "143/388, train_loss: 0.1063, step time: 1.5595\n",
      "144/388, train_loss: 0.0979, step time: 1.5536\n",
      "145/388, train_loss: 0.0728, step time: 1.5350\n",
      "146/388, train_loss: 0.5374, step time: 1.5337\n",
      "147/388, train_loss: 0.3905, step time: 1.5333\n",
      "148/388, train_loss: 0.1108, step time: 1.5341\n",
      "149/388, train_loss: 0.1469, step time: 1.5329\n",
      "150/388, train_loss: 0.1018, step time: 1.5332\n",
      "151/388, train_loss: 0.2579, step time: 1.5337\n",
      "152/388, train_loss: 0.1687, step time: 1.5343\n",
      "153/388, train_loss: 0.0636, step time: 1.5347\n",
      "154/388, train_loss: 0.2255, step time: 1.5356\n",
      "155/388, train_loss: 0.3369, step time: 1.5307\n",
      "156/388, train_loss: 0.1189, step time: 1.5304\n",
      "157/388, train_loss: 0.0795, step time: 1.5286\n",
      "158/388, train_loss: 0.1407, step time: 1.5321\n",
      "159/388, train_loss: 0.1339, step time: 1.5320\n",
      "160/388, train_loss: 0.1996, step time: 1.5326\n",
      "161/388, train_loss: 0.2011, step time: 1.5273\n",
      "162/388, train_loss: 0.1539, step time: 1.5308\n",
      "163/388, train_loss: 0.0684, step time: 1.5321\n",
      "164/388, train_loss: 0.1690, step time: 1.5323\n",
      "165/388, train_loss: 0.3929, step time: 1.5358\n",
      "166/388, train_loss: 0.1862, step time: 1.5326\n",
      "167/388, train_loss: 0.1745, step time: 1.5334\n",
      "168/388, train_loss: 0.0989, step time: 1.5378\n",
      "169/388, train_loss: 0.1676, step time: 1.5328\n",
      "170/388, train_loss: 0.0766, step time: 1.5378\n",
      "171/388, train_loss: 0.1010, step time: 1.5363\n",
      "172/388, train_loss: 0.0468, step time: 1.5356\n",
      "173/388, train_loss: 0.0927, step time: 1.5320\n",
      "174/388, train_loss: 0.3233, step time: 1.5366\n",
      "175/388, train_loss: 0.3513, step time: 1.5298\n",
      "176/388, train_loss: 0.1017, step time: 1.5319\n",
      "177/388, train_loss: 0.1007, step time: 1.5368\n",
      "178/388, train_loss: 0.1811, step time: 1.5356\n",
      "179/388, train_loss: 0.1052, step time: 1.5343\n",
      "180/388, train_loss: 0.1530, step time: 1.5281\n",
      "181/388, train_loss: 0.2716, step time: 1.5335\n",
      "182/388, train_loss: 0.0761, step time: 1.5311\n",
      "183/388, train_loss: 0.4203, step time: 1.5361\n",
      "184/388, train_loss: 0.0889, step time: 1.5359\n",
      "185/388, train_loss: 0.0871, step time: 1.5370\n",
      "186/388, train_loss: 0.0320, step time: 1.5354\n",
      "187/388, train_loss: 0.0796, step time: 1.5302\n",
      "188/388, train_loss: 0.1764, step time: 1.5311\n",
      "189/388, train_loss: 0.0960, step time: 1.5320\n",
      "190/388, train_loss: 0.1847, step time: 1.5313\n",
      "191/388, train_loss: 0.2582, step time: 1.5328\n",
      "192/388, train_loss: 0.1377, step time: 1.5354\n",
      "193/388, train_loss: 0.1121, step time: 1.5367\n",
      "194/388, train_loss: 0.1316, step time: 1.5307\n",
      "195/388, train_loss: 0.1759, step time: 1.5438\n",
      "196/388, train_loss: 0.1075, step time: 1.5343\n",
      "197/388, train_loss: 0.1120, step time: 1.5377\n",
      "198/388, train_loss: 0.2412, step time: 1.5307\n",
      "199/388, train_loss: 0.0916, step time: 1.5289\n",
      "200/388, train_loss: 0.2186, step time: 1.5326\n",
      "201/388, train_loss: 0.0671, step time: 1.5339\n",
      "202/388, train_loss: 0.0837, step time: 1.5352\n",
      "203/388, train_loss: 0.1822, step time: 1.5361\n",
      "204/388, train_loss: 0.2771, step time: 1.5351\n",
      "205/388, train_loss: 0.1764, step time: 1.5311\n",
      "206/388, train_loss: 0.0846, step time: 1.5307\n",
      "207/388, train_loss: 0.1625, step time: 1.5334\n",
      "208/388, train_loss: 0.0962, step time: 1.5348\n",
      "209/388, train_loss: 0.2032, step time: 1.5365\n",
      "210/388, train_loss: 0.1241, step time: 1.5581\n",
      "211/388, train_loss: 0.1947, step time: 1.5343\n",
      "212/388, train_loss: 0.1272, step time: 1.5365\n",
      "213/388, train_loss: 0.2380, step time: 1.5324\n",
      "214/388, train_loss: 0.2144, step time: 1.5350\n",
      "215/388, train_loss: 0.2064, step time: 1.5320\n",
      "216/388, train_loss: 0.2977, step time: 1.5306\n",
      "217/388, train_loss: 0.1185, step time: 1.5320\n",
      "218/388, train_loss: 0.0835, step time: 1.5315\n",
      "219/388, train_loss: 0.1158, step time: 1.5340\n",
      "220/388, train_loss: 0.0590, step time: 1.5369\n",
      "221/388, train_loss: 0.2183, step time: 1.5344\n",
      "222/388, train_loss: 0.1036, step time: 1.5313\n",
      "223/388, train_loss: 0.0987, step time: 1.5287\n",
      "224/388, train_loss: 0.1929, step time: 1.5307\n",
      "225/388, train_loss: 0.1837, step time: 1.5349\n",
      "226/388, train_loss: 0.3702, step time: 1.5340\n",
      "227/388, train_loss: 0.1120, step time: 1.5333\n",
      "228/388, train_loss: 0.2346, step time: 1.5313\n",
      "229/388, train_loss: 0.2353, step time: 1.5377\n",
      "230/388, train_loss: 0.2792, step time: 1.5315\n",
      "231/388, train_loss: 0.1080, step time: 1.5286\n",
      "232/388, train_loss: 0.2025, step time: 1.5374\n",
      "233/388, train_loss: 0.1955, step time: 1.5350\n",
      "234/388, train_loss: 0.2469, step time: 1.5386\n",
      "235/388, train_loss: 0.0526, step time: 1.5308\n",
      "236/388, train_loss: 0.0587, step time: 1.5300\n",
      "237/388, train_loss: 0.2173, step time: 1.5339\n",
      "238/388, train_loss: 0.2218, step time: 1.5320\n",
      "239/388, train_loss: 0.1441, step time: 1.5312\n",
      "240/388, train_loss: 0.1208, step time: 1.5319\n",
      "241/388, train_loss: 0.3834, step time: 1.5329\n",
      "242/388, train_loss: 0.3082, step time: 1.5341\n",
      "243/388, train_loss: 0.4429, step time: 1.5282\n",
      "244/388, train_loss: 0.4421, step time: 1.5468\n",
      "245/388, train_loss: 0.2360, step time: 1.5396\n",
      "246/388, train_loss: 0.0788, step time: 1.5319\n",
      "247/388, train_loss: 0.0595, step time: 1.5336\n",
      "248/388, train_loss: 0.1661, step time: 1.5340\n",
      "249/388, train_loss: 0.1777, step time: 1.5295\n",
      "250/388, train_loss: 0.1162, step time: 1.5308\n",
      "251/388, train_loss: 0.1145, step time: 1.5297\n",
      "252/388, train_loss: 0.1650, step time: 1.5320\n",
      "253/388, train_loss: 0.3982, step time: 1.5506\n",
      "254/388, train_loss: 0.1204, step time: 1.5325\n",
      "255/388, train_loss: 0.2703, step time: 1.5335\n",
      "256/388, train_loss: 0.1284, step time: 1.5317\n",
      "257/388, train_loss: 0.0998, step time: 1.5340\n",
      "258/388, train_loss: 0.1362, step time: 1.5327\n",
      "259/388, train_loss: 0.1044, step time: 1.5403\n",
      "260/388, train_loss: 0.2178, step time: 1.5429\n",
      "261/388, train_loss: 0.2249, step time: 1.5299\n",
      "262/388, train_loss: 0.2364, step time: 1.5320\n",
      "263/388, train_loss: 0.2489, step time: 1.5352\n",
      "264/388, train_loss: 0.2270, step time: 1.5354\n",
      "265/388, train_loss: 0.1796, step time: 1.5346\n",
      "266/388, train_loss: 0.2064, step time: 1.5338\n",
      "267/388, train_loss: 0.0683, step time: 1.5316\n",
      "268/388, train_loss: 0.1761, step time: 1.5332\n",
      "269/388, train_loss: 0.0989, step time: 1.5340\n",
      "270/388, train_loss: 0.0483, step time: 1.5325\n",
      "271/388, train_loss: 0.2798, step time: 1.5289\n",
      "272/388, train_loss: 0.1189, step time: 1.5281\n",
      "273/388, train_loss: 0.1566, step time: 1.5326\n",
      "274/388, train_loss: 0.3236, step time: 1.5338\n",
      "275/388, train_loss: 0.2650, step time: 1.5363\n",
      "276/388, train_loss: 0.1339, step time: 1.5345\n",
      "277/388, train_loss: 0.1385, step time: 1.5344\n",
      "278/388, train_loss: 0.1982, step time: 1.5341\n",
      "279/388, train_loss: 0.3179, step time: 1.5373\n",
      "280/388, train_loss: 0.2216, step time: 1.5316\n",
      "281/388, train_loss: 0.1186, step time: 1.5306\n",
      "282/388, train_loss: 0.1388, step time: 1.5322\n",
      "283/388, train_loss: 0.0931, step time: 1.5344\n",
      "284/388, train_loss: 0.2421, step time: 1.5386\n",
      "285/388, train_loss: 0.0769, step time: 1.5328\n",
      "286/388, train_loss: 0.3724, step time: 1.5316\n",
      "287/388, train_loss: 0.1406, step time: 1.5317\n",
      "288/388, train_loss: 0.3188, step time: 1.5339\n",
      "289/388, train_loss: 0.2136, step time: 1.5345\n",
      "290/388, train_loss: 0.5923, step time: 1.5324\n",
      "291/388, train_loss: 0.0783, step time: 1.5285\n",
      "292/388, train_loss: 0.2033, step time: 1.5319\n",
      "293/388, train_loss: 0.1043, step time: 1.5323\n",
      "294/388, train_loss: 0.2024, step time: 1.5367\n",
      "295/388, train_loss: 0.1045, step time: 1.5348\n",
      "296/388, train_loss: 0.0889, step time: 1.5357\n",
      "297/388, train_loss: 0.0937, step time: 1.5344\n",
      "298/388, train_loss: 0.1024, step time: 1.5301\n",
      "299/388, train_loss: 0.0943, step time: 1.5345\n",
      "300/388, train_loss: 0.4054, step time: 1.5422\n",
      "301/388, train_loss: 0.3146, step time: 1.5339\n",
      "302/388, train_loss: 0.2236, step time: 1.5304\n",
      "303/388, train_loss: 0.2217, step time: 1.5298\n",
      "304/388, train_loss: 0.2635, step time: 1.5336\n",
      "305/388, train_loss: 0.1144, step time: 1.5335\n",
      "306/388, train_loss: 0.0426, step time: 1.5343\n",
      "307/388, train_loss: 0.1718, step time: 1.5299\n",
      "308/388, train_loss: 0.1974, step time: 1.5301\n",
      "309/388, train_loss: 0.1659, step time: 1.5282\n",
      "310/388, train_loss: 0.1611, step time: 1.5327\n",
      "311/388, train_loss: 0.1439, step time: 1.5351\n",
      "312/388, train_loss: 0.2511, step time: 1.5385\n",
      "313/388, train_loss: 0.2191, step time: 1.5346\n",
      "314/388, train_loss: 0.0693, step time: 1.5338\n",
      "315/388, train_loss: 0.2511, step time: 1.5284\n",
      "316/388, train_loss: 0.1895, step time: 1.5304\n",
      "317/388, train_loss: 0.1171, step time: 1.5328\n",
      "318/388, train_loss: 0.1205, step time: 1.5360\n",
      "319/388, train_loss: 0.2446, step time: 1.5335\n",
      "320/388, train_loss: 0.0796, step time: 1.5484\n",
      "321/388, train_loss: 0.2513, step time: 1.5325\n",
      "322/388, train_loss: 0.2420, step time: 1.5325\n",
      "323/388, train_loss: 0.1420, step time: 1.5304\n",
      "324/388, train_loss: 0.0672, step time: 1.5328\n",
      "325/388, train_loss: 0.2549, step time: 1.5335\n",
      "326/388, train_loss: 0.1366, step time: 1.5331\n",
      "327/388, train_loss: 0.2345, step time: 1.5353\n",
      "328/388, train_loss: 0.0470, step time: 1.5316\n",
      "329/388, train_loss: 0.2294, step time: 1.5278\n",
      "330/388, train_loss: 0.2800, step time: 1.5321\n",
      "331/388, train_loss: 0.0508, step time: 1.5307\n",
      "332/388, train_loss: 0.1306, step time: 1.5323\n",
      "333/388, train_loss: 0.1146, step time: 1.5329\n",
      "334/388, train_loss: 0.1510, step time: 1.5346\n",
      "335/388, train_loss: 0.1442, step time: 1.5311\n",
      "336/388, train_loss: 0.3179, step time: 1.5324\n",
      "337/388, train_loss: 0.1409, step time: 1.5428\n",
      "338/388, train_loss: 0.0599, step time: 1.5340\n",
      "339/388, train_loss: 0.2453, step time: 1.5328\n",
      "340/388, train_loss: 0.3217, step time: 1.5359\n",
      "341/388, train_loss: 0.3743, step time: 1.5317\n",
      "342/388, train_loss: 0.0990, step time: 1.5308\n",
      "343/388, train_loss: 0.0929, step time: 1.5288\n",
      "344/388, train_loss: 0.1411, step time: 1.5274\n",
      "345/388, train_loss: 0.0780, step time: 1.5329\n",
      "346/388, train_loss: 0.2958, step time: 1.5363\n",
      "347/388, train_loss: 0.1091, step time: 1.5340\n",
      "348/388, train_loss: 0.4813, step time: 1.5340\n",
      "349/388, train_loss: 0.1019, step time: 1.5315\n",
      "350/388, train_loss: 0.1907, step time: 1.5308\n",
      "351/388, train_loss: 0.0831, step time: 1.5308\n",
      "352/388, train_loss: 0.0843, step time: 1.5306\n",
      "353/388, train_loss: 0.1131, step time: 1.5347\n",
      "354/388, train_loss: 0.4150, step time: 1.5343\n",
      "355/388, train_loss: 0.2486, step time: 1.5319\n",
      "356/388, train_loss: 0.0897, step time: 1.5311\n",
      "357/388, train_loss: 0.0611, step time: 1.5305\n",
      "358/388, train_loss: 0.1188, step time: 1.5320\n",
      "359/388, train_loss: 0.1421, step time: 1.5322\n",
      "360/388, train_loss: 0.1623, step time: 1.5396\n",
      "361/388, train_loss: 0.1982, step time: 1.5356\n",
      "362/388, train_loss: 0.0906, step time: 1.5290\n",
      "363/388, train_loss: 0.1455, step time: 1.5320\n",
      "364/388, train_loss: 0.2519, step time: 1.5314\n",
      "365/388, train_loss: 0.1061, step time: 1.5331\n",
      "366/388, train_loss: 0.0798, step time: 1.5328\n",
      "367/388, train_loss: 0.2178, step time: 1.5370\n",
      "368/388, train_loss: 0.1667, step time: 1.5340\n",
      "369/388, train_loss: 0.1790, step time: 1.5342\n",
      "370/388, train_loss: 0.0997, step time: 1.5315\n",
      "371/388, train_loss: 0.1988, step time: 1.5376\n",
      "372/388, train_loss: 0.2318, step time: 1.5354\n",
      "373/388, train_loss: 0.2373, step time: 1.5340\n",
      "374/388, train_loss: 0.2013, step time: 1.5355\n",
      "375/388, train_loss: 0.1891, step time: 1.5305\n",
      "376/388, train_loss: 0.1421, step time: 1.5312\n",
      "377/388, train_loss: 0.1805, step time: 1.5311\n",
      "378/388, train_loss: 0.1433, step time: 1.5320\n",
      "379/388, train_loss: 0.4590, step time: 1.5335\n",
      "380/388, train_loss: 0.3016, step time: 1.5356\n",
      "381/388, train_loss: 0.1530, step time: 1.5325\n",
      "382/388, train_loss: 0.0264, step time: 1.5311\n",
      "383/388, train_loss: 0.0506, step time: 1.5319\n",
      "384/388, train_loss: 0.1203, step time: 1.5338\n",
      "385/388, train_loss: 0.4348, step time: 1.5328\n",
      "386/388, train_loss: 0.1541, step time: 1.5364\n",
      "387/388, train_loss: 0.2288, step time: 1.5340\n",
      "388/388, train_loss: 0.1904, step time: 1.5391\n",
      "epoch 61 average loss: 0.1770\n",
      "current epoch: 61 current mean dice: 0.7678 tc: 0.8177 wt: 0.9017 et: 0.5839\n",
      "best mean dice: 0.7722 at epoch: 58\n",
      "time consuming of epoch 61 is: 703.6109\n",
      "----------\n",
      "epoch 62/100\n",
      "1/388, train_loss: 0.1008, step time: 1.5487\n",
      "2/388, train_loss: 0.3409, step time: 1.5311\n",
      "3/388, train_loss: 0.2548, step time: 1.5345\n",
      "4/388, train_loss: 0.0280, step time: 1.5338\n",
      "5/388, train_loss: 0.1396, step time: 1.5363\n",
      "6/388, train_loss: 0.0830, step time: 1.5377\n",
      "7/388, train_loss: 0.1994, step time: 1.5324\n",
      "8/388, train_loss: 0.1573, step time: 1.5334\n",
      "9/388, train_loss: 0.2606, step time: 1.5358\n",
      "10/388, train_loss: 0.1025, step time: 1.5363\n",
      "11/388, train_loss: 0.3371, step time: 1.5372\n",
      "12/388, train_loss: 0.0882, step time: 1.5378\n",
      "13/388, train_loss: 0.0952, step time: 1.5326\n",
      "14/388, train_loss: 0.2882, step time: 1.5313\n",
      "15/388, train_loss: 0.0909, step time: 1.5371\n",
      "16/388, train_loss: 0.1915, step time: 1.5323\n",
      "17/388, train_loss: 0.0564, step time: 1.5339\n",
      "18/388, train_loss: 0.1958, step time: 1.5396\n",
      "19/388, train_loss: 0.1582, step time: 1.5303\n",
      "20/388, train_loss: 0.0884, step time: 1.5334\n",
      "21/388, train_loss: 0.0975, step time: 1.5290\n",
      "22/388, train_loss: 0.2500, step time: 1.5337\n",
      "23/388, train_loss: 0.1763, step time: 1.5325\n",
      "24/388, train_loss: 0.0989, step time: 1.5404\n",
      "25/388, train_loss: 0.1149, step time: 1.5410\n",
      "26/388, train_loss: 0.2134, step time: 1.5304\n",
      "27/388, train_loss: 0.0575, step time: 1.5322\n",
      "28/388, train_loss: 0.0978, step time: 1.5320\n",
      "29/388, train_loss: 0.1203, step time: 1.5337\n",
      "30/388, train_loss: 0.1803, step time: 1.5354\n",
      "31/388, train_loss: 0.1819, step time: 1.5356\n",
      "32/388, train_loss: 0.2790, step time: 1.5324\n",
      "33/388, train_loss: 0.1153, step time: 1.5330\n",
      "34/388, train_loss: 0.0926, step time: 1.5345\n",
      "35/388, train_loss: 0.0901, step time: 1.5356\n",
      "36/388, train_loss: 0.1277, step time: 1.5358\n",
      "37/388, train_loss: 0.1880, step time: 1.5371\n",
      "38/388, train_loss: 0.2136, step time: 1.5327\n",
      "39/388, train_loss: 0.2368, step time: 1.5347\n",
      "40/388, train_loss: 0.1210, step time: 1.5335\n",
      "41/388, train_loss: 0.2538, step time: 1.5343\n",
      "42/388, train_loss: 0.2136, step time: 1.5360\n",
      "43/388, train_loss: 0.2485, step time: 1.5312\n",
      "44/388, train_loss: 0.1053, step time: 1.5614\n",
      "45/388, train_loss: 0.1747, step time: 1.5335\n",
      "46/388, train_loss: 0.0983, step time: 1.5342\n",
      "47/388, train_loss: 0.1260, step time: 1.5312\n",
      "48/388, train_loss: 0.4882, step time: 1.5355\n",
      "49/388, train_loss: 0.1115, step time: 1.5352\n",
      "50/388, train_loss: 0.1775, step time: 1.5335\n",
      "51/388, train_loss: 0.0633, step time: 1.5319\n",
      "52/388, train_loss: 0.0939, step time: 1.5336\n",
      "53/388, train_loss: 0.2436, step time: 1.5342\n",
      "54/388, train_loss: 0.1377, step time: 1.5357\n",
      "55/388, train_loss: 0.1725, step time: 1.5358\n",
      "56/388, train_loss: 0.1143, step time: 1.5348\n",
      "57/388, train_loss: 0.1434, step time: 1.5308\n",
      "58/388, train_loss: 0.1723, step time: 1.5322\n",
      "59/388, train_loss: 0.1445, step time: 1.5398\n",
      "60/388, train_loss: 0.1403, step time: 1.5331\n",
      "61/388, train_loss: 0.1072, step time: 1.5311\n",
      "62/388, train_loss: 0.0976, step time: 1.5370\n",
      "63/388, train_loss: 0.1070, step time: 1.5295\n",
      "64/388, train_loss: 0.1024, step time: 1.5309\n",
      "65/388, train_loss: 0.4658, step time: 1.5352\n",
      "66/388, train_loss: 0.1802, step time: 1.5381\n",
      "67/388, train_loss: 0.0927, step time: 1.5335\n",
      "68/388, train_loss: 0.1218, step time: 1.5309\n",
      "69/388, train_loss: 0.1369, step time: 1.5313\n",
      "70/388, train_loss: 0.2867, step time: 1.5311\n",
      "71/388, train_loss: 0.1855, step time: 1.5341\n",
      "72/388, train_loss: 0.1534, step time: 1.5352\n",
      "73/388, train_loss: 0.3764, step time: 1.5357\n",
      "74/388, train_loss: 0.0709, step time: 1.5337\n",
      "75/388, train_loss: 0.0682, step time: 1.5327\n",
      "76/388, train_loss: 0.1795, step time: 1.5322\n",
      "77/388, train_loss: 0.1054, step time: 1.5313\n",
      "78/388, train_loss: 0.4141, step time: 1.5335\n",
      "79/388, train_loss: 0.0880, step time: 1.5338\n",
      "80/388, train_loss: 0.3542, step time: 1.5357\n",
      "81/388, train_loss: 0.1325, step time: 1.5452\n",
      "82/388, train_loss: 0.1462, step time: 1.5331\n",
      "83/388, train_loss: 0.2680, step time: 1.5286\n",
      "84/388, train_loss: 0.2668, step time: 1.5302\n",
      "85/388, train_loss: 0.0825, step time: 1.5296\n",
      "86/388, train_loss: 0.2053, step time: 1.5302\n",
      "87/388, train_loss: 0.0645, step time: 1.5322\n",
      "88/388, train_loss: 0.1008, step time: 1.5329\n",
      "89/388, train_loss: 0.1572, step time: 1.5346\n",
      "90/388, train_loss: 0.0597, step time: 1.5360\n",
      "91/388, train_loss: 0.0899, step time: 1.5310\n",
      "92/388, train_loss: 0.0988, step time: 1.5314\n",
      "93/388, train_loss: 0.0937, step time: 1.5321\n",
      "94/388, train_loss: 0.1107, step time: 1.5329\n",
      "95/388, train_loss: 0.2079, step time: 1.5351\n",
      "96/388, train_loss: 0.0747, step time: 1.5363\n",
      "97/388, train_loss: 0.0834, step time: 1.5318\n",
      "98/388, train_loss: 0.4894, step time: 1.5301\n",
      "99/388, train_loss: 0.1697, step time: 1.5342\n",
      "100/388, train_loss: 0.2022, step time: 1.5346\n",
      "101/388, train_loss: 0.1330, step time: 1.5388\n",
      "102/388, train_loss: 0.2382, step time: 1.5307\n",
      "103/388, train_loss: 0.0694, step time: 1.5293\n",
      "104/388, train_loss: 0.1175, step time: 1.5307\n",
      "105/388, train_loss: 0.3687, step time: 1.5324\n",
      "106/388, train_loss: 0.3037, step time: 1.5352\n",
      "107/388, train_loss: 0.2470, step time: 1.5318\n",
      "108/388, train_loss: 0.0980, step time: 1.5327\n",
      "109/388, train_loss: 0.1671, step time: 1.5324\n",
      "110/388, train_loss: 0.0732, step time: 1.5296\n",
      "111/388, train_loss: 0.1288, step time: 1.5353\n",
      "112/388, train_loss: 0.1352, step time: 1.5333\n",
      "113/388, train_loss: 0.2172, step time: 1.5367\n",
      "114/388, train_loss: 0.0866, step time: 1.5352\n",
      "115/388, train_loss: 0.1372, step time: 1.5327\n",
      "116/388, train_loss: 0.0830, step time: 1.5331\n",
      "117/388, train_loss: 0.1396, step time: 1.5286\n",
      "118/388, train_loss: 0.1611, step time: 1.5298\n",
      "119/388, train_loss: 0.2460, step time: 1.5333\n",
      "120/388, train_loss: 0.2046, step time: 1.5385\n",
      "121/388, train_loss: 0.0761, step time: 1.5355\n",
      "122/388, train_loss: 0.2003, step time: 1.5318\n",
      "123/388, train_loss: 0.0431, step time: 1.5344\n",
      "124/388, train_loss: 0.2332, step time: 1.5343\n",
      "125/388, train_loss: 0.3935, step time: 1.5319\n",
      "126/388, train_loss: 0.0640, step time: 1.5326\n",
      "127/388, train_loss: 0.1684, step time: 1.5352\n",
      "128/388, train_loss: 0.0683, step time: 1.5320\n",
      "129/388, train_loss: 0.2090, step time: 1.5336\n",
      "130/388, train_loss: 0.1336, step time: 1.5345\n",
      "131/388, train_loss: 0.1547, step time: 1.5319\n",
      "132/388, train_loss: 0.1478, step time: 1.5332\n",
      "133/388, train_loss: 0.2446, step time: 1.5331\n",
      "134/388, train_loss: 0.0804, step time: 1.5304\n",
      "135/388, train_loss: 0.1036, step time: 1.5319\n",
      "136/388, train_loss: 0.1103, step time: 1.5318\n",
      "137/388, train_loss: 0.1178, step time: 1.5319\n",
      "138/388, train_loss: 0.1260, step time: 1.5350\n",
      "139/388, train_loss: 0.1509, step time: 1.5361\n",
      "140/388, train_loss: 0.1447, step time: 1.5308\n",
      "141/388, train_loss: 0.1614, step time: 1.5272\n",
      "142/388, train_loss: 0.1301, step time: 1.5405\n",
      "143/388, train_loss: 0.1284, step time: 1.5358\n",
      "144/388, train_loss: 0.1234, step time: 1.5351\n",
      "145/388, train_loss: 0.0926, step time: 1.5304\n",
      "146/388, train_loss: 0.1863, step time: 1.5315\n",
      "147/388, train_loss: 0.1014, step time: 1.5344\n",
      "148/388, train_loss: 0.0934, step time: 1.5372\n",
      "149/388, train_loss: 0.2759, step time: 1.5512\n",
      "150/388, train_loss: 0.1273, step time: 1.5299\n",
      "151/388, train_loss: 0.2712, step time: 1.5307\n",
      "152/388, train_loss: 0.1753, step time: 1.5318\n",
      "153/388, train_loss: 0.0556, step time: 1.5289\n",
      "154/388, train_loss: 0.0949, step time: 1.5393\n",
      "155/388, train_loss: 0.3453, step time: 1.5391\n",
      "156/388, train_loss: 0.1204, step time: 1.5318\n",
      "157/388, train_loss: 0.1228, step time: 1.5320\n",
      "158/388, train_loss: 0.0766, step time: 1.5318\n",
      "159/388, train_loss: 0.3162, step time: 1.5328\n",
      "160/388, train_loss: 0.1154, step time: 1.5350\n",
      "161/388, train_loss: 0.1201, step time: 1.5310\n",
      "162/388, train_loss: 0.2289, step time: 1.5318\n",
      "163/388, train_loss: 0.1190, step time: 1.5299\n",
      "164/388, train_loss: 0.2465, step time: 1.5376\n",
      "165/388, train_loss: 0.0865, step time: 1.5365\n",
      "166/388, train_loss: 0.2869, step time: 1.5305\n",
      "167/388, train_loss: 0.2293, step time: 1.5328\n",
      "168/388, train_loss: 0.1195, step time: 1.5293\n",
      "169/388, train_loss: 0.1226, step time: 1.5345\n",
      "170/388, train_loss: 0.1771, step time: 1.5345\n",
      "171/388, train_loss: 0.1520, step time: 1.5340\n",
      "172/388, train_loss: 0.0980, step time: 1.5343\n",
      "173/388, train_loss: 0.0872, step time: 1.5329\n",
      "174/388, train_loss: 0.1705, step time: 1.5335\n",
      "175/388, train_loss: 0.1881, step time: 1.5298\n",
      "176/388, train_loss: 0.0894, step time: 1.5346\n",
      "177/388, train_loss: 0.0539, step time: 1.5360\n",
      "178/388, train_loss: 0.0814, step time: 1.5372\n",
      "179/388, train_loss: 0.1712, step time: 1.5323\n",
      "180/388, train_loss: 0.1537, step time: 1.5296\n",
      "181/388, train_loss: 0.0745, step time: 1.5305\n",
      "182/388, train_loss: 0.0415, step time: 1.5358\n",
      "183/388, train_loss: 0.1828, step time: 1.5326\n",
      "184/388, train_loss: 0.0946, step time: 1.5340\n",
      "185/388, train_loss: 0.1237, step time: 1.5336\n",
      "186/388, train_loss: 0.1546, step time: 1.5350\n",
      "187/388, train_loss: 0.1783, step time: 1.5315\n",
      "188/388, train_loss: 0.0848, step time: 1.5356\n",
      "189/388, train_loss: 0.0738, step time: 1.5293\n",
      "190/388, train_loss: 0.1672, step time: 1.5627\n",
      "191/388, train_loss: 0.1217, step time: 1.5343\n",
      "192/388, train_loss: 0.2630, step time: 1.5324\n",
      "193/388, train_loss: 0.2473, step time: 1.5289\n",
      "194/388, train_loss: 0.2085, step time: 1.5310\n",
      "195/388, train_loss: 0.1709, step time: 1.5311\n",
      "196/388, train_loss: 0.1418, step time: 1.5361\n",
      "197/388, train_loss: 0.0865, step time: 1.5341\n",
      "198/388, train_loss: 0.1581, step time: 1.5369\n",
      "199/388, train_loss: 0.2387, step time: 1.5320\n",
      "200/388, train_loss: 0.0461, step time: 1.5434\n",
      "201/388, train_loss: 0.1203, step time: 1.5373\n",
      "202/388, train_loss: 0.1236, step time: 1.5436\n",
      "203/388, train_loss: 0.2072, step time: 1.5415\n",
      "204/388, train_loss: 0.3642, step time: 1.5304\n",
      "205/388, train_loss: 0.1910, step time: 1.5302\n",
      "206/388, train_loss: 0.3825, step time: 1.5366\n",
      "207/388, train_loss: 0.3160, step time: 1.5358\n",
      "208/388, train_loss: 0.3444, step time: 1.5362\n",
      "209/388, train_loss: 0.1147, step time: 1.5345\n",
      "210/388, train_loss: 0.1144, step time: 1.5339\n",
      "211/388, train_loss: 0.0368, step time: 1.5372\n",
      "212/388, train_loss: 0.4030, step time: 1.5344\n",
      "213/388, train_loss: 0.1449, step time: 1.5357\n",
      "214/388, train_loss: 0.0590, step time: 1.5311\n",
      "215/388, train_loss: 0.5110, step time: 1.5312\n",
      "216/388, train_loss: 0.1631, step time: 1.5345\n",
      "217/388, train_loss: 0.1186, step time: 1.5334\n",
      "218/388, train_loss: 0.4257, step time: 1.5466\n",
      "219/388, train_loss: 0.0995, step time: 1.5323\n",
      "220/388, train_loss: 0.3685, step time: 1.5322\n",
      "221/388, train_loss: 0.3601, step time: 1.5406\n",
      "222/388, train_loss: 0.1974, step time: 1.5337\n",
      "223/388, train_loss: 0.0730, step time: 1.5359\n",
      "224/388, train_loss: 0.4819, step time: 1.5358\n",
      "225/388, train_loss: 0.1595, step time: 1.5331\n",
      "226/388, train_loss: 0.0435, step time: 1.5312\n",
      "227/388, train_loss: 0.2463, step time: 1.5357\n",
      "228/388, train_loss: 0.1863, step time: 1.5313\n",
      "229/388, train_loss: 0.2651, step time: 1.5324\n",
      "230/388, train_loss: 0.0604, step time: 1.5312\n",
      "231/388, train_loss: 0.3966, step time: 1.5386\n",
      "232/388, train_loss: 0.3784, step time: 1.5350\n",
      "233/388, train_loss: 0.0910, step time: 1.5343\n",
      "234/388, train_loss: 0.0864, step time: 1.5300\n",
      "235/388, train_loss: 0.3513, step time: 1.5342\n",
      "236/388, train_loss: 0.1853, step time: 1.5330\n",
      "237/388, train_loss: 0.2931, step time: 1.5373\n",
      "238/388, train_loss: 0.3799, step time: 1.5364\n",
      "239/388, train_loss: 0.1489, step time: 1.5313\n",
      "240/388, train_loss: 0.1077, step time: 1.5317\n",
      "241/388, train_loss: 0.5224, step time: 1.5336\n",
      "242/388, train_loss: 0.2471, step time: 1.5341\n",
      "243/388, train_loss: 0.2629, step time: 1.5384\n",
      "244/388, train_loss: 0.1816, step time: 1.5540\n",
      "245/388, train_loss: 0.1964, step time: 1.5327\n",
      "246/388, train_loss: 0.2704, step time: 1.5373\n",
      "247/388, train_loss: 0.0823, step time: 1.5346\n",
      "248/388, train_loss: 0.1015, step time: 1.5337\n",
      "249/388, train_loss: 0.1728, step time: 1.5479\n",
      "250/388, train_loss: 0.0548, step time: 1.5369\n",
      "251/388, train_loss: 0.0999, step time: 1.5339\n",
      "252/388, train_loss: 0.0787, step time: 1.5459\n",
      "253/388, train_loss: 0.1658, step time: 1.5379\n",
      "254/388, train_loss: 0.1394, step time: 1.5408\n",
      "255/388, train_loss: 0.2144, step time: 1.5326\n",
      "256/388, train_loss: 0.1575, step time: 1.5461\n",
      "257/388, train_loss: 0.2132, step time: 1.5466\n",
      "258/388, train_loss: 0.0945, step time: 1.5343\n",
      "259/388, train_loss: 0.3129, step time: 1.5303\n",
      "260/388, train_loss: 0.1497, step time: 1.5347\n",
      "261/388, train_loss: 0.1910, step time: 1.5324\n",
      "262/388, train_loss: 0.1126, step time: 1.5383\n",
      "263/388, train_loss: 0.4603, step time: 1.5317\n",
      "264/388, train_loss: 0.1031, step time: 1.5312\n",
      "265/388, train_loss: 0.1226, step time: 1.5312\n",
      "266/388, train_loss: 0.2103, step time: 1.5323\n",
      "267/388, train_loss: 0.0870, step time: 1.5344\n",
      "268/388, train_loss: 0.1249, step time: 1.5385\n",
      "269/388, train_loss: 0.0756, step time: 1.5292\n",
      "270/388, train_loss: 0.2280, step time: 1.5350\n",
      "271/388, train_loss: 0.0797, step time: 1.5315\n",
      "272/388, train_loss: 0.0879, step time: 1.5306\n",
      "273/388, train_loss: 0.1441, step time: 1.5298\n",
      "274/388, train_loss: 0.1443, step time: 1.5315\n",
      "275/388, train_loss: 0.0611, step time: 1.5368\n",
      "276/388, train_loss: 0.0631, step time: 1.5340\n",
      "277/388, train_loss: 0.1094, step time: 1.5409\n",
      "278/388, train_loss: 0.0976, step time: 1.5305\n",
      "279/388, train_loss: 0.1906, step time: 1.5332\n",
      "280/388, train_loss: 0.1578, step time: 1.5335\n",
      "281/388, train_loss: 0.1547, step time: 1.5353\n",
      "282/388, train_loss: 0.1376, step time: 1.5367\n",
      "283/388, train_loss: 0.6012, step time: 1.5336\n",
      "284/388, train_loss: 0.2089, step time: 1.5318\n",
      "285/388, train_loss: 0.0881, step time: 1.5305\n",
      "286/388, train_loss: 0.2543, step time: 1.5316\n",
      "287/388, train_loss: 0.1401, step time: 1.5305\n",
      "288/388, train_loss: 0.0932, step time: 1.5429\n",
      "289/388, train_loss: 0.0310, step time: 1.5311\n",
      "290/388, train_loss: 0.2452, step time: 1.5333\n",
      "291/388, train_loss: 0.1148, step time: 1.5304\n",
      "292/388, train_loss: 0.1095, step time: 1.5321\n",
      "293/388, train_loss: 0.0612, step time: 1.5297\n",
      "294/388, train_loss: 0.3521, step time: 1.5339\n",
      "295/388, train_loss: 0.3385, step time: 1.5354\n",
      "296/388, train_loss: 0.2753, step time: 1.5332\n",
      "297/388, train_loss: 0.2072, step time: 1.5349\n",
      "298/388, train_loss: 0.5023, step time: 1.5309\n",
      "299/388, train_loss: 0.1548, step time: 1.5317\n",
      "300/388, train_loss: 0.1164, step time: 1.5324\n",
      "301/388, train_loss: 0.4042, step time: 1.5355\n",
      "302/388, train_loss: 0.0998, step time: 1.5349\n",
      "303/388, train_loss: 0.1646, step time: 1.5343\n",
      "304/388, train_loss: 0.2480, step time: 1.5351\n",
      "305/388, train_loss: 0.0982, step time: 1.5387\n",
      "306/388, train_loss: 0.2046, step time: 1.5293\n",
      "307/388, train_loss: 0.1916, step time: 1.5302\n",
      "308/388, train_loss: 0.1869, step time: 1.5324\n",
      "309/388, train_loss: 0.0917, step time: 1.5358\n",
      "310/388, train_loss: 0.0731, step time: 1.5523\n",
      "311/388, train_loss: 0.2310, step time: 1.5325\n",
      "312/388, train_loss: 0.0942, step time: 1.5309\n",
      "313/388, train_loss: 0.1698, step time: 1.5338\n",
      "314/388, train_loss: 0.1107, step time: 1.5337\n",
      "315/388, train_loss: 0.2632, step time: 1.5316\n",
      "316/388, train_loss: 0.2722, step time: 1.5317\n",
      "317/388, train_loss: 0.0934, step time: 1.5307\n",
      "318/388, train_loss: 0.1873, step time: 1.5310\n",
      "319/388, train_loss: 0.0688, step time: 1.5314\n",
      "320/388, train_loss: 0.1611, step time: 1.5298\n",
      "321/388, train_loss: 0.1116, step time: 1.5330\n",
      "322/388, train_loss: 0.2312, step time: 1.5338\n",
      "323/388, train_loss: 0.2289, step time: 1.5350\n",
      "324/388, train_loss: 0.1715, step time: 1.5367\n",
      "325/388, train_loss: 0.1902, step time: 1.5306\n",
      "326/388, train_loss: 0.1056, step time: 1.5320\n",
      "327/388, train_loss: 0.0659, step time: 1.5303\n",
      "328/388, train_loss: 0.0936, step time: 1.5345\n",
      "329/388, train_loss: 0.0557, step time: 1.5349\n",
      "330/388, train_loss: 0.2634, step time: 1.5328\n",
      "331/388, train_loss: 0.2868, step time: 1.5352\n",
      "332/388, train_loss: 0.1959, step time: 1.5328\n",
      "333/388, train_loss: 0.1003, step time: 1.5304\n",
      "334/388, train_loss: 0.0844, step time: 1.5294\n",
      "335/388, train_loss: 0.3829, step time: 1.5295\n",
      "336/388, train_loss: 0.2200, step time: 1.5349\n",
      "337/388, train_loss: 0.2473, step time: 1.5328\n",
      "338/388, train_loss: 0.1834, step time: 1.5355\n",
      "339/388, train_loss: 0.2085, step time: 1.5333\n",
      "340/388, train_loss: 0.1062, step time: 1.5339\n",
      "341/388, train_loss: 0.2106, step time: 1.5309\n",
      "342/388, train_loss: 0.2280, step time: 1.5275\n",
      "343/388, train_loss: 0.1722, step time: 1.5315\n",
      "344/388, train_loss: 0.2060, step time: 1.5306\n",
      "345/388, train_loss: 0.1962, step time: 1.5350\n",
      "346/388, train_loss: 0.1012, step time: 1.5435\n",
      "347/388, train_loss: 0.1377, step time: 1.5319\n",
      "348/388, train_loss: 0.2006, step time: 1.5274\n",
      "349/388, train_loss: 0.1387, step time: 1.5355\n",
      "350/388, train_loss: 0.1952, step time: 1.5336\n",
      "351/388, train_loss: 0.0649, step time: 1.5362\n",
      "352/388, train_loss: 0.2742, step time: 1.5359\n",
      "353/388, train_loss: 0.2952, step time: 1.5326\n",
      "354/388, train_loss: 0.0598, step time: 1.5328\n",
      "355/388, train_loss: 0.0823, step time: 1.5317\n",
      "356/388, train_loss: 0.3656, step time: 1.5327\n",
      "357/388, train_loss: 0.1092, step time: 1.5351\n",
      "358/388, train_loss: 0.1505, step time: 1.5347\n",
      "359/388, train_loss: 0.3486, step time: 1.5345\n",
      "360/388, train_loss: 0.1579, step time: 1.5322\n",
      "361/388, train_loss: 0.1090, step time: 1.5302\n",
      "362/388, train_loss: 0.1275, step time: 1.5303\n",
      "363/388, train_loss: 0.0646, step time: 1.5296\n",
      "364/388, train_loss: 0.2436, step time: 1.5333\n",
      "365/388, train_loss: 0.2032, step time: 1.5360\n",
      "366/388, train_loss: 0.1313, step time: 1.5342\n",
      "367/388, train_loss: 0.2711, step time: 1.5327\n",
      "368/388, train_loss: 0.2300, step time: 1.5311\n",
      "369/388, train_loss: 0.1151, step time: 1.5323\n",
      "370/388, train_loss: 0.2570, step time: 1.5340\n",
      "371/388, train_loss: 0.1958, step time: 1.5314\n",
      "372/388, train_loss: 0.2312, step time: 1.5348\n",
      "373/388, train_loss: 0.1029, step time: 1.5337\n",
      "374/388, train_loss: 0.1663, step time: 1.5325\n",
      "375/388, train_loss: 0.0751, step time: 1.5277\n",
      "376/388, train_loss: 0.1541, step time: 1.5313\n",
      "377/388, train_loss: 0.1429, step time: 1.5328\n",
      "378/388, train_loss: 0.2615, step time: 1.5303\n",
      "379/388, train_loss: 0.1463, step time: 1.5362\n",
      "380/388, train_loss: 0.3151, step time: 1.5366\n",
      "381/388, train_loss: 0.1386, step time: 1.5318\n",
      "382/388, train_loss: 0.1964, step time: 1.5315\n",
      "383/388, train_loss: 0.1901, step time: 1.5324\n",
      "384/388, train_loss: 0.1421, step time: 1.5310\n",
      "385/388, train_loss: 0.2523, step time: 1.5316\n",
      "386/388, train_loss: 0.1224, step time: 1.5310\n",
      "387/388, train_loss: 0.0379, step time: 1.5318\n",
      "388/388, train_loss: 0.3901, step time: 1.5362\n",
      "epoch 62 average loss: 0.1735\n",
      "current epoch: 62 current mean dice: 0.7709 tc: 0.8205 wt: 0.9049 et: 0.5872\n",
      "best mean dice: 0.7722 at epoch: 58\n",
      "time consuming of epoch 62 is: 703.1286\n",
      "----------\n",
      "epoch 63/100\n",
      "1/388, train_loss: 0.0614, step time: 1.5490\n",
      "2/388, train_loss: 0.0927, step time: 1.5497\n",
      "3/388, train_loss: 0.1015, step time: 1.5319\n",
      "4/388, train_loss: 0.1024, step time: 1.5351\n",
      "5/388, train_loss: 0.0712, step time: 1.5354\n",
      "6/388, train_loss: 0.0730, step time: 1.5345\n",
      "7/388, train_loss: 0.1544, step time: 1.5329\n",
      "8/388, train_loss: 0.1087, step time: 1.5322\n",
      "9/388, train_loss: 0.1130, step time: 1.5334\n",
      "10/388, train_loss: 0.1107, step time: 1.5340\n",
      "11/388, train_loss: 0.0916, step time: 1.5317\n",
      "12/388, train_loss: 0.5260, step time: 1.5339\n",
      "13/388, train_loss: 0.1235, step time: 1.5298\n",
      "14/388, train_loss: 0.1970, step time: 1.5310\n",
      "15/388, train_loss: 0.3711, step time: 1.5345\n",
      "16/388, train_loss: 0.0893, step time: 1.5335\n",
      "17/388, train_loss: 0.1555, step time: 1.5357\n",
      "18/388, train_loss: 0.1418, step time: 1.5327\n",
      "19/388, train_loss: 0.1440, step time: 1.5323\n",
      "20/388, train_loss: 0.0960, step time: 1.5312\n",
      "21/388, train_loss: 0.1804, step time: 1.5306\n",
      "22/388, train_loss: 0.1443, step time: 1.5345\n",
      "23/388, train_loss: 0.0896, step time: 1.5353\n",
      "24/388, train_loss: 0.0749, step time: 1.5326\n",
      "25/388, train_loss: 0.0519, step time: 1.5317\n",
      "26/388, train_loss: 0.4632, step time: 1.5320\n",
      "27/388, train_loss: 0.0925, step time: 1.5331\n",
      "28/388, train_loss: 0.1195, step time: 1.5361\n",
      "29/388, train_loss: 0.2129, step time: 1.5342\n",
      "30/388, train_loss: 0.0436, step time: 1.5357\n",
      "31/388, train_loss: 0.2185, step time: 1.5356\n",
      "32/388, train_loss: 0.1267, step time: 1.5342\n",
      "33/388, train_loss: 0.2091, step time: 1.5314\n",
      "34/388, train_loss: 0.1945, step time: 1.5337\n",
      "35/388, train_loss: 0.1675, step time: 1.5300\n",
      "36/388, train_loss: 0.0963, step time: 1.5559\n",
      "37/388, train_loss: 0.2564, step time: 1.5411\n",
      "38/388, train_loss: 0.0298, step time: 1.5318\n",
      "39/388, train_loss: 0.2265, step time: 1.5397\n",
      "40/388, train_loss: 0.3460, step time: 1.5544\n",
      "41/388, train_loss: 0.0441, step time: 1.5317\n",
      "42/388, train_loss: 0.0992, step time: 1.5363\n",
      "43/388, train_loss: 0.1123, step time: 1.5388\n",
      "44/388, train_loss: 0.5270, step time: 1.5324\n",
      "45/388, train_loss: 0.2879, step time: 1.5336\n",
      "46/388, train_loss: 0.1863, step time: 1.5336\n",
      "47/388, train_loss: 0.1107, step time: 1.5325\n",
      "48/388, train_loss: 0.1068, step time: 1.5339\n",
      "49/388, train_loss: 0.2598, step time: 1.5343\n",
      "50/388, train_loss: 0.1092, step time: 1.5344\n",
      "51/388, train_loss: 0.2515, step time: 1.5329\n",
      "52/388, train_loss: 0.1415, step time: 1.5308\n",
      "53/388, train_loss: 0.2037, step time: 1.5298\n",
      "54/388, train_loss: 0.1128, step time: 1.5312\n",
      "55/388, train_loss: 0.1545, step time: 1.5320\n",
      "56/388, train_loss: 0.3817, step time: 1.5339\n",
      "57/388, train_loss: 0.2090, step time: 1.5334\n",
      "58/388, train_loss: 0.0398, step time: 1.5341\n",
      "59/388, train_loss: 0.1494, step time: 1.5335\n",
      "60/388, train_loss: 0.0571, step time: 1.5310\n",
      "61/388, train_loss: 0.0998, step time: 1.5315\n",
      "62/388, train_loss: 0.1023, step time: 1.5320\n",
      "63/388, train_loss: 0.2092, step time: 1.5288\n",
      "64/388, train_loss: 0.1131, step time: 1.5335\n",
      "65/388, train_loss: 0.2423, step time: 1.5326\n",
      "66/388, train_loss: 0.2224, step time: 1.5332\n",
      "67/388, train_loss: 0.0793, step time: 1.5272\n",
      "68/388, train_loss: 0.2048, step time: 1.5299\n",
      "69/388, train_loss: 0.0862, step time: 1.5311\n",
      "70/388, train_loss: 0.2506, step time: 1.5307\n",
      "71/388, train_loss: 0.0639, step time: 1.5301\n",
      "72/388, train_loss: 0.1835, step time: 1.5340\n",
      "73/388, train_loss: 0.2332, step time: 1.5358\n",
      "74/388, train_loss: 0.1999, step time: 1.5326\n",
      "75/388, train_loss: 0.1854, step time: 1.5305\n",
      "76/388, train_loss: 0.2274, step time: 1.5301\n",
      "77/388, train_loss: 0.1209, step time: 1.5296\n",
      "78/388, train_loss: 0.1567, step time: 1.5304\n",
      "79/388, train_loss: 0.3825, step time: 1.5333\n",
      "80/388, train_loss: 0.2374, step time: 1.5334\n",
      "81/388, train_loss: 0.0828, step time: 1.5341\n",
      "82/388, train_loss: 0.0960, step time: 1.5344\n",
      "83/388, train_loss: 0.2272, step time: 1.5305\n",
      "84/388, train_loss: 0.2067, step time: 1.5298\n",
      "85/388, train_loss: 0.0601, step time: 1.5321\n",
      "86/388, train_loss: 0.1877, step time: 1.5306\n",
      "87/388, train_loss: 0.1099, step time: 1.5329\n",
      "88/388, train_loss: 0.0967, step time: 1.5404\n",
      "89/388, train_loss: 0.0951, step time: 1.5350\n",
      "90/388, train_loss: 0.2754, step time: 1.5343\n",
      "91/388, train_loss: 0.1028, step time: 1.5329\n",
      "92/388, train_loss: 0.1069, step time: 1.5291\n",
      "93/388, train_loss: 0.1983, step time: 1.5297\n",
      "94/388, train_loss: 0.0649, step time: 1.5351\n",
      "95/388, train_loss: 0.1774, step time: 1.5332\n",
      "96/388, train_loss: 0.1220, step time: 1.5305\n",
      "97/388, train_loss: 0.0859, step time: 1.5312\n",
      "98/388, train_loss: 0.0972, step time: 1.5348\n",
      "99/388, train_loss: 0.1384, step time: 1.5359\n",
      "100/388, train_loss: 0.0633, step time: 1.5311\n",
      "101/388, train_loss: 0.1160, step time: 1.5336\n",
      "102/388, train_loss: 0.0835, step time: 1.5303\n",
      "103/388, train_loss: 0.2245, step time: 1.5305\n",
      "104/388, train_loss: 0.2443, step time: 1.5292\n",
      "105/388, train_loss: 0.3102, step time: 1.5379\n",
      "106/388, train_loss: 0.0919, step time: 1.5552\n",
      "107/388, train_loss: 0.1578, step time: 1.5318\n",
      "108/388, train_loss: 0.1729, step time: 1.5305\n",
      "109/388, train_loss: 0.0910, step time: 1.5363\n",
      "110/388, train_loss: 0.2835, step time: 1.5340\n",
      "111/388, train_loss: 0.0662, step time: 1.5310\n",
      "112/388, train_loss: 0.2633, step time: 1.5287\n",
      "113/388, train_loss: 0.1138, step time: 1.5286\n",
      "114/388, train_loss: 0.3646, step time: 1.5317\n",
      "115/388, train_loss: 0.0992, step time: 1.5350\n",
      "116/388, train_loss: 0.3052, step time: 1.5364\n",
      "117/388, train_loss: 0.1909, step time: 1.5356\n",
      "118/388, train_loss: 0.1493, step time: 1.5352\n",
      "119/388, train_loss: 0.1149, step time: 1.5337\n",
      "120/388, train_loss: 0.3007, step time: 1.5306\n",
      "121/388, train_loss: 0.2047, step time: 1.5299\n",
      "122/388, train_loss: 0.1341, step time: 1.5378\n",
      "123/388, train_loss: 0.1737, step time: 1.5361\n",
      "124/388, train_loss: 0.3197, step time: 1.5302\n",
      "125/388, train_loss: 0.1630, step time: 1.5286\n",
      "126/388, train_loss: 0.3158, step time: 1.5320\n",
      "127/388, train_loss: 0.0892, step time: 1.5307\n",
      "128/388, train_loss: 0.0819, step time: 1.5396\n",
      "129/388, train_loss: 0.2995, step time: 1.5430\n",
      "130/388, train_loss: 0.1725, step time: 1.5281\n",
      "131/388, train_loss: 0.0989, step time: 1.5294\n",
      "132/388, train_loss: 0.1687, step time: 1.5379\n",
      "133/388, train_loss: 0.2529, step time: 1.5393\n",
      "134/388, train_loss: 0.1130, step time: 1.5350\n",
      "135/388, train_loss: 0.1818, step time: 1.5311\n",
      "136/388, train_loss: 0.0416, step time: 1.5434\n",
      "137/388, train_loss: 0.1476, step time: 1.5329\n",
      "138/388, train_loss: 0.1993, step time: 1.5314\n",
      "139/388, train_loss: 0.1757, step time: 1.5325\n",
      "140/388, train_loss: 0.0778, step time: 1.5299\n",
      "141/388, train_loss: 0.0392, step time: 1.5304\n",
      "142/388, train_loss: 0.1245, step time: 1.5336\n",
      "143/388, train_loss: 0.0532, step time: 1.5321\n",
      "144/388, train_loss: 0.1591, step time: 1.5338\n",
      "145/388, train_loss: 0.1414, step time: 1.5326\n",
      "146/388, train_loss: 0.2645, step time: 1.5314\n",
      "147/388, train_loss: 0.4763, step time: 1.5338\n",
      "148/388, train_loss: 0.0966, step time: 1.5313\n",
      "149/388, train_loss: 0.1715, step time: 1.5308\n",
      "150/388, train_loss: 0.0786, step time: 1.5314\n",
      "151/388, train_loss: 0.3337, step time: 1.5312\n",
      "152/388, train_loss: 0.2677, step time: 1.5334\n",
      "153/388, train_loss: 0.2054, step time: 1.5320\n",
      "154/388, train_loss: 0.1147, step time: 1.5349\n",
      "155/388, train_loss: 0.1865, step time: 1.5347\n",
      "156/388, train_loss: 0.0947, step time: 1.5286\n",
      "157/388, train_loss: 0.1906, step time: 1.5308\n",
      "158/388, train_loss: 0.1772, step time: 1.5325\n",
      "159/388, train_loss: 0.1370, step time: 1.5369\n",
      "160/388, train_loss: 0.0577, step time: 1.5339\n",
      "161/388, train_loss: 0.0551, step time: 1.5278\n",
      "162/388, train_loss: 0.2058, step time: 1.5303\n",
      "163/388, train_loss: 0.1153, step time: 1.5339\n",
      "164/388, train_loss: 0.0682, step time: 1.5285\n",
      "165/388, train_loss: 0.3642, step time: 1.5334\n",
      "166/388, train_loss: 0.1619, step time: 1.5378\n",
      "167/388, train_loss: 0.1680, step time: 1.5333\n",
      "168/388, train_loss: 0.0895, step time: 1.5347\n",
      "169/388, train_loss: 0.1320, step time: 1.5302\n",
      "170/388, train_loss: 0.4000, step time: 1.5438\n",
      "171/388, train_loss: 0.3201, step time: 1.5344\n",
      "172/388, train_loss: 0.0460, step time: 1.5321\n",
      "173/388, train_loss: 0.1509, step time: 1.5300\n",
      "174/388, train_loss: 0.1995, step time: 1.5303\n",
      "175/388, train_loss: 0.2750, step time: 1.5316\n",
      "176/388, train_loss: 0.2925, step time: 1.5325\n",
      "177/388, train_loss: 0.2551, step time: 1.5359\n",
      "178/388, train_loss: 0.1668, step time: 1.5328\n",
      "179/388, train_loss: 0.0513, step time: 1.5319\n",
      "180/388, train_loss: 0.3809, step time: 1.5601\n",
      "181/388, train_loss: 0.0849, step time: 1.5347\n",
      "182/388, train_loss: 0.1475, step time: 1.5292\n",
      "183/388, train_loss: 0.2599, step time: 1.5293\n",
      "184/388, train_loss: 0.0915, step time: 1.5321\n",
      "185/388, train_loss: 0.1465, step time: 1.5391\n",
      "186/388, train_loss: 0.2594, step time: 1.5335\n",
      "187/388, train_loss: 0.0678, step time: 1.5317\n",
      "188/388, train_loss: 0.0558, step time: 1.5305\n",
      "189/388, train_loss: 0.2959, step time: 1.5302\n",
      "190/388, train_loss: 0.2056, step time: 1.5366\n",
      "191/388, train_loss: 0.4032, step time: 1.5395\n",
      "192/388, train_loss: 0.2998, step time: 1.5322\n",
      "193/388, train_loss: 0.1327, step time: 1.5319\n",
      "194/388, train_loss: 0.1834, step time: 1.5306\n",
      "195/388, train_loss: 0.1153, step time: 1.5313\n",
      "196/388, train_loss: 0.1475, step time: 1.5326\n",
      "197/388, train_loss: 0.2052, step time: 1.5300\n",
      "198/388, train_loss: 0.2221, step time: 1.5311\n",
      "199/388, train_loss: 0.1001, step time: 1.5316\n",
      "200/388, train_loss: 0.2306, step time: 1.5325\n",
      "201/388, train_loss: 0.0658, step time: 1.5342\n",
      "202/388, train_loss: 0.1546, step time: 1.5289\n",
      "203/388, train_loss: 0.0871, step time: 1.5323\n",
      "204/388, train_loss: 0.1208, step time: 1.5372\n",
      "205/388, train_loss: 0.2206, step time: 1.5352\n",
      "206/388, train_loss: 0.2434, step time: 1.5334\n",
      "207/388, train_loss: 0.1442, step time: 1.5319\n",
      "208/388, train_loss: 0.1187, step time: 1.5308\n",
      "209/388, train_loss: 0.1624, step time: 1.5338\n",
      "210/388, train_loss: 0.1032, step time: 1.5333\n",
      "211/388, train_loss: 0.1069, step time: 1.5347\n",
      "212/388, train_loss: 0.2426, step time: 1.5355\n",
      "213/388, train_loss: 0.0399, step time: 1.5319\n",
      "214/388, train_loss: 0.0702, step time: 1.5331\n",
      "215/388, train_loss: 0.0876, step time: 1.5365\n",
      "216/388, train_loss: 0.1948, step time: 1.5359\n",
      "217/388, train_loss: 0.1428, step time: 1.5332\n",
      "218/388, train_loss: 0.0904, step time: 1.5342\n",
      "219/388, train_loss: 0.1327, step time: 1.5310\n",
      "220/388, train_loss: 0.0970, step time: 1.5335\n",
      "221/388, train_loss: 0.1156, step time: 1.5330\n",
      "222/388, train_loss: 0.2000, step time: 1.5344\n",
      "223/388, train_loss: 0.0901, step time: 1.5354\n",
      "224/388, train_loss: 0.2274, step time: 1.5349\n",
      "225/388, train_loss: 0.3436, step time: 1.5306\n",
      "226/388, train_loss: 0.2057, step time: 1.5314\n",
      "227/388, train_loss: 0.1152, step time: 1.5292\n",
      "228/388, train_loss: 0.2802, step time: 1.5331\n",
      "229/388, train_loss: 0.2196, step time: 1.5298\n",
      "230/388, train_loss: 0.1953, step time: 1.5310\n",
      "231/388, train_loss: 0.1360, step time: 1.5343\n",
      "232/388, train_loss: 0.1493, step time: 1.5345\n",
      "233/388, train_loss: 0.2802, step time: 1.5364\n",
      "234/388, train_loss: 0.1862, step time: 1.5315\n",
      "235/388, train_loss: 0.1339, step time: 1.5323\n",
      "236/388, train_loss: 0.1449, step time: 1.5320\n",
      "237/388, train_loss: 0.1138, step time: 1.5302\n",
      "238/388, train_loss: 0.1965, step time: 1.5328\n",
      "239/388, train_loss: 0.0598, step time: 1.5398\n",
      "240/388, train_loss: 0.0731, step time: 1.5324\n",
      "241/388, train_loss: 0.1409, step time: 1.5322\n",
      "242/388, train_loss: 0.5172, step time: 1.5321\n",
      "243/388, train_loss: 0.0873, step time: 1.5329\n",
      "244/388, train_loss: 0.2891, step time: 1.5311\n",
      "245/388, train_loss: 0.0940, step time: 1.5307\n",
      "246/388, train_loss: 0.1302, step time: 1.5356\n",
      "247/388, train_loss: 0.1256, step time: 1.5351\n",
      "248/388, train_loss: 0.1500, step time: 1.5349\n",
      "249/388, train_loss: 0.1399, step time: 1.5317\n",
      "250/388, train_loss: 0.3281, step time: 1.5299\n",
      "251/388, train_loss: 0.1570, step time: 1.5305\n",
      "252/388, train_loss: 0.1777, step time: 1.5314\n",
      "253/388, train_loss: 0.1843, step time: 1.5422\n",
      "254/388, train_loss: 0.0931, step time: 1.5298\n",
      "255/388, train_loss: 0.0981, step time: 1.5318\n",
      "256/388, train_loss: 0.1210, step time: 1.5333\n",
      "257/388, train_loss: 0.3708, step time: 1.5348\n",
      "258/388, train_loss: 0.1576, step time: 1.5321\n",
      "259/388, train_loss: 0.4697, step time: 1.5338\n",
      "260/388, train_loss: 0.0956, step time: 1.5291\n",
      "261/388, train_loss: 0.1157, step time: 1.5299\n",
      "262/388, train_loss: 0.1090, step time: 1.5337\n",
      "263/388, train_loss: 0.1528, step time: 1.5307\n",
      "264/388, train_loss: 0.1306, step time: 1.5343\n",
      "265/388, train_loss: 0.2953, step time: 1.5349\n",
      "266/388, train_loss: 0.1248, step time: 1.5306\n",
      "267/388, train_loss: 0.1756, step time: 1.5297\n",
      "268/388, train_loss: 0.2160, step time: 1.5298\n",
      "269/388, train_loss: 0.1287, step time: 1.5349\n",
      "270/388, train_loss: 0.4624, step time: 1.5353\n",
      "271/388, train_loss: 0.2046, step time: 1.5354\n",
      "272/388, train_loss: 0.0707, step time: 1.5276\n",
      "273/388, train_loss: 0.0549, step time: 1.5329\n",
      "274/388, train_loss: 0.1005, step time: 1.5296\n",
      "275/388, train_loss: 0.1637, step time: 1.5631\n",
      "276/388, train_loss: 0.1892, step time: 1.5342\n",
      "277/388, train_loss: 0.2221, step time: 1.5332\n",
      "278/388, train_loss: 0.0788, step time: 1.5333\n",
      "279/388, train_loss: 0.3185, step time: 1.5319\n",
      "280/388, train_loss: 0.0637, step time: 1.5352\n",
      "281/388, train_loss: 0.1938, step time: 1.5357\n",
      "282/388, train_loss: 0.2493, step time: 1.5307\n",
      "283/388, train_loss: 0.3472, step time: 1.5327\n",
      "284/388, train_loss: 0.2037, step time: 1.5308\n",
      "285/388, train_loss: 0.2006, step time: 1.5322\n",
      "286/388, train_loss: 0.1330, step time: 1.5317\n",
      "287/388, train_loss: 0.0888, step time: 1.5340\n",
      "288/388, train_loss: 0.2116, step time: 1.5352\n",
      "289/388, train_loss: 0.2329, step time: 1.5331\n",
      "290/388, train_loss: 0.0532, step time: 1.5402\n",
      "291/388, train_loss: 0.1137, step time: 1.5292\n",
      "292/388, train_loss: 0.3604, step time: 1.5315\n",
      "293/388, train_loss: 0.1714, step time: 1.5302\n",
      "294/388, train_loss: 0.1534, step time: 1.5335\n",
      "295/388, train_loss: 0.1621, step time: 1.5335\n",
      "296/388, train_loss: 0.1202, step time: 1.5348\n",
      "297/388, train_loss: 0.1083, step time: 1.5334\n",
      "298/388, train_loss: 0.1394, step time: 1.5309\n",
      "299/388, train_loss: 0.0861, step time: 1.5276\n",
      "300/388, train_loss: 0.1059, step time: 1.5307\n",
      "301/388, train_loss: 0.0849, step time: 1.5357\n",
      "302/388, train_loss: 0.2210, step time: 1.5352\n",
      "303/388, train_loss: 0.2066, step time: 1.5326\n",
      "304/388, train_loss: 0.1481, step time: 1.5358\n",
      "305/388, train_loss: 0.0794, step time: 1.5310\n",
      "306/388, train_loss: 0.0403, step time: 1.5325\n",
      "307/388, train_loss: 0.0924, step time: 1.5318\n",
      "308/388, train_loss: 0.1848, step time: 1.5345\n",
      "309/388, train_loss: 0.2332, step time: 1.5319\n",
      "310/388, train_loss: 0.1663, step time: 1.5371\n",
      "311/388, train_loss: 0.1190, step time: 1.5317\n",
      "312/388, train_loss: 0.1908, step time: 1.5459\n",
      "313/388, train_loss: 0.2231, step time: 1.5322\n",
      "314/388, train_loss: 0.0945, step time: 1.5367\n",
      "315/388, train_loss: 0.0895, step time: 1.5358\n",
      "316/388, train_loss: 0.0956, step time: 1.5353\n",
      "317/388, train_loss: 0.1419, step time: 1.5319\n",
      "318/388, train_loss: 0.1680, step time: 1.5324\n",
      "319/388, train_loss: 0.1390, step time: 1.5330\n",
      "320/388, train_loss: 0.2399, step time: 1.5353\n",
      "321/388, train_loss: 0.2495, step time: 1.5337\n",
      "322/388, train_loss: 0.1679, step time: 1.5331\n",
      "323/388, train_loss: 0.0832, step time: 1.5311\n",
      "324/388, train_loss: 0.2500, step time: 1.5396\n",
      "325/388, train_loss: 0.1460, step time: 1.5345\n",
      "326/388, train_loss: 0.0879, step time: 1.5339\n",
      "327/388, train_loss: 0.3887, step time: 1.5317\n",
      "328/388, train_loss: 0.0978, step time: 1.5357\n",
      "329/388, train_loss: 0.2677, step time: 1.5363\n",
      "330/388, train_loss: 0.1739, step time: 1.5323\n",
      "331/388, train_loss: 0.3534, step time: 1.5311\n",
      "332/388, train_loss: 0.1056, step time: 1.5341\n",
      "333/388, train_loss: 0.2287, step time: 1.5369\n",
      "334/388, train_loss: 0.1658, step time: 1.5399\n",
      "335/388, train_loss: 0.0838, step time: 1.5344\n",
      "336/388, train_loss: 0.2153, step time: 1.5317\n",
      "337/388, train_loss: 0.0979, step time: 1.5344\n",
      "338/388, train_loss: 0.0617, step time: 1.5364\n",
      "339/388, train_loss: 0.2392, step time: 1.5363\n",
      "340/388, train_loss: 0.2269, step time: 1.5339\n",
      "341/388, train_loss: 0.2516, step time: 1.5560\n",
      "342/388, train_loss: 0.2820, step time: 1.5374\n",
      "343/388, train_loss: 0.1717, step time: 1.5376\n",
      "344/388, train_loss: 0.1200, step time: 1.5357\n",
      "345/388, train_loss: 0.1950, step time: 1.5312\n",
      "346/388, train_loss: 0.1078, step time: 1.5346\n",
      "347/388, train_loss: 0.0856, step time: 1.5364\n",
      "348/388, train_loss: 0.1404, step time: 1.5344\n",
      "349/388, train_loss: 0.1723, step time: 1.5315\n",
      "350/388, train_loss: 0.1315, step time: 1.5372\n",
      "351/388, train_loss: 0.0629, step time: 1.5365\n",
      "352/388, train_loss: 0.1686, step time: 1.5335\n",
      "353/388, train_loss: 0.1932, step time: 1.5321\n",
      "354/388, train_loss: 0.1691, step time: 1.5331\n",
      "355/388, train_loss: 0.1619, step time: 1.5338\n",
      "356/388, train_loss: 0.1177, step time: 1.5349\n",
      "357/388, train_loss: 0.1092, step time: 1.5388\n",
      "358/388, train_loss: 0.0666, step time: 1.5323\n",
      "359/388, train_loss: 0.2163, step time: 1.5332\n",
      "360/388, train_loss: 0.2096, step time: 1.5329\n",
      "361/388, train_loss: 0.1403, step time: 1.5364\n",
      "362/388, train_loss: 0.2719, step time: 1.5391\n",
      "363/388, train_loss: 0.0795, step time: 1.5344\n",
      "364/388, train_loss: 0.1652, step time: 1.5326\n",
      "365/388, train_loss: 0.2392, step time: 1.5356\n",
      "366/388, train_loss: 0.1497, step time: 1.5366\n",
      "367/388, train_loss: 0.4362, step time: 1.5362\n",
      "368/388, train_loss: 0.3016, step time: 1.5465\n",
      "369/388, train_loss: 0.1174, step time: 1.5324\n",
      "370/388, train_loss: 0.1996, step time: 1.5302\n",
      "371/388, train_loss: 0.0703, step time: 1.5359\n",
      "372/388, train_loss: 0.2365, step time: 1.5360\n",
      "373/388, train_loss: 0.2285, step time: 1.5330\n",
      "374/388, train_loss: 0.0899, step time: 1.5339\n",
      "375/388, train_loss: 0.0782, step time: 1.5316\n",
      "376/388, train_loss: 0.4813, step time: 1.5354\n",
      "377/388, train_loss: 0.1126, step time: 1.5338\n",
      "378/388, train_loss: 0.1139, step time: 1.5322\n",
      "379/388, train_loss: 0.4489, step time: 1.5347\n",
      "380/388, train_loss: 0.0997, step time: 1.5325\n",
      "381/388, train_loss: 0.1478, step time: 1.5368\n",
      "382/388, train_loss: 0.2236, step time: 1.5376\n",
      "383/388, train_loss: 0.1334, step time: 1.5325\n",
      "384/388, train_loss: 0.3024, step time: 1.5306\n",
      "385/388, train_loss: 0.1071, step time: 1.5326\n",
      "386/388, train_loss: 0.0961, step time: 1.5508\n",
      "387/388, train_loss: 0.1366, step time: 1.5352\n",
      "388/388, train_loss: 0.2703, step time: 1.5380\n",
      "epoch 63 average loss: 0.1706\n",
      "saved new best metric model\n",
      "current epoch: 63 current mean dice: 0.7743 tc: 0.8222 wt: 0.9036 et: 0.5971\n",
      "best mean dice: 0.7743 at epoch: 63\n",
      "time consuming of epoch 63 is: 706.0639\n",
      "----------\n",
      "epoch 64/100\n",
      "1/388, train_loss: 0.3849, step time: 1.5475\n",
      "2/388, train_loss: 0.1167, step time: 1.5360\n",
      "3/388, train_loss: 0.3060, step time: 1.5335\n",
      "4/388, train_loss: 0.2008, step time: 1.5362\n",
      "5/388, train_loss: 0.0936, step time: 1.5358\n",
      "6/388, train_loss: 0.1413, step time: 1.5369\n",
      "7/388, train_loss: 0.1000, step time: 1.5346\n",
      "8/388, train_loss: 0.2499, step time: 1.5356\n",
      "9/388, train_loss: 0.3971, step time: 1.5332\n",
      "10/388, train_loss: 0.0925, step time: 1.5368\n",
      "11/388, train_loss: 0.3600, step time: 1.5369\n",
      "12/388, train_loss: 0.0438, step time: 1.5343\n",
      "13/388, train_loss: 0.0852, step time: 1.5289\n",
      "14/388, train_loss: 0.0866, step time: 1.5371\n",
      "15/388, train_loss: 0.2380, step time: 1.5342\n",
      "16/388, train_loss: 0.3136, step time: 1.5367\n",
      "17/388, train_loss: 0.2748, step time: 1.5322\n",
      "18/388, train_loss: 0.0634, step time: 1.5351\n",
      "19/388, train_loss: 0.2126, step time: 1.5318\n",
      "20/388, train_loss: 0.1812, step time: 1.5359\n",
      "21/388, train_loss: 0.0953, step time: 1.5359\n",
      "22/388, train_loss: 0.1664, step time: 1.5367\n",
      "23/388, train_loss: 0.4168, step time: 1.5345\n",
      "24/388, train_loss: 0.0842, step time: 1.5308\n",
      "25/388, train_loss: 0.1502, step time: 1.5346\n",
      "26/388, train_loss: 0.2341, step time: 1.5384\n",
      "27/388, train_loss: 0.1877, step time: 1.5350\n",
      "28/388, train_loss: 0.1221, step time: 1.5436\n",
      "29/388, train_loss: 0.1912, step time: 1.5345\n",
      "30/388, train_loss: 0.2502, step time: 1.5334\n",
      "31/388, train_loss: 0.1685, step time: 1.5380\n",
      "32/388, train_loss: 0.0694, step time: 1.5466\n",
      "33/388, train_loss: 0.0766, step time: 1.5285\n",
      "34/388, train_loss: 0.1367, step time: 1.5336\n",
      "35/388, train_loss: 0.1220, step time: 1.5323\n",
      "36/388, train_loss: 0.1913, step time: 1.5302\n",
      "37/388, train_loss: 0.2204, step time: 1.5324\n",
      "38/388, train_loss: 0.0507, step time: 1.5342\n",
      "39/388, train_loss: 0.1815, step time: 1.5365\n",
      "40/388, train_loss: 0.1296, step time: 1.5337\n",
      "41/388, train_loss: 0.3494, step time: 1.5324\n",
      "42/388, train_loss: 0.0689, step time: 1.5313\n",
      "43/388, train_loss: 0.1131, step time: 1.5301\n",
      "44/388, train_loss: 0.0810, step time: 1.5320\n",
      "45/388, train_loss: 0.2901, step time: 1.5316\n",
      "46/388, train_loss: 0.0389, step time: 1.5374\n",
      "47/388, train_loss: 0.1470, step time: 1.5363\n",
      "48/388, train_loss: 0.3003, step time: 1.5353\n",
      "49/388, train_loss: 0.4031, step time: 1.5314\n",
      "50/388, train_loss: 0.1550, step time: 1.5284\n",
      "51/388, train_loss: 0.0876, step time: 1.5305\n",
      "52/388, train_loss: 0.4474, step time: 1.5340\n",
      "53/388, train_loss: 0.0682, step time: 1.5350\n",
      "54/388, train_loss: 0.0560, step time: 1.5568\n",
      "55/388, train_loss: 0.1370, step time: 1.5415\n",
      "56/388, train_loss: 0.0574, step time: 1.5341\n",
      "57/388, train_loss: 0.1561, step time: 1.5329\n",
      "58/388, train_loss: 0.1047, step time: 1.5327\n",
      "59/388, train_loss: 0.2088, step time: 1.5296\n",
      "60/388, train_loss: 0.2809, step time: 1.5299\n",
      "61/388, train_loss: 0.5136, step time: 1.5289\n",
      "62/388, train_loss: 0.2450, step time: 1.5360\n",
      "63/388, train_loss: 0.1360, step time: 1.5335\n",
      "64/388, train_loss: 0.0958, step time: 1.5351\n",
      "65/388, train_loss: 0.1456, step time: 1.5320\n",
      "66/388, train_loss: 0.5239, step time: 1.5312\n",
      "67/388, train_loss: 0.0840, step time: 1.5286\n",
      "68/388, train_loss: 0.1942, step time: 1.5279\n",
      "69/388, train_loss: 0.0939, step time: 1.5323\n",
      "70/388, train_loss: 0.2214, step time: 1.5364\n",
      "71/388, train_loss: 0.0919, step time: 1.5361\n",
      "72/388, train_loss: 0.2488, step time: 1.5349\n",
      "73/388, train_loss: 0.0795, step time: 1.5315\n",
      "74/388, train_loss: 0.2628, step time: 1.5394\n",
      "75/388, train_loss: 0.0974, step time: 1.5292\n",
      "76/388, train_loss: 0.1115, step time: 1.5352\n",
      "77/388, train_loss: 0.1830, step time: 1.5346\n",
      "78/388, train_loss: 0.1259, step time: 1.5359\n",
      "79/388, train_loss: 0.0919, step time: 1.5339\n",
      "80/388, train_loss: 0.1129, step time: 1.5331\n",
      "81/388, train_loss: 0.1699, step time: 1.5351\n",
      "82/388, train_loss: 0.0888, step time: 1.5333\n",
      "83/388, train_loss: 0.0726, step time: 1.5359\n",
      "84/388, train_loss: 0.1541, step time: 1.5309\n",
      "85/388, train_loss: 0.0707, step time: 1.5288\n",
      "86/388, train_loss: 0.2196, step time: 1.5339\n",
      "87/388, train_loss: 0.1035, step time: 1.5295\n",
      "88/388, train_loss: 0.1894, step time: 1.5326\n",
      "89/388, train_loss: 0.1317, step time: 1.5338\n",
      "90/388, train_loss: 0.2248, step time: 1.5374\n",
      "91/388, train_loss: 0.0805, step time: 1.5371\n",
      "92/388, train_loss: 0.3102, step time: 1.5319\n",
      "93/388, train_loss: 0.2770, step time: 1.5335\n",
      "94/388, train_loss: 0.3176, step time: 1.5336\n",
      "95/388, train_loss: 0.1362, step time: 1.5324\n",
      "96/388, train_loss: 0.1773, step time: 1.5371\n",
      "97/388, train_loss: 0.1706, step time: 1.5329\n",
      "98/388, train_loss: 0.0925, step time: 1.5331\n",
      "99/388, train_loss: 0.1120, step time: 1.5431\n",
      "100/388, train_loss: 0.0910, step time: 1.5330\n",
      "101/388, train_loss: 0.1533, step time: 1.5598\n",
      "102/388, train_loss: 0.1683, step time: 1.5299\n",
      "103/388, train_loss: 0.2072, step time: 1.5340\n",
      "104/388, train_loss: 0.0550, step time: 1.5355\n",
      "105/388, train_loss: 0.1337, step time: 1.5342\n",
      "106/388, train_loss: 0.1088, step time: 1.5317\n",
      "107/388, train_loss: 0.1535, step time: 1.5584\n",
      "108/388, train_loss: 0.1820, step time: 1.5367\n",
      "109/388, train_loss: 0.2610, step time: 1.5299\n",
      "110/388, train_loss: 0.1098, step time: 1.5640\n",
      "111/388, train_loss: 0.0638, step time: 1.5373\n",
      "112/388, train_loss: 0.1305, step time: 1.5398\n",
      "113/388, train_loss: 0.4502, step time: 1.5322\n",
      "114/388, train_loss: 0.1522, step time: 1.5352\n",
      "115/388, train_loss: 0.4071, step time: 1.5357\n",
      "116/388, train_loss: 0.0851, step time: 1.5356\n",
      "117/388, train_loss: 0.1523, step time: 1.5319\n",
      "118/388, train_loss: 0.2393, step time: 1.5320\n",
      "119/388, train_loss: 0.0999, step time: 1.5309\n",
      "120/388, train_loss: 0.0875, step time: 1.5310\n",
      "121/388, train_loss: 0.1434, step time: 1.5362\n",
      "122/388, train_loss: 0.0948, step time: 1.5337\n",
      "123/388, train_loss: 0.1577, step time: 1.5327\n",
      "124/388, train_loss: 0.1458, step time: 1.5342\n",
      "125/388, train_loss: 0.1380, step time: 1.5295\n",
      "126/388, train_loss: 0.1217, step time: 1.5302\n",
      "127/388, train_loss: 0.0750, step time: 1.5317\n",
      "128/388, train_loss: 0.2534, step time: 1.5361\n",
      "129/388, train_loss: 0.2783, step time: 1.5422\n",
      "130/388, train_loss: 0.1912, step time: 1.5291\n",
      "131/388, train_loss: 0.1296, step time: 1.5273\n",
      "132/388, train_loss: 0.4687, step time: 1.5316\n",
      "133/388, train_loss: 0.2060, step time: 1.5321\n",
      "134/388, train_loss: 0.1523, step time: 1.5359\n",
      "135/388, train_loss: 0.1240, step time: 1.5339\n",
      "136/388, train_loss: 0.1502, step time: 1.5324\n",
      "137/388, train_loss: 0.2082, step time: 1.5338\n",
      "138/388, train_loss: 0.1166, step time: 1.5305\n",
      "139/388, train_loss: 0.0669, step time: 1.5334\n",
      "140/388, train_loss: 0.1144, step time: 1.5353\n",
      "141/388, train_loss: 0.1640, step time: 1.5592\n",
      "142/388, train_loss: 0.4074, step time: 1.5325\n",
      "143/388, train_loss: 0.2418, step time: 1.5331\n",
      "144/388, train_loss: 0.1039, step time: 1.5352\n",
      "145/388, train_loss: 0.1682, step time: 1.5333\n",
      "146/388, train_loss: 0.1594, step time: 1.5360\n",
      "147/388, train_loss: 0.2838, step time: 1.5329\n",
      "148/388, train_loss: 0.2089, step time: 1.5329\n",
      "149/388, train_loss: 0.2347, step time: 1.5292\n",
      "150/388, train_loss: 0.1944, step time: 1.5321\n",
      "151/388, train_loss: 0.1835, step time: 1.5303\n",
      "152/388, train_loss: 0.1187, step time: 1.5343\n",
      "153/388, train_loss: 0.1147, step time: 1.5279\n",
      "154/388, train_loss: 0.3777, step time: 1.5381\n",
      "155/388, train_loss: 0.0992, step time: 1.5333\n",
      "156/388, train_loss: 0.0559, step time: 1.5374\n",
      "157/388, train_loss: 0.3036, step time: 1.5323\n",
      "158/388, train_loss: 0.1362, step time: 1.5349\n",
      "159/388, train_loss: 0.1067, step time: 1.5355\n",
      "160/388, train_loss: 0.2480, step time: 1.5417\n",
      "161/388, train_loss: 0.1649, step time: 1.5431\n",
      "162/388, train_loss: 0.2668, step time: 1.5345\n",
      "163/388, train_loss: 0.2656, step time: 1.5379\n",
      "164/388, train_loss: 0.2033, step time: 1.5316\n",
      "165/388, train_loss: 0.0694, step time: 1.5336\n",
      "166/388, train_loss: 0.2172, step time: 1.5324\n",
      "167/388, train_loss: 0.1914, step time: 1.5287\n",
      "168/388, train_loss: 0.2763, step time: 1.5322\n",
      "169/388, train_loss: 0.0306, step time: 1.5333\n",
      "170/388, train_loss: 0.2021, step time: 1.5354\n",
      "171/388, train_loss: 0.2181, step time: 1.5324\n",
      "172/388, train_loss: 0.0887, step time: 1.5345\n",
      "173/388, train_loss: 0.2199, step time: 1.5341\n",
      "174/388, train_loss: 0.0990, step time: 1.5325\n",
      "175/388, train_loss: 0.0800, step time: 1.5413\n",
      "176/388, train_loss: 0.0994, step time: 1.5358\n",
      "177/388, train_loss: 0.0948, step time: 1.5321\n",
      "178/388, train_loss: 0.1678, step time: 1.5331\n",
      "179/388, train_loss: 0.1981, step time: 1.5294\n",
      "180/388, train_loss: 0.0852, step time: 1.5310\n",
      "181/388, train_loss: 0.0943, step time: 1.5356\n",
      "182/388, train_loss: 0.1092, step time: 1.5343\n",
      "183/388, train_loss: 0.2694, step time: 1.5364\n",
      "184/388, train_loss: 0.1523, step time: 1.5348\n",
      "185/388, train_loss: 0.1861, step time: 1.5343\n",
      "186/388, train_loss: 0.0817, step time: 1.5317\n",
      "187/388, train_loss: 0.1811, step time: 1.5304\n",
      "188/388, train_loss: 0.0971, step time: 1.5326\n",
      "189/388, train_loss: 0.2149, step time: 1.5347\n",
      "190/388, train_loss: 0.2114, step time: 1.5370\n",
      "191/388, train_loss: 0.1393, step time: 1.5362\n",
      "192/388, train_loss: 0.2180, step time: 1.5322\n",
      "193/388, train_loss: 0.1730, step time: 1.5329\n",
      "194/388, train_loss: 0.2457, step time: 1.5323\n",
      "195/388, train_loss: 0.3514, step time: 1.5388\n",
      "196/388, train_loss: 0.1281, step time: 1.5395\n",
      "197/388, train_loss: 0.1992, step time: 1.5339\n",
      "198/388, train_loss: 0.1848, step time: 1.5317\n",
      "199/388, train_loss: 0.1199, step time: 1.5289\n",
      "200/388, train_loss: 0.1003, step time: 1.5328\n",
      "201/388, train_loss: 0.1928, step time: 1.5371\n",
      "202/388, train_loss: 0.0878, step time: 1.5371\n",
      "203/388, train_loss: 0.0863, step time: 1.5599\n",
      "204/388, train_loss: 0.1432, step time: 1.5321\n",
      "205/388, train_loss: 0.1924, step time: 1.5321\n",
      "206/388, train_loss: 0.2759, step time: 1.5348\n",
      "207/388, train_loss: 0.0865, step time: 1.5316\n",
      "208/388, train_loss: 0.1337, step time: 1.5354\n",
      "209/388, train_loss: 0.1324, step time: 1.5319\n",
      "210/388, train_loss: 0.1813, step time: 1.5322\n",
      "211/388, train_loss: 0.0625, step time: 1.5334\n",
      "212/388, train_loss: 0.2002, step time: 1.5322\n",
      "213/388, train_loss: 0.1272, step time: 1.5362\n",
      "214/388, train_loss: 0.1096, step time: 1.5344\n",
      "215/388, train_loss: 0.2065, step time: 1.5367\n",
      "216/388, train_loss: 0.1280, step time: 1.5329\n",
      "217/388, train_loss: 0.0827, step time: 1.5329\n",
      "218/388, train_loss: 0.2422, step time: 1.5342\n",
      "219/388, train_loss: 0.3386, step time: 1.5368\n",
      "220/388, train_loss: 0.0999, step time: 1.5345\n",
      "221/388, train_loss: 0.1431, step time: 1.5368\n",
      "222/388, train_loss: 0.1204, step time: 1.5333\n",
      "223/388, train_loss: 0.0749, step time: 1.5334\n",
      "224/388, train_loss: 0.2759, step time: 1.5354\n",
      "225/388, train_loss: 0.5372, step time: 1.5380\n",
      "226/388, train_loss: 0.0790, step time: 1.5384\n",
      "227/388, train_loss: 0.0835, step time: 1.5391\n",
      "228/388, train_loss: 0.1117, step time: 1.5312\n",
      "229/388, train_loss: 0.0658, step time: 1.5298\n",
      "230/388, train_loss: 0.0568, step time: 1.5323\n",
      "231/388, train_loss: 0.0918, step time: 1.5335\n",
      "232/388, train_loss: 0.3134, step time: 1.5331\n",
      "233/388, train_loss: 0.1753, step time: 1.5359\n",
      "234/388, train_loss: 0.2408, step time: 1.5370\n",
      "235/388, train_loss: 0.1244, step time: 1.5325\n",
      "236/388, train_loss: 0.0516, step time: 1.5330\n",
      "237/388, train_loss: 0.0879, step time: 1.5306\n",
      "238/388, train_loss: 0.1065, step time: 1.5345\n",
      "239/388, train_loss: 0.1602, step time: 1.5338\n",
      "240/388, train_loss: 0.1284, step time: 1.5357\n",
      "241/388, train_loss: 0.0909, step time: 1.5332\n",
      "242/388, train_loss: 0.1264, step time: 1.5316\n",
      "243/388, train_loss: 0.1758, step time: 1.5349\n",
      "244/388, train_loss: 0.2319, step time: 1.5340\n",
      "245/388, train_loss: 0.2325, step time: 1.5366\n",
      "246/388, train_loss: 0.2535, step time: 1.5410\n",
      "247/388, train_loss: 0.1364, step time: 1.5363\n",
      "248/388, train_loss: 0.1796, step time: 1.5361\n",
      "249/388, train_loss: 0.5818, step time: 1.5384\n",
      "250/388, train_loss: 0.1610, step time: 1.5322\n",
      "251/388, train_loss: 0.1810, step time: 1.5324\n",
      "252/388, train_loss: 0.2013, step time: 1.5302\n",
      "253/388, train_loss: 0.2235, step time: 1.5313\n",
      "254/388, train_loss: 0.2694, step time: 1.5346\n",
      "255/388, train_loss: 0.1856, step time: 1.5348\n",
      "256/388, train_loss: 0.1333, step time: 1.5329\n",
      "257/388, train_loss: 0.0674, step time: 1.5389\n",
      "258/388, train_loss: 0.4172, step time: 1.5343\n",
      "259/388, train_loss: 0.2021, step time: 1.5323\n",
      "260/388, train_loss: 0.2112, step time: 1.5324\n",
      "261/388, train_loss: 0.0865, step time: 1.5326\n",
      "262/388, train_loss: 0.2223, step time: 1.5339\n",
      "263/388, train_loss: 0.1187, step time: 1.5357\n",
      "264/388, train_loss: 0.0571, step time: 1.5439\n",
      "265/388, train_loss: 0.1759, step time: 1.5335\n",
      "266/388, train_loss: 0.1258, step time: 1.5344\n",
      "267/388, train_loss: 0.0820, step time: 1.5328\n",
      "268/388, train_loss: 0.0284, step time: 1.5358\n",
      "269/388, train_loss: 0.1298, step time: 1.5338\n",
      "270/388, train_loss: 0.1285, step time: 1.5340\n",
      "271/388, train_loss: 0.1658, step time: 1.5310\n",
      "272/388, train_loss: 0.1958, step time: 1.5298\n",
      "273/388, train_loss: 0.0695, step time: 1.5354\n",
      "274/388, train_loss: 0.2406, step time: 1.5363\n",
      "275/388, train_loss: 0.2180, step time: 1.5372\n",
      "276/388, train_loss: 0.3139, step time: 1.5362\n",
      "277/388, train_loss: 0.2731, step time: 1.5320\n",
      "278/388, train_loss: 0.1432, step time: 1.5329\n",
      "279/388, train_loss: 0.1224, step time: 1.5332\n",
      "280/388, train_loss: 0.2486, step time: 1.5374\n",
      "281/388, train_loss: 0.0648, step time: 1.5359\n",
      "282/388, train_loss: 0.0656, step time: 1.5324\n",
      "283/388, train_loss: 0.1487, step time: 1.5609\n",
      "284/388, train_loss: 0.2514, step time: 1.5396\n",
      "285/388, train_loss: 0.2468, step time: 1.5333\n",
      "286/388, train_loss: 0.2286, step time: 1.5384\n",
      "287/388, train_loss: 0.0948, step time: 1.5356\n",
      "288/388, train_loss: 0.2907, step time: 1.5361\n",
      "289/388, train_loss: 0.2306, step time: 1.5349\n",
      "290/388, train_loss: 0.2253, step time: 1.5366\n",
      "291/388, train_loss: 0.0721, step time: 1.5305\n",
      "292/388, train_loss: 0.1968, step time: 1.5331\n",
      "293/388, train_loss: 0.2594, step time: 1.5321\n",
      "294/388, train_loss: 0.1101, step time: 1.5316\n",
      "295/388, train_loss: 0.1312, step time: 1.5370\n",
      "296/388, train_loss: 0.1074, step time: 1.5357\n",
      "297/388, train_loss: 0.3086, step time: 1.5330\n",
      "298/388, train_loss: 0.0722, step time: 1.5337\n",
      "299/388, train_loss: 0.0958, step time: 1.5331\n",
      "300/388, train_loss: 0.1787, step time: 1.5316\n",
      "301/388, train_loss: 0.2491, step time: 1.5352\n",
      "302/388, train_loss: 0.2154, step time: 1.5370\n",
      "303/388, train_loss: 0.0690, step time: 1.5381\n",
      "304/388, train_loss: 0.1702, step time: 1.5319\n",
      "305/388, train_loss: 0.0464, step time: 1.5354\n",
      "306/388, train_loss: 0.2004, step time: 1.5371\n",
      "307/388, train_loss: 0.2615, step time: 1.5349\n",
      "308/388, train_loss: 0.0952, step time: 1.5365\n",
      "309/388, train_loss: 0.2761, step time: 1.5309\n",
      "310/388, train_loss: 0.0538, step time: 1.5310\n",
      "311/388, train_loss: 0.0588, step time: 1.5358\n",
      "312/388, train_loss: 0.0412, step time: 1.5371\n",
      "313/388, train_loss: 0.1844, step time: 1.5321\n",
      "314/388, train_loss: 0.1016, step time: 1.5323\n",
      "315/388, train_loss: 0.1957, step time: 1.5301\n",
      "316/388, train_loss: 0.0866, step time: 1.5357\n",
      "317/388, train_loss: 0.1878, step time: 1.5386\n",
      "318/388, train_loss: 0.1438, step time: 1.5371\n",
      "319/388, train_loss: 0.2265, step time: 1.5329\n",
      "320/388, train_loss: 0.1402, step time: 1.5313\n",
      "321/388, train_loss: 0.0938, step time: 1.5350\n",
      "322/388, train_loss: 0.0998, step time: 1.5354\n",
      "323/388, train_loss: 0.0966, step time: 1.5361\n",
      "324/388, train_loss: 0.2236, step time: 1.5320\n",
      "325/388, train_loss: 0.1775, step time: 1.5333\n",
      "326/388, train_loss: 0.1838, step time: 1.5337\n",
      "327/388, train_loss: 0.2108, step time: 1.5359\n",
      "328/388, train_loss: 0.1488, step time: 1.5353\n",
      "329/388, train_loss: 0.0657, step time: 1.5359\n",
      "330/388, train_loss: 0.1386, step time: 1.5328\n",
      "331/388, train_loss: 0.0764, step time: 1.5307\n",
      "332/388, train_loss: 0.1836, step time: 1.5293\n",
      "333/388, train_loss: 0.0954, step time: 1.5318\n",
      "334/388, train_loss: 0.0352, step time: 1.5359\n",
      "335/388, train_loss: 0.0745, step time: 1.5349\n",
      "336/388, train_loss: 0.0912, step time: 1.5353\n",
      "337/388, train_loss: 0.1382, step time: 1.5303\n",
      "338/388, train_loss: 0.1298, step time: 1.5307\n",
      "339/388, train_loss: 0.0945, step time: 1.5335\n",
      "340/388, train_loss: 0.1468, step time: 1.5351\n",
      "341/388, train_loss: 0.1743, step time: 1.5344\n",
      "342/388, train_loss: 0.0394, step time: 1.5392\n",
      "343/388, train_loss: 0.3594, step time: 1.5313\n",
      "344/388, train_loss: 0.0903, step time: 1.5336\n",
      "345/388, train_loss: 0.3131, step time: 1.5343\n",
      "346/388, train_loss: 0.1948, step time: 1.5340\n",
      "347/388, train_loss: 0.2158, step time: 1.5331\n",
      "348/388, train_loss: 0.2839, step time: 1.5346\n",
      "349/388, train_loss: 0.2460, step time: 1.5348\n",
      "350/388, train_loss: 0.1575, step time: 1.5331\n",
      "351/388, train_loss: 0.1610, step time: 1.5338\n",
      "352/388, train_loss: 0.0738, step time: 1.5381\n",
      "353/388, train_loss: 0.0444, step time: 1.5311\n",
      "354/388, train_loss: 0.3175, step time: 1.5363\n",
      "355/388, train_loss: 0.2183, step time: 1.5311\n",
      "356/388, train_loss: 0.1162, step time: 1.5325\n",
      "357/388, train_loss: 0.3498, step time: 1.5571\n",
      "358/388, train_loss: 0.1100, step time: 1.5340\n",
      "359/388, train_loss: 0.1066, step time: 1.5322\n",
      "360/388, train_loss: 0.1536, step time: 1.5360\n",
      "361/388, train_loss: 0.1607, step time: 1.5597\n",
      "362/388, train_loss: 0.0995, step time: 1.5308\n",
      "363/388, train_loss: 0.1769, step time: 1.5317\n",
      "364/388, train_loss: 0.2135, step time: 1.5330\n",
      "365/388, train_loss: 0.1462, step time: 1.5336\n",
      "366/388, train_loss: 0.0804, step time: 1.5344\n",
      "367/388, train_loss: 0.0855, step time: 1.5323\n",
      "368/388, train_loss: 0.0830, step time: 1.5306\n",
      "369/388, train_loss: 0.2884, step time: 1.5327\n",
      "370/388, train_loss: 0.3224, step time: 1.5305\n",
      "371/388, train_loss: 0.4097, step time: 1.5281\n",
      "372/388, train_loss: 0.0898, step time: 1.5316\n",
      "373/388, train_loss: 0.2910, step time: 1.5369\n",
      "374/388, train_loss: 0.2293, step time: 1.5326\n",
      "375/388, train_loss: 0.1225, step time: 1.5349\n",
      "376/388, train_loss: 0.1382, step time: 1.5284\n",
      "377/388, train_loss: 0.1991, step time: 1.5362\n",
      "378/388, train_loss: 0.1174, step time: 1.5367\n",
      "379/388, train_loss: 0.1792, step time: 1.5342\n",
      "380/388, train_loss: 0.2080, step time: 1.5388\n",
      "381/388, train_loss: 0.1414, step time: 1.5324\n",
      "382/388, train_loss: 0.1048, step time: 1.5326\n",
      "383/388, train_loss: 0.1590, step time: 1.5314\n",
      "384/388, train_loss: 0.2101, step time: 1.5375\n",
      "385/388, train_loss: 0.2416, step time: 1.5379\n",
      "386/388, train_loss: 0.1137, step time: 1.5307\n",
      "387/388, train_loss: 0.0956, step time: 1.5371\n",
      "388/388, train_loss: 0.1170, step time: 1.5321\n",
      "epoch 64 average loss: 0.1699\n",
      "current epoch: 64 current mean dice: 0.7740 tc: 0.8223 wt: 0.9039 et: 0.5957\n",
      "best mean dice: 0.7743 at epoch: 63\n",
      "time consuming of epoch 64 is: 703.3872\n",
      "----------\n",
      "epoch 65/100\n",
      "1/388, train_loss: 0.1713, step time: 1.5469\n",
      "2/388, train_loss: 0.1127, step time: 1.5398\n",
      "3/388, train_loss: 0.1458, step time: 1.5352\n",
      "4/388, train_loss: 0.1553, step time: 1.5436\n",
      "5/388, train_loss: 0.2517, step time: 1.5316\n",
      "6/388, train_loss: 0.3485, step time: 1.5312\n",
      "7/388, train_loss: 0.0726, step time: 1.5359\n",
      "8/388, train_loss: 0.1929, step time: 1.5374\n",
      "9/388, train_loss: 0.1653, step time: 1.5313\n",
      "10/388, train_loss: 0.1770, step time: 1.5358\n",
      "11/388, train_loss: 0.2903, step time: 1.5333\n",
      "12/388, train_loss: 0.1720, step time: 1.5376\n",
      "13/388, train_loss: 0.1975, step time: 1.5330\n",
      "14/388, train_loss: 0.1572, step time: 1.5319\n",
      "15/388, train_loss: 0.1898, step time: 1.5333\n",
      "16/388, train_loss: 0.2456, step time: 1.5341\n",
      "17/388, train_loss: 0.1689, step time: 1.5344\n",
      "18/388, train_loss: 0.1127, step time: 1.5355\n",
      "19/388, train_loss: 0.2322, step time: 1.5315\n",
      "20/388, train_loss: 0.1669, step time: 1.5344\n",
      "21/388, train_loss: 0.2716, step time: 1.5289\n",
      "22/388, train_loss: 0.1015, step time: 1.5299\n",
      "23/388, train_loss: 0.0868, step time: 1.5330\n",
      "24/388, train_loss: 0.2578, step time: 1.5338\n",
      "25/388, train_loss: 0.1165, step time: 1.5366\n",
      "26/388, train_loss: 0.0965, step time: 1.5344\n",
      "27/388, train_loss: 0.2605, step time: 1.5318\n",
      "28/388, train_loss: 0.1586, step time: 1.5318\n",
      "29/388, train_loss: 0.1278, step time: 1.5338\n",
      "30/388, train_loss: 0.1594, step time: 1.5329\n",
      "31/388, train_loss: 0.0515, step time: 1.5354\n",
      "32/388, train_loss: 0.1876, step time: 1.5337\n",
      "33/388, train_loss: 0.1983, step time: 1.5352\n",
      "34/388, train_loss: 0.0899, step time: 1.5294\n",
      "35/388, train_loss: 0.0890, step time: 1.5301\n",
      "36/388, train_loss: 0.1852, step time: 1.5314\n",
      "37/388, train_loss: 0.1992, step time: 1.5295\n",
      "38/388, train_loss: 0.2963, step time: 1.5325\n",
      "39/388, train_loss: 0.1096, step time: 1.5310\n",
      "40/388, train_loss: 0.0657, step time: 1.5342\n",
      "41/388, train_loss: 0.0981, step time: 1.5351\n",
      "42/388, train_loss: 0.1877, step time: 1.5319\n",
      "43/388, train_loss: 0.2275, step time: 1.5311\n",
      "44/388, train_loss: 0.0637, step time: 1.5338\n",
      "45/388, train_loss: 0.1313, step time: 1.5298\n",
      "46/388, train_loss: 0.1093, step time: 1.5315\n",
      "47/388, train_loss: 0.0997, step time: 1.5327\n",
      "48/388, train_loss: 0.2236, step time: 1.5353\n",
      "49/388, train_loss: 0.2352, step time: 1.5339\n",
      "50/388, train_loss: 0.1807, step time: 1.5321\n",
      "51/388, train_loss: 0.0839, step time: 1.5321\n",
      "52/388, train_loss: 0.0795, step time: 1.5342\n",
      "53/388, train_loss: 0.1067, step time: 1.5286\n",
      "54/388, train_loss: 0.2317, step time: 1.5386\n",
      "55/388, train_loss: 0.0982, step time: 1.5343\n",
      "56/388, train_loss: 0.1673, step time: 1.5342\n",
      "57/388, train_loss: 0.2870, step time: 1.5342\n",
      "58/388, train_loss: 0.5207, step time: 1.5342\n",
      "59/388, train_loss: 0.0318, step time: 1.5315\n",
      "60/388, train_loss: 0.1234, step time: 1.5346\n",
      "61/388, train_loss: 0.0852, step time: 1.5359\n",
      "62/388, train_loss: 0.1077, step time: 1.5344\n",
      "63/388, train_loss: 0.0703, step time: 1.5333\n",
      "64/388, train_loss: 0.1361, step time: 1.5324\n",
      "65/388, train_loss: 0.1219, step time: 1.5313\n",
      "66/388, train_loss: 0.0908, step time: 1.5639\n",
      "67/388, train_loss: 0.0782, step time: 1.5318\n",
      "68/388, train_loss: 0.2347, step time: 1.5302\n",
      "69/388, train_loss: 0.1372, step time: 1.5346\n",
      "70/388, train_loss: 0.0493, step time: 1.5377\n",
      "71/388, train_loss: 0.1689, step time: 1.5342\n",
      "72/388, train_loss: 0.1607, step time: 1.5369\n",
      "73/388, train_loss: 0.1626, step time: 1.5296\n",
      "74/388, train_loss: 0.0860, step time: 1.5325\n",
      "75/388, train_loss: 0.1258, step time: 1.5305\n",
      "76/388, train_loss: 0.3111, step time: 1.5327\n",
      "77/388, train_loss: 0.1964, step time: 1.5333\n",
      "78/388, train_loss: 0.1004, step time: 1.5331\n",
      "79/388, train_loss: 0.1145, step time: 1.5314\n",
      "80/388, train_loss: 0.1723, step time: 1.5321\n",
      "81/388, train_loss: 0.2296, step time: 1.5325\n",
      "82/388, train_loss: 0.0948, step time: 1.5421\n",
      "83/388, train_loss: 0.0997, step time: 1.5351\n",
      "84/388, train_loss: 0.2221, step time: 1.5333\n",
      "85/388, train_loss: 0.0502, step time: 1.5320\n",
      "86/388, train_loss: 0.0435, step time: 1.5290\n",
      "87/388, train_loss: 0.1374, step time: 1.5301\n",
      "88/388, train_loss: 0.3706, step time: 1.5463\n",
      "89/388, train_loss: 0.1402, step time: 1.5363\n",
      "90/388, train_loss: 0.1779, step time: 1.5336\n",
      "91/388, train_loss: 0.0794, step time: 1.5301\n",
      "92/388, train_loss: 0.5653, step time: 1.5357\n",
      "93/388, train_loss: 0.0802, step time: 1.5302\n",
      "94/388, train_loss: 0.1856, step time: 1.5331\n",
      "95/388, train_loss: 0.2784, step time: 1.5331\n",
      "96/388, train_loss: 0.0916, step time: 1.5331\n",
      "97/388, train_loss: 0.1512, step time: 1.5310\n",
      "98/388, train_loss: 0.1006, step time: 1.5319\n",
      "99/388, train_loss: 0.0882, step time: 1.5350\n",
      "100/388, train_loss: 0.1948, step time: 1.5367\n",
      "101/388, train_loss: 0.2200, step time: 1.5329\n",
      "102/388, train_loss: 0.2780, step time: 1.5353\n",
      "103/388, train_loss: 0.2855, step time: 1.5330\n",
      "104/388, train_loss: 0.1115, step time: 1.5313\n",
      "105/388, train_loss: 0.1537, step time: 1.5300\n",
      "106/388, train_loss: 0.2294, step time: 1.5351\n",
      "107/388, train_loss: 0.1097, step time: 1.5363\n",
      "108/388, train_loss: 0.5167, step time: 1.5346\n",
      "109/388, train_loss: 0.1760, step time: 1.5302\n",
      "110/388, train_loss: 0.1779, step time: 1.5346\n",
      "111/388, train_loss: 0.0963, step time: 1.5316\n",
      "112/388, train_loss: 0.1357, step time: 1.5412\n",
      "113/388, train_loss: 0.0620, step time: 1.5336\n",
      "114/388, train_loss: 0.2760, step time: 1.5320\n",
      "115/388, train_loss: 0.2715, step time: 1.5331\n",
      "116/388, train_loss: 0.3075, step time: 1.5307\n",
      "117/388, train_loss: 0.0674, step time: 1.5344\n",
      "118/388, train_loss: 0.0990, step time: 1.5439\n",
      "119/388, train_loss: 0.1967, step time: 1.5315\n",
      "120/388, train_loss: 0.1800, step time: 1.5378\n",
      "121/388, train_loss: 0.1490, step time: 1.5322\n",
      "122/388, train_loss: 0.0818, step time: 1.5375\n",
      "123/388, train_loss: 0.0420, step time: 1.5361\n",
      "124/388, train_loss: 0.2549, step time: 1.5314\n",
      "125/388, train_loss: 0.0453, step time: 1.5340\n",
      "126/388, train_loss: 0.2126, step time: 1.5332\n",
      "127/388, train_loss: 0.1415, step time: 1.5325\n",
      "128/388, train_loss: 0.0690, step time: 1.5379\n",
      "129/388, train_loss: 0.1677, step time: 1.5334\n",
      "130/388, train_loss: 0.0282, step time: 1.5374\n",
      "131/388, train_loss: 0.1963, step time: 1.5352\n",
      "132/388, train_loss: 0.4189, step time: 1.5329\n",
      "133/388, train_loss: 0.1375, step time: 1.5365\n",
      "134/388, train_loss: 0.1047, step time: 1.5326\n",
      "135/388, train_loss: 0.4160, step time: 1.5324\n",
      "136/388, train_loss: 0.1458, step time: 1.5315\n",
      "137/388, train_loss: 0.1076, step time: 1.5355\n",
      "138/388, train_loss: 0.5102, step time: 1.5366\n",
      "139/388, train_loss: 0.1870, step time: 1.5312\n",
      "140/388, train_loss: 0.0884, step time: 1.5321\n",
      "141/388, train_loss: 0.1112, step time: 1.5388\n",
      "142/388, train_loss: 0.2636, step time: 1.5394\n",
      "143/388, train_loss: 0.3184, step time: 1.5359\n",
      "144/388, train_loss: 0.2356, step time: 1.5347\n",
      "145/388, train_loss: 0.2949, step time: 1.5299\n",
      "146/388, train_loss: 0.1279, step time: 1.5316\n",
      "147/388, train_loss: 0.2056, step time: 1.5329\n",
      "148/388, train_loss: 0.1854, step time: 1.5329\n",
      "149/388, train_loss: 0.1252, step time: 1.5362\n",
      "150/388, train_loss: 0.1942, step time: 1.5306\n",
      "151/388, train_loss: 0.1354, step time: 1.5299\n",
      "152/388, train_loss: 0.0810, step time: 1.5294\n",
      "153/388, train_loss: 0.2263, step time: 1.5324\n",
      "154/388, train_loss: 0.1143, step time: 1.5358\n",
      "155/388, train_loss: 0.1560, step time: 1.5331\n",
      "156/388, train_loss: 0.2319, step time: 1.5380\n",
      "157/388, train_loss: 0.1464, step time: 1.5327\n",
      "158/388, train_loss: 0.3789, step time: 1.5351\n",
      "159/388, train_loss: 0.2133, step time: 1.5558\n",
      "160/388, train_loss: 0.3867, step time: 1.5385\n",
      "161/388, train_loss: 0.1869, step time: 1.5360\n",
      "162/388, train_loss: 0.2176, step time: 1.5340\n",
      "163/388, train_loss: 0.1309, step time: 1.5420\n",
      "164/388, train_loss: 0.1707, step time: 1.5310\n",
      "165/388, train_loss: 0.0979, step time: 1.5321\n",
      "166/388, train_loss: 0.1063, step time: 1.5386\n",
      "167/388, train_loss: 0.1849, step time: 1.5623\n",
      "168/388, train_loss: 0.1719, step time: 1.5376\n",
      "169/388, train_loss: 0.2364, step time: 1.5364\n",
      "170/388, train_loss: 0.1691, step time: 1.5371\n",
      "171/388, train_loss: 0.2568, step time: 1.5311\n",
      "172/388, train_loss: 0.0632, step time: 1.5317\n",
      "173/388, train_loss: 0.0998, step time: 1.5567\n",
      "174/388, train_loss: 0.1371, step time: 1.5318\n",
      "175/388, train_loss: 0.0491, step time: 1.5310\n",
      "176/388, train_loss: 0.1570, step time: 1.5351\n",
      "177/388, train_loss: 0.0883, step time: 1.5365\n",
      "178/388, train_loss: 0.5144, step time: 1.5376\n",
      "179/388, train_loss: 0.1245, step time: 1.5384\n",
      "180/388, train_loss: 0.1176, step time: 1.5391\n",
      "181/388, train_loss: 0.0707, step time: 1.5306\n",
      "182/388, train_loss: 0.1018, step time: 1.5318\n",
      "183/388, train_loss: 0.2325, step time: 1.5337\n",
      "184/388, train_loss: 0.0754, step time: 1.5365\n",
      "185/388, train_loss: 0.1973, step time: 1.5333\n",
      "186/388, train_loss: 0.2541, step time: 1.5360\n",
      "187/388, train_loss: 0.3191, step time: 1.5343\n",
      "188/388, train_loss: 0.1052, step time: 1.5317\n",
      "189/388, train_loss: 0.3633, step time: 1.5344\n",
      "190/388, train_loss: 0.0878, step time: 1.5390\n",
      "191/388, train_loss: 0.1722, step time: 1.5337\n",
      "192/388, train_loss: 0.3278, step time: 1.5305\n",
      "193/388, train_loss: 0.1227, step time: 1.5332\n",
      "194/388, train_loss: 0.0896, step time: 1.5336\n",
      "195/388, train_loss: 0.0879, step time: 1.5687\n",
      "196/388, train_loss: 0.0877, step time: 1.5360\n",
      "197/388, train_loss: 0.0878, step time: 1.5346\n",
      "198/388, train_loss: 0.1555, step time: 1.5352\n",
      "199/388, train_loss: 0.4185, step time: 1.5443\n",
      "200/388, train_loss: 0.1351, step time: 1.5396\n",
      "201/388, train_loss: 0.2408, step time: 1.5343\n",
      "202/388, train_loss: 0.2017, step time: 1.5371\n",
      "203/388, train_loss: 0.1617, step time: 1.5343\n",
      "204/388, train_loss: 0.1949, step time: 1.5367\n",
      "205/388, train_loss: 0.1497, step time: 1.5356\n",
      "206/388, train_loss: 0.1090, step time: 1.5347\n",
      "207/388, train_loss: 0.0751, step time: 1.5348\n",
      "208/388, train_loss: 0.3352, step time: 1.5328\n",
      "209/388, train_loss: 0.2677, step time: 1.5311\n",
      "210/388, train_loss: 0.1396, step time: 1.5340\n",
      "211/388, train_loss: 0.1040, step time: 1.5334\n",
      "212/388, train_loss: 0.2287, step time: 1.5333\n",
      "213/388, train_loss: 0.0536, step time: 1.5350\n",
      "214/388, train_loss: 0.2454, step time: 1.5337\n",
      "215/388, train_loss: 0.1603, step time: 1.5326\n",
      "216/388, train_loss: 0.2203, step time: 1.5323\n",
      "217/388, train_loss: 0.1133, step time: 1.5323\n",
      "218/388, train_loss: 0.1512, step time: 1.5361\n",
      "219/388, train_loss: 0.1774, step time: 1.5368\n",
      "220/388, train_loss: 0.0696, step time: 1.5320\n",
      "221/388, train_loss: 0.2065, step time: 1.5337\n",
      "222/388, train_loss: 0.4330, step time: 1.5394\n",
      "223/388, train_loss: 0.4051, step time: 1.5383\n",
      "224/388, train_loss: 0.0852, step time: 1.5362\n",
      "225/388, train_loss: 0.0908, step time: 1.5376\n",
      "226/388, train_loss: 0.1046, step time: 1.5330\n",
      "227/388, train_loss: 0.1001, step time: 1.5333\n",
      "228/388, train_loss: 0.2760, step time: 1.5307\n",
      "229/388, train_loss: 0.0883, step time: 1.5364\n",
      "230/388, train_loss: 0.2875, step time: 1.5402\n",
      "231/388, train_loss: 0.2244, step time: 1.5342\n",
      "232/388, train_loss: 0.1064, step time: 1.5321\n",
      "233/388, train_loss: 0.0612, step time: 1.5316\n",
      "234/388, train_loss: 0.1641, step time: 1.5398\n",
      "235/388, train_loss: 0.1272, step time: 1.5378\n",
      "236/388, train_loss: 0.1653, step time: 1.5359\n",
      "237/388, train_loss: 0.1345, step time: 1.5445\n",
      "238/388, train_loss: 0.0884, step time: 1.5360\n",
      "239/388, train_loss: 0.2549, step time: 1.5363\n",
      "240/388, train_loss: 0.1477, step time: 1.5358\n",
      "241/388, train_loss: 0.1092, step time: 1.5330\n",
      "242/388, train_loss: 0.3086, step time: 1.5306\n",
      "243/388, train_loss: 0.2063, step time: 1.5330\n",
      "244/388, train_loss: 0.2239, step time: 1.5371\n",
      "245/388, train_loss: 0.0775, step time: 1.5356\n",
      "246/388, train_loss: 0.2750, step time: 1.5340\n",
      "247/388, train_loss: 0.0840, step time: 1.5296\n",
      "248/388, train_loss: 0.1183, step time: 1.5317\n",
      "249/388, train_loss: 0.1713, step time: 1.5381\n",
      "250/388, train_loss: 0.1934, step time: 1.5327\n",
      "251/388, train_loss: 0.0850, step time: 1.5321\n",
      "252/388, train_loss: 0.1482, step time: 1.5345\n",
      "253/388, train_loss: 0.2970, step time: 1.5459\n",
      "254/388, train_loss: 0.1454, step time: 1.5346\n",
      "255/388, train_loss: 0.0744, step time: 1.5327\n",
      "256/388, train_loss: 0.0758, step time: 1.5331\n",
      "257/388, train_loss: 0.0954, step time: 1.5384\n",
      "258/388, train_loss: 0.2263, step time: 1.5377\n",
      "259/388, train_loss: 0.1418, step time: 1.5354\n",
      "260/388, train_loss: 0.1992, step time: 1.5358\n",
      "261/388, train_loss: 0.0786, step time: 1.5347\n",
      "262/388, train_loss: 0.4073, step time: 1.5338\n",
      "263/388, train_loss: 0.0672, step time: 1.5355\n",
      "264/388, train_loss: 0.2341, step time: 1.5355\n",
      "265/388, train_loss: 0.2674, step time: 1.5354\n",
      "266/388, train_loss: 0.1016, step time: 1.5338\n",
      "267/388, train_loss: 0.2014, step time: 1.5345\n",
      "268/388, train_loss: 0.1889, step time: 1.5293\n",
      "269/388, train_loss: 0.1403, step time: 1.5374\n",
      "270/388, train_loss: 0.1546, step time: 1.5370\n",
      "271/388, train_loss: 0.0834, step time: 1.5291\n",
      "272/388, train_loss: 0.5297, step time: 1.5320\n",
      "273/388, train_loss: 0.3956, step time: 1.5347\n",
      "274/388, train_loss: 0.1741, step time: 1.5390\n",
      "275/388, train_loss: 0.0858, step time: 1.5331\n",
      "276/388, train_loss: 0.2335, step time: 1.5344\n",
      "277/388, train_loss: 0.0883, step time: 1.5343\n",
      "278/388, train_loss: 0.0817, step time: 1.5361\n",
      "279/388, train_loss: 0.0491, step time: 1.5364\n",
      "280/388, train_loss: 0.0880, step time: 1.5333\n",
      "281/388, train_loss: 0.1731, step time: 1.5369\n",
      "282/388, train_loss: 0.2010, step time: 1.5435\n",
      "283/388, train_loss: 0.1489, step time: 1.5322\n",
      "284/388, train_loss: 0.2110, step time: 1.5317\n",
      "285/388, train_loss: 0.1076, step time: 1.5382\n",
      "286/388, train_loss: 0.0421, step time: 1.5381\n",
      "287/388, train_loss: 0.2132, step time: 1.5324\n",
      "288/388, train_loss: 0.1060, step time: 1.5316\n",
      "289/388, train_loss: 0.2714, step time: 1.5329\n",
      "290/388, train_loss: 0.1097, step time: 1.5378\n",
      "291/388, train_loss: 0.0655, step time: 1.5326\n",
      "292/388, train_loss: 0.1197, step time: 1.5334\n",
      "293/388, train_loss: 0.1048, step time: 1.5333\n",
      "294/388, train_loss: 0.2508, step time: 1.5309\n",
      "295/388, train_loss: 0.2409, step time: 1.5333\n",
      "296/388, train_loss: 0.2987, step time: 1.5341\n",
      "297/388, train_loss: 0.2267, step time: 1.5367\n",
      "298/388, train_loss: 0.0583, step time: 1.5339\n",
      "299/388, train_loss: 0.1289, step time: 1.5325\n",
      "300/388, train_loss: 0.1243, step time: 1.5307\n",
      "301/388, train_loss: 0.1639, step time: 1.5327\n",
      "302/388, train_loss: 0.2592, step time: 1.5337\n",
      "303/388, train_loss: 0.1983, step time: 1.5357\n",
      "304/388, train_loss: 0.1640, step time: 1.5432\n",
      "305/388, train_loss: 0.3581, step time: 1.5331\n",
      "306/388, train_loss: 0.1569, step time: 1.5335\n",
      "307/388, train_loss: 0.1356, step time: 1.5340\n",
      "308/388, train_loss: 0.1254, step time: 1.5339\n",
      "309/388, train_loss: 0.1223, step time: 1.5359\n",
      "310/388, train_loss: 0.1180, step time: 1.5363\n",
      "311/388, train_loss: 0.1949, step time: 1.5335\n",
      "312/388, train_loss: 0.1558, step time: 1.5342\n",
      "313/388, train_loss: 0.2832, step time: 1.5383\n",
      "314/388, train_loss: 0.1546, step time: 1.5359\n",
      "315/388, train_loss: 0.1636, step time: 1.5306\n",
      "316/388, train_loss: 0.3220, step time: 1.5298\n",
      "317/388, train_loss: 0.0722, step time: 1.5345\n",
      "318/388, train_loss: 0.3227, step time: 1.5322\n",
      "319/388, train_loss: 0.1966, step time: 1.5365\n",
      "320/388, train_loss: 0.2124, step time: 1.5362\n",
      "321/388, train_loss: 0.0802, step time: 1.5322\n",
      "322/388, train_loss: 0.2494, step time: 1.5320\n",
      "323/388, train_loss: 0.0765, step time: 1.5285\n",
      "324/388, train_loss: 0.0969, step time: 1.5304\n",
      "325/388, train_loss: 0.0967, step time: 1.5339\n",
      "326/388, train_loss: 0.0496, step time: 1.5315\n",
      "327/388, train_loss: 0.4477, step time: 1.5318\n",
      "328/388, train_loss: 0.1324, step time: 1.5363\n",
      "329/388, train_loss: 0.2571, step time: 1.5383\n",
      "330/388, train_loss: 0.1938, step time: 1.5334\n",
      "331/388, train_loss: 0.2469, step time: 1.5332\n",
      "332/388, train_loss: 0.1379, step time: 1.5323\n",
      "333/388, train_loss: 0.1932, step time: 1.5366\n",
      "334/388, train_loss: 0.1560, step time: 1.5391\n",
      "335/388, train_loss: 0.3833, step time: 1.5322\n",
      "336/388, train_loss: 0.0601, step time: 1.5328\n",
      "337/388, train_loss: 0.1615, step time: 1.5328\n",
      "338/388, train_loss: 0.2348, step time: 1.5361\n",
      "339/388, train_loss: 0.0964, step time: 1.5369\n",
      "340/388, train_loss: 0.1043, step time: 1.5364\n",
      "341/388, train_loss: 0.2117, step time: 1.5327\n",
      "342/388, train_loss: 0.1810, step time: 1.5360\n",
      "343/388, train_loss: 0.5520, step time: 1.5369\n",
      "344/388, train_loss: 0.1839, step time: 1.5363\n",
      "345/388, train_loss: 0.1642, step time: 1.5321\n",
      "346/388, train_loss: 0.2677, step time: 1.5329\n",
      "347/388, train_loss: 0.1980, step time: 1.5360\n",
      "348/388, train_loss: 0.0891, step time: 1.5376\n",
      "349/388, train_loss: 0.1157, step time: 1.5347\n",
      "350/388, train_loss: 0.0337, step time: 1.5379\n",
      "351/388, train_loss: 0.2076, step time: 1.5348\n",
      "352/388, train_loss: 0.0464, step time: 1.5306\n",
      "353/388, train_loss: 0.2421, step time: 1.5357\n",
      "354/388, train_loss: 0.4006, step time: 1.5340\n",
      "355/388, train_loss: 0.1127, step time: 1.5297\n",
      "356/388, train_loss: 0.0861, step time: 1.5362\n",
      "357/388, train_loss: 0.1218, step time: 1.5343\n",
      "358/388, train_loss: 0.1005, step time: 1.5309\n",
      "359/388, train_loss: 0.1397, step time: 1.5295\n",
      "360/388, train_loss: 0.1849, step time: 1.5305\n",
      "361/388, train_loss: 0.1186, step time: 1.5287\n",
      "362/388, train_loss: 0.0685, step time: 1.5362\n",
      "363/388, train_loss: 0.2011, step time: 1.5365\n",
      "364/388, train_loss: 0.1510, step time: 1.5351\n",
      "365/388, train_loss: 0.0674, step time: 1.5323\n",
      "366/388, train_loss: 0.1027, step time: 1.5311\n",
      "367/388, train_loss: 0.1137, step time: 1.5309\n",
      "368/388, train_loss: 0.1775, step time: 1.5322\n",
      "369/388, train_loss: 0.3003, step time: 1.5326\n",
      "370/388, train_loss: 0.1253, step time: 1.5324\n",
      "371/388, train_loss: 0.1074, step time: 1.5351\n",
      "372/388, train_loss: 0.1611, step time: 1.5343\n",
      "373/388, train_loss: 0.3676, step time: 1.5359\n",
      "374/388, train_loss: 0.1269, step time: 1.5326\n",
      "375/388, train_loss: 0.1319, step time: 1.5295\n",
      "376/388, train_loss: 0.0820, step time: 1.5306\n",
      "377/388, train_loss: 0.1268, step time: 1.5344\n",
      "378/388, train_loss: 0.2196, step time: 1.5354\n",
      "379/388, train_loss: 0.1203, step time: 1.5353\n",
      "380/388, train_loss: 0.0763, step time: 1.5377\n",
      "381/388, train_loss: 0.1199, step time: 1.5313\n",
      "382/388, train_loss: 0.3806, step time: 1.5316\n",
      "383/388, train_loss: 0.2688, step time: 1.5306\n",
      "384/388, train_loss: 0.0824, step time: 1.5311\n",
      "385/388, train_loss: 0.1497, step time: 1.5320\n",
      "386/388, train_loss: 0.2314, step time: 1.5333\n",
      "387/388, train_loss: 0.3059, step time: 1.5340\n",
      "388/388, train_loss: 0.2520, step time: 1.5370\n",
      "epoch 65 average loss: 0.1736\n",
      "saved new best metric model\n",
      "current epoch: 65 current mean dice: 0.7761 tc: 0.8226 wt: 0.9060 et: 0.5996\n",
      "best mean dice: 0.7761 at epoch: 65\n",
      "time consuming of epoch 65 is: 703.9834\n",
      "----------\n",
      "epoch 66/100\n",
      "1/388, train_loss: 0.2233, step time: 1.5446\n",
      "2/388, train_loss: 0.0906, step time: 1.5337\n",
      "3/388, train_loss: 0.1582, step time: 1.5345\n",
      "4/388, train_loss: 0.2045, step time: 1.5324\n",
      "5/388, train_loss: 0.0850, step time: 1.5356\n",
      "6/388, train_loss: 0.1733, step time: 1.5341\n",
      "7/388, train_loss: 0.2363, step time: 1.5314\n",
      "8/388, train_loss: 0.0926, step time: 1.5393\n",
      "9/388, train_loss: 0.1592, step time: 1.5330\n",
      "10/388, train_loss: 0.1469, step time: 1.5355\n",
      "11/388, train_loss: 0.2269, step time: 1.5299\n",
      "12/388, train_loss: 0.0986, step time: 1.5316\n",
      "13/388, train_loss: 0.1768, step time: 1.5366\n",
      "14/388, train_loss: 0.2174, step time: 1.5337\n",
      "15/388, train_loss: 0.2139, step time: 1.5349\n",
      "16/388, train_loss: 0.3155, step time: 1.5396\n",
      "17/388, train_loss: 0.1589, step time: 1.5333\n",
      "18/388, train_loss: 0.4249, step time: 1.5324\n",
      "19/388, train_loss: 0.0871, step time: 1.5398\n",
      "20/388, train_loss: 0.2608, step time: 1.5394\n",
      "21/388, train_loss: 0.3770, step time: 1.5334\n",
      "22/388, train_loss: 0.1964, step time: 1.5326\n",
      "23/388, train_loss: 0.2031, step time: 1.5369\n",
      "24/388, train_loss: 0.1197, step time: 1.5324\n",
      "25/388, train_loss: 0.2503, step time: 1.5354\n",
      "26/388, train_loss: 0.2369, step time: 1.5343\n",
      "27/388, train_loss: 0.2723, step time: 1.5306\n",
      "28/388, train_loss: 0.2342, step time: 1.5333\n",
      "29/388, train_loss: 0.3302, step time: 1.5327\n",
      "30/388, train_loss: 0.0777, step time: 1.5376\n",
      "31/388, train_loss: 0.3203, step time: 1.5365\n",
      "32/388, train_loss: 0.1580, step time: 1.5356\n",
      "33/388, train_loss: 0.0841, step time: 1.5347\n",
      "34/388, train_loss: 0.0643, step time: 1.5322\n",
      "35/388, train_loss: 0.1692, step time: 1.5335\n",
      "36/388, train_loss: 0.1218, step time: 1.5398\n",
      "37/388, train_loss: 0.2566, step time: 1.5364\n",
      "38/388, train_loss: 0.1323, step time: 1.5345\n",
      "39/388, train_loss: 0.1690, step time: 1.5315\n",
      "40/388, train_loss: 0.1038, step time: 1.5346\n",
      "41/388, train_loss: 0.0791, step time: 1.5414\n",
      "42/388, train_loss: 0.1561, step time: 1.5396\n",
      "43/388, train_loss: 0.1399, step time: 1.5343\n",
      "44/388, train_loss: 0.0579, step time: 1.5315\n",
      "45/388, train_loss: 0.1809, step time: 1.5371\n",
      "46/388, train_loss: 0.5116, step time: 1.5379\n",
      "47/388, train_loss: 0.0742, step time: 1.5349\n",
      "48/388, train_loss: 0.1794, step time: 1.5326\n",
      "49/388, train_loss: 0.2550, step time: 1.5315\n",
      "50/388, train_loss: 0.2962, step time: 1.5328\n",
      "51/388, train_loss: 0.1034, step time: 1.5347\n",
      "52/388, train_loss: 0.2848, step time: 1.5378\n",
      "53/388, train_loss: 0.1994, step time: 1.5387\n",
      "54/388, train_loss: 0.2357, step time: 1.5342\n",
      "55/388, train_loss: 0.0959, step time: 1.5331\n",
      "56/388, train_loss: 0.2337, step time: 1.5365\n",
      "57/388, train_loss: 0.0504, step time: 1.5349\n",
      "58/388, train_loss: 0.1450, step time: 1.5359\n",
      "59/388, train_loss: 0.0327, step time: 1.5460\n",
      "60/388, train_loss: 0.3181, step time: 1.5368\n",
      "61/388, train_loss: 0.1159, step time: 1.5352\n",
      "62/388, train_loss: 0.1035, step time: 1.5342\n",
      "63/388, train_loss: 0.0842, step time: 1.5325\n",
      "64/388, train_loss: 0.1896, step time: 1.5527\n",
      "65/388, train_loss: 0.2401, step time: 1.5332\n",
      "66/388, train_loss: 0.1741, step time: 1.5335\n",
      "67/388, train_loss: 0.1712, step time: 1.5359\n",
      "68/388, train_loss: 0.2060, step time: 1.5395\n",
      "69/388, train_loss: 0.0822, step time: 1.5354\n",
      "70/388, train_loss: 0.1151, step time: 1.5335\n",
      "71/388, train_loss: 0.1420, step time: 1.5324\n",
      "72/388, train_loss: 0.0544, step time: 1.5328\n",
      "73/388, train_loss: 0.0367, step time: 1.5475\n",
      "74/388, train_loss: 0.0771, step time: 1.5339\n",
      "75/388, train_loss: 0.1017, step time: 1.5317\n",
      "76/388, train_loss: 0.0851, step time: 1.5353\n",
      "77/388, train_loss: 0.0936, step time: 1.5362\n",
      "78/388, train_loss: 0.1414, step time: 1.5354\n",
      "79/388, train_loss: 0.1746, step time: 1.5349\n",
      "80/388, train_loss: 0.0732, step time: 1.5347\n",
      "81/388, train_loss: 0.1677, step time: 1.5331\n",
      "82/388, train_loss: 0.1372, step time: 1.5448\n",
      "83/388, train_loss: 0.0506, step time: 1.5382\n",
      "84/388, train_loss: 0.0976, step time: 1.5369\n",
      "85/388, train_loss: 0.1763, step time: 1.5375\n",
      "86/388, train_loss: 0.1559, step time: 1.5330\n",
      "87/388, train_loss: 0.0933, step time: 1.5312\n",
      "88/388, train_loss: 0.0923, step time: 1.5331\n",
      "89/388, train_loss: 0.2666, step time: 1.5378\n",
      "90/388, train_loss: 0.2195, step time: 1.5360\n",
      "91/388, train_loss: 0.1656, step time: 1.5343\n",
      "92/388, train_loss: 0.0790, step time: 1.5347\n",
      "93/388, train_loss: 0.1077, step time: 1.5334\n",
      "94/388, train_loss: 0.2283, step time: 1.5352\n",
      "95/388, train_loss: 0.1464, step time: 1.5537\n",
      "96/388, train_loss: 0.0401, step time: 1.5320\n",
      "97/388, train_loss: 0.1997, step time: 1.5337\n",
      "98/388, train_loss: 0.1983, step time: 1.5343\n",
      "99/388, train_loss: 0.0453, step time: 1.5376\n",
      "100/388, train_loss: 0.1956, step time: 1.5364\n",
      "101/388, train_loss: 0.3395, step time: 1.5384\n",
      "102/388, train_loss: 0.3273, step time: 1.5325\n",
      "103/388, train_loss: 0.2163, step time: 1.5298\n",
      "104/388, train_loss: 0.1326, step time: 1.5321\n",
      "105/388, train_loss: 0.1259, step time: 1.5329\n",
      "106/388, train_loss: 0.0966, step time: 1.5380\n",
      "107/388, train_loss: 0.1164, step time: 1.5385\n",
      "108/388, train_loss: 0.1709, step time: 1.5394\n",
      "109/388, train_loss: 0.1539, step time: 1.5302\n",
      "110/388, train_loss: 0.4326, step time: 1.5324\n",
      "111/388, train_loss: 0.0937, step time: 1.5352\n",
      "112/388, train_loss: 0.0829, step time: 1.5374\n",
      "113/388, train_loss: 0.4576, step time: 1.5467\n",
      "114/388, train_loss: 0.1366, step time: 1.5336\n",
      "115/388, train_loss: 0.0990, step time: 1.5350\n",
      "116/388, train_loss: 0.3141, step time: 1.5322\n",
      "117/388, train_loss: 0.1618, step time: 1.5385\n",
      "118/388, train_loss: 0.0331, step time: 1.5364\n",
      "119/388, train_loss: 0.2058, step time: 1.5351\n",
      "120/388, train_loss: 0.1390, step time: 1.5334\n",
      "121/388, train_loss: 0.0977, step time: 1.5325\n",
      "122/388, train_loss: 0.2382, step time: 1.5325\n",
      "123/388, train_loss: 0.0949, step time: 1.5355\n",
      "124/388, train_loss: 0.1254, step time: 1.5334\n",
      "125/388, train_loss: 0.1974, step time: 1.5370\n",
      "126/388, train_loss: 0.2082, step time: 1.5366\n",
      "127/388, train_loss: 0.2039, step time: 1.5330\n",
      "128/388, train_loss: 0.1525, step time: 1.5309\n",
      "129/388, train_loss: 0.1287, step time: 1.5340\n",
      "130/388, train_loss: 0.1941, step time: 1.5420\n",
      "131/388, train_loss: 0.1915, step time: 1.5369\n",
      "132/388, train_loss: 0.0872, step time: 1.5327\n",
      "133/388, train_loss: 0.0622, step time: 1.5326\n",
      "134/388, train_loss: 0.1156, step time: 1.5391\n",
      "135/388, train_loss: 0.1956, step time: 1.5364\n",
      "136/388, train_loss: 0.2609, step time: 1.5365\n",
      "137/388, train_loss: 0.0900, step time: 1.5365\n",
      "138/388, train_loss: 0.0926, step time: 1.5363\n",
      "139/388, train_loss: 0.2119, step time: 1.5336\n",
      "140/388, train_loss: 0.1202, step time: 1.5618\n",
      "141/388, train_loss: 0.1878, step time: 1.5363\n",
      "142/388, train_loss: 0.2150, step time: 1.5420\n",
      "143/388, train_loss: 0.4646, step time: 1.5316\n",
      "144/388, train_loss: 0.1746, step time: 1.5325\n",
      "145/388, train_loss: 0.4492, step time: 1.5321\n",
      "146/388, train_loss: 0.1288, step time: 1.5366\n",
      "147/388, train_loss: 0.2234, step time: 1.5328\n",
      "148/388, train_loss: 0.1183, step time: 1.5352\n",
      "149/388, train_loss: 0.1363, step time: 1.5324\n",
      "150/388, train_loss: 0.2599, step time: 1.5345\n",
      "151/388, train_loss: 0.4709, step time: 1.5361\n",
      "152/388, train_loss: 0.0942, step time: 1.5371\n",
      "153/388, train_loss: 0.1640, step time: 1.5368\n",
      "154/388, train_loss: 0.1303, step time: 1.5360\n",
      "155/388, train_loss: 0.0596, step time: 1.5300\n",
      "156/388, train_loss: 0.2385, step time: 1.5372\n",
      "157/388, train_loss: 0.1784, step time: 1.5340\n",
      "158/388, train_loss: 0.3763, step time: 1.5375\n",
      "159/388, train_loss: 0.1840, step time: 1.5363\n",
      "160/388, train_loss: 0.1094, step time: 1.5333\n",
      "161/388, train_loss: 0.1136, step time: 1.5330\n",
      "162/388, train_loss: 0.0588, step time: 1.5322\n",
      "163/388, train_loss: 0.1621, step time: 1.5333\n",
      "164/388, train_loss: 0.0964, step time: 1.5339\n",
      "165/388, train_loss: 0.2010, step time: 1.5364\n",
      "166/388, train_loss: 0.0783, step time: 1.5365\n",
      "167/388, train_loss: 0.1159, step time: 1.5316\n",
      "168/388, train_loss: 0.1205, step time: 1.5352\n",
      "169/388, train_loss: 0.1286, step time: 1.5324\n",
      "170/388, train_loss: 0.2308, step time: 1.5347\n",
      "171/388, train_loss: 0.0624, step time: 1.5384\n",
      "172/388, train_loss: 0.1676, step time: 1.5326\n",
      "173/388, train_loss: 0.3190, step time: 1.5357\n",
      "174/388, train_loss: 0.2118, step time: 1.5365\n",
      "175/388, train_loss: 0.1730, step time: 1.5377\n",
      "176/388, train_loss: 0.1043, step time: 1.5359\n",
      "177/388, train_loss: 0.3226, step time: 1.5315\n",
      "178/388, train_loss: 0.0983, step time: 1.5302\n",
      "179/388, train_loss: 0.1835, step time: 1.5318\n",
      "180/388, train_loss: 0.0430, step time: 1.5295\n",
      "181/388, train_loss: 0.1735, step time: 1.5353\n",
      "182/388, train_loss: 0.1332, step time: 1.5377\n",
      "183/388, train_loss: 0.1423, step time: 1.5355\n",
      "184/388, train_loss: 0.2352, step time: 1.5353\n",
      "185/388, train_loss: 0.1166, step time: 1.5336\n",
      "186/388, train_loss: 0.0825, step time: 1.5333\n",
      "187/388, train_loss: 0.0633, step time: 1.5355\n",
      "188/388, train_loss: 0.1271, step time: 1.5342\n",
      "189/388, train_loss: 0.1786, step time: 1.5346\n",
      "190/388, train_loss: 0.2312, step time: 1.5325\n",
      "191/388, train_loss: 0.1066, step time: 1.5326\n",
      "192/388, train_loss: 0.0976, step time: 1.5308\n",
      "193/388, train_loss: 0.2529, step time: 1.5366\n",
      "194/388, train_loss: 0.1100, step time: 1.5371\n",
      "195/388, train_loss: 0.1829, step time: 1.5345\n",
      "196/388, train_loss: 0.1999, step time: 1.5315\n",
      "197/388, train_loss: 0.2125, step time: 1.5373\n",
      "198/388, train_loss: 0.1602, step time: 1.5361\n",
      "199/388, train_loss: 0.0981, step time: 1.5383\n",
      "200/388, train_loss: 0.0745, step time: 1.5353\n",
      "201/388, train_loss: 0.2176, step time: 1.5309\n",
      "202/388, train_loss: 0.0482, step time: 1.5336\n",
      "203/388, train_loss: 0.3360, step time: 1.5337\n",
      "204/388, train_loss: 0.1543, step time: 1.5340\n",
      "205/388, train_loss: 0.2285, step time: 1.5345\n",
      "206/388, train_loss: 0.0815, step time: 1.5347\n",
      "207/388, train_loss: 0.2344, step time: 1.5355\n",
      "208/388, train_loss: 0.1319, step time: 1.5353\n",
      "209/388, train_loss: 0.0873, step time: 1.5328\n",
      "210/388, train_loss: 0.1213, step time: 1.5314\n",
      "211/388, train_loss: 0.2925, step time: 1.5340\n",
      "212/388, train_loss: 0.0721, step time: 1.5357\n",
      "213/388, train_loss: 0.1189, step time: 1.5355\n",
      "214/388, train_loss: 0.1137, step time: 1.5377\n",
      "215/388, train_loss: 0.2761, step time: 1.5329\n",
      "216/388, train_loss: 0.0514, step time: 1.5319\n",
      "217/388, train_loss: 0.3434, step time: 1.5410\n",
      "218/388, train_loss: 0.3351, step time: 1.5344\n",
      "219/388, train_loss: 0.0658, step time: 1.5342\n",
      "220/388, train_loss: 0.1341, step time: 1.5372\n",
      "221/388, train_loss: 0.1650, step time: 1.5341\n",
      "222/388, train_loss: 0.1723, step time: 1.5348\n",
      "223/388, train_loss: 0.0917, step time: 1.5330\n",
      "224/388, train_loss: 0.1055, step time: 1.5373\n",
      "225/388, train_loss: 0.2429, step time: 1.5347\n",
      "226/388, train_loss: 0.1183, step time: 1.5326\n",
      "227/388, train_loss: 0.1077, step time: 1.5344\n",
      "228/388, train_loss: 0.0593, step time: 1.5365\n",
      "229/388, train_loss: 0.5455, step time: 1.5332\n",
      "230/388, train_loss: 0.4505, step time: 1.5309\n",
      "231/388, train_loss: 0.1120, step time: 1.5337\n",
      "232/388, train_loss: 0.1045, step time: 1.5329\n",
      "233/388, train_loss: 0.1809, step time: 1.5357\n",
      "234/388, train_loss: 0.1038, step time: 1.5337\n",
      "235/388, train_loss: 0.0952, step time: 1.5310\n",
      "236/388, train_loss: 0.1284, step time: 1.5293\n",
      "237/388, train_loss: 0.1542, step time: 1.5353\n",
      "238/388, train_loss: 0.1804, step time: 1.5525\n",
      "239/388, train_loss: 0.0991, step time: 1.5342\n",
      "240/388, train_loss: 0.2025, step time: 1.5326\n",
      "241/388, train_loss: 0.2041, step time: 1.5295\n",
      "242/388, train_loss: 0.0824, step time: 1.5343\n",
      "243/388, train_loss: 0.0796, step time: 1.5345\n",
      "244/388, train_loss: 0.0619, step time: 1.5367\n",
      "245/388, train_loss: 0.0844, step time: 1.5352\n",
      "246/388, train_loss: 0.0999, step time: 1.5326\n",
      "247/388, train_loss: 0.0678, step time: 1.5313\n",
      "248/388, train_loss: 0.0491, step time: 1.5384\n",
      "249/388, train_loss: 0.1038, step time: 1.5372\n",
      "250/388, train_loss: 0.1484, step time: 1.5373\n",
      "251/388, train_loss: 0.1569, step time: 1.5327\n",
      "252/388, train_loss: 0.1205, step time: 1.5310\n",
      "253/388, train_loss: 0.0714, step time: 1.5332\n",
      "254/388, train_loss: 0.2210, step time: 1.5324\n",
      "255/388, train_loss: 0.4222, step time: 1.5364\n",
      "256/388, train_loss: 0.2283, step time: 1.5359\n",
      "257/388, train_loss: 0.3461, step time: 1.5358\n",
      "258/388, train_loss: 0.0960, step time: 1.5305\n",
      "259/388, train_loss: 0.3874, step time: 1.5355\n",
      "260/388, train_loss: 0.1958, step time: 1.5335\n",
      "261/388, train_loss: 0.0668, step time: 1.5474\n",
      "262/388, train_loss: 0.3972, step time: 1.5351\n",
      "263/388, train_loss: 0.2281, step time: 1.5357\n",
      "264/388, train_loss: 0.2099, step time: 1.5346\n",
      "265/388, train_loss: 0.1147, step time: 1.5330\n",
      "266/388, train_loss: 0.1469, step time: 1.5337\n",
      "267/388, train_loss: 0.1590, step time: 1.5333\n",
      "268/388, train_loss: 0.1297, step time: 1.5394\n",
      "269/388, train_loss: 0.0970, step time: 1.5346\n",
      "270/388, train_loss: 0.1543, step time: 1.5333\n",
      "271/388, train_loss: 0.0974, step time: 1.5344\n",
      "272/388, train_loss: 0.2827, step time: 1.5303\n",
      "273/388, train_loss: 0.3889, step time: 1.5323\n",
      "274/388, train_loss: 0.1624, step time: 1.5327\n",
      "275/388, train_loss: 0.3225, step time: 1.5379\n",
      "276/388, train_loss: 0.2430, step time: 1.5346\n",
      "277/388, train_loss: 0.0837, step time: 1.5341\n",
      "278/388, train_loss: 0.1359, step time: 1.5328\n",
      "279/388, train_loss: 0.0623, step time: 1.5306\n",
      "280/388, train_loss: 0.3668, step time: 1.5398\n",
      "281/388, train_loss: 0.0945, step time: 1.5366\n",
      "282/388, train_loss: 0.1080, step time: 1.5344\n",
      "283/388, train_loss: 0.1335, step time: 1.5321\n",
      "284/388, train_loss: 0.3567, step time: 1.5358\n",
      "285/388, train_loss: 0.4563, step time: 1.5345\n",
      "286/388, train_loss: 0.1310, step time: 1.5406\n",
      "287/388, train_loss: 0.0735, step time: 1.5356\n",
      "288/388, train_loss: 0.0488, step time: 1.5322\n",
      "289/388, train_loss: 0.1085, step time: 1.5319\n",
      "290/388, train_loss: 0.1023, step time: 1.5343\n",
      "291/388, train_loss: 0.2094, step time: 1.5339\n",
      "292/388, train_loss: 0.1043, step time: 1.5376\n",
      "293/388, train_loss: 0.2302, step time: 1.5336\n",
      "294/388, train_loss: 0.0838, step time: 1.5322\n",
      "295/388, train_loss: 0.2535, step time: 1.5326\n",
      "296/388, train_loss: 0.2159, step time: 1.5333\n",
      "297/388, train_loss: 0.1234, step time: 1.5422\n",
      "298/388, train_loss: 0.1814, step time: 1.5390\n",
      "299/388, train_loss: 0.0975, step time: 1.5434\n",
      "300/388, train_loss: 0.1529, step time: 1.5327\n",
      "301/388, train_loss: 0.2500, step time: 1.5354\n",
      "302/388, train_loss: 0.3698, step time: 1.5368\n",
      "303/388, train_loss: 0.0739, step time: 1.5346\n",
      "304/388, train_loss: 0.1851, step time: 1.5333\n",
      "305/388, train_loss: 0.1694, step time: 1.5357\n",
      "306/388, train_loss: 0.2053, step time: 1.5327\n",
      "307/388, train_loss: 0.0937, step time: 1.5328\n",
      "308/388, train_loss: 0.1373, step time: 1.5426\n",
      "309/388, train_loss: 0.2106, step time: 1.5366\n",
      "310/388, train_loss: 0.0915, step time: 1.5325\n",
      "311/388, train_loss: 0.0932, step time: 1.5303\n",
      "312/388, train_loss: 0.0712, step time: 1.5334\n",
      "313/388, train_loss: 0.0981, step time: 1.5337\n",
      "314/388, train_loss: 0.2945, step time: 1.5389\n",
      "315/388, train_loss: 0.1582, step time: 1.5348\n",
      "316/388, train_loss: 0.2022, step time: 1.5347\n",
      "317/388, train_loss: 0.1665, step time: 1.5306\n",
      "318/388, train_loss: 0.3072, step time: 1.5333\n",
      "319/388, train_loss: 0.1202, step time: 1.5358\n",
      "320/388, train_loss: 0.2420, step time: 1.5377\n",
      "321/388, train_loss: 0.1088, step time: 1.5470\n",
      "322/388, train_loss: 0.2090, step time: 1.5426\n",
      "323/388, train_loss: 0.2815, step time: 1.5343\n",
      "324/388, train_loss: 0.1398, step time: 1.5345\n",
      "325/388, train_loss: 0.1038, step time: 1.5357\n",
      "326/388, train_loss: 0.2791, step time: 1.5372\n",
      "327/388, train_loss: 0.1804, step time: 1.5328\n",
      "328/388, train_loss: 0.1843, step time: 1.5346\n",
      "329/388, train_loss: 0.2415, step time: 1.5308\n",
      "330/388, train_loss: 0.2911, step time: 1.5326\n",
      "331/388, train_loss: 0.0864, step time: 1.5382\n",
      "332/388, train_loss: 0.3889, step time: 1.5363\n",
      "333/388, train_loss: 0.1916, step time: 1.5365\n",
      "334/388, train_loss: 0.2035, step time: 1.5342\n",
      "335/388, train_loss: 0.1493, step time: 1.5343\n",
      "336/388, train_loss: 0.2409, step time: 1.5339\n",
      "337/388, train_loss: 0.2152, step time: 1.5350\n",
      "338/388, train_loss: 0.2071, step time: 1.5385\n",
      "339/388, train_loss: 0.1946, step time: 1.5344\n",
      "340/388, train_loss: 0.1750, step time: 1.5375\n",
      "341/388, train_loss: 0.1885, step time: 1.5325\n",
      "342/388, train_loss: 0.2256, step time: 1.5302\n",
      "343/388, train_loss: 0.3559, step time: 1.5336\n",
      "344/388, train_loss: 0.1869, step time: 1.5370\n",
      "345/388, train_loss: 0.0933, step time: 1.5350\n",
      "346/388, train_loss: 0.2377, step time: 1.5496\n",
      "347/388, train_loss: 0.1051, step time: 1.5318\n",
      "348/388, train_loss: 0.1256, step time: 1.5356\n",
      "349/388, train_loss: 0.1081, step time: 1.5428\n",
      "350/388, train_loss: 0.1081, step time: 1.5339\n",
      "351/388, train_loss: 0.2730, step time: 1.5320\n",
      "352/388, train_loss: 0.0628, step time: 1.5326\n",
      "353/388, train_loss: 0.1969, step time: 1.5362\n",
      "354/388, train_loss: 0.0547, step time: 1.5369\n",
      "355/388, train_loss: 0.4778, step time: 1.5357\n",
      "356/388, train_loss: 0.2210, step time: 1.5490\n",
      "357/388, train_loss: 0.1416, step time: 1.5306\n",
      "358/388, train_loss: 0.0877, step time: 1.5323\n",
      "359/388, train_loss: 0.1685, step time: 1.5331\n",
      "360/388, train_loss: 0.0460, step time: 1.5359\n",
      "361/388, train_loss: 0.1766, step time: 1.5358\n",
      "362/388, train_loss: 0.1694, step time: 1.5354\n",
      "363/388, train_loss: 0.1581, step time: 1.5330\n",
      "364/388, train_loss: 0.0264, step time: 1.5340\n",
      "365/388, train_loss: 0.1559, step time: 1.5326\n",
      "366/388, train_loss: 0.2695, step time: 1.5373\n",
      "367/388, train_loss: 0.1341, step time: 1.5380\n",
      "368/388, train_loss: 0.1173, step time: 1.5358\n",
      "369/388, train_loss: 0.2429, step time: 1.5349\n",
      "370/388, train_loss: 0.1950, step time: 1.5335\n",
      "371/388, train_loss: 0.1853, step time: 1.5363\n",
      "372/388, train_loss: 0.1370, step time: 1.5372\n",
      "373/388, train_loss: 0.0948, step time: 1.5329\n",
      "374/388, train_loss: 0.1267, step time: 1.5339\n",
      "375/388, train_loss: 0.2273, step time: 1.5310\n",
      "376/388, train_loss: 0.2611, step time: 1.5360\n",
      "377/388, train_loss: 0.2119, step time: 1.5382\n",
      "378/388, train_loss: 0.5234, step time: 1.5345\n",
      "379/388, train_loss: 0.1290, step time: 1.5328\n",
      "380/388, train_loss: 0.1994, step time: 1.5423\n",
      "381/388, train_loss: 0.3140, step time: 1.5313\n",
      "382/388, train_loss: 0.0680, step time: 1.5335\n",
      "383/388, train_loss: 0.1102, step time: 1.5395\n",
      "384/388, train_loss: 0.0907, step time: 1.5355\n",
      "385/388, train_loss: 0.1079, step time: 1.5352\n",
      "386/388, train_loss: 0.2302, step time: 1.5411\n",
      "387/388, train_loss: 0.0676, step time: 1.5327\n",
      "388/388, train_loss: 0.0714, step time: 1.5321\n",
      "epoch 66 average loss: 0.1724\n",
      "current epoch: 66 current mean dice: 0.7741 tc: 0.8255 wt: 0.9066 et: 0.5901\n",
      "best mean dice: 0.7761 at epoch: 65\n",
      "time consuming of epoch 66 is: 704.5335\n",
      "----------\n",
      "epoch 67/100\n",
      "1/388, train_loss: 0.1494, step time: 1.5421\n",
      "2/388, train_loss: 0.1070, step time: 1.5313\n",
      "3/388, train_loss: 0.1007, step time: 1.5343\n",
      "4/388, train_loss: 0.6044, step time: 1.5368\n",
      "5/388, train_loss: 0.1129, step time: 1.5334\n",
      "6/388, train_loss: 0.3324, step time: 1.5385\n",
      "7/388, train_loss: 0.0646, step time: 1.5369\n",
      "8/388, train_loss: 0.2158, step time: 1.5330\n",
      "9/388, train_loss: 0.1391, step time: 1.5375\n",
      "10/388, train_loss: 0.1339, step time: 1.5373\n",
      "11/388, train_loss: 0.1325, step time: 1.5335\n",
      "12/388, train_loss: 0.0706, step time: 1.5318\n",
      "13/388, train_loss: 0.0649, step time: 1.5328\n",
      "14/388, train_loss: 0.2493, step time: 1.5375\n",
      "15/388, train_loss: 0.1198, step time: 1.5381\n",
      "16/388, train_loss: 0.1475, step time: 1.5369\n",
      "17/388, train_loss: 0.2226, step time: 1.5348\n",
      "18/388, train_loss: 0.1262, step time: 1.5328\n",
      "19/388, train_loss: 0.4155, step time: 1.5335\n",
      "20/388, train_loss: 0.2127, step time: 1.5428\n",
      "21/388, train_loss: 0.0491, step time: 1.5341\n",
      "22/388, train_loss: 0.1277, step time: 1.5303\n",
      "23/388, train_loss: 0.1847, step time: 1.5306\n",
      "24/388, train_loss: 0.0855, step time: 1.5334\n",
      "25/388, train_loss: 0.3516, step time: 1.5329\n",
      "26/388, train_loss: 0.1341, step time: 1.5338\n",
      "27/388, train_loss: 0.1025, step time: 1.5350\n",
      "28/388, train_loss: 0.0806, step time: 1.5319\n",
      "29/388, train_loss: 0.2696, step time: 1.5382\n",
      "30/388, train_loss: 0.1912, step time: 1.5394\n",
      "31/388, train_loss: 0.1567, step time: 1.5336\n",
      "32/388, train_loss: 0.2078, step time: 1.5323\n",
      "33/388, train_loss: 0.1325, step time: 1.5307\n",
      "34/388, train_loss: 0.0524, step time: 1.5382\n",
      "35/388, train_loss: 0.0977, step time: 1.5393\n",
      "36/388, train_loss: 0.1149, step time: 1.5362\n",
      "37/388, train_loss: 0.1816, step time: 1.5381\n",
      "38/388, train_loss: 0.1170, step time: 1.5350\n",
      "39/388, train_loss: 0.1478, step time: 1.5356\n",
      "40/388, train_loss: 0.1265, step time: 1.5392\n",
      "41/388, train_loss: 0.3593, step time: 1.5355\n",
      "42/388, train_loss: 0.1468, step time: 1.5348\n",
      "43/388, train_loss: 0.2903, step time: 1.5311\n",
      "44/388, train_loss: 0.0904, step time: 1.5354\n",
      "45/388, train_loss: 0.1222, step time: 1.5355\n",
      "46/388, train_loss: 0.2988, step time: 1.5412\n",
      "47/388, train_loss: 0.1971, step time: 1.5334\n",
      "48/388, train_loss: 0.1445, step time: 1.5357\n",
      "49/388, train_loss: 0.1364, step time: 1.5327\n",
      "50/388, train_loss: 0.5683, step time: 1.5341\n",
      "51/388, train_loss: 0.2107, step time: 1.5379\n",
      "52/388, train_loss: 0.0965, step time: 1.5381\n",
      "53/388, train_loss: 0.1345, step time: 1.5328\n",
      "54/388, train_loss: 0.4336, step time: 1.5321\n",
      "55/388, train_loss: 0.1379, step time: 1.5340\n",
      "56/388, train_loss: 0.0502, step time: 1.5339\n",
      "57/388, train_loss: 0.0633, step time: 1.5337\n",
      "58/388, train_loss: 0.1927, step time: 1.5349\n",
      "59/388, train_loss: 0.1207, step time: 1.5332\n",
      "60/388, train_loss: 0.1320, step time: 1.5367\n",
      "61/388, train_loss: 0.0886, step time: 1.5308\n",
      "62/388, train_loss: 0.1005, step time: 1.5340\n",
      "63/388, train_loss: 0.1975, step time: 1.5315\n",
      "64/388, train_loss: 0.1120, step time: 1.5427\n",
      "65/388, train_loss: 0.0865, step time: 1.5334\n",
      "66/388, train_loss: 0.0632, step time: 1.5351\n",
      "67/388, train_loss: 0.1013, step time: 1.5301\n",
      "68/388, train_loss: 0.2077, step time: 1.5346\n",
      "69/388, train_loss: 0.2606, step time: 1.5347\n",
      "70/388, train_loss: 0.2671, step time: 1.5360\n",
      "71/388, train_loss: 0.0726, step time: 1.5461\n",
      "72/388, train_loss: 0.1978, step time: 1.5321\n",
      "73/388, train_loss: 0.0956, step time: 1.5339\n",
      "74/388, train_loss: 0.1170, step time: 1.5334\n",
      "75/388, train_loss: 0.1268, step time: 1.5321\n",
      "76/388, train_loss: 0.3398, step time: 1.5361\n",
      "77/388, train_loss: 0.0930, step time: 1.5369\n",
      "78/388, train_loss: 0.1737, step time: 1.5318\n",
      "79/388, train_loss: 0.1797, step time: 1.5335\n",
      "80/388, train_loss: 0.0981, step time: 1.5303\n",
      "81/388, train_loss: 0.2833, step time: 1.5342\n",
      "82/388, train_loss: 0.2573, step time: 1.5366\n",
      "83/388, train_loss: 0.1326, step time: 1.5358\n",
      "84/388, train_loss: 0.1605, step time: 1.5353\n",
      "85/388, train_loss: 0.1251, step time: 1.5317\n",
      "86/388, train_loss: 0.1816, step time: 1.5335\n",
      "87/388, train_loss: 0.1571, step time: 1.5337\n",
      "88/388, train_loss: 0.0903, step time: 1.5341\n",
      "89/388, train_loss: 0.0717, step time: 1.5326\n",
      "90/388, train_loss: 0.0855, step time: 1.5359\n",
      "91/388, train_loss: 0.1306, step time: 1.5415\n",
      "92/388, train_loss: 0.1769, step time: 1.5337\n",
      "93/388, train_loss: 0.0959, step time: 1.5344\n",
      "94/388, train_loss: 0.0944, step time: 1.5325\n",
      "95/388, train_loss: 0.2168, step time: 1.5310\n",
      "96/388, train_loss: 0.2010, step time: 1.5356\n",
      "97/388, train_loss: 0.1699, step time: 1.5333\n",
      "98/388, train_loss: 0.1603, step time: 1.5481\n",
      "99/388, train_loss: 0.0966, step time: 1.5303\n",
      "100/388, train_loss: 0.3879, step time: 1.5330\n",
      "101/388, train_loss: 0.1696, step time: 1.5326\n",
      "102/388, train_loss: 0.0908, step time: 1.5387\n",
      "103/388, train_loss: 0.2285, step time: 1.5365\n",
      "104/388, train_loss: 0.1790, step time: 1.5415\n",
      "105/388, train_loss: 0.1032, step time: 1.5343\n",
      "106/388, train_loss: 0.2003, step time: 1.5382\n",
      "107/388, train_loss: 0.1429, step time: 1.5356\n",
      "108/388, train_loss: 0.1072, step time: 1.5347\n",
      "109/388, train_loss: 0.2279, step time: 1.5305\n",
      "110/388, train_loss: 0.1910, step time: 1.5337\n",
      "111/388, train_loss: 0.0625, step time: 1.5349\n",
      "112/388, train_loss: 0.1690, step time: 1.5377\n",
      "113/388, train_loss: 0.2101, step time: 1.5344\n",
      "114/388, train_loss: 0.0921, step time: 1.5352\n",
      "115/388, train_loss: 0.2051, step time: 1.5319\n",
      "116/388, train_loss: 0.1154, step time: 1.5321\n",
      "117/388, train_loss: 0.2296, step time: 1.5331\n",
      "118/388, train_loss: 0.2018, step time: 1.5344\n",
      "119/388, train_loss: 0.0759, step time: 1.5348\n",
      "120/388, train_loss: 0.2478, step time: 1.5369\n",
      "121/388, train_loss: 0.2659, step time: 1.5325\n",
      "122/388, train_loss: 0.0423, step time: 1.5372\n",
      "123/388, train_loss: 0.1571, step time: 1.5516\n",
      "124/388, train_loss: 0.1685, step time: 1.5377\n",
      "125/388, train_loss: 0.0844, step time: 1.5341\n",
      "126/388, train_loss: 0.0519, step time: 1.5327\n",
      "127/388, train_loss: 0.0661, step time: 1.5318\n",
      "128/388, train_loss: 0.1297, step time: 1.5323\n",
      "129/388, train_loss: 0.1112, step time: 1.5326\n",
      "130/388, train_loss: 0.1041, step time: 1.5333\n",
      "131/388, train_loss: 0.1584, step time: 1.5328\n",
      "132/388, train_loss: 0.2420, step time: 1.5351\n",
      "133/388, train_loss: 0.1291, step time: 1.5333\n",
      "134/388, train_loss: 0.3390, step time: 1.5317\n",
      "135/388, train_loss: 0.1565, step time: 1.5305\n",
      "136/388, train_loss: 0.1844, step time: 1.5331\n",
      "137/388, train_loss: 0.1243, step time: 1.5357\n",
      "138/388, train_loss: 0.0704, step time: 1.5396\n",
      "139/388, train_loss: 0.0874, step time: 1.5350\n",
      "140/388, train_loss: 0.3178, step time: 1.5337\n",
      "141/388, train_loss: 0.0882, step time: 1.5320\n",
      "142/388, train_loss: 0.4194, step time: 1.5343\n",
      "143/388, train_loss: 0.1298, step time: 1.5332\n",
      "144/388, train_loss: 0.2660, step time: 1.5369\n",
      "145/388, train_loss: 0.3206, step time: 1.5348\n",
      "146/388, train_loss: 0.0755, step time: 1.5314\n",
      "147/388, train_loss: 0.0994, step time: 1.5364\n",
      "148/388, train_loss: 0.0646, step time: 1.5357\n",
      "149/388, train_loss: 0.2392, step time: 1.5379\n",
      "150/388, train_loss: 0.0819, step time: 1.5353\n",
      "151/388, train_loss: 0.1878, step time: 1.5309\n",
      "152/388, train_loss: 0.1861, step time: 1.5308\n",
      "153/388, train_loss: 0.0722, step time: 1.5337\n",
      "154/388, train_loss: 0.3761, step time: 1.5325\n",
      "155/388, train_loss: 0.3209, step time: 1.5378\n",
      "156/388, train_loss: 0.2579, step time: 1.5372\n",
      "157/388, train_loss: 0.0730, step time: 1.5333\n",
      "158/388, train_loss: 0.0894, step time: 1.5333\n",
      "159/388, train_loss: 0.2428, step time: 1.5328\n",
      "160/388, train_loss: 0.1290, step time: 1.5407\n",
      "161/388, train_loss: 0.1872, step time: 1.5367\n",
      "162/388, train_loss: 0.2141, step time: 1.5370\n",
      "163/388, train_loss: 0.2184, step time: 1.5315\n",
      "164/388, train_loss: 0.1096, step time: 1.5339\n",
      "165/388, train_loss: 0.2203, step time: 1.5334\n",
      "166/388, train_loss: 0.0483, step time: 1.5341\n",
      "167/388, train_loss: 0.2093, step time: 1.5390\n",
      "168/388, train_loss: 0.0882, step time: 1.5341\n",
      "169/388, train_loss: 0.1136, step time: 1.5337\n",
      "170/388, train_loss: 0.1923, step time: 1.5346\n",
      "171/388, train_loss: 0.1775, step time: 1.5365\n",
      "172/388, train_loss: 0.2171, step time: 1.5392\n",
      "173/388, train_loss: 0.0893, step time: 1.5314\n",
      "174/388, train_loss: 0.1813, step time: 1.5351\n",
      "175/388, train_loss: 0.2521, step time: 1.5317\n",
      "176/388, train_loss: 0.1097, step time: 1.5357\n",
      "177/388, train_loss: 0.1922, step time: 1.5360\n",
      "178/388, train_loss: 0.1756, step time: 1.5366\n",
      "179/388, train_loss: 0.1529, step time: 1.5333\n",
      "180/388, train_loss: 0.1715, step time: 1.5343\n",
      "181/388, train_loss: 0.3175, step time: 1.5293\n",
      "182/388, train_loss: 0.1433, step time: 1.5339\n",
      "183/388, train_loss: 0.1933, step time: 1.5331\n",
      "184/388, train_loss: 0.2867, step time: 1.5353\n",
      "185/388, train_loss: 0.1399, step time: 1.5340\n",
      "186/388, train_loss: 0.1795, step time: 1.5350\n",
      "187/388, train_loss: 0.1016, step time: 1.5332\n",
      "188/388, train_loss: 0.1421, step time: 1.5344\n",
      "189/388, train_loss: 0.1067, step time: 1.5338\n",
      "190/388, train_loss: 0.1335, step time: 1.5336\n",
      "191/388, train_loss: 0.0302, step time: 1.5379\n",
      "192/388, train_loss: 0.0872, step time: 1.5374\n",
      "193/388, train_loss: 0.0808, step time: 1.5337\n",
      "194/388, train_loss: 0.0939, step time: 1.5345\n",
      "195/388, train_loss: 0.2622, step time: 1.5305\n",
      "196/388, train_loss: 0.2964, step time: 1.5322\n",
      "197/388, train_loss: 0.2141, step time: 1.5352\n",
      "198/388, train_loss: 0.2146, step time: 1.5402\n",
      "199/388, train_loss: 0.0551, step time: 1.5365\n",
      "200/388, train_loss: 0.3496, step time: 1.5338\n",
      "201/388, train_loss: 0.1770, step time: 1.5321\n",
      "202/388, train_loss: 0.0809, step time: 1.5308\n",
      "203/388, train_loss: 0.3194, step time: 1.5336\n",
      "204/388, train_loss: 0.2474, step time: 1.5381\n",
      "205/388, train_loss: 0.0383, step time: 1.5353\n",
      "206/388, train_loss: 0.0958, step time: 1.5321\n",
      "207/388, train_loss: 0.1549, step time: 1.5326\n",
      "208/388, train_loss: 0.1208, step time: 1.5338\n",
      "209/388, train_loss: 0.0345, step time: 1.5364\n",
      "210/388, train_loss: 0.0507, step time: 1.5361\n",
      "211/388, train_loss: 0.0897, step time: 1.5374\n",
      "212/388, train_loss: 0.1675, step time: 1.5312\n",
      "213/388, train_loss: 0.1754, step time: 1.5303\n",
      "214/388, train_loss: 0.1456, step time: 1.5324\n",
      "215/388, train_loss: 0.1691, step time: 1.5368\n",
      "216/388, train_loss: 0.0946, step time: 1.5349\n",
      "217/388, train_loss: 0.1449, step time: 1.5322\n",
      "218/388, train_loss: 0.1033, step time: 1.5327\n",
      "219/388, train_loss: 0.0600, step time: 1.5318\n",
      "220/388, train_loss: 0.0836, step time: 1.5317\n",
      "221/388, train_loss: 0.1557, step time: 1.5350\n",
      "222/388, train_loss: 0.1222, step time: 1.5369\n",
      "223/388, train_loss: 0.2126, step time: 1.5362\n",
      "224/388, train_loss: 0.0267, step time: 1.5345\n",
      "225/388, train_loss: 0.1947, step time: 1.5324\n",
      "226/388, train_loss: 0.2545, step time: 1.5344\n",
      "227/388, train_loss: 0.0836, step time: 1.5377\n",
      "228/388, train_loss: 0.3098, step time: 1.5360\n",
      "229/388, train_loss: 0.1370, step time: 1.5321\n",
      "230/388, train_loss: 0.2440, step time: 1.5319\n",
      "231/388, train_loss: 0.0662, step time: 1.5329\n",
      "232/388, train_loss: 0.2700, step time: 1.5372\n",
      "233/388, train_loss: 0.2191, step time: 1.5353\n",
      "234/388, train_loss: 0.2251, step time: 1.5346\n",
      "235/388, train_loss: 0.0915, step time: 1.5430\n",
      "236/388, train_loss: 0.0787, step time: 1.5302\n",
      "237/388, train_loss: 0.2209, step time: 1.5320\n",
      "238/388, train_loss: 0.3270, step time: 1.5343\n",
      "239/388, train_loss: 0.1234, step time: 1.5344\n",
      "240/388, train_loss: 0.3283, step time: 1.5345\n",
      "241/388, train_loss: 0.1096, step time: 1.5319\n",
      "242/388, train_loss: 0.2517, step time: 1.5315\n",
      "243/388, train_loss: 0.0440, step time: 1.5337\n",
      "244/388, train_loss: 0.1020, step time: 1.5335\n",
      "245/388, train_loss: 0.0862, step time: 1.5328\n",
      "246/388, train_loss: 0.1010, step time: 1.5337\n",
      "247/388, train_loss: 0.2356, step time: 1.5366\n",
      "248/388, train_loss: 0.2632, step time: 1.5360\n",
      "249/388, train_loss: 0.1868, step time: 1.5337\n",
      "250/388, train_loss: 0.1433, step time: 1.5301\n",
      "251/388, train_loss: 0.2422, step time: 1.5354\n",
      "252/388, train_loss: 0.0964, step time: 1.5368\n",
      "253/388, train_loss: 0.0689, step time: 1.5343\n",
      "254/388, train_loss: 0.0982, step time: 1.5368\n",
      "255/388, train_loss: 0.1540, step time: 1.5339\n",
      "256/388, train_loss: 0.1134, step time: 1.5303\n",
      "257/388, train_loss: 0.1018, step time: 1.5297\n",
      "258/388, train_loss: 0.1548, step time: 1.5320\n",
      "259/388, train_loss: 0.0470, step time: 1.5329\n",
      "260/388, train_loss: 0.0781, step time: 1.5403\n",
      "261/388, train_loss: 0.1275, step time: 1.5467\n",
      "262/388, train_loss: 0.1902, step time: 1.5320\n",
      "263/388, train_loss: 0.1923, step time: 1.5313\n",
      "264/388, train_loss: 0.1199, step time: 1.5330\n",
      "265/388, train_loss: 0.0954, step time: 1.5369\n",
      "266/388, train_loss: 0.1120, step time: 1.5359\n",
      "267/388, train_loss: 0.0873, step time: 1.5323\n",
      "268/388, train_loss: 0.1418, step time: 1.5331\n",
      "269/388, train_loss: 0.2267, step time: 1.5315\n",
      "270/388, train_loss: 0.2118, step time: 1.5343\n",
      "271/388, train_loss: 0.2185, step time: 1.5376\n",
      "272/388, train_loss: 0.1031, step time: 1.5347\n",
      "273/388, train_loss: 0.3497, step time: 1.5345\n",
      "274/388, train_loss: 0.3147, step time: 1.5322\n",
      "275/388, train_loss: 0.0800, step time: 1.5320\n",
      "276/388, train_loss: 0.1702, step time: 1.5326\n",
      "277/388, train_loss: 0.1207, step time: 1.5339\n",
      "278/388, train_loss: 0.3294, step time: 1.5333\n",
      "279/388, train_loss: 0.1557, step time: 1.5360\n",
      "280/388, train_loss: 0.3699, step time: 1.5305\n",
      "281/388, train_loss: 0.1071, step time: 1.5326\n",
      "282/388, train_loss: 0.0912, step time: 1.5301\n",
      "283/388, train_loss: 0.2073, step time: 1.5350\n",
      "284/388, train_loss: 0.1265, step time: 1.5387\n",
      "285/388, train_loss: 0.0627, step time: 1.5390\n",
      "286/388, train_loss: 0.1556, step time: 1.5318\n",
      "287/388, train_loss: 0.1187, step time: 1.5301\n",
      "288/388, train_loss: 0.1152, step time: 1.5328\n",
      "289/388, train_loss: 0.1154, step time: 1.5582\n",
      "290/388, train_loss: 0.0796, step time: 1.5370\n",
      "291/388, train_loss: 0.1931, step time: 1.5319\n",
      "292/388, train_loss: 0.1597, step time: 1.5332\n",
      "293/388, train_loss: 0.1281, step time: 1.5325\n",
      "294/388, train_loss: 0.1287, step time: 1.5388\n",
      "295/388, train_loss: 0.2726, step time: 1.5375\n",
      "296/388, train_loss: 0.0956, step time: 1.5500\n",
      "297/388, train_loss: 0.0835, step time: 1.5317\n",
      "298/388, train_loss: 0.1064, step time: 1.5326\n",
      "299/388, train_loss: 0.1029, step time: 1.5350\n",
      "300/388, train_loss: 0.1807, step time: 1.5350\n",
      "301/388, train_loss: 0.3529, step time: 1.5371\n",
      "302/388, train_loss: 0.0718, step time: 1.5333\n",
      "303/388, train_loss: 0.1927, step time: 1.5344\n",
      "304/388, train_loss: 0.3363, step time: 1.5317\n",
      "305/388, train_loss: 0.3327, step time: 1.5354\n",
      "306/388, train_loss: 0.1305, step time: 1.5363\n",
      "307/388, train_loss: 0.2396, step time: 1.5361\n",
      "308/388, train_loss: 0.2474, step time: 1.5347\n",
      "309/388, train_loss: 0.1738, step time: 1.5318\n",
      "310/388, train_loss: 0.2808, step time: 1.5336\n",
      "311/388, train_loss: 0.2202, step time: 1.5315\n",
      "312/388, train_loss: 0.1225, step time: 1.5379\n",
      "313/388, train_loss: 0.2147, step time: 1.5361\n",
      "314/388, train_loss: 0.1820, step time: 1.5359\n",
      "315/388, train_loss: 0.0785, step time: 1.5337\n",
      "316/388, train_loss: 0.0847, step time: 1.5310\n",
      "317/388, train_loss: 0.1402, step time: 1.5343\n",
      "318/388, train_loss: 0.1971, step time: 1.5356\n",
      "319/388, train_loss: 0.2252, step time: 1.5363\n",
      "320/388, train_loss: 0.0405, step time: 1.5365\n",
      "321/388, train_loss: 0.1698, step time: 1.5341\n",
      "322/388, train_loss: 0.0816, step time: 1.5316\n",
      "323/388, train_loss: 0.2236, step time: 1.5316\n",
      "324/388, train_loss: 0.1497, step time: 1.5603\n",
      "325/388, train_loss: 0.0711, step time: 1.5337\n",
      "326/388, train_loss: 0.4424, step time: 1.5324\n",
      "327/388, train_loss: 0.2345, step time: 1.5356\n",
      "328/388, train_loss: 0.0662, step time: 1.5372\n",
      "329/388, train_loss: 0.1617, step time: 1.5371\n",
      "330/388, train_loss: 0.1171, step time: 1.5325\n",
      "331/388, train_loss: 0.3345, step time: 1.5322\n",
      "332/388, train_loss: 0.1519, step time: 1.5335\n",
      "333/388, train_loss: 0.2070, step time: 1.5350\n",
      "334/388, train_loss: 0.0853, step time: 1.5386\n",
      "335/388, train_loss: 0.2923, step time: 1.5341\n",
      "336/388, train_loss: 0.1845, step time: 1.5333\n",
      "337/388, train_loss: 0.0877, step time: 1.5338\n",
      "338/388, train_loss: 0.1324, step time: 1.5372\n",
      "339/388, train_loss: 0.2247, step time: 1.5381\n",
      "340/388, train_loss: 0.0738, step time: 1.5330\n",
      "341/388, train_loss: 0.1986, step time: 1.5316\n",
      "342/388, train_loss: 0.0639, step time: 1.5313\n",
      "343/388, train_loss: 0.0691, step time: 1.5328\n",
      "344/388, train_loss: 0.2373, step time: 1.5360\n",
      "345/388, train_loss: 0.1100, step time: 1.5340\n",
      "346/388, train_loss: 0.1041, step time: 1.5385\n",
      "347/388, train_loss: 0.1581, step time: 1.5340\n",
      "348/388, train_loss: 0.0666, step time: 1.5319\n",
      "349/388, train_loss: 0.2254, step time: 1.5300\n",
      "350/388, train_loss: 0.1028, step time: 1.5322\n",
      "351/388, train_loss: 0.1725, step time: 1.5324\n",
      "352/388, train_loss: 0.1377, step time: 1.5345\n",
      "353/388, train_loss: 0.2300, step time: 1.5342\n",
      "354/388, train_loss: 0.0834, step time: 1.5335\n",
      "355/388, train_loss: 0.2093, step time: 1.5303\n",
      "356/388, train_loss: 0.3610, step time: 1.5336\n",
      "357/388, train_loss: 0.1279, step time: 1.5368\n",
      "358/388, train_loss: 0.5273, step time: 1.5349\n",
      "359/388, train_loss: 0.0798, step time: 1.5376\n",
      "360/388, train_loss: 0.2166, step time: 1.5345\n",
      "361/388, train_loss: 0.0796, step time: 1.5327\n",
      "362/388, train_loss: 0.0985, step time: 1.5338\n",
      "363/388, train_loss: 0.1758, step time: 1.5351\n",
      "364/388, train_loss: 0.1170, step time: 1.5362\n",
      "365/388, train_loss: 0.0758, step time: 1.5329\n",
      "366/388, train_loss: 0.2409, step time: 1.5338\n",
      "367/388, train_loss: 0.2732, step time: 1.5301\n",
      "368/388, train_loss: 0.2491, step time: 1.5315\n",
      "369/388, train_loss: 0.1525, step time: 1.5484\n",
      "370/388, train_loss: 0.1769, step time: 1.5373\n",
      "371/388, train_loss: 0.1479, step time: 1.5500\n",
      "372/388, train_loss: 0.1699, step time: 1.5357\n",
      "373/388, train_loss: 0.0988, step time: 1.5417\n",
      "374/388, train_loss: 0.2441, step time: 1.5379\n",
      "375/388, train_loss: 0.4495, step time: 1.5331\n",
      "376/388, train_loss: 0.1410, step time: 1.5325\n",
      "377/388, train_loss: 0.3211, step time: 1.5302\n",
      "378/388, train_loss: 0.1983, step time: 1.5323\n",
      "379/388, train_loss: 0.2860, step time: 1.5382\n",
      "380/388, train_loss: 0.1850, step time: 1.5370\n",
      "381/388, train_loss: 0.4553, step time: 1.5357\n",
      "382/388, train_loss: 0.1614, step time: 1.5347\n",
      "383/388, train_loss: 0.0650, step time: 1.5345\n",
      "384/388, train_loss: 0.0460, step time: 1.5304\n",
      "385/388, train_loss: 0.1595, step time: 1.5335\n",
      "386/388, train_loss: 0.3688, step time: 1.5318\n",
      "387/388, train_loss: 0.1587, step time: 1.5332\n",
      "388/388, train_loss: 0.1509, step time: 1.5353\n",
      "epoch 67 average loss: 0.1677\n",
      "saved new best metric model\n",
      "current epoch: 67 current mean dice: 0.7781 tc: 0.8251 wt: 0.9067 et: 0.6027\n",
      "best mean dice: 0.7781 at epoch: 67\n",
      "time consuming of epoch 67 is: 702.7271\n",
      "----------\n",
      "epoch 68/100\n",
      "1/388, train_loss: 0.1651, step time: 1.5491\n",
      "2/388, train_loss: 0.1070, step time: 1.5328\n",
      "3/388, train_loss: 0.2080, step time: 1.5345\n",
      "4/388, train_loss: 0.1469, step time: 1.5354\n",
      "5/388, train_loss: 0.3238, step time: 1.5368\n",
      "6/388, train_loss: 0.1445, step time: 1.5362\n",
      "7/388, train_loss: 0.0820, step time: 1.5324\n",
      "8/388, train_loss: 0.1906, step time: 1.5362\n",
      "9/388, train_loss: 0.1579, step time: 1.5357\n",
      "10/388, train_loss: 0.1259, step time: 1.5379\n",
      "11/388, train_loss: 0.1213, step time: 1.5319\n",
      "12/388, train_loss: 0.0686, step time: 1.5323\n",
      "13/388, train_loss: 0.1713, step time: 1.5323\n",
      "14/388, train_loss: 0.1764, step time: 1.5314\n",
      "15/388, train_loss: 0.5660, step time: 1.5358\n",
      "16/388, train_loss: 0.1459, step time: 1.5379\n",
      "17/388, train_loss: 0.2048, step time: 1.5356\n",
      "18/388, train_loss: 0.2808, step time: 1.5335\n",
      "19/388, train_loss: 0.2090, step time: 1.5329\n",
      "20/388, train_loss: 0.0667, step time: 1.5352\n",
      "21/388, train_loss: 0.1693, step time: 1.5361\n",
      "22/388, train_loss: 0.0858, step time: 1.5391\n",
      "23/388, train_loss: 0.1136, step time: 1.5386\n",
      "24/388, train_loss: 0.2104, step time: 1.5325\n",
      "25/388, train_loss: 0.1887, step time: 1.5352\n",
      "26/388, train_loss: 0.0986, step time: 1.5348\n",
      "27/388, train_loss: 0.0999, step time: 1.5370\n",
      "28/388, train_loss: 0.0652, step time: 1.5363\n",
      "29/388, train_loss: 0.2381, step time: 1.5445\n",
      "30/388, train_loss: 0.0347, step time: 1.5398\n",
      "31/388, train_loss: 0.0649, step time: 1.5350\n",
      "32/388, train_loss: 0.1750, step time: 1.5396\n",
      "33/388, train_loss: 0.2565, step time: 1.5340\n",
      "34/388, train_loss: 0.2072, step time: 1.5370\n",
      "35/388, train_loss: 0.1111, step time: 1.5355\n",
      "36/388, train_loss: 0.0830, step time: 1.5338\n",
      "37/388, train_loss: 0.2663, step time: 1.5350\n",
      "38/388, train_loss: 0.3663, step time: 1.5407\n",
      "39/388, train_loss: 0.2781, step time: 1.5361\n",
      "40/388, train_loss: 0.0815, step time: 1.5313\n",
      "41/388, train_loss: 0.1156, step time: 1.5333\n",
      "42/388, train_loss: 0.3009, step time: 1.5367\n",
      "43/388, train_loss: 0.2821, step time: 1.5373\n",
      "44/388, train_loss: 0.2882, step time: 1.5369\n",
      "45/388, train_loss: 0.1369, step time: 1.5370\n",
      "46/388, train_loss: 0.0669, step time: 1.5325\n",
      "47/388, train_loss: 0.0929, step time: 1.5322\n",
      "48/388, train_loss: 0.3983, step time: 1.5338\n",
      "49/388, train_loss: 0.2460, step time: 1.5355\n",
      "50/388, train_loss: 0.1767, step time: 1.5362\n",
      "51/388, train_loss: 0.3306, step time: 1.5365\n",
      "52/388, train_loss: 0.0548, step time: 1.5358\n",
      "53/388, train_loss: 0.2902, step time: 1.5346\n",
      "54/388, train_loss: 0.1622, step time: 1.5322\n",
      "55/388, train_loss: 0.1501, step time: 1.5319\n",
      "56/388, train_loss: 0.2314, step time: 1.5360\n",
      "57/388, train_loss: 0.0885, step time: 1.5442\n",
      "58/388, train_loss: 0.0867, step time: 1.5347\n",
      "59/388, train_loss: 0.0599, step time: 1.5302\n",
      "60/388, train_loss: 0.1547, step time: 1.5324\n",
      "61/388, train_loss: 0.4113, step time: 1.5326\n",
      "62/388, train_loss: 0.1789, step time: 1.5360\n",
      "63/388, train_loss: 0.2871, step time: 1.5363\n",
      "64/388, train_loss: 0.2458, step time: 1.5352\n",
      "65/388, train_loss: 0.1708, step time: 1.5331\n",
      "66/388, train_loss: 0.0813, step time: 1.5333\n",
      "67/388, train_loss: 0.1720, step time: 1.5334\n",
      "68/388, train_loss: 0.0927, step time: 1.5364\n",
      "69/388, train_loss: 0.1498, step time: 1.5366\n",
      "70/388, train_loss: 0.2292, step time: 1.5365\n",
      "71/388, train_loss: 0.0944, step time: 1.5343\n",
      "72/388, train_loss: 0.1108, step time: 1.5800\n",
      "73/388, train_loss: 0.0318, step time: 1.5345\n",
      "74/388, train_loss: 0.0722, step time: 1.5315\n",
      "75/388, train_loss: 0.1162, step time: 1.5321\n",
      "76/388, train_loss: 0.1015, step time: 1.5369\n",
      "77/388, train_loss: 0.1489, step time: 1.5395\n",
      "78/388, train_loss: 0.1230, step time: 1.5334\n",
      "79/388, train_loss: 0.1880, step time: 1.5346\n",
      "80/388, train_loss: 0.3897, step time: 1.5340\n",
      "81/388, train_loss: 0.2769, step time: 1.5331\n",
      "82/388, train_loss: 0.2941, step time: 1.5386\n",
      "83/388, train_loss: 0.2261, step time: 1.5362\n",
      "84/388, train_loss: 0.4427, step time: 1.5346\n",
      "85/388, train_loss: 0.1235, step time: 1.5311\n",
      "86/388, train_loss: 0.2165, step time: 1.5340\n",
      "87/388, train_loss: 0.0988, step time: 1.5331\n",
      "88/388, train_loss: 0.1694, step time: 1.5351\n",
      "89/388, train_loss: 0.0623, step time: 1.5349\n",
      "90/388, train_loss: 0.1889, step time: 1.5392\n",
      "91/388, train_loss: 0.1274, step time: 1.5353\n",
      "92/388, train_loss: 0.2493, step time: 1.5341\n",
      "93/388, train_loss: 0.1209, step time: 1.5364\n",
      "94/388, train_loss: 0.2060, step time: 1.5341\n",
      "95/388, train_loss: 0.1143, step time: 1.5366\n",
      "96/388, train_loss: 0.1276, step time: 1.5365\n",
      "97/388, train_loss: 0.1160, step time: 1.5369\n",
      "98/388, train_loss: 0.1832, step time: 1.5337\n",
      "99/388, train_loss: 0.1468, step time: 1.5327\n",
      "100/388, train_loss: 0.0801, step time: 1.5359\n",
      "101/388, train_loss: 0.0906, step time: 1.5363\n",
      "102/388, train_loss: 0.1412, step time: 1.5473\n",
      "103/388, train_loss: 0.0833, step time: 1.5343\n",
      "104/388, train_loss: 0.1442, step time: 1.5347\n",
      "105/388, train_loss: 0.2526, step time: 1.5317\n",
      "106/388, train_loss: 0.3503, step time: 1.5323\n",
      "107/388, train_loss: 0.2152, step time: 1.5359\n",
      "108/388, train_loss: 0.0492, step time: 1.5362\n",
      "109/388, train_loss: 0.2155, step time: 1.5329\n",
      "110/388, train_loss: 0.0970, step time: 1.5391\n",
      "111/388, train_loss: 0.1556, step time: 1.5382\n",
      "112/388, train_loss: 0.2739, step time: 1.5406\n",
      "113/388, train_loss: 0.1199, step time: 1.5366\n",
      "114/388, train_loss: 0.2324, step time: 1.5342\n",
      "115/388, train_loss: 0.1851, step time: 1.5353\n",
      "116/388, train_loss: 0.1871, step time: 1.5328\n",
      "117/388, train_loss: 0.1897, step time: 1.5315\n",
      "118/388, train_loss: 0.1970, step time: 1.5343\n",
      "119/388, train_loss: 0.0927, step time: 1.5347\n",
      "120/388, train_loss: 0.1780, step time: 1.5385\n",
      "121/388, train_loss: 0.1624, step time: 1.5336\n",
      "122/388, train_loss: 0.2564, step time: 1.5336\n",
      "123/388, train_loss: 0.1634, step time: 1.5326\n",
      "124/388, train_loss: 0.0472, step time: 1.5342\n",
      "125/388, train_loss: 0.1639, step time: 1.5365\n",
      "126/388, train_loss: 0.0746, step time: 1.5456\n",
      "127/388, train_loss: 0.1569, step time: 1.5337\n",
      "128/388, train_loss: 0.2119, step time: 1.5352\n",
      "129/388, train_loss: 0.2950, step time: 1.5329\n",
      "130/388, train_loss: 0.0452, step time: 1.5358\n",
      "131/388, train_loss: 0.0658, step time: 1.5339\n",
      "132/388, train_loss: 0.1259, step time: 1.5327\n",
      "133/388, train_loss: 0.2116, step time: 1.5317\n",
      "134/388, train_loss: 0.1778, step time: 1.5378\n",
      "135/388, train_loss: 0.1102, step time: 1.5359\n",
      "136/388, train_loss: 0.1278, step time: 1.5384\n",
      "137/388, train_loss: 0.1403, step time: 1.5334\n",
      "138/388, train_loss: 0.1179, step time: 1.5566\n",
      "139/388, train_loss: 0.3492, step time: 1.5361\n",
      "140/388, train_loss: 0.0667, step time: 1.5369\n",
      "141/388, train_loss: 0.2339, step time: 1.5377\n",
      "142/388, train_loss: 0.1475, step time: 1.5328\n",
      "143/388, train_loss: 0.1503, step time: 1.5330\n",
      "144/388, train_loss: 0.2299, step time: 1.5318\n",
      "145/388, train_loss: 0.3691, step time: 1.5336\n",
      "146/388, train_loss: 0.0988, step time: 1.5342\n",
      "147/388, train_loss: 0.1657, step time: 1.5342\n",
      "148/388, train_loss: 0.1069, step time: 1.5348\n",
      "149/388, train_loss: 0.1527, step time: 1.5311\n",
      "150/388, train_loss: 0.0897, step time: 1.5349\n",
      "151/388, train_loss: 0.0876, step time: 1.5380\n",
      "152/388, train_loss: 0.1821, step time: 1.5374\n",
      "153/388, train_loss: 0.1022, step time: 1.5315\n",
      "154/388, train_loss: 0.2093, step time: 1.5358\n",
      "155/388, train_loss: 0.1042, step time: 1.5327\n",
      "156/388, train_loss: 0.1047, step time: 1.5429\n",
      "157/388, train_loss: 0.1647, step time: 1.5402\n",
      "158/388, train_loss: 0.0609, step time: 1.5366\n",
      "159/388, train_loss: 0.2132, step time: 1.5350\n",
      "160/388, train_loss: 0.1088, step time: 1.5350\n",
      "161/388, train_loss: 0.0826, step time: 1.5423\n",
      "162/388, train_loss: 0.0729, step time: 1.5344\n",
      "163/388, train_loss: 0.1360, step time: 1.5421\n",
      "164/388, train_loss: 0.1026, step time: 1.5322\n",
      "165/388, train_loss: 0.0842, step time: 1.5353\n",
      "166/388, train_loss: 0.0979, step time: 1.5382\n",
      "167/388, train_loss: 0.0364, step time: 1.5405\n",
      "168/388, train_loss: 0.0848, step time: 1.5356\n",
      "169/388, train_loss: 0.0853, step time: 1.5331\n",
      "170/388, train_loss: 0.4230, step time: 1.5309\n",
      "171/388, train_loss: 0.0885, step time: 1.5340\n",
      "172/388, train_loss: 0.1780, step time: 1.5365\n",
      "173/388, train_loss: 0.0793, step time: 1.5351\n",
      "174/388, train_loss: 0.1080, step time: 1.5360\n",
      "175/388, train_loss: 0.1704, step time: 1.5336\n",
      "176/388, train_loss: 0.3213, step time: 1.5340\n",
      "177/388, train_loss: 0.2130, step time: 1.5336\n",
      "178/388, train_loss: 0.0909, step time: 1.5345\n",
      "179/388, train_loss: 0.1401, step time: 1.5356\n",
      "180/388, train_loss: 0.2542, step time: 1.5377\n",
      "181/388, train_loss: 0.1525, step time: 1.5369\n",
      "182/388, train_loss: 0.1018, step time: 1.5333\n",
      "183/388, train_loss: 0.4327, step time: 1.5301\n",
      "184/388, train_loss: 0.2479, step time: 1.5366\n",
      "185/388, train_loss: 0.1995, step time: 1.5375\n",
      "186/388, train_loss: 0.2432, step time: 1.5388\n",
      "187/388, train_loss: 0.2020, step time: 1.5333\n",
      "188/388, train_loss: 0.2542, step time: 1.5364\n",
      "189/388, train_loss: 0.2952, step time: 1.5351\n",
      "190/388, train_loss: 0.1139, step time: 1.5359\n",
      "191/388, train_loss: 0.1471, step time: 1.5311\n",
      "192/388, train_loss: 0.3389, step time: 1.5347\n",
      "193/388, train_loss: 0.1466, step time: 1.5317\n",
      "194/388, train_loss: 0.1807, step time: 1.5315\n",
      "195/388, train_loss: 0.3988, step time: 1.5408\n",
      "196/388, train_loss: 0.1182, step time: 1.5383\n",
      "197/388, train_loss: 0.0833, step time: 1.5517\n",
      "198/388, train_loss: 0.1589, step time: 1.5348\n",
      "199/388, train_loss: 0.0760, step time: 1.5353\n",
      "200/388, train_loss: 0.1366, step time: 1.5381\n",
      "201/388, train_loss: 0.1048, step time: 1.5355\n",
      "202/388, train_loss: 0.1475, step time: 1.5357\n",
      "203/388, train_loss: 0.1138, step time: 1.5336\n",
      "204/388, train_loss: 0.1964, step time: 1.5362\n",
      "205/388, train_loss: 0.2085, step time: 1.5376\n",
      "206/388, train_loss: 0.0911, step time: 1.5355\n",
      "207/388, train_loss: 0.2876, step time: 1.5324\n",
      "208/388, train_loss: 0.1021, step time: 1.5316\n",
      "209/388, train_loss: 0.1913, step time: 1.5344\n",
      "210/388, train_loss: 0.0669, step time: 1.5334\n",
      "211/388, train_loss: 0.0719, step time: 1.5358\n",
      "212/388, train_loss: 0.0758, step time: 1.5350\n",
      "213/388, train_loss: 0.1287, step time: 1.5344\n",
      "214/388, train_loss: 0.2477, step time: 1.5328\n",
      "215/388, train_loss: 0.1878, step time: 1.5302\n",
      "216/388, train_loss: 0.0750, step time: 1.5318\n",
      "217/388, train_loss: 0.1785, step time: 1.5357\n",
      "218/388, train_loss: 0.0929, step time: 1.5394\n",
      "219/388, train_loss: 0.0485, step time: 1.5371\n",
      "220/388, train_loss: 0.1316, step time: 1.5327\n",
      "221/388, train_loss: 0.1027, step time: 1.5340\n",
      "222/388, train_loss: 0.0647, step time: 1.5322\n",
      "223/388, train_loss: 0.0405, step time: 1.5372\n",
      "224/388, train_loss: 0.3601, step time: 1.5380\n",
      "225/388, train_loss: 0.2837, step time: 1.5343\n",
      "226/388, train_loss: 0.0647, step time: 1.5322\n",
      "227/388, train_loss: 0.1830, step time: 1.5339\n",
      "228/388, train_loss: 0.0892, step time: 1.5304\n",
      "229/388, train_loss: 0.1479, step time: 1.5335\n",
      "230/388, train_loss: 0.3150, step time: 1.5353\n",
      "231/388, train_loss: 0.2266, step time: 1.5345\n",
      "232/388, train_loss: 0.0873, step time: 1.5353\n",
      "233/388, train_loss: 0.0597, step time: 1.5367\n",
      "234/388, train_loss: 0.1271, step time: 1.5321\n",
      "235/388, train_loss: 0.0927, step time: 1.5301\n",
      "236/388, train_loss: 0.1902, step time: 1.5295\n",
      "237/388, train_loss: 0.1969, step time: 1.5324\n",
      "238/388, train_loss: 0.1643, step time: 1.5359\n",
      "239/388, train_loss: 0.0922, step time: 1.5358\n",
      "240/388, train_loss: 0.2449, step time: 1.5324\n",
      "241/388, train_loss: 0.1635, step time: 1.5320\n",
      "242/388, train_loss: 0.1007, step time: 1.5330\n",
      "243/388, train_loss: 0.0973, step time: 1.5333\n",
      "244/388, train_loss: 0.0720, step time: 1.5386\n",
      "245/388, train_loss: 0.1440, step time: 1.5375\n",
      "246/388, train_loss: 0.1092, step time: 1.5327\n",
      "247/388, train_loss: 0.3814, step time: 1.5301\n",
      "248/388, train_loss: 0.1227, step time: 1.5338\n",
      "249/388, train_loss: 0.1738, step time: 1.5323\n",
      "250/388, train_loss: 0.2008, step time: 1.5363\n",
      "251/388, train_loss: 0.1323, step time: 1.5365\n",
      "252/388, train_loss: 0.2043, step time: 1.5323\n",
      "253/388, train_loss: 0.2633, step time: 1.5320\n",
      "254/388, train_loss: 0.3567, step time: 1.5321\n",
      "255/388, train_loss: 0.0995, step time: 1.5326\n",
      "256/388, train_loss: 0.1563, step time: 1.5347\n",
      "257/388, train_loss: 0.1629, step time: 1.5355\n",
      "258/388, train_loss: 0.0791, step time: 1.5363\n",
      "259/388, train_loss: 0.2266, step time: 1.5336\n",
      "260/388, train_loss: 0.1190, step time: 1.5299\n",
      "261/388, train_loss: 0.0858, step time: 1.5298\n",
      "262/388, train_loss: 0.2318, step time: 1.5333\n",
      "263/388, train_loss: 0.0977, step time: 1.5425\n",
      "264/388, train_loss: 0.1576, step time: 1.5350\n",
      "265/388, train_loss: 0.1544, step time: 1.5348\n",
      "266/388, train_loss: 0.1687, step time: 1.5300\n",
      "267/388, train_loss: 0.0839, step time: 1.5362\n",
      "268/388, train_loss: 0.3474, step time: 1.5372\n",
      "269/388, train_loss: 0.2626, step time: 1.5375\n",
      "270/388, train_loss: 0.2043, step time: 1.5367\n",
      "271/388, train_loss: 0.1668, step time: 1.5341\n",
      "272/388, train_loss: 0.1565, step time: 1.5328\n",
      "273/388, train_loss: 0.1423, step time: 1.5323\n",
      "274/388, train_loss: 0.1257, step time: 1.5323\n",
      "275/388, train_loss: 0.2768, step time: 1.5394\n",
      "276/388, train_loss: 0.1880, step time: 1.5366\n",
      "277/388, train_loss: 0.1995, step time: 1.5373\n",
      "278/388, train_loss: 0.2993, step time: 1.5359\n",
      "279/388, train_loss: 0.3230, step time: 1.5356\n",
      "280/388, train_loss: 0.0999, step time: 1.5348\n",
      "281/388, train_loss: 0.0268, step time: 1.5393\n",
      "282/388, train_loss: 0.0832, step time: 1.5360\n",
      "283/388, train_loss: 0.5134, step time: 1.5327\n",
      "284/388, train_loss: 0.1687, step time: 1.5306\n",
      "285/388, train_loss: 0.0555, step time: 1.5341\n",
      "286/388, train_loss: 0.1442, step time: 1.5322\n",
      "287/388, train_loss: 0.1169, step time: 1.5348\n",
      "288/388, train_loss: 0.2983, step time: 1.5575\n",
      "289/388, train_loss: 0.1756, step time: 1.5326\n",
      "290/388, train_loss: 0.2747, step time: 1.5331\n",
      "291/388, train_loss: 0.1000, step time: 1.5374\n",
      "292/388, train_loss: 0.2089, step time: 1.5380\n",
      "293/388, train_loss: 0.0538, step time: 1.5319\n",
      "294/388, train_loss: 0.1338, step time: 1.5304\n",
      "295/388, train_loss: 0.2952, step time: 1.5320\n",
      "296/388, train_loss: 0.2058, step time: 1.5338\n",
      "297/388, train_loss: 0.2037, step time: 1.5385\n",
      "298/388, train_loss: 0.2480, step time: 1.5368\n",
      "299/388, train_loss: 0.1497, step time: 1.5414\n",
      "300/388, train_loss: 0.1333, step time: 1.5310\n",
      "301/388, train_loss: 0.0499, step time: 1.5350\n",
      "302/388, train_loss: 0.0723, step time: 1.5377\n",
      "303/388, train_loss: 0.2384, step time: 1.5373\n",
      "304/388, train_loss: 0.0809, step time: 1.5331\n",
      "305/388, train_loss: 0.1855, step time: 1.5363\n",
      "306/388, train_loss: 0.1656, step time: 1.5328\n",
      "307/388, train_loss: 0.0981, step time: 1.5479\n",
      "308/388, train_loss: 0.1103, step time: 1.5369\n",
      "309/388, train_loss: 0.4092, step time: 1.5312\n",
      "310/388, train_loss: 0.2922, step time: 1.5329\n",
      "311/388, train_loss: 0.2022, step time: 1.5459\n",
      "312/388, train_loss: 0.1021, step time: 1.5368\n",
      "313/388, train_loss: 0.5101, step time: 1.5333\n",
      "314/388, train_loss: 0.1236, step time: 1.5339\n",
      "315/388, train_loss: 0.2398, step time: 1.5459\n",
      "316/388, train_loss: 0.2333, step time: 1.5397\n",
      "317/388, train_loss: 0.0863, step time: 1.5370\n",
      "318/388, train_loss: 0.2285, step time: 1.5323\n",
      "319/388, train_loss: 0.1109, step time: 1.5436\n",
      "320/388, train_loss: 0.0834, step time: 1.5341\n",
      "321/388, train_loss: 0.0646, step time: 1.5364\n",
      "322/388, train_loss: 0.0450, step time: 1.5364\n",
      "323/388, train_loss: 0.0928, step time: 1.5422\n",
      "324/388, train_loss: 0.1240, step time: 1.5313\n",
      "325/388, train_loss: 0.0598, step time: 1.5635\n",
      "326/388, train_loss: 0.1501, step time: 1.5296\n",
      "327/388, train_loss: 0.2311, step time: 1.5468\n",
      "328/388, train_loss: 0.0871, step time: 1.5358\n",
      "329/388, train_loss: 0.3198, step time: 1.5342\n",
      "330/388, train_loss: 0.1885, step time: 1.5357\n",
      "331/388, train_loss: 0.0987, step time: 1.5425\n",
      "332/388, train_loss: 0.0769, step time: 1.5363\n",
      "333/388, train_loss: 0.1953, step time: 1.5365\n",
      "334/388, train_loss: 0.2375, step time: 1.5322\n",
      "335/388, train_loss: 0.2459, step time: 1.5412\n",
      "336/388, train_loss: 0.1225, step time: 1.5323\n",
      "337/388, train_loss: 0.1634, step time: 1.5351\n",
      "338/388, train_loss: 0.0885, step time: 1.5337\n",
      "339/388, train_loss: 0.1281, step time: 1.5471\n",
      "340/388, train_loss: 0.1561, step time: 1.5303\n",
      "341/388, train_loss: 0.1053, step time: 1.5318\n",
      "342/388, train_loss: 0.2673, step time: 1.5339\n",
      "343/388, train_loss: 0.3246, step time: 1.5485\n",
      "344/388, train_loss: 0.2962, step time: 1.5379\n",
      "345/388, train_loss: 0.1292, step time: 1.5313\n",
      "346/388, train_loss: 0.0835, step time: 1.5331\n",
      "347/388, train_loss: 0.1415, step time: 1.5457\n",
      "348/388, train_loss: 0.3108, step time: 1.5355\n",
      "349/388, train_loss: 0.1229, step time: 1.5370\n",
      "350/388, train_loss: 0.0765, step time: 1.5321\n",
      "351/388, train_loss: 0.1665, step time: 1.5411\n",
      "352/388, train_loss: 0.0909, step time: 1.5357\n",
      "353/388, train_loss: 0.0682, step time: 1.5363\n",
      "354/388, train_loss: 0.1019, step time: 1.5356\n",
      "355/388, train_loss: 0.1617, step time: 1.5449\n",
      "356/388, train_loss: 0.0815, step time: 1.5353\n",
      "357/388, train_loss: 0.1663, step time: 1.5307\n",
      "358/388, train_loss: 0.3284, step time: 1.5369\n",
      "359/388, train_loss: 0.0781, step time: 1.5493\n",
      "360/388, train_loss: 0.0975, step time: 1.5343\n",
      "361/388, train_loss: 0.2152, step time: 1.5336\n",
      "362/388, train_loss: 0.0603, step time: 1.5320\n",
      "363/388, train_loss: 0.1252, step time: 1.5451\n",
      "364/388, train_loss: 0.1389, step time: 1.5351\n",
      "365/388, train_loss: 0.4920, step time: 1.5353\n",
      "366/388, train_loss: 0.0854, step time: 1.5319\n",
      "367/388, train_loss: 0.1068, step time: 1.5437\n",
      "368/388, train_loss: 0.2684, step time: 1.5359\n",
      "369/388, train_loss: 0.1933, step time: 1.5323\n",
      "370/388, train_loss: 0.0674, step time: 1.5320\n",
      "371/388, train_loss: 0.1147, step time: 1.5449\n",
      "372/388, train_loss: 0.1221, step time: 1.5368\n",
      "373/388, train_loss: 0.0441, step time: 1.5372\n",
      "374/388, train_loss: 0.2357, step time: 1.5329\n",
      "375/388, train_loss: 0.1213, step time: 1.5409\n",
      "376/388, train_loss: 0.2160, step time: 1.5339\n",
      "377/388, train_loss: 0.1429, step time: 1.5360\n",
      "378/388, train_loss: 0.1305, step time: 1.5361\n",
      "379/388, train_loss: 0.1380, step time: 1.5496\n",
      "380/388, train_loss: 0.2869, step time: 1.5308\n",
      "381/388, train_loss: 0.1313, step time: 1.5336\n",
      "382/388, train_loss: 0.4436, step time: 1.5317\n",
      "383/388, train_loss: 0.1233, step time: 1.5468\n",
      "384/388, train_loss: 0.2917, step time: 1.5360\n",
      "385/388, train_loss: 0.1356, step time: 1.5334\n",
      "386/388, train_loss: 0.0975, step time: 1.5350\n",
      "387/388, train_loss: 0.0737, step time: 1.5453\n",
      "388/388, train_loss: 0.2753, step time: 1.5385\n",
      "epoch 68 average loss: 0.1680\n",
      "saved new best metric model\n",
      "current epoch: 68 current mean dice: 0.7785 tc: 0.8266 wt: 0.9064 et: 0.6026\n",
      "best mean dice: 0.7785 at epoch: 68\n",
      "time consuming of epoch 68 is: 702.1355\n",
      "----------\n",
      "epoch 69/100\n",
      "1/388, train_loss: 0.2149, step time: 1.5500\n",
      "2/388, train_loss: 0.1593, step time: 1.5409\n",
      "3/388, train_loss: 0.0769, step time: 1.5377\n",
      "4/388, train_loss: 0.1768, step time: 1.5326\n",
      "5/388, train_loss: 0.1245, step time: 1.5315\n",
      "6/388, train_loss: 0.1219, step time: 1.5367\n",
      "7/388, train_loss: 0.1125, step time: 1.5363\n",
      "8/388, train_loss: 0.1525, step time: 1.5360\n",
      "9/388, train_loss: 0.1780, step time: 1.5363\n",
      "10/388, train_loss: 0.2076, step time: 1.5365\n",
      "11/388, train_loss: 0.1714, step time: 1.5332\n",
      "12/388, train_loss: 0.1527, step time: 1.5326\n",
      "13/388, train_loss: 0.0700, step time: 1.5346\n",
      "14/388, train_loss: 0.1536, step time: 1.5363\n",
      "15/388, train_loss: 0.1854, step time: 1.5338\n",
      "16/388, train_loss: 0.1475, step time: 1.5347\n",
      "17/388, train_loss: 0.1031, step time: 1.5321\n",
      "18/388, train_loss: 0.0713, step time: 1.5324\n",
      "19/388, train_loss: 0.1127, step time: 1.5318\n",
      "20/388, train_loss: 0.0820, step time: 1.5334\n",
      "21/388, train_loss: 0.1697, step time: 1.5397\n",
      "22/388, train_loss: 0.1219, step time: 1.5573\n",
      "23/388, train_loss: 0.0969, step time: 1.5665\n",
      "24/388, train_loss: 0.1813, step time: 1.5363\n",
      "25/388, train_loss: 0.0931, step time: 1.5345\n",
      "26/388, train_loss: 0.0597, step time: 1.5327\n",
      "27/388, train_loss: 0.1224, step time: 1.5347\n",
      "28/388, train_loss: 0.0920, step time: 1.5361\n",
      "29/388, train_loss: 0.0916, step time: 1.5447\n",
      "30/388, train_loss: 0.1214, step time: 1.5350\n",
      "31/388, train_loss: 0.2308, step time: 1.5351\n",
      "32/388, train_loss: 0.0935, step time: 1.5391\n",
      "33/388, train_loss: 0.0276, step time: 1.5411\n",
      "34/388, train_loss: 0.1524, step time: 1.5470\n",
      "35/388, train_loss: 0.1200, step time: 1.5361\n",
      "36/388, train_loss: 0.0869, step time: 1.5350\n",
      "37/388, train_loss: 0.1125, step time: 1.5364\n",
      "38/388, train_loss: 0.1993, step time: 1.5366\n",
      "39/388, train_loss: 0.0738, step time: 1.5324\n",
      "40/388, train_loss: 0.1515, step time: 1.5327\n",
      "41/388, train_loss: 0.1493, step time: 1.5352\n",
      "42/388, train_loss: 0.1099, step time: 1.5380\n",
      "43/388, train_loss: 0.0802, step time: 1.5372\n",
      "44/388, train_loss: 0.1886, step time: 1.5395\n",
      "45/388, train_loss: 0.2518, step time: 1.5412\n",
      "46/388, train_loss: 0.1757, step time: 1.5366\n",
      "47/388, train_loss: 0.0939, step time: 1.5366\n",
      "48/388, train_loss: 0.3610, step time: 1.5392\n",
      "49/388, train_loss: 0.0752, step time: 1.5340\n",
      "50/388, train_loss: 0.2901, step time: 1.5356\n",
      "51/388, train_loss: 0.2232, step time: 1.5337\n",
      "52/388, train_loss: 0.0509, step time: 1.5338\n",
      "53/388, train_loss: 0.1627, step time: 1.5361\n",
      "54/388, train_loss: 0.2602, step time: 1.5392\n",
      "55/388, train_loss: 0.2582, step time: 1.5425\n",
      "56/388, train_loss: 0.0890, step time: 1.5348\n",
      "57/388, train_loss: 0.2537, step time: 1.5364\n",
      "58/388, train_loss: 0.0980, step time: 1.5334\n",
      "59/388, train_loss: 0.3818, step time: 1.5363\n",
      "60/388, train_loss: 0.0925, step time: 1.5347\n",
      "61/388, train_loss: 0.1138, step time: 1.5352\n",
      "62/388, train_loss: 0.2236, step time: 1.5357\n",
      "63/388, train_loss: 0.2090, step time: 1.5397\n",
      "64/388, train_loss: 0.1446, step time: 1.5447\n",
      "65/388, train_loss: 0.0964, step time: 1.5336\n",
      "66/388, train_loss: 0.1129, step time: 1.5328\n",
      "67/388, train_loss: 0.1222, step time: 1.5372\n",
      "68/388, train_loss: 0.3829, step time: 1.5371\n",
      "69/388, train_loss: 0.1380, step time: 1.5424\n",
      "70/388, train_loss: 0.1642, step time: 1.5327\n",
      "71/388, train_loss: 0.1196, step time: 1.5311\n",
      "72/388, train_loss: 0.2082, step time: 1.5325\n",
      "73/388, train_loss: 0.3286, step time: 1.5339\n",
      "74/388, train_loss: 0.0952, step time: 1.5347\n",
      "75/388, train_loss: 0.0920, step time: 1.5342\n",
      "76/388, train_loss: 0.1174, step time: 1.5365\n",
      "77/388, train_loss: 0.2986, step time: 1.5361\n",
      "78/388, train_loss: 0.1644, step time: 1.5328\n",
      "79/388, train_loss: 0.1330, step time: 1.5302\n",
      "80/388, train_loss: 0.1259, step time: 1.5374\n",
      "81/388, train_loss: 0.1678, step time: 1.5377\n",
      "82/388, train_loss: 0.1304, step time: 1.5366\n",
      "83/388, train_loss: 0.1059, step time: 1.5352\n",
      "84/388, train_loss: 0.1265, step time: 1.5351\n",
      "85/388, train_loss: 0.0630, step time: 1.5357\n",
      "86/388, train_loss: 0.1698, step time: 1.5338\n",
      "87/388, train_loss: 0.1841, step time: 1.5328\n",
      "88/388, train_loss: 0.1384, step time: 1.5378\n",
      "89/388, train_loss: 0.2585, step time: 1.5340\n",
      "90/388, train_loss: 0.1830, step time: 1.5348\n",
      "91/388, train_loss: 0.1377, step time: 1.5352\n",
      "92/388, train_loss: 0.0992, step time: 1.5376\n",
      "93/388, train_loss: 0.1563, step time: 1.5388\n",
      "94/388, train_loss: 0.2973, step time: 1.5399\n",
      "95/388, train_loss: 0.1418, step time: 1.5360\n",
      "96/388, train_loss: 0.3398, step time: 1.5355\n",
      "97/388, train_loss: 0.2064, step time: 1.5312\n",
      "98/388, train_loss: 0.1295, step time: 1.5337\n",
      "99/388, train_loss: 0.3147, step time: 1.5483\n",
      "100/388, train_loss: 0.2087, step time: 1.5328\n",
      "101/388, train_loss: 0.0704, step time: 1.5335\n",
      "102/388, train_loss: 0.1223, step time: 1.5290\n",
      "103/388, train_loss: 0.1779, step time: 1.5378\n",
      "104/388, train_loss: 0.0868, step time: 1.5376\n",
      "105/388, train_loss: 0.1712, step time: 1.5366\n",
      "106/388, train_loss: 0.0954, step time: 1.5361\n",
      "107/388, train_loss: 0.1549, step time: 1.5317\n",
      "108/388, train_loss: 0.1264, step time: 1.5339\n",
      "109/388, train_loss: 0.1699, step time: 1.5357\n",
      "110/388, train_loss: 0.0652, step time: 1.5357\n",
      "111/388, train_loss: 0.0984, step time: 1.5355\n",
      "112/388, train_loss: 0.3110, step time: 1.5359\n",
      "113/388, train_loss: 0.2497, step time: 1.5358\n",
      "114/388, train_loss: 0.1929, step time: 1.5355\n",
      "115/388, train_loss: 0.2145, step time: 1.5354\n",
      "116/388, train_loss: 0.1299, step time: 1.5364\n",
      "117/388, train_loss: 0.1286, step time: 1.5325\n",
      "118/388, train_loss: 0.1022, step time: 1.5322\n",
      "119/388, train_loss: 0.0800, step time: 1.5341\n",
      "120/388, train_loss: 0.2992, step time: 1.5365\n",
      "121/388, train_loss: 0.1626, step time: 1.5353\n",
      "122/388, train_loss: 0.1696, step time: 1.5375\n",
      "123/388, train_loss: 0.1496, step time: 1.5312\n",
      "124/388, train_loss: 0.1649, step time: 1.5336\n",
      "125/388, train_loss: 0.1051, step time: 1.5438\n",
      "126/388, train_loss: 0.1774, step time: 1.5367\n",
      "127/388, train_loss: 0.1241, step time: 1.5368\n",
      "128/388, train_loss: 0.2035, step time: 1.5329\n",
      "129/388, train_loss: 0.1954, step time: 1.5331\n",
      "130/388, train_loss: 0.1046, step time: 1.5316\n",
      "131/388, train_loss: 0.1826, step time: 1.5378\n",
      "132/388, train_loss: 0.3939, step time: 1.5420\n",
      "133/388, train_loss: 0.0671, step time: 1.5331\n",
      "134/388, train_loss: 0.0999, step time: 1.5331\n",
      "135/388, train_loss: 0.1294, step time: 1.5341\n",
      "136/388, train_loss: 0.2176, step time: 1.5362\n",
      "137/388, train_loss: 0.0713, step time: 1.5411\n",
      "138/388, train_loss: 0.1842, step time: 1.5506\n",
      "139/388, train_loss: 0.1155, step time: 1.5342\n",
      "140/388, train_loss: 0.0975, step time: 1.5325\n",
      "141/388, train_loss: 0.1804, step time: 1.5367\n",
      "142/388, train_loss: 0.0788, step time: 1.5411\n",
      "143/388, train_loss: 0.1281, step time: 1.5372\n",
      "144/388, train_loss: 0.1766, step time: 1.5351\n",
      "145/388, train_loss: 0.1117, step time: 1.5325\n",
      "146/388, train_loss: 0.0909, step time: 1.5489\n",
      "147/388, train_loss: 0.0645, step time: 1.5385\n",
      "148/388, train_loss: 0.1034, step time: 1.5313\n",
      "149/388, train_loss: 0.2134, step time: 1.5301\n",
      "150/388, train_loss: 0.3113, step time: 1.5321\n",
      "151/388, train_loss: 0.0369, step time: 1.5302\n",
      "152/388, train_loss: 0.1487, step time: 1.5327\n",
      "153/388, train_loss: 0.1169, step time: 1.5632\n",
      "154/388, train_loss: 0.3769, step time: 1.5365\n",
      "155/388, train_loss: 0.1289, step time: 1.5363\n",
      "156/388, train_loss: 0.2064, step time: 1.5371\n",
      "157/388, train_loss: 0.2280, step time: 1.5380\n",
      "158/388, train_loss: 0.1730, step time: 1.5423\n",
      "159/388, train_loss: 0.1611, step time: 1.5326\n",
      "160/388, train_loss: 0.3191, step time: 1.5324\n",
      "161/388, train_loss: 0.6221, step time: 1.5348\n",
      "162/388, train_loss: 0.1059, step time: 1.5365\n",
      "163/388, train_loss: 0.5298, step time: 1.5355\n",
      "164/388, train_loss: 0.2583, step time: 1.5347\n",
      "165/388, train_loss: 0.1885, step time: 1.5314\n",
      "166/388, train_loss: 0.0667, step time: 1.5318\n",
      "167/388, train_loss: 0.1498, step time: 1.5323\n",
      "168/388, train_loss: 0.0983, step time: 1.5377\n",
      "169/388, train_loss: 0.2494, step time: 1.5329\n",
      "170/388, train_loss: 0.1371, step time: 1.5351\n",
      "171/388, train_loss: 0.0761, step time: 1.5361\n",
      "172/388, train_loss: 0.0990, step time: 1.5345\n",
      "173/388, train_loss: 0.4823, step time: 1.5338\n",
      "174/388, train_loss: 0.1653, step time: 1.5380\n",
      "175/388, train_loss: 0.1712, step time: 1.5401\n",
      "176/388, train_loss: 0.1180, step time: 1.5358\n",
      "177/388, train_loss: 0.2730, step time: 1.5309\n",
      "178/388, train_loss: 0.1605, step time: 1.5317\n",
      "179/388, train_loss: 0.2871, step time: 1.5334\n",
      "180/388, train_loss: 0.2348, step time: 1.5314\n",
      "181/388, train_loss: 0.1671, step time: 1.5361\n",
      "182/388, train_loss: 0.0886, step time: 1.5397\n",
      "183/388, train_loss: 0.1054, step time: 1.5365\n",
      "184/388, train_loss: 0.2341, step time: 1.5376\n",
      "185/388, train_loss: 0.2120, step time: 1.5314\n",
      "186/388, train_loss: 0.0863, step time: 1.5357\n",
      "187/388, train_loss: 0.2407, step time: 1.5338\n",
      "188/388, train_loss: 0.1361, step time: 1.5397\n",
      "189/388, train_loss: 0.0652, step time: 1.5321\n",
      "190/388, train_loss: 0.1333, step time: 1.5362\n",
      "191/388, train_loss: 0.1177, step time: 1.5336\n",
      "192/388, train_loss: 0.0872, step time: 1.5360\n",
      "193/388, train_loss: 0.0327, step time: 1.5448\n",
      "194/388, train_loss: 0.2403, step time: 1.5332\n",
      "195/388, train_loss: 0.2535, step time: 1.5392\n",
      "196/388, train_loss: 0.2470, step time: 1.5368\n",
      "197/388, train_loss: 0.0993, step time: 1.5398\n",
      "198/388, train_loss: 0.1701, step time: 1.5359\n",
      "199/388, train_loss: 0.3610, step time: 1.5360\n",
      "200/388, train_loss: 0.2854, step time: 1.5320\n",
      "201/388, train_loss: 0.1455, step time: 1.5373\n",
      "202/388, train_loss: 0.0994, step time: 1.5363\n",
      "203/388, train_loss: 0.2381, step time: 1.5334\n",
      "204/388, train_loss: 0.0578, step time: 1.5336\n",
      "205/388, train_loss: 0.1878, step time: 1.5326\n",
      "206/388, train_loss: 0.1960, step time: 1.5327\n",
      "207/388, train_loss: 0.0833, step time: 1.5379\n",
      "208/388, train_loss: 0.2491, step time: 1.5395\n",
      "209/388, train_loss: 0.0843, step time: 1.5350\n",
      "210/388, train_loss: 0.0533, step time: 1.5338\n",
      "211/388, train_loss: 0.4168, step time: 1.5338\n",
      "212/388, train_loss: 0.0850, step time: 1.5385\n",
      "213/388, train_loss: 0.0823, step time: 1.5385\n",
      "214/388, train_loss: 0.1667, step time: 1.5370\n",
      "215/388, train_loss: 0.2122, step time: 1.5350\n",
      "216/388, train_loss: 0.0981, step time: 1.5326\n",
      "217/388, train_loss: 0.1753, step time: 1.5293\n",
      "218/388, train_loss: 0.0525, step time: 1.5315\n",
      "219/388, train_loss: 0.1461, step time: 1.5357\n",
      "220/388, train_loss: 0.1266, step time: 1.5375\n",
      "221/388, train_loss: 0.1974, step time: 1.5335\n",
      "222/388, train_loss: 0.0564, step time: 1.5367\n",
      "223/388, train_loss: 0.1345, step time: 1.5481\n",
      "224/388, train_loss: 0.0314, step time: 1.5349\n",
      "225/388, train_loss: 0.2511, step time: 1.5327\n",
      "226/388, train_loss: 0.0620, step time: 1.5326\n",
      "227/388, train_loss: 0.2039, step time: 1.5344\n",
      "228/388, train_loss: 0.2059, step time: 1.5482\n",
      "229/388, train_loss: 0.1482, step time: 1.5346\n",
      "230/388, train_loss: 0.3041, step time: 1.5351\n",
      "231/388, train_loss: 0.0789, step time: 1.5357\n",
      "232/388, train_loss: 0.1427, step time: 1.5362\n",
      "233/388, train_loss: 0.1331, step time: 1.5437\n",
      "234/388, train_loss: 0.3014, step time: 1.5496\n",
      "235/388, train_loss: 0.0739, step time: 1.5319\n",
      "236/388, train_loss: 0.0977, step time: 1.5324\n",
      "237/388, train_loss: 0.3005, step time: 1.5319\n",
      "238/388, train_loss: 0.1587, step time: 1.5379\n",
      "239/388, train_loss: 0.1209, step time: 1.5372\n",
      "240/388, train_loss: 0.1580, step time: 1.5332\n",
      "241/388, train_loss: 0.1028, step time: 1.5423\n",
      "242/388, train_loss: 0.2196, step time: 1.5344\n",
      "243/388, train_loss: 0.2945, step time: 1.5338\n",
      "244/388, train_loss: 0.0513, step time: 1.5618\n",
      "245/388, train_loss: 0.1494, step time: 1.5348\n",
      "246/388, train_loss: 0.3432, step time: 1.5349\n",
      "247/388, train_loss: 0.1020, step time: 1.5302\n",
      "248/388, train_loss: 0.1334, step time: 1.5337\n",
      "249/388, train_loss: 0.3515, step time: 1.5425\n",
      "250/388, train_loss: 0.1488, step time: 1.5411\n",
      "251/388, train_loss: 0.1899, step time: 1.5359\n",
      "252/388, train_loss: 0.1881, step time: 1.5378\n",
      "253/388, train_loss: 0.1513, step time: 1.5344\n",
      "254/388, train_loss: 0.1640, step time: 1.5329\n",
      "255/388, train_loss: 0.1229, step time: 1.5311\n",
      "256/388, train_loss: 0.2183, step time: 1.5341\n",
      "257/388, train_loss: 0.1677, step time: 1.5379\n",
      "258/388, train_loss: 0.0622, step time: 1.5370\n",
      "259/388, train_loss: 0.1603, step time: 1.5338\n",
      "260/388, train_loss: 0.1034, step time: 1.5338\n",
      "261/388, train_loss: 0.0916, step time: 1.5318\n",
      "262/388, train_loss: 0.0813, step time: 1.5341\n",
      "263/388, train_loss: 0.1283, step time: 1.5370\n",
      "264/388, train_loss: 0.2478, step time: 1.5388\n",
      "265/388, train_loss: 0.3475, step time: 1.5360\n",
      "266/388, train_loss: 0.0797, step time: 1.5328\n",
      "267/388, train_loss: 0.1644, step time: 1.5328\n",
      "268/388, train_loss: 0.1433, step time: 1.5366\n",
      "269/388, train_loss: 0.1464, step time: 1.5330\n",
      "270/388, train_loss: 0.3459, step time: 1.5360\n",
      "271/388, train_loss: 0.0835, step time: 1.5371\n",
      "272/388, train_loss: 0.2279, step time: 1.5362\n",
      "273/388, train_loss: 0.4569, step time: 1.5345\n",
      "274/388, train_loss: 0.2315, step time: 1.5337\n",
      "275/388, train_loss: 0.1183, step time: 1.5322\n",
      "276/388, train_loss: 0.1139, step time: 1.5388\n",
      "277/388, train_loss: 0.0674, step time: 1.5510\n",
      "278/388, train_loss: 0.1883, step time: 1.5370\n",
      "279/388, train_loss: 0.1430, step time: 1.5357\n",
      "280/388, train_loss: 0.0816, step time: 1.5360\n",
      "281/388, train_loss: 0.0788, step time: 1.5378\n",
      "282/388, train_loss: 0.2836, step time: 1.5364\n",
      "283/388, train_loss: 0.0648, step time: 1.5372\n",
      "284/388, train_loss: 0.1105, step time: 1.5330\n",
      "285/388, train_loss: 0.2024, step time: 1.5346\n",
      "286/388, train_loss: 0.2582, step time: 1.5364\n",
      "287/388, train_loss: 0.0968, step time: 1.5369\n",
      "288/388, train_loss: 0.1155, step time: 1.5320\n",
      "289/388, train_loss: 0.1296, step time: 1.5423\n",
      "290/388, train_loss: 0.0513, step time: 1.5355\n",
      "291/388, train_loss: 0.2638, step time: 1.5378\n",
      "292/388, train_loss: 0.2999, step time: 1.5350\n",
      "293/388, train_loss: 0.4189, step time: 1.5339\n",
      "294/388, train_loss: 0.2278, step time: 1.5345\n",
      "295/388, train_loss: 0.0868, step time: 1.5390\n",
      "296/388, train_loss: 0.2033, step time: 1.5378\n",
      "297/388, train_loss: 0.0695, step time: 1.5393\n",
      "298/388, train_loss: 0.1441, step time: 1.5343\n",
      "299/388, train_loss: 0.2002, step time: 1.5339\n",
      "300/388, train_loss: 0.1947, step time: 1.5490\n",
      "301/388, train_loss: 0.1093, step time: 1.5351\n",
      "302/388, train_loss: 0.0429, step time: 1.5383\n",
      "303/388, train_loss: 0.2157, step time: 1.5322\n",
      "304/388, train_loss: 0.0869, step time: 1.5433\n",
      "305/388, train_loss: 0.2753, step time: 1.5380\n",
      "306/388, train_loss: 0.1407, step time: 1.5359\n",
      "307/388, train_loss: 0.1458, step time: 1.5344\n",
      "308/388, train_loss: 0.0496, step time: 1.5475\n",
      "309/388, train_loss: 0.1960, step time: 1.5356\n",
      "310/388, train_loss: 0.1886, step time: 1.5387\n",
      "311/388, train_loss: 0.2941, step time: 1.5371\n",
      "312/388, train_loss: 0.2207, step time: 1.5400\n",
      "313/388, train_loss: 0.1088, step time: 1.5327\n",
      "314/388, train_loss: 0.2077, step time: 1.5341\n",
      "315/388, train_loss: 0.3552, step time: 1.5365\n",
      "316/388, train_loss: 0.0712, step time: 1.5485\n",
      "317/388, train_loss: 0.0569, step time: 1.5339\n",
      "318/388, train_loss: 0.3205, step time: 1.5336\n",
      "319/388, train_loss: 0.2387, step time: 1.5340\n",
      "320/388, train_loss: 0.0873, step time: 1.5464\n",
      "321/388, train_loss: 0.0880, step time: 1.5331\n",
      "322/388, train_loss: 0.1416, step time: 1.5333\n",
      "323/388, train_loss: 0.0777, step time: 1.5333\n",
      "324/388, train_loss: 0.0831, step time: 1.5425\n",
      "325/388, train_loss: 0.3118, step time: 1.5388\n",
      "326/388, train_loss: 0.2054, step time: 1.5399\n",
      "327/388, train_loss: 0.0908, step time: 1.5360\n",
      "328/388, train_loss: 0.0496, step time: 1.5464\n",
      "329/388, train_loss: 0.3075, step time: 1.5346\n",
      "330/388, train_loss: 0.2768, step time: 1.5353\n",
      "331/388, train_loss: 0.1729, step time: 1.5356\n",
      "332/388, train_loss: 0.2252, step time: 1.5454\n",
      "333/388, train_loss: 0.1910, step time: 1.5315\n",
      "334/388, train_loss: 0.0597, step time: 1.5333\n",
      "335/388, train_loss: 0.0852, step time: 1.5363\n",
      "336/388, train_loss: 0.1136, step time: 1.5486\n",
      "337/388, train_loss: 0.2348, step time: 1.5309\n",
      "338/388, train_loss: 0.0975, step time: 1.5335\n",
      "339/388, train_loss: 0.1857, step time: 1.5357\n",
      "340/388, train_loss: 0.2673, step time: 1.5492\n",
      "341/388, train_loss: 0.0936, step time: 1.5342\n",
      "342/388, train_loss: 0.0870, step time: 1.5329\n",
      "343/388, train_loss: 0.0969, step time: 1.5340\n",
      "344/388, train_loss: 0.0893, step time: 1.5449\n",
      "345/388, train_loss: 0.1952, step time: 1.5334\n",
      "346/388, train_loss: 0.0824, step time: 1.5374\n",
      "347/388, train_loss: 0.1601, step time: 1.5357\n",
      "348/388, train_loss: 0.1906, step time: 1.5455\n",
      "349/388, train_loss: 0.0870, step time: 1.5337\n",
      "350/388, train_loss: 0.1513, step time: 1.5368\n",
      "351/388, train_loss: 0.2118, step time: 1.5353\n",
      "352/388, train_loss: 0.3977, step time: 1.5491\n",
      "353/388, train_loss: 0.1017, step time: 1.5313\n",
      "354/388, train_loss: 0.1693, step time: 1.5329\n",
      "355/388, train_loss: 0.2362, step time: 1.5346\n",
      "356/388, train_loss: 0.3362, step time: 1.5491\n",
      "357/388, train_loss: 0.0672, step time: 1.5320\n",
      "358/388, train_loss: 0.3300, step time: 1.5332\n",
      "359/388, train_loss: 0.1703, step time: 1.5336\n",
      "360/388, train_loss: 0.2229, step time: 1.5554\n",
      "361/388, train_loss: 0.1886, step time: 1.5306\n",
      "362/388, train_loss: 0.0651, step time: 1.5314\n",
      "363/388, train_loss: 0.1117, step time: 1.5325\n",
      "364/388, train_loss: 0.1865, step time: 1.5472\n",
      "365/388, train_loss: 0.2068, step time: 1.5412\n",
      "366/388, train_loss: 0.2332, step time: 1.5322\n",
      "367/388, train_loss: 0.1064, step time: 1.5314\n",
      "368/388, train_loss: 0.2444, step time: 1.5450\n",
      "369/388, train_loss: 0.1073, step time: 1.5508\n",
      "370/388, train_loss: 0.1035, step time: 1.5336\n",
      "371/388, train_loss: 0.2333, step time: 1.5336\n",
      "372/388, train_loss: 0.3008, step time: 1.5406\n",
      "373/388, train_loss: 0.1439, step time: 1.5332\n",
      "374/388, train_loss: 0.1416, step time: 1.5398\n",
      "375/388, train_loss: 0.0981, step time: 1.5375\n",
      "376/388, train_loss: 0.4231, step time: 1.5470\n",
      "377/388, train_loss: 0.1408, step time: 1.5335\n",
      "378/388, train_loss: 0.1031, step time: 1.5347\n",
      "379/388, train_loss: 0.1293, step time: 1.5353\n",
      "380/388, train_loss: 0.2062, step time: 1.5496\n",
      "381/388, train_loss: 0.1171, step time: 1.5327\n",
      "382/388, train_loss: 0.1180, step time: 1.5362\n",
      "383/388, train_loss: 0.1510, step time: 1.5346\n",
      "384/388, train_loss: 0.2466, step time: 1.5475\n",
      "385/388, train_loss: 0.1624, step time: 1.5361\n",
      "386/388, train_loss: 0.5153, step time: 1.5363\n",
      "387/388, train_loss: 0.0469, step time: 1.5320\n",
      "388/388, train_loss: 0.2030, step time: 1.5426\n",
      "epoch 69 average loss: 0.1671\n",
      "current epoch: 69 current mean dice: 0.7774 tc: 0.8253 wt: 0.9030 et: 0.6039\n",
      "best mean dice: 0.7785 at epoch: 68\n",
      "time consuming of epoch 69 is: 706.1876\n",
      "----------\n",
      "epoch 70/100\n",
      "1/388, train_loss: 0.1218, step time: 1.5496\n",
      "2/388, train_loss: 0.0775, step time: 1.5339\n",
      "3/388, train_loss: 0.1173, step time: 1.5319\n",
      "4/388, train_loss: 0.1228, step time: 1.5361\n",
      "5/388, train_loss: 0.1944, step time: 1.5331\n",
      "6/388, train_loss: 0.1428, step time: 1.5346\n",
      "7/388, train_loss: 0.1386, step time: 1.5338\n",
      "8/388, train_loss: 0.1264, step time: 1.5371\n",
      "9/388, train_loss: 0.0981, step time: 1.5315\n",
      "10/388, train_loss: 0.0948, step time: 1.5327\n",
      "11/388, train_loss: 0.2272, step time: 1.5314\n",
      "12/388, train_loss: 0.2845, step time: 1.5352\n",
      "13/388, train_loss: 0.1188, step time: 1.5348\n",
      "14/388, train_loss: 0.1608, step time: 1.5370\n",
      "15/388, train_loss: 0.0679, step time: 1.5362\n",
      "16/388, train_loss: 0.1819, step time: 1.5351\n",
      "17/388, train_loss: 0.1354, step time: 1.5320\n",
      "18/388, train_loss: 0.0908, step time: 1.5330\n",
      "19/388, train_loss: 0.1957, step time: 1.5368\n",
      "20/388, train_loss: 0.1332, step time: 1.5368\n",
      "21/388, train_loss: 0.0795, step time: 1.5355\n",
      "22/388, train_loss: 0.2519, step time: 1.5355\n",
      "23/388, train_loss: 0.1042, step time: 1.5333\n",
      "24/388, train_loss: 0.1448, step time: 1.5329\n",
      "25/388, train_loss: 0.0862, step time: 1.5362\n",
      "26/388, train_loss: 0.0727, step time: 1.5336\n",
      "27/388, train_loss: 0.0940, step time: 1.5328\n",
      "28/388, train_loss: 0.2658, step time: 1.5337\n",
      "29/388, train_loss: 0.1647, step time: 1.5335\n",
      "30/388, train_loss: 0.2140, step time: 1.5317\n",
      "31/388, train_loss: 0.3162, step time: 1.5451\n",
      "32/388, train_loss: 0.0893, step time: 1.5376\n",
      "33/388, train_loss: 0.0809, step time: 1.5375\n",
      "34/388, train_loss: 0.1907, step time: 1.5355\n",
      "35/388, train_loss: 0.1960, step time: 1.5335\n",
      "36/388, train_loss: 0.0888, step time: 1.5338\n",
      "37/388, train_loss: 0.1973, step time: 1.5357\n",
      "38/388, train_loss: 0.1179, step time: 1.5360\n",
      "39/388, train_loss: 0.2583, step time: 1.5357\n",
      "40/388, train_loss: 0.1574, step time: 1.5345\n",
      "41/388, train_loss: 0.1244, step time: 1.5323\n",
      "42/388, train_loss: 0.2861, step time: 1.5320\n",
      "43/388, train_loss: 0.0969, step time: 1.5316\n",
      "44/388, train_loss: 0.2012, step time: 1.5358\n",
      "45/388, train_loss: 0.1292, step time: 1.5367\n",
      "46/388, train_loss: 0.2079, step time: 1.5447\n",
      "47/388, train_loss: 0.2459, step time: 1.5366\n",
      "48/388, train_loss: 0.1306, step time: 1.5338\n",
      "49/388, train_loss: 0.0950, step time: 1.5400\n",
      "50/388, train_loss: 0.3079, step time: 1.5372\n",
      "51/388, train_loss: 0.2091, step time: 1.5360\n",
      "52/388, train_loss: 0.1747, step time: 1.5337\n",
      "53/388, train_loss: 0.2475, step time: 1.5318\n",
      "54/388, train_loss: 0.1648, step time: 1.5341\n",
      "55/388, train_loss: 0.1455, step time: 1.5352\n",
      "56/388, train_loss: 0.2048, step time: 1.5384\n",
      "57/388, train_loss: 0.0606, step time: 1.5353\n",
      "58/388, train_loss: 0.1971, step time: 1.5332\n",
      "59/388, train_loss: 0.1621, step time: 1.5324\n",
      "60/388, train_loss: 0.1722, step time: 1.5327\n",
      "61/388, train_loss: 0.1141, step time: 1.5334\n",
      "62/388, train_loss: 0.2374, step time: 1.5371\n",
      "63/388, train_loss: 0.2178, step time: 1.5332\n",
      "64/388, train_loss: 0.3132, step time: 1.5331\n",
      "65/388, train_loss: 0.1079, step time: 1.5355\n",
      "66/388, train_loss: 0.1581, step time: 1.5381\n",
      "67/388, train_loss: 0.2499, step time: 1.5365\n",
      "68/388, train_loss: 0.2730, step time: 1.5362\n",
      "69/388, train_loss: 0.2251, step time: 1.5350\n",
      "70/388, train_loss: 0.1570, step time: 1.5306\n",
      "71/388, train_loss: 0.1241, step time: 1.5328\n",
      "72/388, train_loss: 0.2243, step time: 1.5444\n",
      "73/388, train_loss: 0.1148, step time: 1.5358\n",
      "74/388, train_loss: 0.1204, step time: 1.5336\n",
      "75/388, train_loss: 0.1434, step time: 1.5367\n",
      "76/388, train_loss: 0.3080, step time: 1.5353\n",
      "77/388, train_loss: 0.1003, step time: 1.5603\n",
      "78/388, train_loss: 0.0953, step time: 1.5364\n",
      "79/388, train_loss: 0.0761, step time: 1.5593\n",
      "80/388, train_loss: 0.1807, step time: 1.5302\n",
      "81/388, train_loss: 0.1467, step time: 1.5332\n",
      "82/388, train_loss: 0.1095, step time: 1.5341\n",
      "83/388, train_loss: 0.0549, step time: 1.5378\n",
      "84/388, train_loss: 0.1455, step time: 1.5383\n",
      "85/388, train_loss: 0.0740, step time: 1.5374\n",
      "86/388, train_loss: 0.0868, step time: 1.5342\n",
      "87/388, train_loss: 0.1276, step time: 1.5346\n",
      "88/388, train_loss: 0.2489, step time: 1.5352\n",
      "89/388, train_loss: 0.0870, step time: 1.5469\n",
      "90/388, train_loss: 0.0761, step time: 1.5318\n",
      "91/388, train_loss: 0.1949, step time: 1.5326\n",
      "92/388, train_loss: 0.1376, step time: 1.5316\n",
      "93/388, train_loss: 0.1288, step time: 1.5379\n",
      "94/388, train_loss: 0.2277, step time: 1.5356\n",
      "95/388, train_loss: 0.1069, step time: 1.5362\n",
      "96/388, train_loss: 0.0843, step time: 1.5393\n",
      "97/388, train_loss: 0.0469, step time: 1.5364\n",
      "98/388, train_loss: 0.1030, step time: 1.5416\n",
      "99/388, train_loss: 0.2113, step time: 1.5354\n",
      "100/388, train_loss: 0.1284, step time: 1.5377\n",
      "101/388, train_loss: 0.3922, step time: 1.5325\n",
      "102/388, train_loss: 0.2063, step time: 1.5359\n",
      "103/388, train_loss: 0.0522, step time: 1.5338\n",
      "104/388, train_loss: 0.0449, step time: 1.5361\n",
      "105/388, train_loss: 0.2064, step time: 1.5366\n",
      "106/388, train_loss: 0.0583, step time: 1.5329\n",
      "107/388, train_loss: 0.1297, step time: 1.5325\n",
      "108/388, train_loss: 0.0586, step time: 1.5347\n",
      "109/388, train_loss: 0.1733, step time: 1.5361\n",
      "110/388, train_loss: 0.2508, step time: 1.5364\n",
      "111/388, train_loss: 0.1448, step time: 1.5330\n",
      "112/388, train_loss: 0.0825, step time: 1.5412\n",
      "113/388, train_loss: 0.0320, step time: 1.5339\n",
      "114/388, train_loss: 0.0974, step time: 1.5378\n",
      "115/388, train_loss: 0.1727, step time: 1.5358\n",
      "116/388, train_loss: 0.2101, step time: 1.5368\n",
      "117/388, train_loss: 0.3672, step time: 1.5330\n",
      "118/388, train_loss: 0.1451, step time: 1.5318\n",
      "119/388, train_loss: 0.1561, step time: 1.5328\n",
      "120/388, train_loss: 0.1829, step time: 1.5368\n",
      "121/388, train_loss: 0.2433, step time: 1.5434\n",
      "122/388, train_loss: 0.0611, step time: 1.5435\n",
      "123/388, train_loss: 0.0859, step time: 1.5348\n",
      "124/388, train_loss: 0.1226, step time: 1.5336\n",
      "125/388, train_loss: 0.1230, step time: 1.5445\n",
      "126/388, train_loss: 0.2136, step time: 1.5352\n",
      "127/388, train_loss: 0.2317, step time: 1.5358\n",
      "128/388, train_loss: 0.2434, step time: 1.5337\n",
      "129/388, train_loss: 0.2207, step time: 1.5319\n",
      "130/388, train_loss: 0.1713, step time: 1.5338\n",
      "131/388, train_loss: 0.3919, step time: 1.5330\n",
      "132/388, train_loss: 0.1666, step time: 1.5369\n",
      "133/388, train_loss: 0.1511, step time: 1.5358\n",
      "134/388, train_loss: 0.2013, step time: 1.5356\n",
      "135/388, train_loss: 0.0983, step time: 1.5346\n",
      "136/388, train_loss: 0.1198, step time: 1.5365\n",
      "137/388, train_loss: 0.4145, step time: 1.5347\n",
      "138/388, train_loss: 0.1591, step time: 1.5527\n",
      "139/388, train_loss: 0.1466, step time: 1.5338\n",
      "140/388, train_loss: 0.0945, step time: 1.5328\n",
      "141/388, train_loss: 0.0572, step time: 1.5343\n",
      "142/388, train_loss: 0.0559, step time: 1.5368\n",
      "143/388, train_loss: 0.2496, step time: 1.5380\n",
      "144/388, train_loss: 0.2663, step time: 1.5383\n",
      "145/388, train_loss: 0.1709, step time: 1.5398\n",
      "146/388, train_loss: 0.0853, step time: 1.5326\n",
      "147/388, train_loss: 0.0979, step time: 1.5321\n",
      "148/388, train_loss: 0.1568, step time: 1.5353\n",
      "149/388, train_loss: 0.2297, step time: 1.5382\n",
      "150/388, train_loss: 0.3014, step time: 1.5392\n",
      "151/388, train_loss: 0.1459, step time: 1.5438\n",
      "152/388, train_loss: 0.1195, step time: 1.5319\n",
      "153/388, train_loss: 0.0769, step time: 1.5342\n",
      "154/388, train_loss: 0.1793, step time: 1.5375\n",
      "155/388, train_loss: 0.1824, step time: 1.5387\n",
      "156/388, train_loss: 0.1504, step time: 1.5328\n",
      "157/388, train_loss: 0.1266, step time: 1.5424\n",
      "158/388, train_loss: 0.2057, step time: 1.5325\n",
      "159/388, train_loss: 0.1215, step time: 1.5449\n",
      "160/388, train_loss: 0.1644, step time: 1.5369\n",
      "161/388, train_loss: 0.1246, step time: 1.5335\n",
      "162/388, train_loss: 0.1350, step time: 1.5359\n",
      "163/388, train_loss: 0.0960, step time: 1.5313\n",
      "164/388, train_loss: 0.3178, step time: 1.5378\n",
      "165/388, train_loss: 0.1929, step time: 1.5439\n",
      "166/388, train_loss: 0.0626, step time: 1.5347\n",
      "167/388, train_loss: 0.3084, step time: 1.5340\n",
      "168/388, train_loss: 0.1411, step time: 1.5369\n",
      "169/388, train_loss: 0.1149, step time: 1.5388\n",
      "170/388, train_loss: 0.0875, step time: 1.5380\n",
      "171/388, train_loss: 0.0707, step time: 1.5340\n",
      "172/388, train_loss: 0.0875, step time: 1.5325\n",
      "173/388, train_loss: 0.1270, step time: 1.5347\n",
      "174/388, train_loss: 0.0974, step time: 1.5497\n",
      "175/388, train_loss: 0.2513, step time: 1.5342\n",
      "176/388, train_loss: 0.0270, step time: 1.5332\n",
      "177/388, train_loss: 0.0845, step time: 1.5302\n",
      "178/388, train_loss: 0.1018, step time: 1.5297\n",
      "179/388, train_loss: 0.0960, step time: 1.5374\n",
      "180/388, train_loss: 0.2840, step time: 1.5343\n",
      "181/388, train_loss: 0.1656, step time: 1.5309\n",
      "182/388, train_loss: 0.2575, step time: 1.5336\n",
      "183/388, train_loss: 0.1780, step time: 1.5327\n",
      "184/388, train_loss: 0.1787, step time: 1.5313\n",
      "185/388, train_loss: 0.0694, step time: 1.5324\n",
      "186/388, train_loss: 0.1524, step time: 1.5373\n",
      "187/388, train_loss: 0.3628, step time: 1.5381\n",
      "188/388, train_loss: 0.2429, step time: 1.5323\n",
      "189/388, train_loss: 0.0730, step time: 1.5347\n",
      "190/388, train_loss: 0.0662, step time: 1.5464\n",
      "191/388, train_loss: 0.0698, step time: 1.5391\n",
      "192/388, train_loss: 0.2207, step time: 1.5341\n",
      "193/388, train_loss: 0.1135, step time: 1.5333\n",
      "194/388, train_loss: 0.1096, step time: 1.5364\n",
      "195/388, train_loss: 0.2052, step time: 1.5402\n",
      "196/388, train_loss: 0.0953, step time: 1.5333\n",
      "197/388, train_loss: 0.1130, step time: 1.5351\n",
      "198/388, train_loss: 0.0708, step time: 1.5353\n",
      "199/388, train_loss: 0.1086, step time: 1.5378\n",
      "200/388, train_loss: 0.2022, step time: 1.5367\n",
      "201/388, train_loss: 0.2256, step time: 1.5328\n",
      "202/388, train_loss: 0.2533, step time: 1.5351\n",
      "203/388, train_loss: 0.2451, step time: 1.5356\n",
      "204/388, train_loss: 0.1272, step time: 1.5506\n",
      "205/388, train_loss: 0.1516, step time: 1.5358\n",
      "206/388, train_loss: 0.2216, step time: 1.5337\n",
      "207/388, train_loss: 0.3522, step time: 1.5350\n",
      "208/388, train_loss: 0.1878, step time: 1.5382\n",
      "209/388, train_loss: 0.0487, step time: 1.5387\n",
      "210/388, train_loss: 0.1709, step time: 1.5400\n",
      "211/388, train_loss: 0.1051, step time: 1.5334\n",
      "212/388, train_loss: 0.3041, step time: 1.5342\n",
      "213/388, train_loss: 0.2470, step time: 1.5309\n",
      "214/388, train_loss: 0.1467, step time: 1.5371\n",
      "215/388, train_loss: 0.1156, step time: 1.5402\n",
      "216/388, train_loss: 0.1030, step time: 1.5339\n",
      "217/388, train_loss: 0.1321, step time: 1.5338\n",
      "218/388, train_loss: 0.0653, step time: 1.5400\n",
      "219/388, train_loss: 0.1387, step time: 1.5396\n",
      "220/388, train_loss: 0.1739, step time: 1.5449\n",
      "221/388, train_loss: 0.1144, step time: 1.5353\n",
      "222/388, train_loss: 0.2814, step time: 1.5394\n",
      "223/388, train_loss: 0.1083, step time: 1.5383\n",
      "224/388, train_loss: 0.2537, step time: 1.5348\n",
      "225/388, train_loss: 0.4290, step time: 1.5344\n",
      "226/388, train_loss: 0.0897, step time: 1.5383\n",
      "227/388, train_loss: 0.2895, step time: 1.5335\n",
      "228/388, train_loss: 0.1065, step time: 1.5409\n",
      "229/388, train_loss: 0.0643, step time: 1.5318\n",
      "230/388, train_loss: 0.3075, step time: 1.5327\n",
      "231/388, train_loss: 0.4949, step time: 1.5321\n",
      "232/388, train_loss: 0.3396, step time: 1.5373\n",
      "233/388, train_loss: 0.0957, step time: 1.5381\n",
      "234/388, train_loss: 0.1025, step time: 1.5351\n",
      "235/388, train_loss: 0.3265, step time: 1.5308\n",
      "236/388, train_loss: 0.1634, step time: 1.5323\n",
      "237/388, train_loss: 0.3967, step time: 1.5321\n",
      "238/388, train_loss: 0.1296, step time: 1.5356\n",
      "239/388, train_loss: 0.3246, step time: 1.5361\n",
      "240/388, train_loss: 0.1323, step time: 1.5336\n",
      "241/388, train_loss: 0.0660, step time: 1.5364\n",
      "242/388, train_loss: 0.0823, step time: 1.5348\n",
      "243/388, train_loss: 0.1084, step time: 1.5360\n",
      "244/388, train_loss: 0.1461, step time: 1.5332\n",
      "245/388, train_loss: 0.2768, step time: 1.5350\n",
      "246/388, train_loss: 0.0464, step time: 1.5370\n",
      "247/388, train_loss: 0.1475, step time: 1.5411\n",
      "248/388, train_loss: 0.2281, step time: 1.5330\n",
      "249/388, train_loss: 0.2468, step time: 1.5313\n",
      "250/388, train_loss: 0.1553, step time: 1.5349\n",
      "251/388, train_loss: 0.0964, step time: 1.5414\n",
      "252/388, train_loss: 0.0851, step time: 1.5384\n",
      "253/388, train_loss: 0.5282, step time: 1.5378\n",
      "254/388, train_loss: 0.1429, step time: 1.5329\n",
      "255/388, train_loss: 0.1828, step time: 1.5320\n",
      "256/388, train_loss: 0.4034, step time: 1.5321\n",
      "257/388, train_loss: 0.3413, step time: 1.5336\n",
      "258/388, train_loss: 0.1080, step time: 1.5369\n",
      "259/388, train_loss: 0.2260, step time: 1.5378\n",
      "260/388, train_loss: 0.0913, step time: 1.5353\n",
      "261/388, train_loss: 0.4101, step time: 1.5611\n",
      "262/388, train_loss: 0.1106, step time: 1.5346\n",
      "263/388, train_loss: 0.2000, step time: 1.5356\n",
      "264/388, train_loss: 0.0956, step time: 1.5368\n",
      "265/388, train_loss: 0.0782, step time: 1.5324\n",
      "266/388, train_loss: 0.3923, step time: 1.5352\n",
      "267/388, train_loss: 0.2030, step time: 1.5328\n",
      "268/388, train_loss: 0.2408, step time: 1.5369\n",
      "269/388, train_loss: 0.1564, step time: 1.5349\n",
      "270/388, train_loss: 0.1719, step time: 1.5341\n",
      "271/388, train_loss: 0.0863, step time: 1.5321\n",
      "272/388, train_loss: 0.0812, step time: 1.5344\n",
      "273/388, train_loss: 0.2076, step time: 1.5367\n",
      "274/388, train_loss: 0.2254, step time: 1.5379\n",
      "275/388, train_loss: 0.0914, step time: 1.5379\n",
      "276/388, train_loss: 0.3923, step time: 1.5349\n",
      "277/388, train_loss: 0.0960, step time: 1.5314\n",
      "278/388, train_loss: 0.2128, step time: 1.5325\n",
      "279/388, train_loss: 0.2198, step time: 1.5339\n",
      "280/388, train_loss: 0.1004, step time: 1.5360\n",
      "281/388, train_loss: 0.2661, step time: 1.5364\n",
      "282/388, train_loss: 0.3897, step time: 1.5360\n",
      "283/388, train_loss: 0.2137, step time: 1.5306\n",
      "284/388, train_loss: 0.0913, step time: 1.5331\n",
      "285/388, train_loss: 0.2113, step time: 1.5342\n",
      "286/388, train_loss: 0.0878, step time: 1.5352\n",
      "287/388, train_loss: 0.1455, step time: 1.5364\n",
      "288/388, train_loss: 0.0597, step time: 1.5367\n",
      "289/388, train_loss: 0.1728, step time: 1.5343\n",
      "290/388, train_loss: 0.1955, step time: 1.5331\n",
      "291/388, train_loss: 0.4499, step time: 1.5334\n",
      "292/388, train_loss: 0.0663, step time: 1.5350\n",
      "293/388, train_loss: 0.0457, step time: 1.5371\n",
      "294/388, train_loss: 0.1907, step time: 1.5372\n",
      "295/388, train_loss: 0.1696, step time: 1.5352\n",
      "296/388, train_loss: 0.3045, step time: 1.5322\n",
      "297/388, train_loss: 0.0796, step time: 1.5303\n",
      "298/388, train_loss: 0.1542, step time: 1.5351\n",
      "299/388, train_loss: 0.0841, step time: 1.5335\n",
      "300/388, train_loss: 0.0925, step time: 1.5355\n",
      "301/388, train_loss: 0.1235, step time: 1.5335\n",
      "302/388, train_loss: 0.1066, step time: 1.5339\n",
      "303/388, train_loss: 0.0974, step time: 1.5325\n",
      "304/388, train_loss: 0.2126, step time: 1.5332\n",
      "305/388, train_loss: 0.1084, step time: 1.5331\n",
      "306/388, train_loss: 0.0946, step time: 1.5345\n",
      "307/388, train_loss: 0.0921, step time: 1.5366\n",
      "308/388, train_loss: 0.1375, step time: 1.5454\n",
      "309/388, train_loss: 0.1744, step time: 1.5328\n",
      "310/388, train_loss: 0.5577, step time: 1.5336\n",
      "311/388, train_loss: 0.1871, step time: 1.5304\n",
      "312/388, train_loss: 0.0754, step time: 1.5416\n",
      "313/388, train_loss: 0.2599, step time: 1.5411\n",
      "314/388, train_loss: 0.1739, step time: 1.5297\n",
      "315/388, train_loss: 0.2264, step time: 1.5309\n",
      "316/388, train_loss: 0.1390, step time: 1.5320\n",
      "317/388, train_loss: 0.1568, step time: 1.5342\n",
      "318/388, train_loss: 0.1924, step time: 1.5345\n",
      "319/388, train_loss: 0.1354, step time: 1.5330\n",
      "320/388, train_loss: 0.0787, step time: 1.5375\n",
      "321/388, train_loss: 0.1792, step time: 1.5322\n",
      "322/388, train_loss: 0.2410, step time: 1.5315\n",
      "323/388, train_loss: 0.1798, step time: 1.5319\n",
      "324/388, train_loss: 0.1847, step time: 1.5327\n",
      "325/388, train_loss: 0.0466, step time: 1.5358\n",
      "326/388, train_loss: 0.1298, step time: 1.5374\n",
      "327/388, train_loss: 0.1001, step time: 1.5366\n",
      "328/388, train_loss: 0.3132, step time: 1.5327\n",
      "329/388, train_loss: 0.0812, step time: 1.5310\n",
      "330/388, train_loss: 0.1600, step time: 1.5344\n",
      "331/388, train_loss: 0.2504, step time: 1.5323\n",
      "332/388, train_loss: 0.1580, step time: 1.5377\n",
      "333/388, train_loss: 0.0782, step time: 1.5359\n",
      "334/388, train_loss: 0.1072, step time: 1.5346\n",
      "335/388, train_loss: 0.1498, step time: 1.5346\n",
      "336/388, train_loss: 0.0495, step time: 1.5623\n",
      "337/388, train_loss: 0.2606, step time: 1.5329\n",
      "338/388, train_loss: 0.1852, step time: 1.5315\n",
      "339/388, train_loss: 0.0453, step time: 1.5426\n",
      "340/388, train_loss: 0.2571, step time: 1.5349\n",
      "341/388, train_loss: 0.1192, step time: 1.5360\n",
      "342/388, train_loss: 0.1408, step time: 1.5341\n",
      "343/388, train_loss: 0.1178, step time: 1.5329\n",
      "344/388, train_loss: 0.1904, step time: 1.5318\n",
      "345/388, train_loss: 0.1318, step time: 1.5402\n",
      "346/388, train_loss: 0.0687, step time: 1.5367\n",
      "347/388, train_loss: 0.2237, step time: 1.5365\n",
      "348/388, train_loss: 0.0935, step time: 1.5370\n",
      "349/388, train_loss: 0.3107, step time: 1.5314\n",
      "350/388, train_loss: 0.2615, step time: 1.5321\n",
      "351/388, train_loss: 0.1531, step time: 1.5315\n",
      "352/388, train_loss: 0.4680, step time: 1.5335\n",
      "353/388, train_loss: 0.0462, step time: 1.5377\n",
      "354/388, train_loss: 0.2361, step time: 1.5384\n",
      "355/388, train_loss: 0.0963, step time: 1.5308\n",
      "356/388, train_loss: 0.1717, step time: 1.5346\n",
      "357/388, train_loss: 0.2131, step time: 1.5357\n",
      "358/388, train_loss: 0.0749, step time: 1.5378\n",
      "359/388, train_loss: 0.1234, step time: 1.5366\n",
      "360/388, train_loss: 0.1202, step time: 1.5333\n",
      "361/388, train_loss: 0.1772, step time: 1.5307\n",
      "362/388, train_loss: 0.1337, step time: 1.5335\n",
      "363/388, train_loss: 0.1356, step time: 1.5318\n",
      "364/388, train_loss: 0.2029, step time: 1.5393\n",
      "365/388, train_loss: 0.1196, step time: 1.5377\n",
      "366/388, train_loss: 0.1203, step time: 1.5319\n",
      "367/388, train_loss: 0.0793, step time: 1.5329\n",
      "368/388, train_loss: 0.0601, step time: 1.5362\n",
      "369/388, train_loss: 0.2946, step time: 1.5397\n",
      "370/388, train_loss: 0.0915, step time: 1.5342\n",
      "371/388, train_loss: 0.3444, step time: 1.5347\n",
      "372/388, train_loss: 0.1526, step time: 1.5347\n",
      "373/388, train_loss: 0.1671, step time: 1.5369\n",
      "374/388, train_loss: 0.0319, step time: 1.5373\n",
      "375/388, train_loss: 0.1133, step time: 1.5372\n",
      "376/388, train_loss: 0.2190, step time: 1.5354\n",
      "377/388, train_loss: 0.1417, step time: 1.5352\n",
      "378/388, train_loss: 0.2543, step time: 1.5315\n",
      "379/388, train_loss: 0.3097, step time: 1.5345\n",
      "380/388, train_loss: 0.1184, step time: 1.5395\n",
      "381/388, train_loss: 0.1249, step time: 1.5379\n",
      "382/388, train_loss: 0.3274, step time: 1.5374\n",
      "383/388, train_loss: 0.2038, step time: 1.5332\n",
      "384/388, train_loss: 0.0357, step time: 1.5395\n",
      "385/388, train_loss: 0.1981, step time: 1.5345\n",
      "386/388, train_loss: 0.1449, step time: 1.5363\n",
      "387/388, train_loss: 0.1041, step time: 1.5350\n",
      "388/388, train_loss: 0.0894, step time: 1.5301\n",
      "epoch 70 average loss: 0.1671\n",
      "current epoch: 70 current mean dice: 0.7697 tc: 0.8182 wt: 0.8997 et: 0.5912\n",
      "best mean dice: 0.7785 at epoch: 68\n",
      "time consuming of epoch 70 is: 702.4911\n",
      "----------\n",
      "epoch 71/100\n",
      "1/388, train_loss: 0.0470, step time: 1.5473\n",
      "2/388, train_loss: 0.2265, step time: 1.5355\n",
      "3/388, train_loss: 0.0968, step time: 1.5326\n",
      "4/388, train_loss: 0.1672, step time: 1.5348\n",
      "5/388, train_loss: 0.1488, step time: 1.5402\n",
      "6/388, train_loss: 0.2918, step time: 1.5326\n",
      "7/388, train_loss: 0.0630, step time: 1.5350\n",
      "8/388, train_loss: 0.1219, step time: 1.5329\n",
      "9/388, train_loss: 0.1830, step time: 1.5324\n",
      "10/388, train_loss: 0.2121, step time: 1.5352\n",
      "11/388, train_loss: 0.3322, step time: 1.5324\n",
      "12/388, train_loss: 0.0835, step time: 1.5364\n",
      "13/388, train_loss: 0.2472, step time: 1.5357\n",
      "14/388, train_loss: 0.0265, step time: 1.5343\n",
      "15/388, train_loss: 0.1354, step time: 1.5333\n",
      "16/388, train_loss: 0.1794, step time: 1.5316\n",
      "17/388, train_loss: 0.0769, step time: 1.5324\n",
      "18/388, train_loss: 0.2610, step time: 1.5331\n",
      "19/388, train_loss: 0.7484, step time: 1.5353\n",
      "20/388, train_loss: 0.1202, step time: 1.5351\n",
      "21/388, train_loss: 0.5330, step time: 1.5376\n",
      "22/388, train_loss: 0.1399, step time: 1.5326\n",
      "23/388, train_loss: 0.1543, step time: 1.5345\n",
      "24/388, train_loss: 0.1339, step time: 1.5323\n",
      "25/388, train_loss: 0.1608, step time: 1.5371\n",
      "26/388, train_loss: 0.0777, step time: 1.5372\n",
      "27/388, train_loss: 0.2471, step time: 1.5309\n",
      "28/388, train_loss: 0.1092, step time: 1.5334\n",
      "29/388, train_loss: 0.1610, step time: 1.5324\n",
      "30/388, train_loss: 0.1915, step time: 1.5321\n",
      "31/388, train_loss: 0.1697, step time: 1.5548\n",
      "32/388, train_loss: 0.0744, step time: 1.5375\n",
      "33/388, train_loss: 0.0954, step time: 1.5349\n",
      "34/388, train_loss: 0.0944, step time: 1.5333\n",
      "35/388, train_loss: 0.1441, step time: 1.5359\n",
      "36/388, train_loss: 0.3269, step time: 1.5369\n",
      "37/388, train_loss: 0.0511, step time: 1.5374\n",
      "38/388, train_loss: 0.0884, step time: 1.5374\n",
      "39/388, train_loss: 0.2271, step time: 1.5320\n",
      "40/388, train_loss: 0.1877, step time: 1.5306\n",
      "41/388, train_loss: 0.2394, step time: 1.5352\n",
      "42/388, train_loss: 0.1901, step time: 1.5392\n",
      "43/388, train_loss: 0.0704, step time: 1.5357\n",
      "44/388, train_loss: 0.1102, step time: 1.5355\n",
      "45/388, train_loss: 0.2489, step time: 1.5319\n",
      "46/388, train_loss: 0.1556, step time: 1.5426\n",
      "47/388, train_loss: 0.2925, step time: 1.5432\n",
      "48/388, train_loss: 0.1622, step time: 1.5420\n",
      "49/388, train_loss: 0.2132, step time: 1.5356\n",
      "50/388, train_loss: 0.2376, step time: 1.5314\n",
      "51/388, train_loss: 0.0920, step time: 1.5325\n",
      "52/388, train_loss: 0.1184, step time: 1.5340\n",
      "53/388, train_loss: 0.0462, step time: 1.5373\n",
      "54/388, train_loss: 0.3958, step time: 1.5387\n",
      "55/388, train_loss: 0.3549, step time: 1.5345\n",
      "56/388, train_loss: 0.0846, step time: 1.5343\n",
      "57/388, train_loss: 0.4635, step time: 1.5335\n",
      "58/388, train_loss: 0.3003, step time: 1.5363\n",
      "59/388, train_loss: 0.0660, step time: 1.5331\n",
      "60/388, train_loss: 0.1952, step time: 1.5352\n",
      "61/388, train_loss: 0.3908, step time: 1.5352\n",
      "62/388, train_loss: 0.3186, step time: 1.5308\n",
      "63/388, train_loss: 0.1750, step time: 1.5319\n",
      "64/388, train_loss: 0.1042, step time: 1.5342\n",
      "65/388, train_loss: 0.0743, step time: 1.5353\n",
      "66/388, train_loss: 0.0773, step time: 1.5357\n",
      "67/388, train_loss: 0.1114, step time: 1.5384\n",
      "68/388, train_loss: 0.2766, step time: 1.5345\n",
      "69/388, train_loss: 0.0927, step time: 1.5342\n",
      "70/388, train_loss: 0.1033, step time: 1.5338\n",
      "71/388, train_loss: 0.1277, step time: 1.5331\n",
      "72/388, train_loss: 0.2138, step time: 1.5357\n",
      "73/388, train_loss: 0.1056, step time: 1.5378\n",
      "74/388, train_loss: 0.1439, step time: 1.5367\n",
      "75/388, train_loss: 0.0614, step time: 1.5331\n",
      "76/388, train_loss: 0.0836, step time: 1.5322\n",
      "77/388, train_loss: 0.0897, step time: 1.5307\n",
      "78/388, train_loss: 0.1105, step time: 1.5301\n",
      "79/388, train_loss: 0.0825, step time: 1.5422\n",
      "80/388, train_loss: 0.0992, step time: 1.5321\n",
      "81/388, train_loss: 0.1600, step time: 1.5320\n",
      "82/388, train_loss: 0.0800, step time: 1.5341\n",
      "83/388, train_loss: 0.2072, step time: 1.5357\n",
      "84/388, train_loss: 0.1720, step time: 1.5387\n",
      "85/388, train_loss: 0.1197, step time: 1.5411\n",
      "86/388, train_loss: 0.0989, step time: 1.5347\n",
      "87/388, train_loss: 0.1625, step time: 1.5341\n",
      "88/388, train_loss: 0.1044, step time: 1.5349\n",
      "89/388, train_loss: 0.1325, step time: 1.5353\n",
      "90/388, train_loss: 0.2443, step time: 1.5342\n",
      "91/388, train_loss: 0.1109, step time: 1.5323\n",
      "92/388, train_loss: 0.1048, step time: 1.5346\n",
      "93/388, train_loss: 0.2175, step time: 1.5320\n",
      "94/388, train_loss: 0.0896, step time: 1.5414\n",
      "95/388, train_loss: 0.1958, step time: 1.5353\n",
      "96/388, train_loss: 0.0615, step time: 1.5357\n",
      "97/388, train_loss: 0.0739, step time: 1.5301\n",
      "98/388, train_loss: 0.0755, step time: 1.5336\n",
      "99/388, train_loss: 0.1313, step time: 1.5311\n",
      "100/388, train_loss: 0.1805, step time: 1.5340\n",
      "101/388, train_loss: 0.1734, step time: 1.5342\n",
      "102/388, train_loss: 0.0948, step time: 1.5361\n",
      "103/388, train_loss: 0.1514, step time: 1.5305\n",
      "104/388, train_loss: 0.0388, step time: 1.5358\n",
      "105/388, train_loss: 0.0958, step time: 1.5370\n",
      "106/388, train_loss: 0.3753, step time: 1.5375\n",
      "107/388, train_loss: 0.2272, step time: 1.5335\n",
      "108/388, train_loss: 0.1561, step time: 1.5323\n",
      "109/388, train_loss: 0.3161, step time: 1.5335\n",
      "110/388, train_loss: 0.2109, step time: 1.5366\n",
      "111/388, train_loss: 0.1460, step time: 1.5369\n",
      "112/388, train_loss: 0.1804, step time: 1.5345\n",
      "113/388, train_loss: 0.1980, step time: 1.5331\n",
      "114/388, train_loss: 0.2480, step time: 1.5323\n",
      "115/388, train_loss: 0.0662, step time: 1.5329\n",
      "116/388, train_loss: 0.1211, step time: 1.5355\n",
      "117/388, train_loss: 0.0840, step time: 1.5356\n",
      "118/388, train_loss: 0.1079, step time: 1.5370\n",
      "119/388, train_loss: 0.2465, step time: 1.5336\n",
      "120/388, train_loss: 0.2412, step time: 1.5328\n",
      "121/388, train_loss: 0.1436, step time: 1.5313\n",
      "122/388, train_loss: 0.2299, step time: 1.5376\n",
      "123/388, train_loss: 0.0846, step time: 1.5353\n",
      "124/388, train_loss: 0.2486, step time: 1.5355\n",
      "125/388, train_loss: 0.1341, step time: 1.5365\n",
      "126/388, train_loss: 0.0582, step time: 1.5331\n",
      "127/388, train_loss: 0.0828, step time: 1.5331\n",
      "128/388, train_loss: 0.0725, step time: 1.5302\n",
      "129/388, train_loss: 0.0491, step time: 1.5331\n",
      "130/388, train_loss: 0.0777, step time: 1.5331\n",
      "131/388, train_loss: 0.2798, step time: 1.5367\n",
      "132/388, train_loss: 0.2355, step time: 1.5327\n",
      "133/388, train_loss: 0.2053, step time: 1.5334\n",
      "134/388, train_loss: 0.1150, step time: 1.5343\n",
      "135/388, train_loss: 0.1405, step time: 1.5320\n",
      "136/388, train_loss: 0.2047, step time: 1.5339\n",
      "137/388, train_loss: 0.0455, step time: 1.5382\n",
      "138/388, train_loss: 0.1015, step time: 1.5345\n",
      "139/388, train_loss: 0.1386, step time: 1.5467\n",
      "140/388, train_loss: 0.1547, step time: 1.5322\n",
      "141/388, train_loss: 0.1353, step time: 1.5348\n",
      "142/388, train_loss: 0.1856, step time: 1.5347\n",
      "143/388, train_loss: 0.2040, step time: 1.5350\n",
      "144/388, train_loss: 0.1379, step time: 1.5355\n",
      "145/388, train_loss: 0.0839, step time: 1.5331\n",
      "146/388, train_loss: 0.1264, step time: 1.5332\n",
      "147/388, train_loss: 0.2086, step time: 1.5319\n",
      "148/388, train_loss: 0.2152, step time: 1.5341\n",
      "149/388, train_loss: 0.4155, step time: 1.5334\n",
      "150/388, train_loss: 0.2379, step time: 1.5340\n",
      "151/388, train_loss: 0.1287, step time: 1.5356\n",
      "152/388, train_loss: 0.0792, step time: 1.5334\n",
      "153/388, train_loss: 0.1534, step time: 1.5353\n",
      "154/388, train_loss: 0.5919, step time: 1.5353\n",
      "155/388, train_loss: 0.0507, step time: 1.5335\n",
      "156/388, train_loss: 0.1650, step time: 1.5307\n",
      "157/388, train_loss: 0.0957, step time: 1.5369\n",
      "158/388, train_loss: 0.2344, step time: 1.5346\n",
      "159/388, train_loss: 0.1485, step time: 1.5366\n",
      "160/388, train_loss: 0.2611, step time: 1.5306\n",
      "161/388, train_loss: 0.0621, step time: 1.5302\n",
      "162/388, train_loss: 0.3080, step time: 1.5331\n",
      "163/388, train_loss: 0.2371, step time: 1.5320\n",
      "164/388, train_loss: 0.2408, step time: 1.5426\n",
      "165/388, train_loss: 0.0658, step time: 1.5357\n",
      "166/388, train_loss: 0.1177, step time: 1.5343\n",
      "167/388, train_loss: 0.0922, step time: 1.5315\n",
      "168/388, train_loss: 0.0688, step time: 1.5347\n",
      "169/388, train_loss: 0.1482, step time: 1.5333\n",
      "170/388, train_loss: 0.0997, step time: 1.5356\n",
      "171/388, train_loss: 0.0969, step time: 1.5345\n",
      "172/388, train_loss: 0.1279, step time: 1.5436\n",
      "173/388, train_loss: 0.1355, step time: 1.5383\n",
      "174/388, train_loss: 0.1519, step time: 1.5356\n",
      "175/388, train_loss: 0.2272, step time: 1.5354\n",
      "176/388, train_loss: 0.2290, step time: 1.5339\n",
      "177/388, train_loss: 0.0835, step time: 1.5334\n",
      "178/388, train_loss: 0.0995, step time: 1.5293\n",
      "179/388, train_loss: 0.0680, step time: 1.5316\n",
      "180/388, train_loss: 0.1763, step time: 1.5322\n",
      "181/388, train_loss: 0.0679, step time: 1.5379\n",
      "182/388, train_loss: 0.1237, step time: 1.5368\n",
      "183/388, train_loss: 0.1875, step time: 1.5357\n",
      "184/388, train_loss: 0.0519, step time: 1.5337\n",
      "185/388, train_loss: 0.1882, step time: 1.5307\n",
      "186/388, train_loss: 0.0764, step time: 1.5350\n",
      "187/388, train_loss: 0.0651, step time: 1.5351\n",
      "188/388, train_loss: 0.1467, step time: 1.5376\n",
      "189/388, train_loss: 0.2447, step time: 1.5361\n",
      "190/388, train_loss: 0.0902, step time: 1.5347\n",
      "191/388, train_loss: 0.2057, step time: 1.5351\n",
      "192/388, train_loss: 0.1541, step time: 1.5313\n",
      "193/388, train_loss: 0.1440, step time: 1.5332\n",
      "194/388, train_loss: 0.1920, step time: 1.5324\n",
      "195/388, train_loss: 0.1898, step time: 1.5359\n",
      "196/388, train_loss: 0.1428, step time: 1.5342\n",
      "197/388, train_loss: 0.2008, step time: 1.5301\n",
      "198/388, train_loss: 0.0954, step time: 1.5324\n",
      "199/388, train_loss: 0.0769, step time: 1.5341\n",
      "200/388, train_loss: 0.1463, step time: 1.5317\n",
      "201/388, train_loss: 0.2725, step time: 1.5355\n",
      "202/388, train_loss: 0.0952, step time: 1.5370\n",
      "203/388, train_loss: 0.2116, step time: 1.5318\n",
      "204/388, train_loss: 0.1539, step time: 1.5290\n",
      "205/388, train_loss: 0.1517, step time: 1.5324\n",
      "206/388, train_loss: 0.3823, step time: 1.5414\n",
      "207/388, train_loss: 0.1073, step time: 1.5371\n",
      "208/388, train_loss: 0.2174, step time: 1.5359\n",
      "209/388, train_loss: 0.1157, step time: 1.5341\n",
      "210/388, train_loss: 0.0989, step time: 1.5350\n",
      "211/388, train_loss: 0.1184, step time: 1.5328\n",
      "212/388, train_loss: 0.1797, step time: 1.5306\n",
      "213/388, train_loss: 0.2734, step time: 1.5376\n",
      "214/388, train_loss: 0.0654, step time: 1.5354\n",
      "215/388, train_loss: 0.2206, step time: 1.5328\n",
      "216/388, train_loss: 0.2130, step time: 1.5338\n",
      "217/388, train_loss: 0.1185, step time: 1.5332\n",
      "218/388, train_loss: 0.4326, step time: 1.5352\n",
      "219/388, train_loss: 0.1370, step time: 1.5347\n",
      "220/388, train_loss: 0.2074, step time: 1.5368\n",
      "221/388, train_loss: 0.1561, step time: 1.5303\n",
      "222/388, train_loss: 0.0564, step time: 1.5344\n",
      "223/388, train_loss: 0.2272, step time: 1.5338\n",
      "224/388, train_loss: 0.1140, step time: 1.5329\n",
      "225/388, train_loss: 0.1967, step time: 1.5382\n",
      "226/388, train_loss: 0.2071, step time: 1.5352\n",
      "227/388, train_loss: 0.1674, step time: 1.5345\n",
      "228/388, train_loss: 0.1508, step time: 1.5345\n",
      "229/388, train_loss: 0.0929, step time: 1.5371\n",
      "230/388, train_loss: 0.1934, step time: 1.5313\n",
      "231/388, train_loss: 0.0310, step time: 1.5323\n",
      "232/388, train_loss: 0.1595, step time: 1.5334\n",
      "233/388, train_loss: 0.0768, step time: 1.5338\n",
      "234/388, train_loss: 0.1002, step time: 1.5345\n",
      "235/388, train_loss: 0.0967, step time: 1.5343\n",
      "236/388, train_loss: 0.3636, step time: 1.5348\n",
      "237/388, train_loss: 0.3708, step time: 1.5328\n",
      "238/388, train_loss: 0.1433, step time: 1.5298\n",
      "239/388, train_loss: 0.1078, step time: 1.5304\n",
      "240/388, train_loss: 0.1563, step time: 1.5448\n",
      "241/388, train_loss: 0.1586, step time: 1.5399\n",
      "242/388, train_loss: 0.1481, step time: 1.5379\n",
      "243/388, train_loss: 0.1582, step time: 1.5340\n",
      "244/388, train_loss: 0.1774, step time: 1.5301\n",
      "245/388, train_loss: 0.1977, step time: 1.5328\n",
      "246/388, train_loss: 0.1627, step time: 1.5345\n",
      "247/388, train_loss: 0.1649, step time: 1.5347\n",
      "248/388, train_loss: 0.0563, step time: 1.5360\n",
      "249/388, train_loss: 0.1497, step time: 1.5346\n",
      "250/388, train_loss: 0.2595, step time: 1.5322\n",
      "251/388, train_loss: 0.2221, step time: 1.5290\n",
      "252/388, train_loss: 0.0302, step time: 1.5319\n",
      "253/388, train_loss: 0.1679, step time: 1.5352\n",
      "254/388, train_loss: 0.1696, step time: 1.5350\n",
      "255/388, train_loss: 0.0835, step time: 1.5344\n",
      "256/388, train_loss: 0.0845, step time: 1.5313\n",
      "257/388, train_loss: 0.0845, step time: 1.5315\n",
      "258/388, train_loss: 0.2454, step time: 1.5307\n",
      "259/388, train_loss: 0.2716, step time: 1.5315\n",
      "260/388, train_loss: 0.1287, step time: 1.5323\n",
      "261/388, train_loss: 0.2189, step time: 1.5362\n",
      "262/388, train_loss: 0.2957, step time: 1.5347\n",
      "263/388, train_loss: 0.0842, step time: 1.5340\n",
      "264/388, train_loss: 0.0921, step time: 1.5329\n",
      "265/388, train_loss: 0.0618, step time: 1.5347\n",
      "266/388, train_loss: 0.1515, step time: 1.5302\n",
      "267/388, train_loss: 0.2734, step time: 1.5341\n",
      "268/388, train_loss: 0.1647, step time: 1.5369\n",
      "269/388, train_loss: 0.2515, step time: 1.5352\n",
      "270/388, train_loss: 0.1079, step time: 1.5324\n",
      "271/388, train_loss: 0.1428, step time: 1.5325\n",
      "272/388, train_loss: 0.1889, step time: 1.5357\n",
      "273/388, train_loss: 0.2884, step time: 1.5358\n",
      "274/388, train_loss: 0.3118, step time: 1.5354\n",
      "275/388, train_loss: 0.3538, step time: 1.5368\n",
      "276/388, train_loss: 0.0825, step time: 1.5321\n",
      "277/388, train_loss: 0.1212, step time: 1.5327\n",
      "278/388, train_loss: 0.2050, step time: 1.5329\n",
      "279/388, train_loss: 0.3960, step time: 1.5323\n",
      "280/388, train_loss: 0.0506, step time: 1.5306\n",
      "281/388, train_loss: 0.2085, step time: 1.5314\n",
      "282/388, train_loss: 0.1851, step time: 1.5316\n",
      "283/388, train_loss: 0.1524, step time: 1.5319\n",
      "284/388, train_loss: 0.1261, step time: 1.5381\n",
      "285/388, train_loss: 0.1645, step time: 1.5358\n",
      "286/388, train_loss: 0.0918, step time: 1.5350\n",
      "287/388, train_loss: 0.0923, step time: 1.5320\n",
      "288/388, train_loss: 0.1020, step time: 1.5303\n",
      "289/388, train_loss: 0.3754, step time: 1.5312\n",
      "290/388, train_loss: 0.1455, step time: 1.5341\n",
      "291/388, train_loss: 0.2182, step time: 1.5369\n",
      "292/388, train_loss: 0.2697, step time: 1.5326\n",
      "293/388, train_loss: 0.1373, step time: 1.5427\n",
      "294/388, train_loss: 0.2234, step time: 1.5323\n",
      "295/388, train_loss: 0.1333, step time: 1.5343\n",
      "296/388, train_loss: 0.0934, step time: 1.5375\n",
      "297/388, train_loss: 0.1126, step time: 1.5327\n",
      "298/388, train_loss: 0.0687, step time: 1.5456\n",
      "299/388, train_loss: 0.1266, step time: 1.5289\n",
      "300/388, train_loss: 0.1085, step time: 1.5325\n",
      "301/388, train_loss: 0.1043, step time: 1.5321\n",
      "302/388, train_loss: 0.1976, step time: 1.5438\n",
      "303/388, train_loss: 0.1693, step time: 1.5402\n",
      "304/388, train_loss: 0.2998, step time: 1.5357\n",
      "305/388, train_loss: 0.1303, step time: 1.5319\n",
      "306/388, train_loss: 0.0433, step time: 1.5410\n",
      "307/388, train_loss: 0.0805, step time: 1.5352\n",
      "308/388, train_loss: 0.1367, step time: 1.5369\n",
      "309/388, train_loss: 0.1593, step time: 1.5357\n",
      "310/388, train_loss: 0.0873, step time: 1.5340\n",
      "311/388, train_loss: 0.1740, step time: 1.5347\n",
      "312/388, train_loss: 0.2012, step time: 1.5303\n",
      "313/388, train_loss: 0.1259, step time: 1.5377\n",
      "314/388, train_loss: 0.1204, step time: 1.5335\n",
      "315/388, train_loss: 0.0959, step time: 1.5355\n",
      "316/388, train_loss: 0.1854, step time: 1.5347\n",
      "317/388, train_loss: 0.2383, step time: 1.5333\n",
      "318/388, train_loss: 0.1442, step time: 1.5342\n",
      "319/388, train_loss: 0.1932, step time: 1.5381\n",
      "320/388, train_loss: 0.1844, step time: 1.5358\n",
      "321/388, train_loss: 0.1039, step time: 1.5316\n",
      "322/388, train_loss: 0.1504, step time: 1.5327\n",
      "323/388, train_loss: 0.1597, step time: 1.5295\n",
      "324/388, train_loss: 0.1835, step time: 1.5337\n",
      "325/388, train_loss: 0.0683, step time: 1.5380\n",
      "326/388, train_loss: 0.0792, step time: 1.5355\n",
      "327/388, train_loss: 0.3123, step time: 1.5389\n",
      "328/388, train_loss: 0.2023, step time: 1.5345\n",
      "329/388, train_loss: 0.2612, step time: 1.5321\n",
      "330/388, train_loss: 0.1546, step time: 1.5322\n",
      "331/388, train_loss: 0.1673, step time: 1.5326\n",
      "332/388, train_loss: 0.1426, step time: 1.5456\n",
      "333/388, train_loss: 0.0478, step time: 1.5395\n",
      "334/388, train_loss: 0.0992, step time: 1.5332\n",
      "335/388, train_loss: 0.5074, step time: 1.5304\n",
      "336/388, train_loss: 0.0979, step time: 1.5335\n",
      "337/388, train_loss: 0.1082, step time: 1.5362\n",
      "338/388, train_loss: 0.1673, step time: 1.5370\n",
      "339/388, train_loss: 0.1393, step time: 1.5356\n",
      "340/388, train_loss: 0.1748, step time: 1.5322\n",
      "341/388, train_loss: 0.2319, step time: 1.5331\n",
      "342/388, train_loss: 0.1915, step time: 1.5357\n",
      "343/388, train_loss: 0.1140, step time: 1.5336\n",
      "344/388, train_loss: 0.1040, step time: 1.5360\n",
      "345/388, train_loss: 0.0893, step time: 1.5337\n",
      "346/388, train_loss: 0.1020, step time: 1.5321\n",
      "347/388, train_loss: 0.1248, step time: 1.5331\n",
      "348/388, train_loss: 0.2099, step time: 1.5299\n",
      "349/388, train_loss: 0.1953, step time: 1.5293\n",
      "350/388, train_loss: 0.1371, step time: 1.5378\n",
      "351/388, train_loss: 0.2660, step time: 1.5358\n",
      "352/388, train_loss: 0.0944, step time: 1.5362\n",
      "353/388, train_loss: 0.2508, step time: 1.5339\n",
      "354/388, train_loss: 0.1864, step time: 1.5315\n",
      "355/388, train_loss: 0.0875, step time: 1.5338\n",
      "356/388, train_loss: 0.1936, step time: 1.5339\n",
      "357/388, train_loss: 0.3422, step time: 1.5403\n",
      "358/388, train_loss: 0.1433, step time: 1.5338\n",
      "359/388, train_loss: 0.1601, step time: 1.5333\n",
      "360/388, train_loss: 0.0651, step time: 1.5329\n",
      "361/388, train_loss: 0.1976, step time: 1.5369\n",
      "362/388, train_loss: 0.0949, step time: 1.5335\n",
      "363/388, train_loss: 0.2633, step time: 1.5343\n",
      "364/388, train_loss: 0.0951, step time: 1.5349\n",
      "365/388, train_loss: 0.1181, step time: 1.5320\n",
      "366/388, train_loss: 0.0938, step time: 1.5301\n",
      "367/388, train_loss: 0.2301, step time: 1.5391\n",
      "368/388, train_loss: 0.4467, step time: 1.5386\n",
      "369/388, train_loss: 0.1935, step time: 1.5305\n",
      "370/388, train_loss: 0.0681, step time: 1.5371\n",
      "371/388, train_loss: 0.3274, step time: 1.5497\n",
      "372/388, train_loss: 0.1623, step time: 1.5310\n",
      "373/388, train_loss: 0.2812, step time: 1.5330\n",
      "374/388, train_loss: 0.0768, step time: 1.5324\n",
      "375/388, train_loss: 0.2110, step time: 1.5366\n",
      "376/388, train_loss: 0.0586, step time: 1.5472\n",
      "377/388, train_loss: 0.2113, step time: 1.5310\n",
      "378/388, train_loss: 0.0902, step time: 1.5324\n",
      "379/388, train_loss: 0.0902, step time: 1.5296\n",
      "380/388, train_loss: 0.1796, step time: 1.5356\n",
      "381/388, train_loss: 0.1362, step time: 1.5353\n",
      "382/388, train_loss: 0.4396, step time: 1.5344\n",
      "383/388, train_loss: 0.0536, step time: 1.5325\n",
      "384/388, train_loss: 0.1051, step time: 1.5340\n",
      "385/388, train_loss: 0.2123, step time: 1.5335\n",
      "386/388, train_loss: 0.2159, step time: 1.5378\n",
      "387/388, train_loss: 0.1046, step time: 1.5370\n",
      "388/388, train_loss: 0.2542, step time: 1.5318\n",
      "epoch 71 average loss: 0.1656\n",
      "current epoch: 71 current mean dice: 0.7774 tc: 0.8257 wt: 0.9020 et: 0.6046\n",
      "best mean dice: 0.7785 at epoch: 68\n",
      "time consuming of epoch 71 is: 703.2625\n",
      "----------\n",
      "epoch 72/100\n",
      "1/388, train_loss: 0.1477, step time: 1.5459\n",
      "2/388, train_loss: 0.2296, step time: 1.5365\n",
      "3/388, train_loss: 0.1055, step time: 1.5391\n",
      "4/388, train_loss: 0.2154, step time: 1.5353\n",
      "5/388, train_loss: 0.1339, step time: 1.5319\n",
      "6/388, train_loss: 0.1353, step time: 1.5349\n",
      "7/388, train_loss: 0.1311, step time: 1.5351\n",
      "8/388, train_loss: 0.1560, step time: 1.5387\n",
      "9/388, train_loss: 0.2062, step time: 1.5290\n",
      "10/388, train_loss: 0.1911, step time: 1.5335\n",
      "11/388, train_loss: 0.1383, step time: 1.5319\n",
      "12/388, train_loss: 0.0685, step time: 1.5310\n",
      "13/388, train_loss: 0.1091, step time: 1.5323\n",
      "14/388, train_loss: 0.3559, step time: 1.5380\n",
      "15/388, train_loss: 0.1673, step time: 1.5334\n",
      "16/388, train_loss: 0.1407, step time: 1.5349\n",
      "17/388, train_loss: 0.1372, step time: 1.5305\n",
      "18/388, train_loss: 0.2594, step time: 1.5320\n",
      "19/388, train_loss: 0.0565, step time: 1.5323\n",
      "20/388, train_loss: 0.2871, step time: 1.5348\n",
      "21/388, train_loss: 0.3569, step time: 1.5351\n",
      "22/388, train_loss: 0.1908, step time: 1.5324\n",
      "23/388, train_loss: 0.2007, step time: 1.5369\n",
      "24/388, train_loss: 0.3040, step time: 1.5315\n",
      "25/388, train_loss: 0.0824, step time: 1.5333\n",
      "26/388, train_loss: 0.2726, step time: 1.5289\n",
      "27/388, train_loss: 0.1181, step time: 1.5349\n",
      "28/388, train_loss: 0.1671, step time: 1.5357\n",
      "29/388, train_loss: 0.1677, step time: 1.5357\n",
      "30/388, train_loss: 0.1386, step time: 1.5335\n",
      "31/388, train_loss: 0.0727, step time: 1.5335\n",
      "32/388, train_loss: 0.1032, step time: 1.5333\n",
      "33/388, train_loss: 0.1453, step time: 1.5365\n",
      "34/388, train_loss: 0.3538, step time: 1.5353\n",
      "35/388, train_loss: 0.1283, step time: 1.5370\n",
      "36/388, train_loss: 0.1009, step time: 1.5391\n",
      "37/388, train_loss: 0.1680, step time: 1.5325\n",
      "38/388, train_loss: 0.3322, step time: 1.5347\n",
      "39/388, train_loss: 0.0762, step time: 1.5338\n",
      "40/388, train_loss: 0.5295, step time: 1.5391\n",
      "41/388, train_loss: 0.4865, step time: 1.5361\n",
      "42/388, train_loss: 0.2438, step time: 1.5394\n",
      "43/388, train_loss: 0.0649, step time: 1.5375\n",
      "44/388, train_loss: 0.1114, step time: 1.5343\n",
      "45/388, train_loss: 0.2252, step time: 1.5491\n",
      "46/388, train_loss: 0.1026, step time: 1.5365\n",
      "47/388, train_loss: 0.1126, step time: 1.5354\n",
      "48/388, train_loss: 0.0646, step time: 1.5340\n",
      "49/388, train_loss: 0.1597, step time: 1.5292\n",
      "50/388, train_loss: 0.1664, step time: 1.5319\n",
      "51/388, train_loss: 0.0769, step time: 1.5329\n",
      "52/388, train_loss: 0.2635, step time: 1.5375\n",
      "53/388, train_loss: 0.2517, step time: 1.5357\n",
      "54/388, train_loss: 0.0403, step time: 1.5352\n",
      "55/388, train_loss: 0.1466, step time: 1.5338\n",
      "56/388, train_loss: 0.0930, step time: 1.5304\n",
      "57/388, train_loss: 0.2109, step time: 1.5328\n",
      "58/388, train_loss: 0.1444, step time: 1.5313\n",
      "59/388, train_loss: 0.1197, step time: 1.5430\n",
      "60/388, train_loss: 0.1006, step time: 1.5378\n",
      "61/388, train_loss: 0.0553, step time: 1.5640\n",
      "62/388, train_loss: 0.2492, step time: 1.5319\n",
      "63/388, train_loss: 0.1883, step time: 1.5374\n",
      "64/388, train_loss: 0.0794, step time: 1.5329\n",
      "65/388, train_loss: 0.3615, step time: 1.5286\n",
      "66/388, train_loss: 0.1057, step time: 1.5328\n",
      "67/388, train_loss: 0.0640, step time: 1.5362\n",
      "68/388, train_loss: 0.3490, step time: 1.5374\n",
      "69/388, train_loss: 0.1311, step time: 1.5354\n",
      "70/388, train_loss: 0.1245, step time: 1.5327\n",
      "71/388, train_loss: 0.1334, step time: 1.5319\n",
      "72/388, train_loss: 0.1468, step time: 1.5330\n",
      "73/388, train_loss: 0.1760, step time: 1.5359\n",
      "74/388, train_loss: 0.2596, step time: 1.5326\n",
      "75/388, train_loss: 0.2354, step time: 1.5352\n",
      "76/388, train_loss: 0.0925, step time: 1.5316\n",
      "77/388, train_loss: 0.1992, step time: 1.5315\n",
      "78/388, train_loss: 0.0976, step time: 1.5343\n",
      "79/388, train_loss: 0.2129, step time: 1.5466\n",
      "80/388, train_loss: 0.1357, step time: 1.5357\n",
      "81/388, train_loss: 0.2354, step time: 1.5325\n",
      "82/388, train_loss: 0.1925, step time: 1.5346\n",
      "83/388, train_loss: 0.1260, step time: 1.5299\n",
      "84/388, train_loss: 0.1112, step time: 1.5404\n",
      "85/388, train_loss: 0.2499, step time: 1.5354\n",
      "86/388, train_loss: 0.0919, step time: 1.5326\n",
      "87/388, train_loss: 0.1906, step time: 1.5290\n",
      "88/388, train_loss: 0.1533, step time: 1.5317\n",
      "89/388, train_loss: 0.0922, step time: 1.5314\n",
      "90/388, train_loss: 0.4210, step time: 1.5365\n",
      "91/388, train_loss: 0.0775, step time: 1.5365\n",
      "92/388, train_loss: 0.1667, step time: 1.5322\n",
      "93/388, train_loss: 0.1137, step time: 1.5344\n",
      "94/388, train_loss: 0.2632, step time: 1.5293\n",
      "95/388, train_loss: 0.0798, step time: 1.5384\n",
      "96/388, train_loss: 0.2599, step time: 1.5351\n",
      "97/388, train_loss: 0.1053, step time: 1.5384\n",
      "98/388, train_loss: 0.0856, step time: 1.5331\n",
      "99/388, train_loss: 0.2014, step time: 1.5324\n",
      "100/388, train_loss: 0.0829, step time: 1.5368\n",
      "101/388, train_loss: 0.1765, step time: 1.5371\n",
      "102/388, train_loss: 0.2673, step time: 1.5370\n",
      "103/388, train_loss: 0.1111, step time: 1.5320\n",
      "104/388, train_loss: 0.1749, step time: 1.5328\n",
      "105/388, train_loss: 0.0382, step time: 1.5298\n",
      "106/388, train_loss: 0.1248, step time: 1.5307\n",
      "107/388, train_loss: 0.1241, step time: 1.5343\n",
      "108/388, train_loss: 0.1674, step time: 1.5355\n",
      "109/388, train_loss: 0.1090, step time: 1.5331\n",
      "110/388, train_loss: 0.1790, step time: 1.5342\n",
      "111/388, train_loss: 0.1178, step time: 1.5307\n",
      "112/388, train_loss: 0.1055, step time: 1.5351\n",
      "113/388, train_loss: 0.2923, step time: 1.5352\n",
      "114/388, train_loss: 0.2217, step time: 1.5358\n",
      "115/388, train_loss: 0.0986, step time: 1.5325\n",
      "116/388, train_loss: 0.0704, step time: 1.5325\n",
      "117/388, train_loss: 0.0902, step time: 1.5351\n",
      "118/388, train_loss: 0.0880, step time: 1.5350\n",
      "119/388, train_loss: 0.0991, step time: 1.5371\n",
      "120/388, train_loss: 0.0479, step time: 1.5356\n",
      "121/388, train_loss: 0.1978, step time: 1.5340\n",
      "122/388, train_loss: 0.2204, step time: 1.5317\n",
      "123/388, train_loss: 0.1687, step time: 1.5313\n",
      "124/388, train_loss: 0.4129, step time: 1.5330\n",
      "125/388, train_loss: 0.3418, step time: 1.5363\n",
      "126/388, train_loss: 0.1288, step time: 1.5358\n",
      "127/388, train_loss: 0.0881, step time: 1.5335\n",
      "128/388, train_loss: 0.0980, step time: 1.5324\n",
      "129/388, train_loss: 0.1998, step time: 1.5300\n",
      "130/388, train_loss: 0.3563, step time: 1.5329\n",
      "131/388, train_loss: 0.0863, step time: 1.5352\n",
      "132/388, train_loss: 0.2106, step time: 1.5384\n",
      "133/388, train_loss: 0.1044, step time: 1.5311\n",
      "134/388, train_loss: 0.2540, step time: 1.5300\n",
      "135/388, train_loss: 0.0848, step time: 1.5319\n",
      "136/388, train_loss: 0.1666, step time: 1.5328\n",
      "137/388, train_loss: 0.0882, step time: 1.5321\n",
      "138/388, train_loss: 0.1737, step time: 1.5473\n",
      "139/388, train_loss: 0.1203, step time: 1.5363\n",
      "140/388, train_loss: 0.1334, step time: 1.5348\n",
      "141/388, train_loss: 0.0868, step time: 1.5327\n",
      "142/388, train_loss: 0.2492, step time: 1.5309\n",
      "143/388, train_loss: 0.4137, step time: 1.5367\n",
      "144/388, train_loss: 0.0585, step time: 1.5406\n",
      "145/388, train_loss: 0.2936, step time: 1.5363\n",
      "146/388, train_loss: 0.1790, step time: 1.5346\n",
      "147/388, train_loss: 0.1462, step time: 1.5315\n",
      "148/388, train_loss: 0.0826, step time: 1.5326\n",
      "149/388, train_loss: 0.4749, step time: 1.5336\n",
      "150/388, train_loss: 0.2522, step time: 1.5357\n",
      "151/388, train_loss: 0.1211, step time: 1.5420\n",
      "152/388, train_loss: 0.1006, step time: 1.5323\n",
      "153/388, train_loss: 0.0719, step time: 1.5315\n",
      "154/388, train_loss: 0.1061, step time: 1.5329\n",
      "155/388, train_loss: 0.3955, step time: 1.5325\n",
      "156/388, train_loss: 0.3115, step time: 1.5337\n",
      "157/388, train_loss: 0.1009, step time: 1.5359\n",
      "158/388, train_loss: 0.1584, step time: 1.5332\n",
      "159/388, train_loss: 0.1007, step time: 1.5290\n",
      "160/388, train_loss: 0.1170, step time: 1.5327\n",
      "161/388, train_loss: 0.1141, step time: 1.5332\n",
      "162/388, train_loss: 0.1414, step time: 1.5327\n",
      "163/388, train_loss: 0.2093, step time: 1.5365\n",
      "164/388, train_loss: 0.0932, step time: 1.5360\n",
      "165/388, train_loss: 0.1476, step time: 1.5351\n",
      "166/388, train_loss: 0.0436, step time: 1.5324\n",
      "167/388, train_loss: 0.2221, step time: 1.5332\n",
      "168/388, train_loss: 0.1334, step time: 1.5340\n",
      "169/388, train_loss: 0.1206, step time: 1.5316\n",
      "170/388, train_loss: 0.2750, step time: 1.5357\n",
      "171/388, train_loss: 0.1375, step time: 1.5389\n",
      "172/388, train_loss: 0.0805, step time: 1.5300\n",
      "173/388, train_loss: 0.2105, step time: 1.5317\n",
      "174/388, train_loss: 0.5967, step time: 1.5329\n",
      "175/388, train_loss: 0.0823, step time: 1.5316\n",
      "176/388, train_loss: 0.1762, step time: 1.5340\n",
      "177/388, train_loss: 0.2586, step time: 1.5357\n",
      "178/388, train_loss: 0.1311, step time: 1.5372\n",
      "179/388, train_loss: 0.2347, step time: 1.5358\n",
      "180/388, train_loss: 0.2323, step time: 1.5313\n",
      "181/388, train_loss: 0.0596, step time: 1.5316\n",
      "182/388, train_loss: 0.1981, step time: 1.5314\n",
      "183/388, train_loss: 0.1495, step time: 1.5363\n",
      "184/388, train_loss: 0.2057, step time: 1.5365\n",
      "185/388, train_loss: 0.2949, step time: 1.5318\n",
      "186/388, train_loss: 0.2251, step time: 1.5322\n",
      "187/388, train_loss: 0.2147, step time: 1.5292\n",
      "188/388, train_loss: 0.1584, step time: 1.5335\n",
      "189/388, train_loss: 0.0937, step time: 1.5372\n",
      "190/388, train_loss: 0.0455, step time: 1.5472\n",
      "191/388, train_loss: 0.0794, step time: 1.5321\n",
      "192/388, train_loss: 0.1929, step time: 1.5302\n",
      "193/388, train_loss: 0.0443, step time: 1.5306\n",
      "194/388, train_loss: 0.4243, step time: 1.5372\n",
      "195/388, train_loss: 0.1768, step time: 1.5359\n",
      "196/388, train_loss: 0.0891, step time: 1.5365\n",
      "197/388, train_loss: 0.1002, step time: 1.5323\n",
      "198/388, train_loss: 0.1586, step time: 1.5296\n",
      "199/388, train_loss: 0.0947, step time: 1.5319\n",
      "200/388, train_loss: 0.1577, step time: 1.5335\n",
      "201/388, train_loss: 0.1598, step time: 1.5372\n",
      "202/388, train_loss: 0.3304, step time: 1.5326\n",
      "203/388, train_loss: 0.2184, step time: 1.5319\n",
      "204/388, train_loss: 0.2401, step time: 1.5340\n",
      "205/388, train_loss: 0.0493, step time: 1.5324\n",
      "206/388, train_loss: 0.1280, step time: 1.5389\n",
      "207/388, train_loss: 0.1366, step time: 1.5326\n",
      "208/388, train_loss: 0.1204, step time: 1.5332\n",
      "209/388, train_loss: 0.1166, step time: 1.5368\n",
      "210/388, train_loss: 0.2139, step time: 1.5326\n",
      "211/388, train_loss: 0.1806, step time: 1.5307\n",
      "212/388, train_loss: 0.0984, step time: 1.5320\n",
      "213/388, train_loss: 0.2999, step time: 1.5335\n",
      "214/388, train_loss: 0.1278, step time: 1.5371\n",
      "215/388, train_loss: 0.0830, step time: 1.5363\n",
      "216/388, train_loss: 0.0898, step time: 1.5324\n",
      "217/388, train_loss: 0.0774, step time: 1.5322\n",
      "218/388, train_loss: 0.0982, step time: 1.5339\n",
      "219/388, train_loss: 0.1342, step time: 1.5330\n",
      "220/388, train_loss: 0.0749, step time: 1.5346\n",
      "221/388, train_loss: 0.4585, step time: 1.5356\n",
      "222/388, train_loss: 0.0829, step time: 1.5354\n",
      "223/388, train_loss: 0.1635, step time: 1.5335\n",
      "224/388, train_loss: 0.2710, step time: 1.5302\n",
      "225/388, train_loss: 0.1947, step time: 1.5330\n",
      "226/388, train_loss: 0.1915, step time: 1.5346\n",
      "227/388, train_loss: 0.0834, step time: 1.5350\n",
      "228/388, train_loss: 0.0754, step time: 1.5333\n",
      "229/388, train_loss: 0.2471, step time: 1.5301\n",
      "230/388, train_loss: 0.2517, step time: 1.5292\n",
      "231/388, train_loss: 0.0682, step time: 1.5312\n",
      "232/388, train_loss: 0.2466, step time: 1.5324\n",
      "233/388, train_loss: 0.1546, step time: 1.5337\n",
      "234/388, train_loss: 0.1091, step time: 1.5355\n",
      "235/388, train_loss: 0.1347, step time: 1.5377\n",
      "236/388, train_loss: 0.1994, step time: 1.5322\n",
      "237/388, train_loss: 0.1952, step time: 1.5366\n",
      "238/388, train_loss: 0.1619, step time: 1.5304\n",
      "239/388, train_loss: 0.1241, step time: 1.5327\n",
      "240/388, train_loss: 0.0546, step time: 1.5334\n",
      "241/388, train_loss: 0.2048, step time: 1.5328\n",
      "242/388, train_loss: 0.1596, step time: 1.5340\n",
      "243/388, train_loss: 0.0880, step time: 1.5300\n",
      "244/388, train_loss: 0.0751, step time: 1.5319\n",
      "245/388, train_loss: 0.2820, step time: 1.5343\n",
      "246/388, train_loss: 0.0506, step time: 1.5326\n",
      "247/388, train_loss: 0.1440, step time: 1.5355\n",
      "248/388, train_loss: 0.2472, step time: 1.5363\n",
      "249/388, train_loss: 0.0580, step time: 1.5431\n",
      "250/388, train_loss: 0.1243, step time: 1.5331\n",
      "251/388, train_loss: 0.2298, step time: 1.5330\n",
      "252/388, train_loss: 0.1480, step time: 1.5387\n",
      "253/388, train_loss: 0.1686, step time: 1.5355\n",
      "254/388, train_loss: 0.1774, step time: 1.5344\n",
      "255/388, train_loss: 0.0927, step time: 1.5337\n",
      "256/388, train_loss: 0.1334, step time: 1.5598\n",
      "257/388, train_loss: 0.0796, step time: 1.5357\n",
      "258/388, train_loss: 0.1749, step time: 1.5309\n",
      "259/388, train_loss: 0.2109, step time: 1.5299\n",
      "260/388, train_loss: 0.2290, step time: 1.5364\n",
      "261/388, train_loss: 0.1625, step time: 1.5350\n",
      "262/388, train_loss: 0.0528, step time: 1.5316\n",
      "263/388, train_loss: 0.1808, step time: 1.5330\n",
      "264/388, train_loss: 0.0803, step time: 1.5539\n",
      "265/388, train_loss: 0.2448, step time: 1.5339\n",
      "266/388, train_loss: 0.0961, step time: 1.5362\n",
      "267/388, train_loss: 0.0653, step time: 1.5342\n",
      "268/388, train_loss: 0.0918, step time: 1.5330\n",
      "269/388, train_loss: 0.1325, step time: 1.5371\n",
      "270/388, train_loss: 0.1911, step time: 1.5435\n",
      "271/388, train_loss: 0.0544, step time: 1.5372\n",
      "272/388, train_loss: 0.1294, step time: 1.5334\n",
      "273/388, train_loss: 0.1601, step time: 1.5300\n",
      "274/388, train_loss: 0.1469, step time: 1.5304\n",
      "275/388, train_loss: 0.2020, step time: 1.5328\n",
      "276/388, train_loss: 0.2383, step time: 1.5336\n",
      "277/388, train_loss: 0.1478, step time: 1.5366\n",
      "278/388, train_loss: 0.1840, step time: 1.5375\n",
      "279/388, train_loss: 0.1818, step time: 1.5338\n",
      "280/388, train_loss: 0.2663, step time: 1.5321\n",
      "281/388, train_loss: 0.1999, step time: 1.5344\n",
      "282/388, train_loss: 0.0886, step time: 1.5307\n",
      "283/388, train_loss: 0.1030, step time: 1.5337\n",
      "284/388, train_loss: 0.1571, step time: 1.5338\n",
      "285/388, train_loss: 0.1054, step time: 1.5429\n",
      "286/388, train_loss: 0.1438, step time: 1.5334\n",
      "287/388, train_loss: 0.2117, step time: 1.5387\n",
      "288/388, train_loss: 0.0760, step time: 1.5304\n",
      "289/388, train_loss: 0.1827, step time: 1.5381\n",
      "290/388, train_loss: 0.1087, step time: 1.5344\n",
      "291/388, train_loss: 0.2616, step time: 1.5347\n",
      "292/388, train_loss: 0.1655, step time: 1.5351\n",
      "293/388, train_loss: 0.0812, step time: 1.5311\n",
      "294/388, train_loss: 0.0916, step time: 1.5307\n",
      "295/388, train_loss: 0.1834, step time: 1.5289\n",
      "296/388, train_loss: 0.1665, step time: 1.5340\n",
      "297/388, train_loss: 0.3048, step time: 1.5338\n",
      "298/388, train_loss: 0.1047, step time: 1.5354\n",
      "299/388, train_loss: 0.2293, step time: 1.5377\n",
      "300/388, train_loss: 0.1719, step time: 1.5341\n",
      "301/388, train_loss: 0.0779, step time: 1.5345\n",
      "302/388, train_loss: 0.1030, step time: 1.5305\n",
      "303/388, train_loss: 0.2108, step time: 1.5353\n",
      "304/388, train_loss: 0.1063, step time: 1.5372\n",
      "305/388, train_loss: 0.0810, step time: 1.5336\n",
      "306/388, train_loss: 0.1862, step time: 1.5343\n",
      "307/388, train_loss: 0.0803, step time: 1.5301\n",
      "308/388, train_loss: 0.0670, step time: 1.5363\n",
      "309/388, train_loss: 0.0832, step time: 1.5401\n",
      "310/388, train_loss: 0.1975, step time: 1.5332\n",
      "311/388, train_loss: 0.1249, step time: 1.5303\n",
      "312/388, train_loss: 0.2358, step time: 1.5332\n",
      "313/388, train_loss: 0.1664, step time: 1.5297\n",
      "314/388, train_loss: 0.3022, step time: 1.5333\n",
      "315/388, train_loss: 0.0737, step time: 1.5310\n",
      "316/388, train_loss: 0.2175, step time: 1.5301\n",
      "317/388, train_loss: 0.1273, step time: 1.5294\n",
      "318/388, train_loss: 0.2253, step time: 1.5312\n",
      "319/388, train_loss: 0.0873, step time: 1.5356\n",
      "320/388, train_loss: 0.2190, step time: 1.5318\n",
      "321/388, train_loss: 0.1719, step time: 1.5329\n",
      "322/388, train_loss: 0.1000, step time: 1.5365\n",
      "323/388, train_loss: 0.0676, step time: 1.5324\n",
      "324/388, train_loss: 0.1128, step time: 1.5339\n",
      "325/388, train_loss: 0.1272, step time: 1.5354\n",
      "326/388, train_loss: 0.2684, step time: 1.5374\n",
      "327/388, train_loss: 0.1465, step time: 1.5335\n",
      "328/388, train_loss: 0.1670, step time: 1.5364\n",
      "329/388, train_loss: 0.0843, step time: 1.5340\n",
      "330/388, train_loss: 0.1180, step time: 1.5290\n",
      "331/388, train_loss: 0.0874, step time: 1.5326\n",
      "332/388, train_loss: 0.1242, step time: 1.5354\n",
      "333/388, train_loss: 0.0558, step time: 1.5348\n",
      "334/388, train_loss: 0.3476, step time: 1.5323\n",
      "335/388, train_loss: 0.1811, step time: 1.5336\n",
      "336/388, train_loss: 0.3474, step time: 1.5317\n",
      "337/388, train_loss: 0.1455, step time: 1.5320\n",
      "338/388, train_loss: 0.2684, step time: 1.5350\n",
      "339/388, train_loss: 0.3570, step time: 1.5372\n",
      "340/388, train_loss: 0.1760, step time: 1.5330\n",
      "341/388, train_loss: 0.1052, step time: 1.5332\n",
      "342/388, train_loss: 0.0982, step time: 1.5354\n",
      "343/388, train_loss: 0.0424, step time: 1.5335\n",
      "344/388, train_loss: 0.3195, step time: 1.5356\n",
      "345/388, train_loss: 0.4395, step time: 1.5334\n",
      "346/388, train_loss: 0.0269, step time: 1.5353\n",
      "347/388, train_loss: 0.1791, step time: 1.5326\n",
      "348/388, train_loss: 0.1245, step time: 1.5365\n",
      "349/388, train_loss: 0.2669, step time: 1.5354\n",
      "350/388, train_loss: 0.2447, step time: 1.5372\n",
      "351/388, train_loss: 0.0317, step time: 1.5334\n",
      "352/388, train_loss: 0.2084, step time: 1.5333\n",
      "353/388, train_loss: 0.0667, step time: 1.5310\n",
      "354/388, train_loss: 0.1926, step time: 1.5355\n",
      "355/388, train_loss: 0.1800, step time: 1.5358\n",
      "356/388, train_loss: 0.0781, step time: 1.5489\n",
      "357/388, train_loss: 0.2364, step time: 1.5333\n",
      "358/388, train_loss: 0.1587, step time: 1.5334\n",
      "359/388, train_loss: 0.1490, step time: 1.5347\n",
      "360/388, train_loss: 0.1050, step time: 1.5368\n",
      "361/388, train_loss: 0.0727, step time: 1.5334\n",
      "362/388, train_loss: 0.0874, step time: 1.5318\n",
      "363/388, train_loss: 0.0873, step time: 1.5302\n",
      "364/388, train_loss: 0.1948, step time: 1.5307\n",
      "365/388, train_loss: 0.1021, step time: 1.5386\n",
      "366/388, train_loss: 0.1673, step time: 1.5561\n",
      "367/388, train_loss: 0.1097, step time: 1.5317\n",
      "368/388, train_loss: 0.4002, step time: 1.5328\n",
      "369/388, train_loss: 0.1877, step time: 1.5318\n",
      "370/388, train_loss: 0.0989, step time: 1.5355\n",
      "371/388, train_loss: 0.0840, step time: 1.5356\n",
      "372/388, train_loss: 0.3115, step time: 1.5364\n",
      "373/388, train_loss: 0.1963, step time: 1.5334\n",
      "374/388, train_loss: 0.1822, step time: 1.5331\n",
      "375/388, train_loss: 0.1855, step time: 1.5330\n",
      "376/388, train_loss: 0.1240, step time: 1.5425\n",
      "377/388, train_loss: 0.2384, step time: 1.5362\n",
      "378/388, train_loss: 0.1015, step time: 1.5367\n",
      "379/388, train_loss: 0.0907, step time: 1.5353\n",
      "380/388, train_loss: 0.3139, step time: 1.5310\n",
      "381/388, train_loss: 0.2208, step time: 1.5312\n",
      "382/388, train_loss: 0.1542, step time: 1.5325\n",
      "383/388, train_loss: 0.2002, step time: 1.5309\n",
      "384/388, train_loss: 0.1867, step time: 1.5365\n",
      "385/388, train_loss: 0.1662, step time: 1.5339\n",
      "386/388, train_loss: 0.0835, step time: 1.5349\n",
      "387/388, train_loss: 0.0319, step time: 1.5320\n",
      "388/388, train_loss: 0.1836, step time: 1.5310\n",
      "epoch 72 average loss: 0.1653\n",
      "current epoch: 72 current mean dice: 0.7740 tc: 0.8237 wt: 0.9030 et: 0.5954\n",
      "best mean dice: 0.7785 at epoch: 68\n",
      "time consuming of epoch 72 is: 702.0754\n",
      "----------\n",
      "epoch 73/100\n",
      "1/388, train_loss: 0.1156, step time: 1.5440\n",
      "2/388, train_loss: 0.3356, step time: 1.5369\n",
      "3/388, train_loss: 0.1968, step time: 1.5386\n",
      "4/388, train_loss: 0.0872, step time: 1.5390\n",
      "5/388, train_loss: 0.1509, step time: 1.5368\n",
      "6/388, train_loss: 0.2253, step time: 1.5361\n",
      "7/388, train_loss: 0.1770, step time: 1.5328\n",
      "8/388, train_loss: 0.1266, step time: 1.5340\n",
      "9/388, train_loss: 0.1692, step time: 1.5334\n",
      "10/388, train_loss: 0.1376, step time: 1.5350\n",
      "11/388, train_loss: 0.1460, step time: 1.5365\n",
      "12/388, train_loss: 0.1512, step time: 1.5336\n",
      "13/388, train_loss: 0.1874, step time: 1.5329\n",
      "14/388, train_loss: 0.0581, step time: 1.5354\n",
      "15/388, train_loss: 0.1967, step time: 1.5346\n",
      "16/388, train_loss: 0.0900, step time: 1.5339\n",
      "17/388, train_loss: 0.2631, step time: 1.5334\n",
      "18/388, train_loss: 0.1959, step time: 1.5465\n",
      "19/388, train_loss: 0.0707, step time: 1.5359\n",
      "20/388, train_loss: 0.1316, step time: 1.5356\n",
      "21/388, train_loss: 0.0879, step time: 1.5327\n",
      "22/388, train_loss: 0.2556, step time: 1.5317\n",
      "23/388, train_loss: 0.2239, step time: 1.5371\n",
      "24/388, train_loss: 0.3890, step time: 1.5371\n",
      "25/388, train_loss: 0.1274, step time: 1.5502\n",
      "26/388, train_loss: 0.1984, step time: 1.5359\n",
      "27/388, train_loss: 0.2015, step time: 1.5351\n",
      "28/388, train_loss: 0.4236, step time: 1.5359\n",
      "29/388, train_loss: 0.0914, step time: 1.5335\n",
      "30/388, train_loss: 0.1395, step time: 1.5353\n",
      "31/388, train_loss: 0.1227, step time: 1.5367\n",
      "32/388, train_loss: 0.1168, step time: 1.5383\n",
      "33/388, train_loss: 0.0366, step time: 1.5334\n",
      "34/388, train_loss: 0.1991, step time: 1.5316\n",
      "35/388, train_loss: 0.0256, step time: 1.5319\n",
      "36/388, train_loss: 0.0723, step time: 1.5375\n",
      "37/388, train_loss: 0.2583, step time: 1.5381\n",
      "38/388, train_loss: 0.1027, step time: 1.5330\n",
      "39/388, train_loss: 0.1570, step time: 1.5291\n",
      "40/388, train_loss: 0.3129, step time: 1.5303\n",
      "41/388, train_loss: 0.2110, step time: 1.5331\n",
      "42/388, train_loss: 0.3033, step time: 1.5318\n",
      "43/388, train_loss: 0.0669, step time: 1.5338\n",
      "44/388, train_loss: 0.0693, step time: 1.5326\n",
      "45/388, train_loss: 0.2011, step time: 1.5335\n",
      "46/388, train_loss: 0.1408, step time: 1.5349\n",
      "47/388, train_loss: 0.1166, step time: 1.5317\n",
      "48/388, train_loss: 0.2646, step time: 1.5302\n",
      "49/388, train_loss: 0.1737, step time: 1.5370\n",
      "50/388, train_loss: 0.0914, step time: 1.5421\n",
      "51/388, train_loss: 0.1523, step time: 1.5410\n",
      "52/388, train_loss: 0.2601, step time: 1.5345\n",
      "53/388, train_loss: 0.0435, step time: 1.5341\n",
      "54/388, train_loss: 0.0707, step time: 1.5325\n",
      "55/388, train_loss: 0.1289, step time: 1.5312\n",
      "56/388, train_loss: 0.2152, step time: 1.5341\n",
      "57/388, train_loss: 0.1635, step time: 1.5351\n",
      "58/388, train_loss: 0.1460, step time: 1.5346\n",
      "59/388, train_loss: 0.2270, step time: 1.5367\n",
      "60/388, train_loss: 0.1861, step time: 1.5312\n",
      "61/388, train_loss: 0.0652, step time: 1.5325\n",
      "62/388, train_loss: 0.1950, step time: 1.5306\n",
      "63/388, train_loss: 0.0893, step time: 1.5319\n",
      "64/388, train_loss: 0.1773, step time: 1.5358\n",
      "65/388, train_loss: 0.1111, step time: 1.5342\n",
      "66/388, train_loss: 0.0554, step time: 1.5371\n",
      "67/388, train_loss: 0.0964, step time: 1.5331\n",
      "68/388, train_loss: 0.1733, step time: 1.5336\n",
      "69/388, train_loss: 0.1087, step time: 1.5332\n",
      "70/388, train_loss: 0.1313, step time: 1.5608\n",
      "71/388, train_loss: 0.1394, step time: 1.5323\n",
      "72/388, train_loss: 0.0419, step time: 1.5305\n",
      "73/388, train_loss: 0.1118, step time: 1.5318\n",
      "74/388, train_loss: 0.0895, step time: 1.5362\n",
      "75/388, train_loss: 0.2548, step time: 1.5363\n",
      "76/388, train_loss: 0.0999, step time: 1.5318\n",
      "77/388, train_loss: 0.1206, step time: 1.5318\n",
      "78/388, train_loss: 0.1561, step time: 1.5325\n",
      "79/388, train_loss: 0.1308, step time: 1.5316\n",
      "80/388, train_loss: 0.1805, step time: 1.5338\n",
      "81/388, train_loss: 0.0973, step time: 1.5344\n",
      "82/388, train_loss: 0.2097, step time: 1.5383\n",
      "83/388, train_loss: 0.0883, step time: 1.5338\n",
      "84/388, train_loss: 0.0446, step time: 1.5328\n",
      "85/388, train_loss: 0.1557, step time: 1.5344\n",
      "86/388, train_loss: 0.0904, step time: 1.5322\n",
      "87/388, train_loss: 0.1317, step time: 1.5297\n",
      "88/388, train_loss: 0.0422, step time: 1.5325\n",
      "89/388, train_loss: 0.2625, step time: 1.5325\n",
      "90/388, train_loss: 0.2191, step time: 1.5327\n",
      "91/388, train_loss: 0.2784, step time: 1.5383\n",
      "92/388, train_loss: 0.1836, step time: 1.5373\n",
      "93/388, train_loss: 0.5849, step time: 1.5338\n",
      "94/388, train_loss: 0.1756, step time: 1.5345\n",
      "95/388, train_loss: 0.0729, step time: 1.5384\n",
      "96/388, train_loss: 0.2290, step time: 1.5380\n",
      "97/388, train_loss: 0.2357, step time: 1.5308\n",
      "98/388, train_loss: 0.1553, step time: 1.5321\n",
      "99/388, train_loss: 0.1044, step time: 1.5315\n",
      "100/388, train_loss: 0.0656, step time: 1.5330\n",
      "101/388, train_loss: 0.0844, step time: 1.5338\n",
      "102/388, train_loss: 0.1686, step time: 1.5427\n",
      "103/388, train_loss: 0.1184, step time: 1.5324\n",
      "104/388, train_loss: 0.1548, step time: 1.5345\n",
      "105/388, train_loss: 0.0902, step time: 1.5339\n",
      "106/388, train_loss: 0.0922, step time: 1.5314\n",
      "107/388, train_loss: 0.1222, step time: 1.5356\n",
      "108/388, train_loss: 0.2171, step time: 1.5365\n",
      "109/388, train_loss: 0.1228, step time: 1.5333\n",
      "110/388, train_loss: 0.4270, step time: 1.5331\n",
      "111/388, train_loss: 0.2290, step time: 1.5331\n",
      "112/388, train_loss: 0.3009, step time: 1.5372\n",
      "113/388, train_loss: 0.1420, step time: 1.5353\n",
      "114/388, train_loss: 0.1478, step time: 1.5357\n",
      "115/388, train_loss: 0.2078, step time: 1.5350\n",
      "116/388, train_loss: 0.1239, step time: 1.5328\n",
      "117/388, train_loss: 0.0946, step time: 1.5325\n",
      "118/388, train_loss: 0.1024, step time: 1.5322\n",
      "119/388, train_loss: 0.1800, step time: 1.5359\n",
      "120/388, train_loss: 0.0699, step time: 1.5390\n",
      "121/388, train_loss: 0.3212, step time: 1.5342\n",
      "122/388, train_loss: 0.1313, step time: 1.5318\n",
      "123/388, train_loss: 0.0861, step time: 1.5298\n",
      "124/388, train_loss: 0.2014, step time: 1.5307\n",
      "125/388, train_loss: 0.0821, step time: 1.5347\n",
      "126/388, train_loss: 0.0943, step time: 1.5352\n",
      "127/388, train_loss: 0.2502, step time: 1.5343\n",
      "128/388, train_loss: 0.1305, step time: 1.5303\n",
      "129/388, train_loss: 0.0869, step time: 1.5317\n",
      "130/388, train_loss: 0.0869, step time: 1.5330\n",
      "131/388, train_loss: 0.1257, step time: 1.5342\n",
      "132/388, train_loss: 0.1597, step time: 1.5351\n",
      "133/388, train_loss: 0.2034, step time: 1.5316\n",
      "134/388, train_loss: 0.2101, step time: 1.5309\n",
      "135/388, train_loss: 0.1301, step time: 1.5329\n",
      "136/388, train_loss: 0.1340, step time: 1.5320\n",
      "137/388, train_loss: 0.0830, step time: 1.5368\n",
      "138/388, train_loss: 0.2974, step time: 1.5355\n",
      "139/388, train_loss: 0.3733, step time: 1.5372\n",
      "140/388, train_loss: 0.1374, step time: 1.5333\n",
      "141/388, train_loss: 0.2177, step time: 1.5333\n",
      "142/388, train_loss: 0.0962, step time: 1.5301\n",
      "143/388, train_loss: 0.1203, step time: 1.5325\n",
      "144/388, train_loss: 0.0339, step time: 1.5372\n",
      "145/388, train_loss: 0.0845, step time: 1.5379\n",
      "146/388, train_loss: 0.1902, step time: 1.5333\n",
      "147/388, train_loss: 0.1488, step time: 1.5573\n",
      "148/388, train_loss: 0.1635, step time: 1.5363\n",
      "149/388, train_loss: 0.1044, step time: 1.5361\n",
      "150/388, train_loss: 0.5245, step time: 1.5346\n",
      "151/388, train_loss: 0.0969, step time: 1.5347\n",
      "152/388, train_loss: 0.1373, step time: 1.5309\n",
      "153/388, train_loss: 0.3662, step time: 1.5310\n",
      "154/388, train_loss: 0.0983, step time: 1.5322\n",
      "155/388, train_loss: 0.0874, step time: 1.5335\n",
      "156/388, train_loss: 0.0830, step time: 1.5335\n",
      "157/388, train_loss: 0.1064, step time: 1.5354\n",
      "158/388, train_loss: 0.3452, step time: 1.5363\n",
      "159/388, train_loss: 0.1814, step time: 1.5344\n",
      "160/388, train_loss: 0.2631, step time: 1.5304\n",
      "161/388, train_loss: 0.0780, step time: 1.5310\n",
      "162/388, train_loss: 0.1015, step time: 1.5379\n",
      "163/388, train_loss: 0.0812, step time: 1.5356\n",
      "164/388, train_loss: 0.1139, step time: 1.5375\n",
      "165/388, train_loss: 0.0814, step time: 1.5321\n",
      "166/388, train_loss: 0.3022, step time: 1.5306\n",
      "167/388, train_loss: 0.4211, step time: 1.5334\n",
      "168/388, train_loss: 0.1212, step time: 1.5329\n",
      "169/388, train_loss: 0.1980, step time: 1.5351\n",
      "170/388, train_loss: 0.2525, step time: 1.5374\n",
      "171/388, train_loss: 0.0729, step time: 1.5297\n",
      "172/388, train_loss: 0.3205, step time: 1.5344\n",
      "173/388, train_loss: 0.2307, step time: 1.5349\n",
      "174/388, train_loss: 0.1187, step time: 1.5359\n",
      "175/388, train_loss: 0.0622, step time: 1.5371\n",
      "176/388, train_loss: 0.0971, step time: 1.5319\n",
      "177/388, train_loss: 0.1692, step time: 1.5443\n",
      "178/388, train_loss: 0.2388, step time: 1.5334\n",
      "179/388, train_loss: 0.1320, step time: 1.5381\n",
      "180/388, train_loss: 0.1033, step time: 1.5358\n",
      "181/388, train_loss: 0.1063, step time: 1.5334\n",
      "182/388, train_loss: 0.2011, step time: 1.5334\n",
      "183/388, train_loss: 0.2063, step time: 1.5315\n",
      "184/388, train_loss: 0.3151, step time: 1.5311\n",
      "185/388, train_loss: 0.1097, step time: 1.5446\n",
      "186/388, train_loss: 0.0933, step time: 1.5368\n",
      "187/388, train_loss: 0.1546, step time: 1.5348\n",
      "188/388, train_loss: 0.1033, step time: 1.5310\n",
      "189/388, train_loss: 0.2967, step time: 1.5329\n",
      "190/388, train_loss: 0.2525, step time: 1.5614\n",
      "191/388, train_loss: 0.1757, step time: 1.5342\n",
      "192/388, train_loss: 0.3732, step time: 1.5315\n",
      "193/388, train_loss: 0.0608, step time: 1.5320\n",
      "194/388, train_loss: 0.2128, step time: 1.5287\n",
      "195/388, train_loss: 0.3044, step time: 1.5329\n",
      "196/388, train_loss: 0.2340, step time: 1.5390\n",
      "197/388, train_loss: 0.1508, step time: 1.5357\n",
      "198/388, train_loss: 0.1777, step time: 1.5373\n",
      "199/388, train_loss: 0.4263, step time: 1.5328\n",
      "200/388, train_loss: 0.2083, step time: 1.5489\n",
      "201/388, train_loss: 0.0305, step time: 1.5348\n",
      "202/388, train_loss: 0.0659, step time: 1.5340\n",
      "203/388, train_loss: 0.1389, step time: 1.5301\n",
      "204/388, train_loss: 0.0799, step time: 1.5345\n",
      "205/388, train_loss: 0.1561, step time: 1.5315\n",
      "206/388, train_loss: 0.3258, step time: 1.5342\n",
      "207/388, train_loss: 0.0866, step time: 1.5333\n",
      "208/388, train_loss: 0.0952, step time: 1.5341\n",
      "209/388, train_loss: 0.1794, step time: 1.5315\n",
      "210/388, train_loss: 0.2232, step time: 1.5367\n",
      "211/388, train_loss: 0.1061, step time: 1.5378\n",
      "212/388, train_loss: 0.2447, step time: 1.5387\n",
      "213/388, train_loss: 0.2031, step time: 1.5340\n",
      "214/388, train_loss: 0.1873, step time: 1.5324\n",
      "215/388, train_loss: 0.2708, step time: 1.5288\n",
      "216/388, train_loss: 0.2420, step time: 1.5354\n",
      "217/388, train_loss: 0.1884, step time: 1.5385\n",
      "218/388, train_loss: 0.0780, step time: 1.5341\n",
      "219/388, train_loss: 0.2592, step time: 1.5335\n",
      "220/388, train_loss: 0.0710, step time: 1.5332\n",
      "221/388, train_loss: 0.1102, step time: 1.5607\n",
      "222/388, train_loss: 0.2978, step time: 1.5347\n",
      "223/388, train_loss: 0.1699, step time: 1.5319\n",
      "224/388, train_loss: 0.1216, step time: 1.5325\n",
      "225/388, train_loss: 0.3840, step time: 1.5346\n",
      "226/388, train_loss: 0.4021, step time: 1.5348\n",
      "227/388, train_loss: 0.0919, step time: 1.5349\n",
      "228/388, train_loss: 0.0942, step time: 1.5325\n",
      "229/388, train_loss: 0.0915, step time: 1.5311\n",
      "230/388, train_loss: 0.1173, step time: 1.5314\n",
      "231/388, train_loss: 0.0822, step time: 1.5354\n",
      "232/388, train_loss: 0.1574, step time: 1.5360\n",
      "233/388, train_loss: 0.0944, step time: 1.5363\n",
      "234/388, train_loss: 0.0557, step time: 1.5325\n",
      "235/388, train_loss: 0.1004, step time: 1.5330\n",
      "236/388, train_loss: 0.0662, step time: 1.5337\n",
      "237/388, train_loss: 0.1208, step time: 1.5329\n",
      "238/388, train_loss: 0.0483, step time: 1.5385\n",
      "239/388, train_loss: 0.0901, step time: 1.5366\n",
      "240/388, train_loss: 0.2789, step time: 1.5372\n",
      "241/388, train_loss: 0.2871, step time: 1.5336\n",
      "242/388, train_loss: 0.1609, step time: 1.5330\n",
      "243/388, train_loss: 0.2706, step time: 1.5293\n",
      "244/388, train_loss: 0.2713, step time: 1.5365\n",
      "245/388, train_loss: 0.0954, step time: 1.5371\n",
      "246/388, train_loss: 0.0972, step time: 1.5365\n",
      "247/388, train_loss: 0.3999, step time: 1.5326\n",
      "248/388, train_loss: 0.1430, step time: 1.5369\n",
      "249/388, train_loss: 0.1096, step time: 1.5340\n",
      "250/388, train_loss: 0.4277, step time: 1.5369\n",
      "251/388, train_loss: 0.0988, step time: 1.5316\n",
      "252/388, train_loss: 0.3015, step time: 1.5324\n",
      "253/388, train_loss: 0.1456, step time: 1.5281\n",
      "254/388, train_loss: 0.1627, step time: 1.5326\n",
      "255/388, train_loss: 0.0839, step time: 1.5345\n",
      "256/388, train_loss: 0.1640, step time: 1.5342\n",
      "257/388, train_loss: 0.0867, step time: 1.5357\n",
      "258/388, train_loss: 0.1510, step time: 1.5341\n",
      "259/388, train_loss: 0.1182, step time: 1.5322\n",
      "260/388, train_loss: 0.2932, step time: 1.5335\n",
      "261/388, train_loss: 0.1933, step time: 1.5316\n",
      "262/388, train_loss: 0.0826, step time: 1.5374\n",
      "263/388, train_loss: 0.0650, step time: 1.5349\n",
      "264/388, train_loss: 0.1425, step time: 1.5344\n",
      "265/388, train_loss: 0.1140, step time: 1.5358\n",
      "266/388, train_loss: 0.1245, step time: 1.5336\n",
      "267/388, train_loss: 0.0965, step time: 1.5328\n",
      "268/388, train_loss: 0.1686, step time: 1.5311\n",
      "269/388, train_loss: 0.1224, step time: 1.5360\n",
      "270/388, train_loss: 0.0962, step time: 1.5368\n",
      "271/388, train_loss: 0.0894, step time: 1.5367\n",
      "272/388, train_loss: 0.1862, step time: 1.5303\n",
      "273/388, train_loss: 0.2624, step time: 1.5329\n",
      "274/388, train_loss: 0.1670, step time: 1.5328\n",
      "275/388, train_loss: 0.2453, step time: 1.5338\n",
      "276/388, train_loss: 0.1487, step time: 1.5364\n",
      "277/388, train_loss: 0.0958, step time: 1.5351\n",
      "278/388, train_loss: 0.1463, step time: 1.5389\n",
      "279/388, train_loss: 0.3501, step time: 1.5315\n",
      "280/388, train_loss: 0.0967, step time: 1.5341\n",
      "281/388, train_loss: 0.4971, step time: 1.5352\n",
      "282/388, train_loss: 0.2602, step time: 1.5376\n",
      "283/388, train_loss: 0.2019, step time: 1.5345\n",
      "284/388, train_loss: 0.2469, step time: 1.5455\n",
      "285/388, train_loss: 0.1985, step time: 1.5342\n",
      "286/388, train_loss: 0.1172, step time: 1.5397\n",
      "287/388, train_loss: 0.2411, step time: 1.5336\n",
      "288/388, train_loss: 0.4661, step time: 1.5343\n",
      "289/388, train_loss: 0.0686, step time: 1.5325\n",
      "290/388, train_loss: 0.1432, step time: 1.5359\n",
      "291/388, train_loss: 0.2745, step time: 1.5377\n",
      "292/388, train_loss: 0.1205, step time: 1.5370\n",
      "293/388, train_loss: 0.2007, step time: 1.5321\n",
      "294/388, train_loss: 0.1991, step time: 1.5321\n",
      "295/388, train_loss: 0.2367, step time: 1.5325\n",
      "296/388, train_loss: 0.0624, step time: 1.5382\n",
      "297/388, train_loss: 0.1766, step time: 1.5346\n",
      "298/388, train_loss: 0.0816, step time: 1.5324\n",
      "299/388, train_loss: 0.0524, step time: 1.5322\n",
      "300/388, train_loss: 0.1389, step time: 1.5371\n",
      "301/388, train_loss: 0.1199, step time: 1.5366\n",
      "302/388, train_loss: 0.1346, step time: 1.5359\n",
      "303/388, train_loss: 0.2221, step time: 1.5332\n",
      "304/388, train_loss: 0.2076, step time: 1.5301\n",
      "305/388, train_loss: 0.0809, step time: 1.5337\n",
      "306/388, train_loss: 0.0690, step time: 1.5311\n",
      "307/388, train_loss: 0.1912, step time: 1.5351\n",
      "308/388, train_loss: 0.0668, step time: 1.5375\n",
      "309/388, train_loss: 0.3155, step time: 1.5360\n",
      "310/388, train_loss: 0.1329, step time: 1.5368\n",
      "311/388, train_loss: 0.1335, step time: 1.5326\n",
      "312/388, train_loss: 0.0948, step time: 1.5333\n",
      "313/388, train_loss: 0.2301, step time: 1.5357\n",
      "314/388, train_loss: 0.2234, step time: 1.5469\n",
      "315/388, train_loss: 0.0939, step time: 1.5312\n",
      "316/388, train_loss: 0.1324, step time: 1.5337\n",
      "317/388, train_loss: 0.0824, step time: 1.5344\n",
      "318/388, train_loss: 0.1479, step time: 1.5372\n",
      "319/388, train_loss: 0.2232, step time: 1.5350\n",
      "320/388, train_loss: 0.1762, step time: 1.5333\n",
      "321/388, train_loss: 0.0839, step time: 1.5338\n",
      "322/388, train_loss: 0.2696, step time: 1.5332\n",
      "323/388, train_loss: 0.1186, step time: 1.5356\n",
      "324/388, train_loss: 0.1958, step time: 1.5372\n",
      "325/388, train_loss: 0.2655, step time: 1.5352\n",
      "326/388, train_loss: 0.1527, step time: 1.5307\n",
      "327/388, train_loss: 0.1033, step time: 1.5299\n",
      "328/388, train_loss: 0.1449, step time: 1.5326\n",
      "329/388, train_loss: 0.1373, step time: 1.5321\n",
      "330/388, train_loss: 0.0667, step time: 1.5373\n",
      "331/388, train_loss: 0.0678, step time: 1.5364\n",
      "332/388, train_loss: 0.1879, step time: 1.5385\n",
      "333/388, train_loss: 0.2206, step time: 1.5368\n",
      "334/388, train_loss: 0.0684, step time: 1.5331\n",
      "335/388, train_loss: 0.2704, step time: 1.5339\n",
      "336/388, train_loss: 0.1896, step time: 1.5357\n",
      "337/388, train_loss: 0.0975, step time: 1.5356\n",
      "338/388, train_loss: 0.3350, step time: 1.5361\n",
      "339/388, train_loss: 0.1546, step time: 1.5317\n",
      "340/388, train_loss: 0.1620, step time: 1.5386\n",
      "341/388, train_loss: 0.1615, step time: 1.5336\n",
      "342/388, train_loss: 0.1516, step time: 1.5313\n",
      "343/388, train_loss: 0.0420, step time: 1.5330\n",
      "344/388, train_loss: 0.2008, step time: 1.5323\n",
      "345/388, train_loss: 0.1979, step time: 1.5372\n",
      "346/388, train_loss: 0.0593, step time: 1.5366\n",
      "347/388, train_loss: 0.1928, step time: 1.5331\n",
      "348/388, train_loss: 0.0416, step time: 1.5323\n",
      "349/388, train_loss: 0.1315, step time: 1.5318\n",
      "350/388, train_loss: 0.1986, step time: 1.5370\n",
      "351/388, train_loss: 0.2435, step time: 1.5365\n",
      "352/388, train_loss: 0.0967, step time: 1.5328\n",
      "353/388, train_loss: 0.0507, step time: 1.5340\n",
      "354/388, train_loss: 0.1217, step time: 1.5316\n",
      "355/388, train_loss: 0.1100, step time: 1.5374\n",
      "356/388, train_loss: 0.1139, step time: 1.5401\n",
      "357/388, train_loss: 0.4434, step time: 1.5325\n",
      "358/388, train_loss: 0.0742, step time: 1.5325\n",
      "359/388, train_loss: 0.0955, step time: 1.5342\n",
      "360/388, train_loss: 0.1257, step time: 1.5427\n",
      "361/388, train_loss: 0.1035, step time: 1.5351\n",
      "362/388, train_loss: 0.1751, step time: 1.5436\n",
      "363/388, train_loss: 0.0925, step time: 1.5459\n",
      "364/388, train_loss: 0.1397, step time: 1.5524\n",
      "365/388, train_loss: 0.4205, step time: 1.5322\n",
      "366/388, train_loss: 0.2506, step time: 1.5493\n",
      "367/388, train_loss: 0.1276, step time: 1.5472\n",
      "368/388, train_loss: 0.2262, step time: 1.5327\n",
      "369/388, train_loss: 0.1861, step time: 1.5527\n",
      "370/388, train_loss: 0.2224, step time: 1.5460\n",
      "371/388, train_loss: 0.2642, step time: 1.5349\n",
      "372/388, train_loss: 0.2733, step time: 1.5369\n",
      "373/388, train_loss: 0.1857, step time: 1.5368\n",
      "374/388, train_loss: 0.0632, step time: 1.5412\n",
      "375/388, train_loss: 0.0893, step time: 1.5308\n",
      "376/388, train_loss: 0.1697, step time: 1.5432\n",
      "377/388, train_loss: 0.2164, step time: 1.5306\n",
      "378/388, train_loss: 0.2647, step time: 1.5459\n",
      "379/388, train_loss: 0.0825, step time: 1.5381\n",
      "380/388, train_loss: 0.0924, step time: 1.5344\n",
      "381/388, train_loss: 0.1581, step time: 1.5328\n",
      "382/388, train_loss: 0.0459, step time: 1.5442\n",
      "383/388, train_loss: 0.0792, step time: 1.5394\n",
      "384/388, train_loss: 0.0970, step time: 1.5354\n",
      "385/388, train_loss: 0.1931, step time: 1.5379\n",
      "386/388, train_loss: 0.0649, step time: 1.5431\n",
      "387/388, train_loss: 0.1390, step time: 1.5342\n",
      "388/388, train_loss: 0.3155, step time: 1.5346\n",
      "epoch 73 average loss: 0.1659\n",
      "current epoch: 73 current mean dice: 0.7785 tc: 0.8256 wt: 0.9069 et: 0.6030\n",
      "best mean dice: 0.7785 at epoch: 68\n",
      "time consuming of epoch 73 is: 702.4725\n",
      "----------\n",
      "epoch 74/100\n",
      "1/388, train_loss: 0.0690, step time: 1.5514\n",
      "2/388, train_loss: 0.1034, step time: 1.5363\n",
      "3/388, train_loss: 0.1376, step time: 1.5370\n",
      "4/388, train_loss: 0.0647, step time: 1.5572\n",
      "5/388, train_loss: 0.1092, step time: 1.5360\n",
      "6/388, train_loss: 0.1952, step time: 1.5330\n",
      "7/388, train_loss: 0.3700, step time: 1.5334\n",
      "8/388, train_loss: 0.1039, step time: 1.5392\n",
      "9/388, train_loss: 0.1553, step time: 1.5312\n",
      "10/388, train_loss: 0.1994, step time: 1.5339\n",
      "11/388, train_loss: 0.0821, step time: 1.5315\n",
      "12/388, train_loss: 0.0753, step time: 1.5302\n",
      "13/388, train_loss: 0.1101, step time: 1.5410\n",
      "14/388, train_loss: 0.0989, step time: 1.5513\n",
      "15/388, train_loss: 0.1109, step time: 1.5375\n",
      "16/388, train_loss: 0.1830, step time: 1.5364\n",
      "17/388, train_loss: 0.1192, step time: 1.5331\n",
      "18/388, train_loss: 0.1204, step time: 1.5413\n",
      "19/388, train_loss: 0.2963, step time: 1.5355\n",
      "20/388, train_loss: 0.0688, step time: 1.5331\n",
      "21/388, train_loss: 0.1846, step time: 1.5366\n",
      "22/388, train_loss: 0.1621, step time: 1.5352\n",
      "23/388, train_loss: 0.1586, step time: 1.5351\n",
      "24/388, train_loss: 0.1161, step time: 1.5359\n",
      "25/388, train_loss: 0.1999, step time: 1.5340\n",
      "26/388, train_loss: 0.1504, step time: 1.5323\n",
      "27/388, train_loss: 0.1044, step time: 1.5317\n",
      "28/388, train_loss: 0.1124, step time: 1.5375\n",
      "29/388, train_loss: 0.0572, step time: 1.5350\n",
      "30/388, train_loss: 0.1902, step time: 1.5328\n",
      "31/388, train_loss: 0.0854, step time: 1.5320\n",
      "32/388, train_loss: 0.1176, step time: 1.5338\n",
      "33/388, train_loss: 0.1634, step time: 1.5337\n",
      "34/388, train_loss: 0.1744, step time: 1.5371\n",
      "35/388, train_loss: 0.1625, step time: 1.5376\n",
      "36/388, train_loss: 0.1783, step time: 1.5315\n",
      "37/388, train_loss: 0.0681, step time: 1.5321\n",
      "38/388, train_loss: 0.1234, step time: 1.5330\n",
      "39/388, train_loss: 0.0721, step time: 1.5340\n",
      "40/388, train_loss: 0.0784, step time: 1.5400\n",
      "41/388, train_loss: 0.2540, step time: 1.5367\n",
      "42/388, train_loss: 0.3068, step time: 1.5330\n",
      "43/388, train_loss: 0.2488, step time: 1.5348\n",
      "44/388, train_loss: 0.1366, step time: 1.5313\n",
      "45/388, train_loss: 0.1118, step time: 1.5347\n",
      "46/388, train_loss: 0.1976, step time: 1.5359\n",
      "47/388, train_loss: 0.1047, step time: 1.5419\n",
      "48/388, train_loss: 0.1505, step time: 1.5339\n",
      "49/388, train_loss: 0.1830, step time: 1.5319\n",
      "50/388, train_loss: 0.1544, step time: 1.5342\n",
      "51/388, train_loss: 0.1206, step time: 1.5315\n",
      "52/388, train_loss: 0.2046, step time: 1.5417\n",
      "53/388, train_loss: 0.0973, step time: 1.5366\n",
      "54/388, train_loss: 0.1256, step time: 1.5369\n",
      "55/388, train_loss: 0.1614, step time: 1.5332\n",
      "56/388, train_loss: 0.0447, step time: 1.5317\n",
      "57/388, train_loss: 0.0894, step time: 1.5324\n",
      "58/388, train_loss: 0.2199, step time: 1.5407\n",
      "59/388, train_loss: 0.1272, step time: 1.5392\n",
      "60/388, train_loss: 0.2065, step time: 1.5362\n",
      "61/388, train_loss: 0.0872, step time: 1.5327\n",
      "62/388, train_loss: 0.1529, step time: 1.5345\n",
      "63/388, train_loss: 0.1076, step time: 1.5329\n",
      "64/388, train_loss: 0.2538, step time: 1.5375\n",
      "65/388, train_loss: 0.0856, step time: 1.5463\n",
      "66/388, train_loss: 0.1976, step time: 1.5327\n",
      "67/388, train_loss: 0.2395, step time: 1.5317\n",
      "68/388, train_loss: 0.1457, step time: 1.5356\n",
      "69/388, train_loss: 0.0670, step time: 1.5361\n",
      "70/388, train_loss: 0.1318, step time: 1.5341\n",
      "71/388, train_loss: 0.0354, step time: 1.5334\n",
      "72/388, train_loss: 0.0814, step time: 1.5348\n",
      "73/388, train_loss: 0.0669, step time: 1.5297\n",
      "74/388, train_loss: 0.1554, step time: 1.5314\n",
      "75/388, train_loss: 0.0874, step time: 1.5363\n",
      "76/388, train_loss: 0.0982, step time: 1.5370\n",
      "77/388, train_loss: 0.1619, step time: 1.5329\n",
      "78/388, train_loss: 0.2521, step time: 1.5333\n",
      "79/388, train_loss: 0.0521, step time: 1.5350\n",
      "80/388, train_loss: 0.1368, step time: 1.5381\n",
      "81/388, train_loss: 0.2579, step time: 1.5354\n",
      "82/388, train_loss: 0.5080, step time: 1.5334\n",
      "83/388, train_loss: 0.1015, step time: 1.5318\n",
      "84/388, train_loss: 0.1455, step time: 1.5321\n",
      "85/388, train_loss: 0.2511, step time: 1.5315\n",
      "86/388, train_loss: 0.1062, step time: 1.5313\n",
      "87/388, train_loss: 0.1806, step time: 1.5395\n",
      "88/388, train_loss: 0.2135, step time: 1.5329\n",
      "89/388, train_loss: 0.2040, step time: 1.5337\n",
      "90/388, train_loss: 0.1463, step time: 1.5330\n",
      "91/388, train_loss: 0.2178, step time: 1.5347\n",
      "92/388, train_loss: 0.0660, step time: 1.5356\n",
      "93/388, train_loss: 0.0749, step time: 1.5347\n",
      "94/388, train_loss: 0.1001, step time: 1.5320\n",
      "95/388, train_loss: 0.0994, step time: 1.5346\n",
      "96/388, train_loss: 0.1693, step time: 1.5333\n",
      "97/388, train_loss: 0.2144, step time: 1.5352\n",
      "98/388, train_loss: 0.0918, step time: 1.5351\n",
      "99/388, train_loss: 0.1972, step time: 1.5349\n",
      "100/388, train_loss: 0.1276, step time: 1.5350\n",
      "101/388, train_loss: 0.3569, step time: 1.5293\n",
      "102/388, train_loss: 0.1789, step time: 1.5350\n",
      "103/388, train_loss: 0.2477, step time: 1.5326\n",
      "104/388, train_loss: 0.0633, step time: 1.5429\n",
      "105/388, train_loss: 0.1669, step time: 1.5370\n",
      "106/388, train_loss: 0.1831, step time: 1.5331\n",
      "107/388, train_loss: 0.1640, step time: 1.5318\n",
      "108/388, train_loss: 0.1725, step time: 1.5343\n",
      "109/388, train_loss: 0.2156, step time: 1.5342\n",
      "110/388, train_loss: 0.0831, step time: 1.5458\n",
      "111/388, train_loss: 0.1084, step time: 1.5311\n",
      "112/388, train_loss: 0.2227, step time: 1.5339\n",
      "113/388, train_loss: 0.3929, step time: 1.5338\n",
      "114/388, train_loss: 0.1636, step time: 1.5364\n",
      "115/388, train_loss: 0.3558, step time: 1.5357\n",
      "116/388, train_loss: 0.0502, step time: 1.5332\n",
      "117/388, train_loss: 0.1310, step time: 1.5297\n",
      "118/388, train_loss: 0.1282, step time: 1.5315\n",
      "119/388, train_loss: 0.0802, step time: 1.5337\n",
      "120/388, train_loss: 0.1316, step time: 1.5344\n",
      "121/388, train_loss: 0.0639, step time: 1.5339\n",
      "122/388, train_loss: 0.2674, step time: 1.5416\n",
      "123/388, train_loss: 0.1315, step time: 1.5354\n",
      "124/388, train_loss: 0.0848, step time: 1.5325\n",
      "125/388, train_loss: 0.0418, step time: 1.5311\n",
      "126/388, train_loss: 0.1321, step time: 1.5339\n",
      "127/388, train_loss: 0.1551, step time: 1.5355\n",
      "128/388, train_loss: 0.1280, step time: 1.5367\n",
      "129/388, train_loss: 0.1515, step time: 1.5377\n",
      "130/388, train_loss: 0.2308, step time: 1.5336\n",
      "131/388, train_loss: 0.0944, step time: 1.5313\n",
      "132/388, train_loss: 0.0914, step time: 1.5322\n",
      "133/388, train_loss: 0.2113, step time: 1.5343\n",
      "134/388, train_loss: 0.0801, step time: 1.5362\n",
      "135/388, train_loss: 0.0804, step time: 1.5358\n",
      "136/388, train_loss: 0.0715, step time: 1.5335\n",
      "137/388, train_loss: 0.1527, step time: 1.5304\n",
      "138/388, train_loss: 0.1496, step time: 1.5329\n",
      "139/388, train_loss: 0.1056, step time: 1.5358\n",
      "140/388, train_loss: 0.1171, step time: 1.5369\n",
      "141/388, train_loss: 0.0930, step time: 1.5354\n",
      "142/388, train_loss: 0.0945, step time: 1.5319\n",
      "143/388, train_loss: 0.1481, step time: 1.5325\n",
      "144/388, train_loss: 0.0657, step time: 1.5335\n",
      "145/388, train_loss: 0.2401, step time: 1.5388\n",
      "146/388, train_loss: 0.1661, step time: 1.5379\n",
      "147/388, train_loss: 0.0989, step time: 1.5353\n",
      "148/388, train_loss: 0.0538, step time: 1.5331\n",
      "149/388, train_loss: 0.1799, step time: 1.5323\n",
      "150/388, train_loss: 0.2033, step time: 1.5351\n",
      "151/388, train_loss: 0.0829, step time: 1.5393\n",
      "152/388, train_loss: 0.1231, step time: 1.5365\n",
      "153/388, train_loss: 0.3460, step time: 1.5414\n",
      "154/388, train_loss: 0.1047, step time: 1.5319\n",
      "155/388, train_loss: 0.1780, step time: 1.5366\n",
      "156/388, train_loss: 0.2011, step time: 1.5352\n",
      "157/388, train_loss: 0.1659, step time: 1.5360\n",
      "158/388, train_loss: 0.0613, step time: 1.5340\n",
      "159/388, train_loss: 0.2379, step time: 1.5325\n",
      "160/388, train_loss: 0.1158, step time: 1.5377\n",
      "161/388, train_loss: 0.1104, step time: 1.5368\n",
      "162/388, train_loss: 0.0715, step time: 1.5368\n",
      "163/388, train_loss: 0.1591, step time: 1.5342\n",
      "164/388, train_loss: 0.0941, step time: 1.5447\n",
      "165/388, train_loss: 0.2018, step time: 1.5310\n",
      "166/388, train_loss: 0.1430, step time: 1.5344\n",
      "167/388, train_loss: 0.2046, step time: 1.5332\n",
      "168/388, train_loss: 0.0872, step time: 1.5328\n",
      "169/388, train_loss: 0.0907, step time: 1.5318\n",
      "170/388, train_loss: 0.1050, step time: 1.5335\n",
      "171/388, train_loss: 0.2827, step time: 1.5331\n",
      "172/388, train_loss: 0.1468, step time: 1.5344\n",
      "173/388, train_loss: 0.0842, step time: 1.5352\n",
      "174/388, train_loss: 0.4066, step time: 1.5370\n",
      "175/388, train_loss: 0.5685, step time: 1.5316\n",
      "176/388, train_loss: 0.3625, step time: 1.5348\n",
      "177/388, train_loss: 0.1698, step time: 1.5328\n",
      "178/388, train_loss: 0.0694, step time: 1.5376\n",
      "179/388, train_loss: 0.2023, step time: 1.5361\n",
      "180/388, train_loss: 0.1201, step time: 1.5354\n",
      "181/388, train_loss: 0.2394, step time: 1.5357\n",
      "182/388, train_loss: 0.1353, step time: 1.5360\n",
      "183/388, train_loss: 0.0751, step time: 1.5342\n",
      "184/388, train_loss: 0.3384, step time: 1.5313\n",
      "185/388, train_loss: 0.0403, step time: 1.5313\n",
      "186/388, train_loss: 0.0822, step time: 1.5329\n",
      "187/388, train_loss: 0.0789, step time: 1.5312\n",
      "188/388, train_loss: 0.1684, step time: 1.5347\n",
      "189/388, train_loss: 0.4323, step time: 1.5372\n",
      "190/388, train_loss: 0.0572, step time: 1.5355\n",
      "191/388, train_loss: 0.0998, step time: 1.5336\n",
      "192/388, train_loss: 0.2143, step time: 1.5326\n",
      "193/388, train_loss: 0.0947, step time: 1.5325\n",
      "194/388, train_loss: 0.1301, step time: 1.5344\n",
      "195/388, train_loss: 0.0674, step time: 1.5417\n",
      "196/388, train_loss: 0.2769, step time: 1.5360\n",
      "197/388, train_loss: 0.3132, step time: 1.5350\n",
      "198/388, train_loss: 0.1127, step time: 1.5350\n",
      "199/388, train_loss: 0.2634, step time: 1.5335\n",
      "200/388, train_loss: 0.1988, step time: 1.5383\n",
      "201/388, train_loss: 0.2744, step time: 1.5378\n",
      "202/388, train_loss: 0.1037, step time: 1.5321\n",
      "203/388, train_loss: 0.2800, step time: 1.5324\n",
      "204/388, train_loss: 0.2175, step time: 1.5330\n",
      "205/388, train_loss: 0.1174, step time: 1.5358\n",
      "206/388, train_loss: 0.1919, step time: 1.5360\n",
      "207/388, train_loss: 0.2061, step time: 1.5337\n",
      "208/388, train_loss: 0.0912, step time: 1.5338\n",
      "209/388, train_loss: 0.1091, step time: 1.5325\n",
      "210/388, train_loss: 0.2382, step time: 1.5319\n",
      "211/388, train_loss: 0.1734, step time: 1.5328\n",
      "212/388, train_loss: 0.0593, step time: 1.5374\n",
      "213/388, train_loss: 0.1520, step time: 1.5610\n",
      "214/388, train_loss: 0.1126, step time: 1.5333\n",
      "215/388, train_loss: 0.1226, step time: 1.5313\n",
      "216/388, train_loss: 0.5140, step time: 1.5352\n",
      "217/388, train_loss: 0.1376, step time: 1.5368\n",
      "218/388, train_loss: 0.1282, step time: 1.5452\n",
      "219/388, train_loss: 0.1710, step time: 1.5344\n",
      "220/388, train_loss: 0.1519, step time: 1.5321\n",
      "221/388, train_loss: 0.0873, step time: 1.5392\n",
      "222/388, train_loss: 0.2692, step time: 1.5358\n",
      "223/388, train_loss: 0.3160, step time: 1.5379\n",
      "224/388, train_loss: 0.0992, step time: 1.5317\n",
      "225/388, train_loss: 0.3465, step time: 1.5348\n",
      "226/388, train_loss: 0.3508, step time: 1.5348\n",
      "227/388, train_loss: 0.1228, step time: 1.5382\n",
      "228/388, train_loss: 0.2248, step time: 1.5381\n",
      "229/388, train_loss: 0.2697, step time: 1.5303\n",
      "230/388, train_loss: 0.0443, step time: 1.5374\n",
      "231/388, train_loss: 0.1143, step time: 1.5327\n",
      "232/388, train_loss: 0.1184, step time: 1.5328\n",
      "233/388, train_loss: 0.2790, step time: 1.5353\n",
      "234/388, train_loss: 0.0608, step time: 1.5366\n",
      "235/388, train_loss: 0.1366, step time: 1.5329\n",
      "236/388, train_loss: 0.1012, step time: 1.5331\n",
      "237/388, train_loss: 0.3072, step time: 1.5337\n",
      "238/388, train_loss: 0.1770, step time: 1.5340\n",
      "239/388, train_loss: 0.0780, step time: 1.5353\n",
      "240/388, train_loss: 0.1782, step time: 1.5351\n",
      "241/388, train_loss: 0.1365, step time: 1.5333\n",
      "242/388, train_loss: 0.1000, step time: 1.5360\n",
      "243/388, train_loss: 0.1117, step time: 1.5313\n",
      "244/388, train_loss: 0.1019, step time: 1.5331\n",
      "245/388, train_loss: 0.2840, step time: 1.5321\n",
      "246/388, train_loss: 0.2775, step time: 1.5377\n",
      "247/388, train_loss: 0.0330, step time: 1.5348\n",
      "248/388, train_loss: 0.0778, step time: 1.5343\n",
      "249/388, train_loss: 0.1328, step time: 1.5301\n",
      "250/388, train_loss: 0.1602, step time: 1.5303\n",
      "251/388, train_loss: 0.1007, step time: 1.5301\n",
      "252/388, train_loss: 0.2384, step time: 1.5326\n",
      "253/388, train_loss: 0.0747, step time: 1.5356\n",
      "254/388, train_loss: 0.0887, step time: 1.5378\n",
      "255/388, train_loss: 0.0902, step time: 1.5332\n",
      "256/388, train_loss: 0.2669, step time: 1.5327\n",
      "257/388, train_loss: 0.1955, step time: 1.5318\n",
      "258/388, train_loss: 0.0980, step time: 1.5329\n",
      "259/388, train_loss: 0.4363, step time: 1.5342\n",
      "260/388, train_loss: 0.0625, step time: 1.5370\n",
      "261/388, train_loss: 0.2445, step time: 1.5303\n",
      "262/388, train_loss: 0.2721, step time: 1.5345\n",
      "263/388, train_loss: 0.1244, step time: 1.5322\n",
      "264/388, train_loss: 0.0820, step time: 1.5382\n",
      "265/388, train_loss: 0.2025, step time: 1.5356\n",
      "266/388, train_loss: 0.2165, step time: 1.5375\n",
      "267/388, train_loss: 0.1600, step time: 1.5345\n",
      "268/388, train_loss: 0.0972, step time: 1.5336\n",
      "269/388, train_loss: 0.0791, step time: 1.5318\n",
      "270/388, train_loss: 0.0413, step time: 1.5460\n",
      "271/388, train_loss: 0.1908, step time: 1.5337\n",
      "272/388, train_loss: 0.1956, step time: 1.5338\n",
      "273/388, train_loss: 0.2030, step time: 1.5325\n",
      "274/388, train_loss: 0.0941, step time: 1.5330\n",
      "275/388, train_loss: 0.1255, step time: 1.5381\n",
      "276/388, train_loss: 0.0935, step time: 1.5343\n",
      "277/388, train_loss: 0.0804, step time: 1.5318\n",
      "278/388, train_loss: 0.1421, step time: 1.5344\n",
      "279/388, train_loss: 0.2447, step time: 1.5338\n",
      "280/388, train_loss: 0.2244, step time: 1.5374\n",
      "281/388, train_loss: 0.1953, step time: 1.5378\n",
      "282/388, train_loss: 0.2526, step time: 1.5350\n",
      "283/388, train_loss: 0.0988, step time: 1.5332\n",
      "284/388, train_loss: 0.4324, step time: 1.5332\n",
      "285/388, train_loss: 0.1201, step time: 1.5367\n",
      "286/388, train_loss: 0.0583, step time: 1.5420\n",
      "287/388, train_loss: 0.2484, step time: 1.5340\n",
      "288/388, train_loss: 0.1436, step time: 1.5324\n",
      "289/388, train_loss: 0.0717, step time: 1.5299\n",
      "290/388, train_loss: 0.0919, step time: 1.5339\n",
      "291/388, train_loss: 0.1964, step time: 1.5369\n",
      "292/388, train_loss: 0.0896, step time: 1.5359\n",
      "293/388, train_loss: 0.1079, step time: 1.5358\n",
      "294/388, train_loss: 0.2248, step time: 1.5435\n",
      "295/388, train_loss: 0.1560, step time: 1.5298\n",
      "296/388, train_loss: 0.0984, step time: 1.5308\n",
      "297/388, train_loss: 0.1956, step time: 1.5354\n",
      "298/388, train_loss: 0.2806, step time: 1.5339\n",
      "299/388, train_loss: 0.0940, step time: 1.5356\n",
      "300/388, train_loss: 0.2029, step time: 1.5347\n",
      "301/388, train_loss: 0.2395, step time: 1.5355\n",
      "302/388, train_loss: 0.2012, step time: 1.5329\n",
      "303/388, train_loss: 0.1451, step time: 1.5351\n",
      "304/388, train_loss: 0.1623, step time: 1.5378\n",
      "305/388, train_loss: 0.1626, step time: 1.5371\n",
      "306/388, train_loss: 0.1089, step time: 1.5329\n",
      "307/388, train_loss: 0.2495, step time: 1.5319\n",
      "308/388, train_loss: 0.1837, step time: 1.5322\n",
      "309/388, train_loss: 0.2471, step time: 1.5333\n",
      "310/388, train_loss: 0.2657, step time: 1.5348\n",
      "311/388, train_loss: 0.2420, step time: 1.5354\n",
      "312/388, train_loss: 0.2776, step time: 1.5342\n",
      "313/388, train_loss: 0.1279, step time: 1.5312\n",
      "314/388, train_loss: 0.4609, step time: 1.5308\n",
      "315/388, train_loss: 0.1427, step time: 1.5302\n",
      "316/388, train_loss: 0.2402, step time: 1.5391\n",
      "317/388, train_loss: 0.1360, step time: 1.5390\n",
      "318/388, train_loss: 0.1169, step time: 1.5468\n",
      "319/388, train_loss: 0.1783, step time: 1.5382\n",
      "320/388, train_loss: 0.2285, step time: 1.5512\n",
      "321/388, train_loss: 0.2004, step time: 1.5327\n",
      "322/388, train_loss: 0.1010, step time: 1.5325\n",
      "323/388, train_loss: 0.1930, step time: 1.5329\n",
      "324/388, train_loss: 0.0553, step time: 1.5499\n",
      "325/388, train_loss: 0.1103, step time: 1.5442\n",
      "326/388, train_loss: 0.1111, step time: 1.5334\n",
      "327/388, train_loss: 0.0761, step time: 1.5310\n",
      "328/388, train_loss: 0.1802, step time: 1.5356\n",
      "329/388, train_loss: 0.1551, step time: 1.5368\n",
      "330/388, train_loss: 0.0812, step time: 1.5358\n",
      "331/388, train_loss: 0.2990, step time: 1.5346\n",
      "332/388, train_loss: 0.2487, step time: 1.5437\n",
      "333/388, train_loss: 0.2360, step time: 1.5378\n",
      "334/388, train_loss: 0.2678, step time: 1.5370\n",
      "335/388, train_loss: 0.3651, step time: 1.5414\n",
      "336/388, train_loss: 0.1094, step time: 1.5330\n",
      "337/388, train_loss: 0.1385, step time: 1.5327\n",
      "338/388, train_loss: 0.1248, step time: 1.5347\n",
      "339/388, train_loss: 0.0830, step time: 1.5366\n",
      "340/388, train_loss: 0.2027, step time: 1.5365\n",
      "341/388, train_loss: 0.1739, step time: 1.5326\n",
      "342/388, train_loss: 0.2070, step time: 1.5312\n",
      "343/388, train_loss: 0.1905, step time: 1.5322\n",
      "344/388, train_loss: 0.0575, step time: 1.5374\n",
      "345/388, train_loss: 0.2258, step time: 1.5561\n",
      "346/388, train_loss: 0.1746, step time: 1.5334\n",
      "347/388, train_loss: 0.2048, step time: 1.5351\n",
      "348/388, train_loss: 0.1476, step time: 1.5340\n",
      "349/388, train_loss: 0.0857, step time: 1.5376\n",
      "350/388, train_loss: 0.0544, step time: 1.5360\n",
      "351/388, train_loss: 0.0524, step time: 1.5361\n",
      "352/388, train_loss: 0.1001, step time: 1.5340\n",
      "353/388, train_loss: 0.1313, step time: 1.5330\n",
      "354/388, train_loss: 0.0919, step time: 1.5305\n",
      "355/388, train_loss: 0.1906, step time: 1.5350\n",
      "356/388, train_loss: 0.0938, step time: 1.5354\n",
      "357/388, train_loss: 0.2933, step time: 1.5368\n",
      "358/388, train_loss: 0.1916, step time: 1.5351\n",
      "359/388, train_loss: 0.1708, step time: 1.5340\n",
      "360/388, train_loss: 0.1177, step time: 1.5344\n",
      "361/388, train_loss: 0.0945, step time: 1.5349\n",
      "362/388, train_loss: 0.1500, step time: 1.5341\n",
      "363/388, train_loss: 0.0981, step time: 1.5352\n",
      "364/388, train_loss: 0.0918, step time: 1.5384\n",
      "365/388, train_loss: 0.2071, step time: 1.5321\n",
      "366/388, train_loss: 0.1508, step time: 1.5320\n",
      "367/388, train_loss: 0.2015, step time: 1.5319\n",
      "368/388, train_loss: 0.0937, step time: 1.5327\n",
      "369/388, train_loss: 0.2036, step time: 1.5339\n",
      "370/388, train_loss: 0.2176, step time: 1.5382\n",
      "371/388, train_loss: 0.1409, step time: 1.5348\n",
      "372/388, train_loss: 0.4442, step time: 1.5388\n",
      "373/388, train_loss: 0.2213, step time: 1.5318\n",
      "374/388, train_loss: 0.0308, step time: 1.5316\n",
      "375/388, train_loss: 0.1693, step time: 1.5376\n",
      "376/388, train_loss: 0.2173, step time: 1.5359\n",
      "377/388, train_loss: 0.0891, step time: 1.5348\n",
      "378/388, train_loss: 0.2197, step time: 1.5347\n",
      "379/388, train_loss: 0.3164, step time: 1.5319\n",
      "380/388, train_loss: 0.1909, step time: 1.5320\n",
      "381/388, train_loss: 0.2857, step time: 1.5351\n",
      "382/388, train_loss: 0.1423, step time: 1.5385\n",
      "383/388, train_loss: 0.2602, step time: 1.5359\n",
      "384/388, train_loss: 0.1561, step time: 1.5345\n",
      "385/388, train_loss: 0.0263, step time: 1.5336\n",
      "386/388, train_loss: 0.0903, step time: 1.5323\n",
      "387/388, train_loss: 0.2796, step time: 1.5353\n",
      "388/388, train_loss: 0.4076, step time: 1.5345\n",
      "epoch 74 average loss: 0.1632\n",
      "current epoch: 74 current mean dice: 0.7783 tc: 0.8263 wt: 0.9020 et: 0.6066\n",
      "best mean dice: 0.7785 at epoch: 68\n",
      "time consuming of epoch 74 is: 704.3458\n",
      "----------\n",
      "epoch 75/100\n",
      "1/388, train_loss: 0.1794, step time: 1.5564\n",
      "2/388, train_loss: 0.0946, step time: 1.5458\n",
      "3/388, train_loss: 0.0693, step time: 1.5335\n",
      "4/388, train_loss: 0.1667, step time: 1.5437\n",
      "5/388, train_loss: 0.2316, step time: 1.5334\n",
      "6/388, train_loss: 0.1900, step time: 1.5328\n",
      "7/388, train_loss: 0.1338, step time: 1.5611\n",
      "8/388, train_loss: 0.1713, step time: 1.5394\n",
      "9/388, train_loss: 0.2566, step time: 1.5417\n",
      "10/388, train_loss: 0.2001, step time: 1.5338\n",
      "11/388, train_loss: 0.2521, step time: 1.5320\n",
      "12/388, train_loss: 0.1979, step time: 1.5351\n",
      "13/388, train_loss: 0.2132, step time: 1.5370\n",
      "14/388, train_loss: 0.2735, step time: 1.5377\n",
      "15/388, train_loss: 0.0889, step time: 1.5345\n",
      "16/388, train_loss: 0.0829, step time: 1.5317\n",
      "17/388, train_loss: 0.2390, step time: 1.5300\n",
      "18/388, train_loss: 0.0798, step time: 1.5319\n",
      "19/388, train_loss: 0.4072, step time: 1.5362\n",
      "20/388, train_loss: 0.0944, step time: 1.5372\n",
      "21/388, train_loss: 0.0585, step time: 1.5372\n",
      "22/388, train_loss: 0.3520, step time: 1.5323\n",
      "23/388, train_loss: 0.2459, step time: 1.5344\n",
      "24/388, train_loss: 0.1961, step time: 1.5328\n",
      "25/388, train_loss: 0.1215, step time: 1.5380\n",
      "26/388, train_loss: 0.0551, step time: 1.5401\n",
      "27/388, train_loss: 0.2070, step time: 1.5368\n",
      "28/388, train_loss: 0.1499, step time: 1.5342\n",
      "29/388, train_loss: 0.0617, step time: 1.5339\n",
      "30/388, train_loss: 0.0906, step time: 1.5312\n",
      "31/388, train_loss: 0.0885, step time: 1.5366\n",
      "32/388, train_loss: 0.3926, step time: 1.5370\n",
      "33/388, train_loss: 0.4852, step time: 1.5341\n",
      "34/388, train_loss: 0.0884, step time: 1.5337\n",
      "35/388, train_loss: 0.1608, step time: 1.5373\n",
      "36/388, train_loss: 0.1232, step time: 1.5382\n",
      "37/388, train_loss: 0.0873, step time: 1.5303\n",
      "38/388, train_loss: 0.1288, step time: 1.5310\n",
      "39/388, train_loss: 0.1291, step time: 1.5339\n",
      "40/388, train_loss: 0.1990, step time: 1.5345\n",
      "41/388, train_loss: 0.1174, step time: 1.5394\n",
      "42/388, train_loss: 0.2034, step time: 1.5383\n",
      "43/388, train_loss: 0.0803, step time: 1.5315\n",
      "44/388, train_loss: 0.1594, step time: 1.5424\n",
      "45/388, train_loss: 0.6728, step time: 1.5362\n",
      "46/388, train_loss: 0.0597, step time: 1.5358\n",
      "47/388, train_loss: 0.1056, step time: 1.5354\n",
      "48/388, train_loss: 0.1827, step time: 1.5361\n",
      "49/388, train_loss: 0.1989, step time: 1.5348\n",
      "50/388, train_loss: 0.3207, step time: 1.5346\n",
      "51/388, train_loss: 0.1018, step time: 1.5352\n",
      "52/388, train_loss: 0.1429, step time: 1.5396\n",
      "53/388, train_loss: 0.2469, step time: 1.5322\n",
      "54/388, train_loss: 0.0890, step time: 1.5316\n",
      "55/388, train_loss: 0.0614, step time: 1.5392\n",
      "56/388, train_loss: 0.0798, step time: 1.5362\n",
      "57/388, train_loss: 0.2060, step time: 1.5339\n",
      "58/388, train_loss: 0.3132, step time: 1.5300\n",
      "59/388, train_loss: 0.2758, step time: 1.5320\n",
      "60/388, train_loss: 0.3017, step time: 1.5341\n",
      "61/388, train_loss: 0.1406, step time: 1.5365\n",
      "62/388, train_loss: 0.0670, step time: 1.5379\n",
      "63/388, train_loss: 0.0913, step time: 1.5320\n",
      "64/388, train_loss: 0.0730, step time: 1.5356\n",
      "65/388, train_loss: 0.0955, step time: 1.5335\n",
      "66/388, train_loss: 0.2307, step time: 1.5359\n",
      "67/388, train_loss: 0.1138, step time: 1.5364\n",
      "68/388, train_loss: 0.1298, step time: 1.5361\n",
      "69/388, train_loss: 0.2759, step time: 1.5315\n",
      "70/388, train_loss: 0.1137, step time: 1.5325\n",
      "71/388, train_loss: 0.0627, step time: 1.5329\n",
      "72/388, train_loss: 0.0937, step time: 1.5341\n",
      "73/388, train_loss: 0.2504, step time: 1.5355\n",
      "74/388, train_loss: 0.0834, step time: 1.5354\n",
      "75/388, train_loss: 0.2141, step time: 1.5360\n",
      "76/388, train_loss: 0.0574, step time: 1.5317\n",
      "77/388, train_loss: 0.1472, step time: 1.5315\n",
      "78/388, train_loss: 0.0967, step time: 1.5307\n",
      "79/388, train_loss: 0.2782, step time: 1.5362\n",
      "80/388, train_loss: 0.0725, step time: 1.5389\n",
      "81/388, train_loss: 0.0812, step time: 1.5357\n",
      "82/388, train_loss: 0.1784, step time: 1.5348\n",
      "83/388, train_loss: 0.2375, step time: 1.5336\n",
      "84/388, train_loss: 0.2188, step time: 1.5346\n",
      "85/388, train_loss: 0.0612, step time: 1.5342\n",
      "86/388, train_loss: 0.1922, step time: 1.5362\n",
      "87/388, train_loss: 0.1386, step time: 1.5348\n",
      "88/388, train_loss: 0.1561, step time: 1.5324\n",
      "89/388, train_loss: 0.0679, step time: 1.5378\n",
      "90/388, train_loss: 0.1082, step time: 1.5354\n",
      "91/388, train_loss: 0.1500, step time: 1.5356\n",
      "92/388, train_loss: 0.0759, step time: 1.5354\n",
      "93/388, train_loss: 0.2275, step time: 1.5352\n",
      "94/388, train_loss: 0.0789, step time: 1.5377\n",
      "95/388, train_loss: 0.0354, step time: 1.5400\n",
      "96/388, train_loss: 0.2060, step time: 1.5344\n",
      "97/388, train_loss: 0.1047, step time: 1.5321\n",
      "98/388, train_loss: 0.1375, step time: 1.5498\n",
      "99/388, train_loss: 0.0868, step time: 1.5345\n",
      "100/388, train_loss: 0.1119, step time: 1.5364\n",
      "101/388, train_loss: 0.2460, step time: 1.5386\n",
      "102/388, train_loss: 0.1261, step time: 1.5349\n",
      "103/388, train_loss: 0.0955, step time: 1.5326\n",
      "104/388, train_loss: 0.2098, step time: 1.5345\n",
      "105/388, train_loss: 0.3609, step time: 1.5328\n",
      "106/388, train_loss: 0.1493, step time: 1.5362\n",
      "107/388, train_loss: 0.1372, step time: 1.5350\n",
      "108/388, train_loss: 0.0995, step time: 1.5372\n",
      "109/388, train_loss: 0.0670, step time: 1.5332\n",
      "110/388, train_loss: 0.2683, step time: 1.5304\n",
      "111/388, train_loss: 0.3385, step time: 1.5332\n",
      "112/388, train_loss: 0.2509, step time: 1.5375\n",
      "113/388, train_loss: 0.1691, step time: 1.5347\n",
      "114/388, train_loss: 0.2511, step time: 1.5351\n",
      "115/388, train_loss: 0.1295, step time: 1.5331\n",
      "116/388, train_loss: 0.0863, step time: 1.5341\n",
      "117/388, train_loss: 0.1489, step time: 1.5325\n",
      "118/388, train_loss: 0.1886, step time: 1.5370\n",
      "119/388, train_loss: 0.2522, step time: 1.5391\n",
      "120/388, train_loss: 0.1065, step time: 1.5339\n",
      "121/388, train_loss: 0.0961, step time: 1.5319\n",
      "122/388, train_loss: 0.4278, step time: 1.5300\n",
      "123/388, train_loss: 0.0891, step time: 1.5346\n",
      "124/388, train_loss: 0.1120, step time: 1.5351\n",
      "125/388, train_loss: 0.1446, step time: 1.5374\n",
      "126/388, train_loss: 0.3073, step time: 1.5342\n",
      "127/388, train_loss: 0.1276, step time: 1.5342\n",
      "128/388, train_loss: 0.1347, step time: 1.5344\n",
      "129/388, train_loss: 0.1301, step time: 1.5363\n",
      "130/388, train_loss: 0.1149, step time: 1.5377\n",
      "131/388, train_loss: 0.1671, step time: 1.5391\n",
      "132/388, train_loss: 0.0864, step time: 1.5330\n",
      "133/388, train_loss: 0.2371, step time: 1.5316\n",
      "134/388, train_loss: 0.1190, step time: 1.5300\n",
      "135/388, train_loss: 0.1429, step time: 1.5311\n",
      "136/388, train_loss: 0.1815, step time: 1.5372\n",
      "137/388, train_loss: 0.1073, step time: 1.5358\n",
      "138/388, train_loss: 0.0993, step time: 1.5352\n",
      "139/388, train_loss: 0.0550, step time: 1.5316\n",
      "140/388, train_loss: 0.0604, step time: 1.5328\n",
      "141/388, train_loss: 0.2176, step time: 1.5347\n",
      "142/388, train_loss: 0.1633, step time: 1.5331\n",
      "143/388, train_loss: 0.0856, step time: 1.5333\n",
      "144/388, train_loss: 0.1796, step time: 1.5363\n",
      "145/388, train_loss: 0.0938, step time: 1.5384\n",
      "146/388, train_loss: 0.2339, step time: 1.5342\n",
      "147/388, train_loss: 0.2426, step time: 1.5304\n",
      "148/388, train_loss: 0.2270, step time: 1.5338\n",
      "149/388, train_loss: 0.0866, step time: 1.5375\n",
      "150/388, train_loss: 0.1923, step time: 1.5357\n",
      "151/388, train_loss: 0.0864, step time: 1.5373\n",
      "152/388, train_loss: 0.1326, step time: 1.5423\n",
      "153/388, train_loss: 0.1905, step time: 1.5322\n",
      "154/388, train_loss: 0.3583, step time: 1.5348\n",
      "155/388, train_loss: 0.1119, step time: 1.5338\n",
      "156/388, train_loss: 0.1419, step time: 1.5332\n",
      "157/388, train_loss: 0.1408, step time: 1.5387\n",
      "158/388, train_loss: 0.0772, step time: 1.5411\n",
      "159/388, train_loss: 0.2941, step time: 1.5327\n",
      "160/388, train_loss: 0.1005, step time: 1.5327\n",
      "161/388, train_loss: 0.0999, step time: 1.5370\n",
      "162/388, train_loss: 0.0977, step time: 1.5366\n",
      "163/388, train_loss: 0.1578, step time: 1.5356\n",
      "164/388, train_loss: 0.1323, step time: 1.5340\n",
      "165/388, train_loss: 0.0927, step time: 1.5357\n",
      "166/388, train_loss: 0.1986, step time: 1.5323\n",
      "167/388, train_loss: 0.1893, step time: 1.5299\n",
      "168/388, train_loss: 0.2168, step time: 1.5353\n",
      "169/388, train_loss: 0.0901, step time: 1.5412\n",
      "170/388, train_loss: 0.0833, step time: 1.5356\n",
      "171/388, train_loss: 0.0998, step time: 1.5337\n",
      "172/388, train_loss: 0.1763, step time: 1.5337\n",
      "173/388, train_loss: 0.1692, step time: 1.5371\n",
      "174/388, train_loss: 0.2355, step time: 1.5354\n",
      "175/388, train_loss: 0.5271, step time: 1.5368\n",
      "176/388, train_loss: 0.1964, step time: 1.5351\n",
      "177/388, train_loss: 0.2556, step time: 1.5337\n",
      "178/388, train_loss: 0.1433, step time: 1.5323\n",
      "179/388, train_loss: 0.1590, step time: 1.5329\n",
      "180/388, train_loss: 0.1499, step time: 1.5358\n",
      "181/388, train_loss: 0.2027, step time: 1.5349\n",
      "182/388, train_loss: 0.1173, step time: 1.5384\n",
      "183/388, train_loss: 0.1536, step time: 1.5338\n",
      "184/388, train_loss: 0.1904, step time: 1.5338\n",
      "185/388, train_loss: 0.1391, step time: 1.5345\n",
      "186/388, train_loss: 0.1330, step time: 1.5467\n",
      "187/388, train_loss: 0.0786, step time: 1.5368\n",
      "188/388, train_loss: 0.0658, step time: 1.5369\n",
      "189/388, train_loss: 0.1813, step time: 1.5351\n",
      "190/388, train_loss: 0.0777, step time: 1.5336\n",
      "191/388, train_loss: 0.1066, step time: 1.5329\n",
      "192/388, train_loss: 0.2137, step time: 1.5379\n",
      "193/388, train_loss: 0.1581, step time: 1.5349\n",
      "194/388, train_loss: 0.0785, step time: 1.5395\n",
      "195/388, train_loss: 0.0784, step time: 1.5324\n",
      "196/388, train_loss: 0.2200, step time: 1.5310\n",
      "197/388, train_loss: 0.0995, step time: 1.5334\n",
      "198/388, train_loss: 0.1690, step time: 1.5334\n",
      "199/388, train_loss: 0.1270, step time: 1.5365\n",
      "200/388, train_loss: 0.0771, step time: 1.5370\n",
      "201/388, train_loss: 0.2474, step time: 1.5346\n",
      "202/388, train_loss: 0.1455, step time: 1.5471\n",
      "203/388, train_loss: 0.2110, step time: 1.5342\n",
      "204/388, train_loss: 0.3114, step time: 1.5338\n",
      "205/388, train_loss: 0.1395, step time: 1.5446\n",
      "206/388, train_loss: 0.0769, step time: 1.5302\n",
      "207/388, train_loss: 0.0925, step time: 1.5295\n",
      "208/388, train_loss: 0.0707, step time: 1.5324\n",
      "209/388, train_loss: 0.2279, step time: 1.5320\n",
      "210/388, train_loss: 0.1830, step time: 1.5364\n",
      "211/388, train_loss: 0.1885, step time: 1.5471\n",
      "212/388, train_loss: 0.1510, step time: 1.5339\n",
      "213/388, train_loss: 0.1973, step time: 1.5354\n",
      "214/388, train_loss: 0.1394, step time: 1.5356\n",
      "215/388, train_loss: 0.0647, step time: 1.5461\n",
      "216/388, train_loss: 0.0430, step time: 1.5331\n",
      "217/388, train_loss: 0.1956, step time: 1.5318\n",
      "218/388, train_loss: 0.0891, step time: 1.5350\n",
      "219/388, train_loss: 0.2727, step time: 1.5363\n",
      "220/388, train_loss: 0.1357, step time: 1.5378\n",
      "221/388, train_loss: 0.1449, step time: 1.5387\n",
      "222/388, train_loss: 0.1229, step time: 1.5347\n",
      "223/388, train_loss: 0.1171, step time: 1.5355\n",
      "224/388, train_loss: 0.0680, step time: 1.5354\n",
      "225/388, train_loss: 0.1534, step time: 1.5378\n",
      "226/388, train_loss: 0.2369, step time: 1.5366\n",
      "227/388, train_loss: 0.3408, step time: 1.5321\n",
      "228/388, train_loss: 0.1486, step time: 1.5332\n",
      "229/388, train_loss: 0.1483, step time: 1.5338\n",
      "230/388, train_loss: 0.1830, step time: 1.5357\n",
      "231/388, train_loss: 0.1701, step time: 1.5494\n",
      "232/388, train_loss: 0.1023, step time: 1.5345\n",
      "233/388, train_loss: 0.0747, step time: 1.5312\n",
      "234/388, train_loss: 0.1246, step time: 1.5322\n",
      "235/388, train_loss: 0.3032, step time: 1.5397\n",
      "236/388, train_loss: 0.1343, step time: 1.5383\n",
      "237/388, train_loss: 0.1901, step time: 1.5358\n",
      "238/388, train_loss: 0.3966, step time: 1.5339\n",
      "239/388, train_loss: 0.0791, step time: 1.5313\n",
      "240/388, train_loss: 0.1192, step time: 1.5335\n",
      "241/388, train_loss: 0.1727, step time: 1.5388\n",
      "242/388, train_loss: 0.2372, step time: 1.5392\n",
      "243/388, train_loss: 0.0977, step time: 1.5349\n",
      "244/388, train_loss: 0.2664, step time: 1.5337\n",
      "245/388, train_loss: 0.2403, step time: 1.5349\n",
      "246/388, train_loss: 0.0444, step time: 1.5358\n",
      "247/388, train_loss: 0.1795, step time: 1.5360\n",
      "248/388, train_loss: 0.1541, step time: 1.5365\n",
      "249/388, train_loss: 0.1436, step time: 1.5336\n",
      "250/388, train_loss: 0.2982, step time: 1.5328\n",
      "251/388, train_loss: 0.3812, step time: 1.5375\n",
      "252/388, train_loss: 0.0549, step time: 1.5382\n",
      "253/388, train_loss: 0.1885, step time: 1.5362\n",
      "254/388, train_loss: 0.3495, step time: 1.5345\n",
      "255/388, train_loss: 0.3990, step time: 1.5359\n",
      "256/388, train_loss: 0.3234, step time: 1.5445\n",
      "257/388, train_loss: 0.1641, step time: 1.5363\n",
      "258/388, train_loss: 0.1022, step time: 1.5349\n",
      "259/388, train_loss: 0.1794, step time: 1.5333\n",
      "260/388, train_loss: 0.1018, step time: 1.5352\n",
      "261/388, train_loss: 0.1729, step time: 1.5336\n",
      "262/388, train_loss: 0.1113, step time: 1.5414\n",
      "263/388, train_loss: 0.2325, step time: 1.5370\n",
      "264/388, train_loss: 0.1588, step time: 1.5351\n",
      "265/388, train_loss: 0.0437, step time: 1.5319\n",
      "266/388, train_loss: 0.3886, step time: 1.5303\n",
      "267/388, train_loss: 0.2039, step time: 1.5320\n",
      "268/388, train_loss: 0.1677, step time: 1.5388\n",
      "269/388, train_loss: 0.1891, step time: 1.5591\n",
      "270/388, train_loss: 0.1451, step time: 1.5342\n",
      "271/388, train_loss: 0.0745, step time: 1.5338\n",
      "272/388, train_loss: 0.1313, step time: 1.5363\n",
      "273/388, train_loss: 0.1472, step time: 1.5373\n",
      "274/388, train_loss: 0.1051, step time: 1.5342\n",
      "275/388, train_loss: 0.0699, step time: 1.5332\n",
      "276/388, train_loss: 0.0817, step time: 1.5309\n",
      "277/388, train_loss: 0.2804, step time: 1.5346\n",
      "278/388, train_loss: 0.0802, step time: 1.5388\n",
      "279/388, train_loss: 0.1007, step time: 1.5383\n",
      "280/388, train_loss: 0.0730, step time: 1.5311\n",
      "281/388, train_loss: 0.1891, step time: 1.5325\n",
      "282/388, train_loss: 0.5317, step time: 1.5337\n",
      "283/388, train_loss: 0.1405, step time: 1.5389\n",
      "284/388, train_loss: 0.1788, step time: 1.5374\n",
      "285/388, train_loss: 0.0907, step time: 1.5296\n",
      "286/388, train_loss: 0.0817, step time: 1.5330\n",
      "287/388, train_loss: 0.0513, step time: 1.5316\n",
      "288/388, train_loss: 0.0825, step time: 1.5377\n",
      "289/388, train_loss: 0.2791, step time: 1.5348\n",
      "290/388, train_loss: 0.1930, step time: 1.5306\n",
      "291/388, train_loss: 0.1939, step time: 1.5316\n",
      "292/388, train_loss: 0.1075, step time: 1.5355\n",
      "293/388, train_loss: 0.1201, step time: 1.5364\n",
      "294/388, train_loss: 0.0895, step time: 1.5358\n",
      "295/388, train_loss: 0.1489, step time: 1.5337\n",
      "296/388, train_loss: 0.1813, step time: 1.5365\n",
      "297/388, train_loss: 0.0957, step time: 1.5359\n",
      "298/388, train_loss: 0.1819, step time: 1.5331\n",
      "299/388, train_loss: 0.1070, step time: 1.5323\n",
      "300/388, train_loss: 0.2033, step time: 1.5340\n",
      "301/388, train_loss: 0.1961, step time: 1.5371\n",
      "302/388, train_loss: 0.1780, step time: 1.5387\n",
      "303/388, train_loss: 0.0627, step time: 1.5373\n",
      "304/388, train_loss: 0.1131, step time: 1.5346\n",
      "305/388, train_loss: 0.1854, step time: 1.5354\n",
      "306/388, train_loss: 0.0900, step time: 1.5390\n",
      "307/388, train_loss: 0.2589, step time: 1.5438\n",
      "308/388, train_loss: 0.1262, step time: 1.5323\n",
      "309/388, train_loss: 0.1713, step time: 1.5333\n",
      "310/388, train_loss: 0.1379, step time: 1.5374\n",
      "311/388, train_loss: 0.1486, step time: 1.5420\n",
      "312/388, train_loss: 0.1113, step time: 1.5329\n",
      "313/388, train_loss: 0.1956, step time: 1.5313\n",
      "314/388, train_loss: 0.1239, step time: 1.5318\n",
      "315/388, train_loss: 0.2144, step time: 1.5362\n",
      "316/388, train_loss: 0.2326, step time: 1.5365\n",
      "317/388, train_loss: 0.0464, step time: 1.5354\n",
      "318/388, train_loss: 0.2031, step time: 1.5360\n",
      "319/388, train_loss: 0.1540, step time: 1.5323\n",
      "320/388, train_loss: 0.2258, step time: 1.5339\n",
      "321/388, train_loss: 0.2505, step time: 1.5378\n",
      "322/388, train_loss: 0.1253, step time: 1.5354\n",
      "323/388, train_loss: 0.1297, step time: 1.5340\n",
      "324/388, train_loss: 0.2494, step time: 1.5305\n",
      "325/388, train_loss: 0.1771, step time: 1.5321\n",
      "326/388, train_loss: 0.2098, step time: 1.5329\n",
      "327/388, train_loss: 0.1727, step time: 1.5340\n",
      "328/388, train_loss: 0.1423, step time: 1.5358\n",
      "329/388, train_loss: 0.3478, step time: 1.5408\n",
      "330/388, train_loss: 0.0688, step time: 1.5364\n",
      "331/388, train_loss: 0.1905, step time: 1.5414\n",
      "332/388, train_loss: 0.1317, step time: 1.5357\n",
      "333/388, train_loss: 0.2013, step time: 1.5351\n",
      "334/388, train_loss: 0.1984, step time: 1.5382\n",
      "335/388, train_loss: 0.2503, step time: 1.5379\n",
      "336/388, train_loss: 0.2954, step time: 1.5353\n",
      "337/388, train_loss: 0.2995, step time: 1.5304\n",
      "338/388, train_loss: 0.0919, step time: 1.5322\n",
      "339/388, train_loss: 0.1959, step time: 1.5322\n",
      "340/388, train_loss: 0.1410, step time: 1.5350\n",
      "341/388, train_loss: 0.2413, step time: 1.5355\n",
      "342/388, train_loss: 0.0511, step time: 1.5355\n",
      "343/388, train_loss: 0.2685, step time: 1.5371\n",
      "344/388, train_loss: 0.1539, step time: 1.5329\n",
      "345/388, train_loss: 0.1914, step time: 1.5320\n",
      "346/388, train_loss: 0.0337, step time: 1.5325\n",
      "347/388, train_loss: 0.0999, step time: 1.5348\n",
      "348/388, train_loss: 0.0258, step time: 1.5397\n",
      "349/388, train_loss: 0.1212, step time: 1.5376\n",
      "350/388, train_loss: 0.1597, step time: 1.5349\n",
      "351/388, train_loss: 0.1104, step time: 1.5389\n",
      "352/388, train_loss: 0.3660, step time: 1.5332\n",
      "353/388, train_loss: 0.2694, step time: 1.5355\n",
      "354/388, train_loss: 0.1504, step time: 1.5381\n",
      "355/388, train_loss: 0.1121, step time: 1.5345\n",
      "356/388, train_loss: 0.2937, step time: 1.5334\n",
      "357/388, train_loss: 0.3672, step time: 1.5320\n",
      "358/388, train_loss: 0.2339, step time: 1.5341\n",
      "359/388, train_loss: 0.1081, step time: 1.5479\n",
      "360/388, train_loss: 0.0863, step time: 1.5342\n",
      "361/388, train_loss: 0.0861, step time: 1.5333\n",
      "362/388, train_loss: 0.1083, step time: 1.5374\n",
      "363/388, train_loss: 0.0993, step time: 1.5361\n",
      "364/388, train_loss: 0.1074, step time: 1.5355\n",
      "365/388, train_loss: 0.1601, step time: 1.5341\n",
      "366/388, train_loss: 0.4395, step time: 1.5334\n",
      "367/388, train_loss: 0.1304, step time: 1.5344\n",
      "368/388, train_loss: 0.0826, step time: 1.5339\n",
      "369/388, train_loss: 0.0984, step time: 1.5375\n",
      "370/388, train_loss: 0.0877, step time: 1.5360\n",
      "371/388, train_loss: 0.1287, step time: 1.5325\n",
      "372/388, train_loss: 0.1323, step time: 1.5367\n",
      "373/388, train_loss: 0.0336, step time: 1.5363\n",
      "374/388, train_loss: 0.1610, step time: 1.5364\n",
      "375/388, train_loss: 0.2073, step time: 1.5426\n",
      "376/388, train_loss: 0.0982, step time: 1.5332\n",
      "377/388, train_loss: 0.0415, step time: 1.5334\n",
      "378/388, train_loss: 0.1316, step time: 1.5345\n",
      "379/388, train_loss: 0.1121, step time: 1.5387\n",
      "380/388, train_loss: 0.1616, step time: 1.5391\n",
      "381/388, train_loss: 0.2448, step time: 1.5350\n",
      "382/388, train_loss: 0.1278, step time: 1.5329\n",
      "383/388, train_loss: 0.0572, step time: 1.5348\n",
      "384/388, train_loss: 0.0782, step time: 1.5342\n",
      "385/388, train_loss: 0.1011, step time: 1.5347\n",
      "386/388, train_loss: 0.0646, step time: 1.5375\n",
      "387/388, train_loss: 0.2114, step time: 1.5390\n",
      "388/388, train_loss: 0.1281, step time: 1.5344\n",
      "epoch 75 average loss: 0.1633\n",
      "saved new best metric model\n",
      "current epoch: 75 current mean dice: 0.7786 tc: 0.8271 wt: 0.9061 et: 0.6026\n",
      "best mean dice: 0.7786 at epoch: 75\n",
      "time consuming of epoch 75 is: 704.1141\n",
      "----------\n",
      "epoch 76/100\n",
      "1/388, train_loss: 0.2338, step time: 1.5394\n",
      "2/388, train_loss: 0.0896, step time: 1.5330\n",
      "3/388, train_loss: 0.1344, step time: 1.5338\n",
      "4/388, train_loss: 0.1908, step time: 1.5358\n",
      "5/388, train_loss: 0.1816, step time: 1.5342\n",
      "6/388, train_loss: 0.2232, step time: 1.5315\n",
      "7/388, train_loss: 0.2262, step time: 1.5351\n",
      "8/388, train_loss: 0.1972, step time: 1.5383\n",
      "9/388, train_loss: 0.1183, step time: 1.5320\n",
      "10/388, train_loss: 0.1031, step time: 1.5325\n",
      "11/388, train_loss: 0.0602, step time: 1.5341\n",
      "12/388, train_loss: 0.1070, step time: 1.5359\n",
      "13/388, train_loss: 0.1799, step time: 1.5340\n",
      "14/388, train_loss: 0.0710, step time: 1.5362\n",
      "15/388, train_loss: 0.0459, step time: 1.5397\n",
      "16/388, train_loss: 0.1811, step time: 1.5328\n",
      "17/388, train_loss: 0.0867, step time: 1.5337\n",
      "18/388, train_loss: 0.0433, step time: 1.5360\n",
      "19/388, train_loss: 0.1262, step time: 1.5320\n",
      "20/388, train_loss: 0.2027, step time: 1.5362\n",
      "21/388, train_loss: 0.1774, step time: 1.5361\n",
      "22/388, train_loss: 0.0869, step time: 1.5321\n",
      "23/388, train_loss: 0.2060, step time: 1.5335\n",
      "24/388, train_loss: 0.1873, step time: 1.5316\n",
      "25/388, train_loss: 0.5505, step time: 1.5381\n",
      "26/388, train_loss: 0.3704, step time: 1.5364\n",
      "27/388, train_loss: 0.1715, step time: 1.5413\n",
      "28/388, train_loss: 0.1262, step time: 1.5328\n",
      "29/388, train_loss: 0.1341, step time: 1.5383\n",
      "30/388, train_loss: 0.1315, step time: 1.5346\n",
      "31/388, train_loss: 0.1867, step time: 1.5378\n",
      "32/388, train_loss: 0.1887, step time: 1.5346\n",
      "33/388, train_loss: 0.0730, step time: 1.5352\n",
      "34/388, train_loss: 0.3875, step time: 1.5324\n",
      "35/388, train_loss: 0.1488, step time: 1.5349\n",
      "36/388, train_loss: 0.1528, step time: 1.5331\n",
      "37/388, train_loss: 0.1970, step time: 1.5350\n",
      "38/388, train_loss: 0.0878, step time: 1.5349\n",
      "39/388, train_loss: 0.0947, step time: 1.5365\n",
      "40/388, train_loss: 0.1152, step time: 1.5318\n",
      "41/388, train_loss: 0.2122, step time: 1.5333\n",
      "42/388, train_loss: 0.1094, step time: 1.5298\n",
      "43/388, train_loss: 0.1854, step time: 1.5325\n",
      "44/388, train_loss: 0.1412, step time: 1.5562\n",
      "45/388, train_loss: 0.0757, step time: 1.5328\n",
      "46/388, train_loss: 0.1019, step time: 1.5325\n",
      "47/388, train_loss: 0.1464, step time: 1.5322\n",
      "48/388, train_loss: 0.1325, step time: 1.5371\n",
      "49/388, train_loss: 0.1275, step time: 1.5365\n",
      "50/388, train_loss: 0.0479, step time: 1.5357\n",
      "51/388, train_loss: 0.4195, step time: 1.5305\n",
      "52/388, train_loss: 0.0679, step time: 1.5322\n",
      "53/388, train_loss: 0.2967, step time: 1.5352\n",
      "54/388, train_loss: 0.0948, step time: 1.5358\n",
      "55/388, train_loss: 0.1836, step time: 1.5384\n",
      "56/388, train_loss: 0.2325, step time: 1.5348\n",
      "57/388, train_loss: 0.2268, step time: 1.5358\n",
      "58/388, train_loss: 0.1211, step time: 1.5344\n",
      "59/388, train_loss: 0.0313, step time: 1.5309\n",
      "60/388, train_loss: 0.1197, step time: 1.5333\n",
      "61/388, train_loss: 0.0959, step time: 1.5351\n",
      "62/388, train_loss: 0.0606, step time: 1.5396\n",
      "63/388, train_loss: 0.0900, step time: 1.5356\n",
      "64/388, train_loss: 0.0970, step time: 1.5350\n",
      "65/388, train_loss: 0.1538, step time: 1.5350\n",
      "66/388, train_loss: 0.1169, step time: 1.5345\n",
      "67/388, train_loss: 0.1669, step time: 1.5604\n",
      "68/388, train_loss: 0.0604, step time: 1.5375\n",
      "69/388, train_loss: 0.3236, step time: 1.5331\n",
      "70/388, train_loss: 0.0689, step time: 1.5336\n",
      "71/388, train_loss: 0.2305, step time: 1.5334\n",
      "72/388, train_loss: 0.2096, step time: 1.5420\n",
      "73/388, train_loss: 0.4113, step time: 1.5370\n",
      "74/388, train_loss: 0.1509, step time: 1.5371\n",
      "75/388, train_loss: 0.0990, step time: 1.5361\n",
      "76/388, train_loss: 0.1413, step time: 1.5344\n",
      "77/388, train_loss: 0.1516, step time: 1.5334\n",
      "78/388, train_loss: 0.1628, step time: 1.5369\n",
      "79/388, train_loss: 0.2173, step time: 1.5372\n",
      "80/388, train_loss: 0.1508, step time: 1.5358\n",
      "81/388, train_loss: 0.2881, step time: 1.5321\n",
      "82/388, train_loss: 0.1365, step time: 1.5335\n",
      "83/388, train_loss: 0.1083, step time: 1.5315\n",
      "84/388, train_loss: 0.0495, step time: 1.5344\n",
      "85/388, train_loss: 0.1594, step time: 1.5360\n",
      "86/388, train_loss: 0.1820, step time: 1.5358\n",
      "87/388, train_loss: 0.1369, step time: 1.5353\n",
      "88/388, train_loss: 0.2016, step time: 1.5339\n",
      "89/388, train_loss: 0.1422, step time: 1.5318\n",
      "90/388, train_loss: 0.2097, step time: 1.5337\n",
      "91/388, train_loss: 0.1980, step time: 1.5368\n",
      "92/388, train_loss: 0.1118, step time: 1.5361\n",
      "93/388, train_loss: 0.0927, step time: 1.5338\n",
      "94/388, train_loss: 0.0963, step time: 1.5334\n",
      "95/388, train_loss: 0.1357, step time: 1.5345\n",
      "96/388, train_loss: 0.0863, step time: 1.5345\n",
      "97/388, train_loss: 0.0870, step time: 1.5350\n",
      "98/388, train_loss: 0.0998, step time: 1.5370\n",
      "99/388, train_loss: 0.0997, step time: 1.5315\n",
      "100/388, train_loss: 0.1258, step time: 1.5329\n",
      "101/388, train_loss: 0.1461, step time: 1.5324\n",
      "102/388, train_loss: 0.2183, step time: 1.5329\n",
      "103/388, train_loss: 0.1315, step time: 1.5342\n",
      "104/388, train_loss: 0.1265, step time: 1.5356\n",
      "105/388, train_loss: 0.1327, step time: 1.5347\n",
      "106/388, train_loss: 0.3251, step time: 1.5373\n",
      "107/388, train_loss: 0.0616, step time: 1.5346\n",
      "108/388, train_loss: 0.3910, step time: 1.5335\n",
      "109/388, train_loss: 0.3520, step time: 1.5331\n",
      "110/388, train_loss: 0.1544, step time: 1.5511\n",
      "111/388, train_loss: 0.0864, step time: 1.5341\n",
      "112/388, train_loss: 0.0871, step time: 1.5339\n",
      "113/388, train_loss: 0.1005, step time: 1.5357\n",
      "114/388, train_loss: 0.1140, step time: 1.5354\n",
      "115/388, train_loss: 0.1455, step time: 1.5365\n",
      "116/388, train_loss: 0.1679, step time: 1.5369\n",
      "117/388, train_loss: 0.0997, step time: 1.5356\n",
      "118/388, train_loss: 0.0985, step time: 1.5343\n",
      "119/388, train_loss: 0.2330, step time: 1.5320\n",
      "120/388, train_loss: 0.0786, step time: 1.5328\n",
      "121/388, train_loss: 0.2098, step time: 1.5360\n",
      "122/388, train_loss: 0.0824, step time: 1.5320\n",
      "123/388, train_loss: 0.1572, step time: 1.5320\n",
      "124/388, train_loss: 0.1094, step time: 1.5334\n",
      "125/388, train_loss: 0.2719, step time: 1.5359\n",
      "126/388, train_loss: 0.0887, step time: 1.5369\n",
      "127/388, train_loss: 0.2120, step time: 1.5349\n",
      "128/388, train_loss: 0.3557, step time: 1.5335\n",
      "129/388, train_loss: 0.2013, step time: 1.5351\n",
      "130/388, train_loss: 0.0804, step time: 1.5380\n",
      "131/388, train_loss: 0.4882, step time: 1.5362\n",
      "132/388, train_loss: 0.1809, step time: 1.5327\n",
      "133/388, train_loss: 0.2188, step time: 1.5323\n",
      "134/388, train_loss: 0.0777, step time: 1.5337\n",
      "135/388, train_loss: 0.1417, step time: 1.5305\n",
      "136/388, train_loss: 0.1317, step time: 1.5348\n",
      "137/388, train_loss: 0.0882, step time: 1.5368\n",
      "138/388, train_loss: 0.2368, step time: 1.5364\n",
      "139/388, train_loss: 0.0751, step time: 1.5345\n",
      "140/388, train_loss: 0.3217, step time: 1.5320\n",
      "141/388, train_loss: 0.1661, step time: 1.5363\n",
      "142/388, train_loss: 0.1001, step time: 1.5420\n",
      "143/388, train_loss: 0.0705, step time: 1.5330\n",
      "144/388, train_loss: 0.2356, step time: 1.5335\n",
      "145/388, train_loss: 0.1417, step time: 1.5344\n",
      "146/388, train_loss: 0.0930, step time: 1.5351\n",
      "147/388, train_loss: 0.2293, step time: 1.5351\n",
      "148/388, train_loss: 0.0671, step time: 1.5312\n",
      "149/388, train_loss: 0.0970, step time: 1.5322\n",
      "150/388, train_loss: 0.0659, step time: 1.5353\n",
      "151/388, train_loss: 0.1985, step time: 1.5349\n",
      "152/388, train_loss: 0.2536, step time: 1.5369\n",
      "153/388, train_loss: 0.0956, step time: 1.5320\n",
      "154/388, train_loss: 0.1070, step time: 1.5323\n",
      "155/388, train_loss: 0.1197, step time: 1.5298\n",
      "156/388, train_loss: 0.2096, step time: 1.5330\n",
      "157/388, train_loss: 0.2588, step time: 1.5379\n",
      "158/388, train_loss: 0.1007, step time: 1.5365\n",
      "159/388, train_loss: 0.3069, step time: 1.5342\n",
      "160/388, train_loss: 0.4382, step time: 1.5351\n",
      "161/388, train_loss: 0.0882, step time: 1.5342\n",
      "162/388, train_loss: 0.1657, step time: 1.5384\n",
      "163/388, train_loss: 0.1255, step time: 1.5366\n",
      "164/388, train_loss: 0.1008, step time: 1.5350\n",
      "165/388, train_loss: 0.0738, step time: 1.5327\n",
      "166/388, train_loss: 0.1416, step time: 1.5332\n",
      "167/388, train_loss: 0.2865, step time: 1.5329\n",
      "168/388, train_loss: 0.0864, step time: 1.5361\n",
      "169/388, train_loss: 0.0966, step time: 1.5356\n",
      "170/388, train_loss: 0.1964, step time: 1.5355\n",
      "171/388, train_loss: 0.1621, step time: 1.5563\n",
      "172/388, train_loss: 0.1379, step time: 1.5327\n",
      "173/388, train_loss: 0.2059, step time: 1.5406\n",
      "174/388, train_loss: 0.0550, step time: 1.5341\n",
      "175/388, train_loss: 0.0856, step time: 1.5314\n",
      "176/388, train_loss: 0.1673, step time: 1.5334\n",
      "177/388, train_loss: 0.1086, step time: 1.5338\n",
      "178/388, train_loss: 0.1356, step time: 1.5363\n",
      "179/388, train_loss: 0.3012, step time: 1.5381\n",
      "180/388, train_loss: 0.0964, step time: 1.5657\n",
      "181/388, train_loss: 0.2294, step time: 1.5325\n",
      "182/388, train_loss: 0.0783, step time: 1.5385\n",
      "183/388, train_loss: 0.1066, step time: 1.5389\n",
      "184/388, train_loss: 0.0919, step time: 1.5382\n",
      "185/388, train_loss: 0.1929, step time: 1.5334\n",
      "186/388, train_loss: 0.1711, step time: 1.5335\n",
      "187/388, train_loss: 0.3066, step time: 1.5375\n",
      "188/388, train_loss: 0.0499, step time: 1.5379\n",
      "189/388, train_loss: 0.1258, step time: 1.5356\n",
      "190/388, train_loss: 0.0946, step time: 1.5360\n",
      "191/388, train_loss: 0.1724, step time: 1.5340\n",
      "192/388, train_loss: 0.1698, step time: 1.5359\n",
      "193/388, train_loss: 0.1988, step time: 1.5362\n",
      "194/388, train_loss: 0.0292, step time: 1.5344\n",
      "195/388, train_loss: 0.2722, step time: 1.5360\n",
      "196/388, train_loss: 0.0700, step time: 1.5351\n",
      "197/388, train_loss: 0.1229, step time: 1.5360\n",
      "198/388, train_loss: 0.1552, step time: 1.5383\n",
      "199/388, train_loss: 0.4656, step time: 1.5377\n",
      "200/388, train_loss: 0.1808, step time: 1.5339\n",
      "201/388, train_loss: 0.1306, step time: 1.5338\n",
      "202/388, train_loss: 0.0424, step time: 1.5327\n",
      "203/388, train_loss: 0.0797, step time: 1.5353\n",
      "204/388, train_loss: 0.1582, step time: 1.5387\n",
      "205/388, train_loss: 0.4593, step time: 1.5392\n",
      "206/388, train_loss: 0.1183, step time: 1.5339\n",
      "207/388, train_loss: 0.2349, step time: 1.5345\n",
      "208/388, train_loss: 0.0576, step time: 1.5347\n",
      "209/388, train_loss: 0.0750, step time: 1.5380\n",
      "210/388, train_loss: 0.0845, step time: 1.5391\n",
      "211/388, train_loss: 0.1528, step time: 1.5331\n",
      "212/388, train_loss: 0.1974, step time: 1.5339\n",
      "213/388, train_loss: 0.0534, step time: 1.5349\n",
      "214/388, train_loss: 0.0778, step time: 1.5398\n",
      "215/388, train_loss: 0.0518, step time: 1.5358\n",
      "216/388, train_loss: 0.2287, step time: 1.5349\n",
      "217/388, train_loss: 0.0919, step time: 1.5353\n",
      "218/388, train_loss: 0.1226, step time: 1.5387\n",
      "219/388, train_loss: 0.1347, step time: 1.5353\n",
      "220/388, train_loss: 0.2406, step time: 1.5373\n",
      "221/388, train_loss: 0.1281, step time: 1.5333\n",
      "222/388, train_loss: 0.0995, step time: 1.5348\n",
      "223/388, train_loss: 0.0817, step time: 1.5337\n",
      "224/388, train_loss: 0.0832, step time: 1.5357\n",
      "225/388, train_loss: 0.1508, step time: 1.5372\n",
      "226/388, train_loss: 0.2197, step time: 1.5377\n",
      "227/388, train_loss: 0.0424, step time: 1.5350\n",
      "228/388, train_loss: 0.0999, step time: 1.5345\n",
      "229/388, train_loss: 0.0796, step time: 1.5340\n",
      "230/388, train_loss: 0.1762, step time: 1.5354\n",
      "231/388, train_loss: 0.1398, step time: 1.5414\n",
      "232/388, train_loss: 0.0638, step time: 1.5348\n",
      "233/388, train_loss: 0.1007, step time: 1.5301\n",
      "234/388, train_loss: 0.2209, step time: 1.5333\n",
      "235/388, train_loss: 0.2130, step time: 1.5355\n",
      "236/388, train_loss: 0.0856, step time: 1.5381\n",
      "237/388, train_loss: 0.1943, step time: 1.5354\n",
      "238/388, train_loss: 0.1878, step time: 1.5326\n",
      "239/388, train_loss: 0.1596, step time: 1.5457\n",
      "240/388, train_loss: 0.3129, step time: 1.5357\n",
      "241/388, train_loss: 0.2288, step time: 1.5325\n",
      "242/388, train_loss: 0.0693, step time: 1.5307\n",
      "243/388, train_loss: 0.1091, step time: 1.5305\n",
      "244/388, train_loss: 0.4019, step time: 1.5339\n",
      "245/388, train_loss: 0.1235, step time: 1.5390\n",
      "246/388, train_loss: 0.1008, step time: 1.5385\n",
      "247/388, train_loss: 0.1523, step time: 1.5355\n",
      "248/388, train_loss: 0.1035, step time: 1.5337\n",
      "249/388, train_loss: 0.1756, step time: 1.5299\n",
      "250/388, train_loss: 0.1017, step time: 1.5389\n",
      "251/388, train_loss: 0.0869, step time: 1.5371\n",
      "252/388, train_loss: 0.0666, step time: 1.5474\n",
      "253/388, train_loss: 0.1626, step time: 1.5321\n",
      "254/388, train_loss: 0.2312, step time: 1.5338\n",
      "255/388, train_loss: 0.1896, step time: 1.5357\n",
      "256/388, train_loss: 0.0540, step time: 1.5346\n",
      "257/388, train_loss: 0.0642, step time: 1.5624\n",
      "258/388, train_loss: 0.1711, step time: 1.5382\n",
      "259/388, train_loss: 0.0948, step time: 1.5385\n",
      "260/388, train_loss: 0.1021, step time: 1.5356\n",
      "261/388, train_loss: 0.1202, step time: 1.5329\n",
      "262/388, train_loss: 0.2287, step time: 1.5314\n",
      "263/388, train_loss: 0.3187, step time: 1.5353\n",
      "264/388, train_loss: 0.1274, step time: 1.5359\n",
      "265/388, train_loss: 0.3014, step time: 1.5360\n",
      "266/388, train_loss: 0.3511, step time: 1.5334\n",
      "267/388, train_loss: 0.1896, step time: 1.5364\n",
      "268/388, train_loss: 0.3491, step time: 1.5309\n",
      "269/388, train_loss: 0.1961, step time: 1.5340\n",
      "270/388, train_loss: 0.0658, step time: 1.5324\n",
      "271/388, train_loss: 0.0430, step time: 1.5326\n",
      "272/388, train_loss: 0.2981, step time: 1.5358\n",
      "273/388, train_loss: 0.1374, step time: 1.5351\n",
      "274/388, train_loss: 0.1855, step time: 1.5328\n",
      "275/388, train_loss: 0.1996, step time: 1.5305\n",
      "276/388, train_loss: 0.0820, step time: 1.5335\n",
      "277/388, train_loss: 0.2692, step time: 1.5375\n",
      "278/388, train_loss: 0.2627, step time: 1.5343\n",
      "279/388, train_loss: 0.2543, step time: 1.5337\n",
      "280/388, train_loss: 0.1695, step time: 1.5357\n",
      "281/388, train_loss: 0.5043, step time: 1.5341\n",
      "282/388, train_loss: 0.2338, step time: 1.5333\n",
      "283/388, train_loss: 0.1323, step time: 1.5388\n",
      "284/388, train_loss: 0.1649, step time: 1.5370\n",
      "285/388, train_loss: 0.2486, step time: 1.5327\n",
      "286/388, train_loss: 0.4144, step time: 1.5328\n",
      "287/388, train_loss: 0.2323, step time: 1.5321\n",
      "288/388, train_loss: 0.1828, step time: 1.5321\n",
      "289/388, train_loss: 0.2909, step time: 1.5333\n",
      "290/388, train_loss: 0.1008, step time: 1.5355\n",
      "291/388, train_loss: 0.3156, step time: 1.5347\n",
      "292/388, train_loss: 0.1954, step time: 1.5345\n",
      "293/388, train_loss: 0.1435, step time: 1.5316\n",
      "294/388, train_loss: 0.2297, step time: 1.5306\n",
      "295/388, train_loss: 0.0886, step time: 1.5334\n",
      "296/388, train_loss: 0.1172, step time: 1.5361\n",
      "297/388, train_loss: 0.0776, step time: 1.5346\n",
      "298/388, train_loss: 0.1141, step time: 1.5359\n",
      "299/388, train_loss: 0.3114, step time: 1.5364\n",
      "300/388, train_loss: 0.1859, step time: 1.5348\n",
      "301/388, train_loss: 0.0264, step time: 1.5356\n",
      "302/388, train_loss: 0.1435, step time: 1.5388\n",
      "303/388, train_loss: 0.2068, step time: 1.5358\n",
      "304/388, train_loss: 0.4232, step time: 1.5345\n",
      "305/388, train_loss: 0.1460, step time: 1.5358\n",
      "306/388, train_loss: 0.0507, step time: 1.5360\n",
      "307/388, train_loss: 0.1762, step time: 1.5357\n",
      "308/388, train_loss: 0.2172, step time: 1.5356\n",
      "309/388, train_loss: 0.1870, step time: 1.5369\n",
      "310/388, train_loss: 0.2922, step time: 1.5355\n",
      "311/388, train_loss: 0.1305, step time: 1.5325\n",
      "312/388, train_loss: 0.1564, step time: 1.5326\n",
      "313/388, train_loss: 0.1838, step time: 1.5326\n",
      "314/388, train_loss: 0.1282, step time: 1.5384\n",
      "315/388, train_loss: 0.2378, step time: 1.5383\n",
      "316/388, train_loss: 0.0922, step time: 1.5339\n",
      "317/388, train_loss: 0.0974, step time: 1.5338\n",
      "318/388, train_loss: 0.0967, step time: 1.5361\n",
      "319/388, train_loss: 0.2036, step time: 1.5379\n",
      "320/388, train_loss: 0.1234, step time: 1.5354\n",
      "321/388, train_loss: 0.0838, step time: 1.5404\n",
      "322/388, train_loss: 0.2455, step time: 1.5376\n",
      "323/388, train_loss: 0.2589, step time: 1.5350\n",
      "324/388, train_loss: 0.2695, step time: 1.5415\n",
      "325/388, train_loss: 0.1967, step time: 1.5319\n",
      "326/388, train_loss: 0.1614, step time: 1.5341\n",
      "327/388, train_loss: 0.1616, step time: 1.5338\n",
      "328/388, train_loss: 0.0855, step time: 1.5315\n",
      "329/388, train_loss: 0.1840, step time: 1.5346\n",
      "330/388, train_loss: 0.1637, step time: 1.5341\n",
      "331/388, train_loss: 0.0863, step time: 1.5334\n",
      "332/388, train_loss: 0.2048, step time: 1.5351\n",
      "333/388, train_loss: 0.1493, step time: 1.5323\n",
      "334/388, train_loss: 0.1047, step time: 1.5320\n",
      "335/388, train_loss: 0.1257, step time: 1.5400\n",
      "336/388, train_loss: 0.1145, step time: 1.5374\n",
      "337/388, train_loss: 0.1473, step time: 1.5383\n",
      "338/388, train_loss: 0.1105, step time: 1.5364\n",
      "339/388, train_loss: 0.0949, step time: 1.5322\n",
      "340/388, train_loss: 0.0849, step time: 1.5345\n",
      "341/388, train_loss: 0.2546, step time: 1.5365\n",
      "342/388, train_loss: 0.0888, step time: 1.5344\n",
      "343/388, train_loss: 0.4095, step time: 1.5369\n",
      "344/388, train_loss: 0.2211, step time: 1.5382\n",
      "345/388, train_loss: 0.1229, step time: 1.5321\n",
      "346/388, train_loss: 0.0815, step time: 1.5335\n",
      "347/388, train_loss: 0.1812, step time: 1.5341\n",
      "348/388, train_loss: 0.2547, step time: 1.5376\n",
      "349/388, train_loss: 0.1665, step time: 1.5348\n",
      "350/388, train_loss: 0.1881, step time: 1.5317\n",
      "351/388, train_loss: 0.1699, step time: 1.5319\n",
      "352/388, train_loss: 0.1967, step time: 1.5341\n",
      "353/388, train_loss: 0.2955, step time: 1.5397\n",
      "354/388, train_loss: 0.1869, step time: 1.5315\n",
      "355/388, train_loss: 0.0701, step time: 1.5318\n",
      "356/388, train_loss: 0.1289, step time: 1.5327\n",
      "357/388, train_loss: 0.0720, step time: 1.5363\n",
      "358/388, train_loss: 0.2453, step time: 1.5337\n",
      "359/388, train_loss: 0.2001, step time: 1.5302\n",
      "360/388, train_loss: 0.1394, step time: 1.5335\n",
      "361/388, train_loss: 0.0782, step time: 1.5396\n",
      "362/388, train_loss: 0.1765, step time: 1.5370\n",
      "363/388, train_loss: 0.2665, step time: 1.5348\n",
      "364/388, train_loss: 0.0854, step time: 1.5338\n",
      "365/388, train_loss: 0.0538, step time: 1.5360\n",
      "366/388, train_loss: 0.1160, step time: 1.5385\n",
      "367/388, train_loss: 0.3020, step time: 1.5377\n",
      "368/388, train_loss: 0.1109, step time: 1.5404\n",
      "369/388, train_loss: 0.1189, step time: 1.5370\n",
      "370/388, train_loss: 0.2963, step time: 1.5326\n",
      "371/388, train_loss: 0.1174, step time: 1.5379\n",
      "372/388, train_loss: 0.1014, step time: 1.5383\n",
      "373/388, train_loss: 0.0984, step time: 1.5330\n",
      "374/388, train_loss: 0.1233, step time: 1.5329\n",
      "375/388, train_loss: 0.1457, step time: 1.5348\n",
      "376/388, train_loss: 0.0571, step time: 1.5375\n",
      "377/388, train_loss: 0.3154, step time: 1.5346\n",
      "378/388, train_loss: 0.1979, step time: 1.5355\n",
      "379/388, train_loss: 0.1885, step time: 1.5409\n",
      "380/388, train_loss: 0.0544, step time: 1.5335\n",
      "381/388, train_loss: 0.1240, step time: 1.5347\n",
      "382/388, train_loss: 0.1401, step time: 1.5394\n",
      "383/388, train_loss: 0.1173, step time: 1.5380\n",
      "384/388, train_loss: 0.1386, step time: 1.5328\n",
      "385/388, train_loss: 0.2260, step time: 1.5346\n",
      "386/388, train_loss: 0.1886, step time: 1.5331\n",
      "387/388, train_loss: 0.2369, step time: 1.5336\n",
      "388/388, train_loss: 0.2416, step time: 1.5335\n",
      "epoch 76 average loss: 0.1631\n",
      "saved new best metric model\n",
      "current epoch: 76 current mean dice: 0.7794 tc: 0.8285 wt: 0.9064 et: 0.6033\n",
      "best mean dice: 0.7794 at epoch: 76\n",
      "time consuming of epoch 76 is: 703.7981\n",
      "----------\n",
      "epoch 77/100\n",
      "1/388, train_loss: 0.1601, step time: 1.5585\n",
      "2/388, train_loss: 0.0445, step time: 1.5359\n",
      "3/388, train_loss: 0.1539, step time: 1.5324\n",
      "4/388, train_loss: 0.0756, step time: 1.5343\n",
      "5/388, train_loss: 0.1370, step time: 1.5354\n",
      "6/388, train_loss: 0.0896, step time: 1.5324\n",
      "7/388, train_loss: 0.2437, step time: 1.5327\n",
      "8/388, train_loss: 0.0985, step time: 1.5361\n",
      "9/388, train_loss: 0.1360, step time: 1.5329\n",
      "10/388, train_loss: 0.2071, step time: 1.5358\n",
      "11/388, train_loss: 0.2091, step time: 1.5399\n",
      "12/388, train_loss: 0.3017, step time: 1.5338\n",
      "13/388, train_loss: 0.1839, step time: 1.5347\n",
      "14/388, train_loss: 0.2556, step time: 1.5369\n",
      "15/388, train_loss: 0.1097, step time: 1.5385\n",
      "16/388, train_loss: 0.2282, step time: 1.5389\n",
      "17/388, train_loss: 0.2834, step time: 1.5343\n",
      "18/388, train_loss: 0.1497, step time: 1.5361\n",
      "19/388, train_loss: 0.1206, step time: 1.5334\n",
      "20/388, train_loss: 0.2397, step time: 1.5396\n",
      "21/388, train_loss: 0.1530, step time: 1.5381\n",
      "22/388, train_loss: 0.3482, step time: 1.5342\n",
      "23/388, train_loss: 0.1672, step time: 1.5330\n",
      "24/388, train_loss: 0.3464, step time: 1.5340\n",
      "25/388, train_loss: 0.0803, step time: 1.5358\n",
      "26/388, train_loss: 0.0689, step time: 1.5341\n",
      "27/388, train_loss: 0.1937, step time: 1.5400\n",
      "28/388, train_loss: 0.0855, step time: 1.5366\n",
      "29/388, train_loss: 0.0476, step time: 1.5363\n",
      "30/388, train_loss: 0.0458, step time: 1.5388\n",
      "31/388, train_loss: 0.2584, step time: 1.5345\n",
      "32/388, train_loss: 0.0947, step time: 1.5359\n",
      "33/388, train_loss: 0.0821, step time: 1.5312\n",
      "34/388, train_loss: 0.1776, step time: 1.5358\n",
      "35/388, train_loss: 0.1930, step time: 1.5363\n",
      "36/388, train_loss: 0.1547, step time: 1.5366\n",
      "37/388, train_loss: 0.0958, step time: 1.5336\n",
      "38/388, train_loss: 0.2475, step time: 1.5299\n",
      "39/388, train_loss: 0.0893, step time: 1.5332\n",
      "40/388, train_loss: 0.1600, step time: 1.5357\n",
      "41/388, train_loss: 0.0675, step time: 1.5377\n",
      "42/388, train_loss: 0.2398, step time: 1.5324\n",
      "43/388, train_loss: 0.0831, step time: 1.5411\n",
      "44/388, train_loss: 0.0382, step time: 1.5404\n",
      "45/388, train_loss: 0.2099, step time: 1.5343\n",
      "46/388, train_loss: 0.1019, step time: 1.5328\n",
      "47/388, train_loss: 0.0659, step time: 1.5308\n",
      "48/388, train_loss: 0.1354, step time: 1.5330\n",
      "49/388, train_loss: 0.2034, step time: 1.5347\n",
      "50/388, train_loss: 0.0967, step time: 1.5341\n",
      "51/388, train_loss: 0.2621, step time: 1.5379\n",
      "52/388, train_loss: 0.1503, step time: 1.5338\n",
      "53/388, train_loss: 0.1727, step time: 1.5344\n",
      "54/388, train_loss: 0.1320, step time: 1.5317\n",
      "55/388, train_loss: 0.1236, step time: 1.5338\n",
      "56/388, train_loss: 0.1755, step time: 1.5324\n",
      "57/388, train_loss: 0.0770, step time: 1.5403\n",
      "58/388, train_loss: 0.2218, step time: 1.5381\n",
      "59/388, train_loss: 0.1523, step time: 1.5368\n",
      "60/388, train_loss: 0.0504, step time: 1.5327\n",
      "61/388, train_loss: 0.1162, step time: 1.5332\n",
      "62/388, train_loss: 0.0622, step time: 1.5346\n",
      "63/388, train_loss: 0.1846, step time: 1.5352\n",
      "64/388, train_loss: 0.0896, step time: 1.5532\n",
      "65/388, train_loss: 0.2045, step time: 1.5345\n",
      "66/388, train_loss: 0.1017, step time: 1.5327\n",
      "67/388, train_loss: 0.3314, step time: 1.5355\n",
      "68/388, train_loss: 0.1078, step time: 1.5366\n",
      "69/388, train_loss: 0.1627, step time: 1.5406\n",
      "70/388, train_loss: 0.0731, step time: 1.5369\n",
      "71/388, train_loss: 0.2243, step time: 1.5330\n",
      "72/388, train_loss: 0.0808, step time: 1.5348\n",
      "73/388, train_loss: 0.1034, step time: 1.5357\n",
      "74/388, train_loss: 0.1139, step time: 1.5352\n",
      "75/388, train_loss: 0.1930, step time: 1.5343\n",
      "76/388, train_loss: 0.1634, step time: 1.5304\n",
      "77/388, train_loss: 0.1899, step time: 1.5312\n",
      "78/388, train_loss: 0.4154, step time: 1.5339\n",
      "79/388, train_loss: 0.2039, step time: 1.5401\n",
      "80/388, train_loss: 0.3948, step time: 1.5338\n",
      "81/388, train_loss: 0.0658, step time: 1.5318\n",
      "82/388, train_loss: 0.1361, step time: 1.5330\n",
      "83/388, train_loss: 0.1953, step time: 1.5333\n",
      "84/388, train_loss: 0.1296, step time: 1.5348\n",
      "85/388, train_loss: 0.1892, step time: 1.5373\n",
      "86/388, train_loss: 0.2928, step time: 1.5314\n",
      "87/388, train_loss: 0.0959, step time: 1.5309\n",
      "88/388, train_loss: 0.0960, step time: 1.5325\n",
      "89/388, train_loss: 0.2071, step time: 1.5316\n",
      "90/388, train_loss: 0.1897, step time: 1.5426\n",
      "91/388, train_loss: 0.1910, step time: 1.5342\n",
      "92/388, train_loss: 0.1662, step time: 1.5343\n",
      "93/388, train_loss: 0.0895, step time: 1.5353\n",
      "94/388, train_loss: 0.3337, step time: 1.5370\n",
      "95/388, train_loss: 0.1156, step time: 1.5348\n",
      "96/388, train_loss: 0.2070, step time: 1.5318\n",
      "97/388, train_loss: 0.1808, step time: 1.5329\n",
      "98/388, train_loss: 0.3643, step time: 1.5324\n",
      "99/388, train_loss: 0.1104, step time: 1.5378\n",
      "100/388, train_loss: 0.1347, step time: 1.5378\n",
      "101/388, train_loss: 0.1440, step time: 1.5354\n",
      "102/388, train_loss: 0.0997, step time: 1.5325\n",
      "103/388, train_loss: 0.0836, step time: 1.5323\n",
      "104/388, train_loss: 0.1017, step time: 1.5302\n",
      "105/388, train_loss: 0.1134, step time: 1.5307\n",
      "106/388, train_loss: 0.1413, step time: 1.5339\n",
      "107/388, train_loss: 0.2562, step time: 1.5363\n",
      "108/388, train_loss: 0.1402, step time: 1.5356\n",
      "109/388, train_loss: 0.1724, step time: 1.5315\n",
      "110/388, train_loss: 0.1374, step time: 1.5328\n",
      "111/388, train_loss: 0.0991, step time: 1.5339\n",
      "112/388, train_loss: 0.1567, step time: 1.5344\n",
      "113/388, train_loss: 0.1270, step time: 1.5366\n",
      "114/388, train_loss: 0.1932, step time: 1.5323\n",
      "115/388, train_loss: 0.2000, step time: 1.5313\n",
      "116/388, train_loss: 0.1724, step time: 1.5343\n",
      "117/388, train_loss: 0.1650, step time: 1.5338\n",
      "118/388, train_loss: 0.1155, step time: 1.5374\n",
      "119/388, train_loss: 0.1125, step time: 1.5353\n",
      "120/388, train_loss: 0.1332, step time: 1.5378\n",
      "121/388, train_loss: 0.2404, step time: 1.5318\n",
      "122/388, train_loss: 0.1900, step time: 1.5342\n",
      "123/388, train_loss: 0.3115, step time: 1.5331\n",
      "124/388, train_loss: 0.2320, step time: 1.5308\n",
      "125/388, train_loss: 0.1047, step time: 1.5378\n",
      "126/388, train_loss: 0.0818, step time: 1.5366\n",
      "127/388, train_loss: 0.4417, step time: 1.5319\n",
      "128/388, train_loss: 0.2183, step time: 1.5318\n",
      "129/388, train_loss: 0.1678, step time: 1.5337\n",
      "130/388, train_loss: 0.1331, step time: 1.5443\n",
      "131/388, train_loss: 0.0985, step time: 1.5367\n",
      "132/388, train_loss: 0.0730, step time: 1.5317\n",
      "133/388, train_loss: 0.1240, step time: 1.5373\n",
      "134/388, train_loss: 0.1284, step time: 1.5323\n",
      "135/388, train_loss: 0.0830, step time: 1.5414\n",
      "136/388, train_loss: 0.3169, step time: 1.5376\n",
      "137/388, train_loss: 0.2344, step time: 1.5349\n",
      "138/388, train_loss: 0.1456, step time: 1.5329\n",
      "139/388, train_loss: 0.1423, step time: 1.5325\n",
      "140/388, train_loss: 0.2044, step time: 1.5339\n",
      "141/388, train_loss: 0.0559, step time: 1.5357\n",
      "142/388, train_loss: 0.0643, step time: 1.5366\n",
      "143/388, train_loss: 0.1009, step time: 1.5350\n",
      "144/388, train_loss: 0.2169, step time: 1.5313\n",
      "145/388, train_loss: 0.0370, step time: 1.5322\n",
      "146/388, train_loss: 0.3513, step time: 1.5319\n",
      "147/388, train_loss: 0.0681, step time: 1.5390\n",
      "148/388, train_loss: 0.2782, step time: 1.5353\n",
      "149/388, train_loss: 0.1436, step time: 1.5376\n",
      "150/388, train_loss: 0.0624, step time: 1.5330\n",
      "151/388, train_loss: 0.0589, step time: 1.5364\n",
      "152/388, train_loss: 0.0926, step time: 1.5326\n",
      "153/388, train_loss: 0.1097, step time: 1.5325\n",
      "154/388, train_loss: 0.1230, step time: 1.5364\n",
      "155/388, train_loss: 0.0974, step time: 1.5341\n",
      "156/388, train_loss: 0.0675, step time: 1.5346\n",
      "157/388, train_loss: 0.1638, step time: 1.5330\n",
      "158/388, train_loss: 0.0384, step time: 1.5334\n",
      "159/388, train_loss: 0.1339, step time: 1.5335\n",
      "160/388, train_loss: 0.0949, step time: 1.5340\n",
      "161/388, train_loss: 0.1825, step time: 1.5355\n",
      "162/388, train_loss: 0.1233, step time: 1.5381\n",
      "163/388, train_loss: 0.4450, step time: 1.5333\n",
      "164/388, train_loss: 0.0764, step time: 1.5304\n",
      "165/388, train_loss: 0.1188, step time: 1.5338\n",
      "166/388, train_loss: 0.1788, step time: 1.5314\n",
      "167/388, train_loss: 0.1524, step time: 1.5359\n",
      "168/388, train_loss: 0.5216, step time: 1.5386\n",
      "169/388, train_loss: 0.1503, step time: 1.5330\n",
      "170/388, train_loss: 0.1006, step time: 1.5304\n",
      "171/388, train_loss: 0.1936, step time: 1.5324\n",
      "172/388, train_loss: 0.0479, step time: 1.5363\n",
      "173/388, train_loss: 0.1316, step time: 1.5334\n",
      "174/388, train_loss: 0.1756, step time: 1.5329\n",
      "175/388, train_loss: 0.2314, step time: 1.5321\n",
      "176/388, train_loss: 0.0592, step time: 1.5356\n",
      "177/388, train_loss: 0.0919, step time: 1.5322\n",
      "178/388, train_loss: 0.3917, step time: 1.5344\n",
      "179/388, train_loss: 0.1234, step time: 1.5370\n",
      "180/388, train_loss: 0.2271, step time: 1.5355\n",
      "181/388, train_loss: 0.2419, step time: 1.5364\n",
      "182/388, train_loss: 0.1881, step time: 1.5330\n",
      "183/388, train_loss: 0.0860, step time: 1.5307\n",
      "184/388, train_loss: 0.1401, step time: 1.5350\n",
      "185/388, train_loss: 0.1046, step time: 1.5348\n",
      "186/388, train_loss: 0.1290, step time: 1.5384\n",
      "187/388, train_loss: 0.0739, step time: 1.5321\n",
      "188/388, train_loss: 0.0800, step time: 1.5357\n",
      "189/388, train_loss: 0.1740, step time: 1.5304\n",
      "190/388, train_loss: 0.2215, step time: 1.5360\n",
      "191/388, train_loss: 0.0910, step time: 1.5377\n",
      "192/388, train_loss: 0.5238, step time: 1.5386\n",
      "193/388, train_loss: 0.0671, step time: 1.5334\n",
      "194/388, train_loss: 0.2008, step time: 1.5303\n",
      "195/388, train_loss: 0.1049, step time: 1.5350\n",
      "196/388, train_loss: 0.3161, step time: 1.5360\n",
      "197/388, train_loss: 0.2497, step time: 1.5357\n",
      "198/388, train_loss: 0.4574, step time: 1.5362\n",
      "199/388, train_loss: 0.1099, step time: 1.5334\n",
      "200/388, train_loss: 0.0972, step time: 1.5355\n",
      "201/388, train_loss: 0.1150, step time: 1.5310\n",
      "202/388, train_loss: 0.1833, step time: 1.5358\n",
      "203/388, train_loss: 0.1663, step time: 1.5342\n",
      "204/388, train_loss: 0.1031, step time: 1.5326\n",
      "205/388, train_loss: 0.1634, step time: 1.5390\n",
      "206/388, train_loss: 0.0953, step time: 1.5365\n",
      "207/388, train_loss: 0.0509, step time: 1.5369\n",
      "208/388, train_loss: 0.0735, step time: 1.5347\n",
      "209/388, train_loss: 0.2507, step time: 1.5320\n",
      "210/388, train_loss: 0.2937, step time: 1.5325\n",
      "211/388, train_loss: 0.2232, step time: 1.5329\n",
      "212/388, train_loss: 0.1846, step time: 1.5343\n",
      "213/388, train_loss: 0.2234, step time: 1.5354\n",
      "214/388, train_loss: 0.0988, step time: 1.5370\n",
      "215/388, train_loss: 0.1028, step time: 1.5342\n",
      "216/388, train_loss: 0.1869, step time: 1.5323\n",
      "217/388, train_loss: 0.1331, step time: 1.5317\n",
      "218/388, train_loss: 0.2349, step time: 1.5442\n",
      "219/388, train_loss: 0.1071, step time: 1.5361\n",
      "220/388, train_loss: 0.2005, step time: 1.5359\n",
      "221/388, train_loss: 0.1976, step time: 1.5301\n",
      "222/388, train_loss: 0.1816, step time: 1.5319\n",
      "223/388, train_loss: 0.1809, step time: 1.5337\n",
      "224/388, train_loss: 0.4000, step time: 1.5395\n",
      "225/388, train_loss: 0.0927, step time: 1.5458\n",
      "226/388, train_loss: 0.2589, step time: 1.5323\n",
      "227/388, train_loss: 0.1356, step time: 1.5318\n",
      "228/388, train_loss: 0.1432, step time: 1.5354\n",
      "229/388, train_loss: 0.1247, step time: 1.5380\n",
      "230/388, train_loss: 0.2513, step time: 1.5363\n",
      "231/388, train_loss: 0.0932, step time: 1.5327\n",
      "232/388, train_loss: 0.0974, step time: 1.5323\n",
      "233/388, train_loss: 0.0891, step time: 1.5337\n",
      "234/388, train_loss: 0.1353, step time: 1.5328\n",
      "235/388, train_loss: 0.0820, step time: 1.5426\n",
      "236/388, train_loss: 0.0848, step time: 1.5385\n",
      "237/388, train_loss: 0.0290, step time: 1.5338\n",
      "238/388, train_loss: 0.1209, step time: 1.5361\n",
      "239/388, train_loss: 0.1961, step time: 1.5325\n",
      "240/388, train_loss: 0.0898, step time: 1.5357\n",
      "241/388, train_loss: 0.1770, step time: 1.5384\n",
      "242/388, train_loss: 0.0911, step time: 1.5375\n",
      "243/388, train_loss: 0.1211, step time: 1.5427\n",
      "244/388, train_loss: 0.4474, step time: 1.5325\n",
      "245/388, train_loss: 0.2516, step time: 1.5369\n",
      "246/388, train_loss: 0.2241, step time: 1.5380\n",
      "247/388, train_loss: 0.0873, step time: 1.5377\n",
      "248/388, train_loss: 0.1196, step time: 1.5341\n",
      "249/388, train_loss: 0.2167, step time: 1.5338\n",
      "250/388, train_loss: 0.1582, step time: 1.5338\n",
      "251/388, train_loss: 0.0493, step time: 1.5360\n",
      "252/388, train_loss: 0.2475, step time: 1.5391\n",
      "253/388, train_loss: 0.0573, step time: 1.5335\n",
      "254/388, train_loss: 0.1756, step time: 1.5341\n",
      "255/388, train_loss: 0.1423, step time: 1.5327\n",
      "256/388, train_loss: 0.0996, step time: 1.5336\n",
      "257/388, train_loss: 0.1717, step time: 1.5359\n",
      "258/388, train_loss: 0.0964, step time: 1.5361\n",
      "259/388, train_loss: 0.2032, step time: 1.5348\n",
      "260/388, train_loss: 0.0976, step time: 1.5330\n",
      "261/388, train_loss: 0.3023, step time: 1.5321\n",
      "262/388, train_loss: 0.4975, step time: 1.5341\n",
      "263/388, train_loss: 0.1185, step time: 1.5357\n",
      "264/388, train_loss: 0.0974, step time: 1.5429\n",
      "265/388, train_loss: 0.1078, step time: 1.5369\n",
      "266/388, train_loss: 0.2056, step time: 1.5344\n",
      "267/388, train_loss: 0.1216, step time: 1.5347\n",
      "268/388, train_loss: 0.2000, step time: 1.5324\n",
      "269/388, train_loss: 0.1012, step time: 1.5302\n",
      "270/388, train_loss: 0.1193, step time: 1.5335\n",
      "271/388, train_loss: 0.2973, step time: 1.5376\n",
      "272/388, train_loss: 0.1020, step time: 1.5363\n",
      "273/388, train_loss: 0.1892, step time: 1.5597\n",
      "274/388, train_loss: 0.1080, step time: 1.5336\n",
      "275/388, train_loss: 0.1127, step time: 1.5324\n",
      "276/388, train_loss: 0.2405, step time: 1.5324\n",
      "277/388, train_loss: 0.0384, step time: 1.5386\n",
      "278/388, train_loss: 0.1967, step time: 1.5387\n",
      "279/388, train_loss: 0.5132, step time: 1.5335\n",
      "280/388, train_loss: 0.1961, step time: 1.5324\n",
      "281/388, train_loss: 0.3171, step time: 1.5350\n",
      "282/388, train_loss: 0.1615, step time: 1.5351\n",
      "283/388, train_loss: 0.1150, step time: 1.5469\n",
      "284/388, train_loss: 0.1522, step time: 1.5294\n",
      "285/388, train_loss: 0.1962, step time: 1.5325\n",
      "286/388, train_loss: 0.1085, step time: 1.5307\n",
      "287/388, train_loss: 0.2571, step time: 1.5369\n",
      "288/388, train_loss: 0.0254, step time: 1.5357\n",
      "289/388, train_loss: 0.1566, step time: 1.5312\n",
      "290/388, train_loss: 0.1599, step time: 1.5327\n",
      "291/388, train_loss: 0.0630, step time: 1.5291\n",
      "292/388, train_loss: 0.2385, step time: 1.5320\n",
      "293/388, train_loss: 0.1413, step time: 1.5364\n",
      "294/388, train_loss: 0.1453, step time: 1.5343\n",
      "295/388, train_loss: 0.1314, step time: 1.5335\n",
      "296/388, train_loss: 0.0855, step time: 1.5321\n",
      "297/388, train_loss: 0.2918, step time: 1.5317\n",
      "298/388, train_loss: 0.0824, step time: 1.5301\n",
      "299/388, train_loss: 0.2708, step time: 1.5341\n",
      "300/388, train_loss: 0.1671, step time: 1.5357\n",
      "301/388, train_loss: 0.2667, step time: 1.5330\n",
      "302/388, train_loss: 0.1157, step time: 1.5296\n",
      "303/388, train_loss: 0.2899, step time: 1.5275\n",
      "304/388, train_loss: 0.2231, step time: 1.5299\n",
      "305/388, train_loss: 0.0894, step time: 1.5316\n",
      "306/388, train_loss: 0.3208, step time: 1.5346\n",
      "307/388, train_loss: 0.0951, step time: 1.5354\n",
      "308/388, train_loss: 0.2572, step time: 1.5307\n",
      "309/388, train_loss: 0.1121, step time: 1.5288\n",
      "310/388, train_loss: 0.0967, step time: 1.5336\n",
      "311/388, train_loss: 0.1647, step time: 1.5339\n",
      "312/388, train_loss: 0.1543, step time: 1.5305\n",
      "313/388, train_loss: 0.0720, step time: 1.5334\n",
      "314/388, train_loss: 0.0926, step time: 1.5327\n",
      "315/388, train_loss: 0.1574, step time: 1.5290\n",
      "316/388, train_loss: 0.1852, step time: 1.5292\n",
      "317/388, train_loss: 0.0838, step time: 1.5297\n",
      "318/388, train_loss: 0.0948, step time: 1.5315\n",
      "319/388, train_loss: 0.0677, step time: 1.5270\n",
      "320/388, train_loss: 0.1577, step time: 1.5290\n",
      "321/388, train_loss: 0.2066, step time: 1.5343\n",
      "322/388, train_loss: 0.1852, step time: 1.5360\n",
      "323/388, train_loss: 0.2102, step time: 1.5284\n",
      "324/388, train_loss: 0.0859, step time: 1.5316\n",
      "325/388, train_loss: 0.1989, step time: 1.5327\n",
      "326/388, train_loss: 0.1332, step time: 1.5317\n",
      "327/388, train_loss: 0.1256, step time: 1.5346\n",
      "328/388, train_loss: 0.0594, step time: 1.5310\n",
      "329/388, train_loss: 0.0896, step time: 1.5322\n",
      "330/388, train_loss: 0.1590, step time: 1.5332\n",
      "331/388, train_loss: 0.1736, step time: 1.5321\n",
      "332/388, train_loss: 0.3649, step time: 1.5295\n",
      "333/388, train_loss: 0.0873, step time: 1.5319\n",
      "334/388, train_loss: 0.0581, step time: 1.5323\n",
      "335/388, train_loss: 0.1691, step time: 1.5351\n",
      "336/388, train_loss: 0.1726, step time: 1.5347\n",
      "337/388, train_loss: 0.2714, step time: 1.5349\n",
      "338/388, train_loss: 0.1514, step time: 1.5292\n",
      "339/388, train_loss: 0.0611, step time: 1.5289\n",
      "340/388, train_loss: 0.1204, step time: 1.5296\n",
      "341/388, train_loss: 0.2521, step time: 1.5300\n",
      "342/388, train_loss: 0.0902, step time: 1.5346\n",
      "343/388, train_loss: 0.1903, step time: 1.5347\n",
      "344/388, train_loss: 0.1177, step time: 1.5292\n",
      "345/388, train_loss: 0.0903, step time: 1.5317\n",
      "346/388, train_loss: 0.1390, step time: 1.5302\n",
      "347/388, train_loss: 0.1136, step time: 1.5292\n",
      "348/388, train_loss: 0.0838, step time: 1.5371\n",
      "349/388, train_loss: 0.2004, step time: 1.5345\n",
      "350/388, train_loss: 0.0918, step time: 1.5319\n",
      "351/388, train_loss: 0.2019, step time: 1.5313\n",
      "352/388, train_loss: 0.1972, step time: 1.5315\n",
      "353/388, train_loss: 0.2473, step time: 1.5266\n",
      "354/388, train_loss: 0.3828, step time: 1.5338\n",
      "355/388, train_loss: 0.2752, step time: 1.5356\n",
      "356/388, train_loss: 0.3203, step time: 1.5362\n",
      "357/388, train_loss: 0.0856, step time: 1.5330\n",
      "358/388, train_loss: 0.1262, step time: 1.5334\n",
      "359/388, train_loss: 0.2172, step time: 1.5300\n",
      "360/388, train_loss: 0.2325, step time: 1.5322\n",
      "361/388, train_loss: 0.1467, step time: 1.5340\n",
      "362/388, train_loss: 0.0761, step time: 1.5328\n",
      "363/388, train_loss: 0.0700, step time: 1.5337\n",
      "364/388, train_loss: 0.0892, step time: 1.5306\n",
      "365/388, train_loss: 0.1474, step time: 1.5308\n",
      "366/388, train_loss: 0.2933, step time: 1.5315\n",
      "367/388, train_loss: 0.1322, step time: 1.5439\n",
      "368/388, train_loss: 0.0975, step time: 1.5314\n",
      "369/388, train_loss: 0.0609, step time: 1.5603\n",
      "370/388, train_loss: 0.2121, step time: 1.5316\n",
      "371/388, train_loss: 0.1509, step time: 1.5304\n",
      "372/388, train_loss: 0.0659, step time: 1.5302\n",
      "373/388, train_loss: 0.1500, step time: 1.5346\n",
      "374/388, train_loss: 0.0313, step time: 1.5375\n",
      "375/388, train_loss: 0.0493, step time: 1.5299\n",
      "376/388, train_loss: 0.1774, step time: 1.5318\n",
      "377/388, train_loss: 0.0774, step time: 1.5297\n",
      "378/388, train_loss: 0.1930, step time: 1.5330\n",
      "379/388, train_loss: 0.1349, step time: 1.5351\n",
      "380/388, train_loss: 0.1263, step time: 1.5428\n",
      "381/388, train_loss: 0.1795, step time: 1.5328\n",
      "382/388, train_loss: 0.2326, step time: 1.5358\n",
      "383/388, train_loss: 0.2742, step time: 1.5332\n",
      "384/388, train_loss: 0.0906, step time: 1.5298\n",
      "385/388, train_loss: 0.2012, step time: 1.5449\n",
      "386/388, train_loss: 0.1667, step time: 1.5326\n",
      "387/388, train_loss: 0.2853, step time: 1.5334\n",
      "388/388, train_loss: 0.1338, step time: 1.5337\n",
      "epoch 77 average loss: 0.1623\n",
      "current epoch: 77 current mean dice: 0.7781 tc: 0.8254 wt: 0.9069 et: 0.6021\n",
      "best mean dice: 0.7794 at epoch: 76\n",
      "time consuming of epoch 77 is: 703.1557\n",
      "----------\n",
      "epoch 78/100\n",
      "1/388, train_loss: 0.1605, step time: 1.5469\n",
      "2/388, train_loss: 0.1060, step time: 1.5369\n",
      "3/388, train_loss: 0.1192, step time: 1.5367\n",
      "4/388, train_loss: 0.0855, step time: 1.5340\n",
      "5/388, train_loss: 0.1925, step time: 1.5345\n",
      "6/388, train_loss: 0.2378, step time: 1.5354\n",
      "7/388, train_loss: 0.3826, step time: 1.5305\n",
      "8/388, train_loss: 0.1192, step time: 1.5309\n",
      "9/388, train_loss: 0.1099, step time: 1.5376\n",
      "10/388, train_loss: 0.2053, step time: 1.5347\n",
      "11/388, train_loss: 0.0805, step time: 1.5343\n",
      "12/388, train_loss: 0.1098, step time: 1.5348\n",
      "13/388, train_loss: 0.1167, step time: 1.5443\n",
      "14/388, train_loss: 0.2156, step time: 1.5321\n",
      "15/388, train_loss: 0.1278, step time: 1.5324\n",
      "16/388, train_loss: 0.0764, step time: 1.5293\n",
      "17/388, train_loss: 0.1743, step time: 1.5371\n",
      "18/388, train_loss: 0.2498, step time: 1.5388\n",
      "19/388, train_loss: 0.1553, step time: 1.5354\n",
      "20/388, train_loss: 0.5168, step time: 1.5324\n",
      "21/388, train_loss: 0.2007, step time: 1.5332\n",
      "22/388, train_loss: 0.1568, step time: 1.5412\n",
      "23/388, train_loss: 0.1611, step time: 1.5304\n",
      "24/388, train_loss: 0.1596, step time: 1.5319\n",
      "25/388, train_loss: 0.2041, step time: 1.5319\n",
      "26/388, train_loss: 0.2613, step time: 1.5317\n",
      "27/388, train_loss: 0.1356, step time: 1.5330\n",
      "28/388, train_loss: 0.1479, step time: 1.5359\n",
      "29/388, train_loss: 0.2574, step time: 1.5356\n",
      "30/388, train_loss: 0.1746, step time: 1.5350\n",
      "31/388, train_loss: 0.3261, step time: 1.5325\n",
      "32/388, train_loss: 0.1827, step time: 1.5303\n",
      "33/388, train_loss: 0.0744, step time: 1.5387\n",
      "34/388, train_loss: 0.1025, step time: 1.5352\n",
      "35/388, train_loss: 0.2964, step time: 1.5358\n",
      "36/388, train_loss: 0.0946, step time: 1.5284\n",
      "37/388, train_loss: 0.1882, step time: 1.5309\n",
      "38/388, train_loss: 0.0769, step time: 1.5374\n",
      "39/388, train_loss: 0.3473, step time: 1.5326\n",
      "40/388, train_loss: 0.1336, step time: 1.5342\n",
      "41/388, train_loss: 0.2015, step time: 1.5492\n",
      "42/388, train_loss: 0.0364, step time: 1.5503\n",
      "43/388, train_loss: 0.0732, step time: 1.5449\n",
      "44/388, train_loss: 0.1071, step time: 1.5434\n",
      "45/388, train_loss: 0.0706, step time: 1.5322\n",
      "46/388, train_loss: 0.1561, step time: 1.5353\n",
      "47/388, train_loss: 0.0979, step time: 1.5375\n",
      "48/388, train_loss: 0.2308, step time: 1.5473\n",
      "49/388, train_loss: 0.3457, step time: 1.5491\n",
      "50/388, train_loss: 0.0896, step time: 1.5458\n",
      "51/388, train_loss: 0.0893, step time: 1.5297\n",
      "52/388, train_loss: 0.1264, step time: 1.5305\n",
      "53/388, train_loss: 0.2256, step time: 1.5523\n",
      "54/388, train_loss: 0.4130, step time: 1.5360\n",
      "55/388, train_loss: 0.0804, step time: 1.5304\n",
      "56/388, train_loss: 0.1316, step time: 1.5325\n",
      "57/388, train_loss: 0.1302, step time: 1.5430\n",
      "58/388, train_loss: 0.0981, step time: 1.5323\n",
      "59/388, train_loss: 0.1927, step time: 1.5431\n",
      "60/388, train_loss: 0.0859, step time: 1.5332\n",
      "61/388, train_loss: 0.0446, step time: 1.5443\n",
      "62/388, train_loss: 0.0957, step time: 1.5368\n",
      "63/388, train_loss: 0.1127, step time: 1.5352\n",
      "64/388, train_loss: 0.1325, step time: 1.5333\n",
      "65/388, train_loss: 0.1341, step time: 1.5435\n",
      "66/388, train_loss: 0.0848, step time: 1.5329\n",
      "67/388, train_loss: 0.1290, step time: 1.5386\n",
      "68/388, train_loss: 0.2026, step time: 1.5340\n",
      "69/388, train_loss: 0.1161, step time: 1.5418\n",
      "70/388, train_loss: 0.0592, step time: 1.5357\n",
      "71/388, train_loss: 0.2351, step time: 1.5392\n",
      "72/388, train_loss: 0.2130, step time: 1.5353\n",
      "73/388, train_loss: 0.1819, step time: 1.5446\n",
      "74/388, train_loss: 0.0898, step time: 1.5478\n",
      "75/388, train_loss: 0.0920, step time: 1.5411\n",
      "76/388, train_loss: 0.1605, step time: 1.5355\n",
      "77/388, train_loss: 0.0524, step time: 1.5466\n",
      "78/388, train_loss: 0.2193, step time: 1.5396\n",
      "79/388, train_loss: 0.1929, step time: 1.5341\n",
      "80/388, train_loss: 0.1320, step time: 1.5367\n",
      "81/388, train_loss: 0.0769, step time: 1.5427\n",
      "82/388, train_loss: 0.2157, step time: 1.5367\n",
      "83/388, train_loss: 0.1939, step time: 1.5379\n",
      "84/388, train_loss: 0.4773, step time: 1.5335\n",
      "85/388, train_loss: 0.2275, step time: 1.5435\n",
      "86/388, train_loss: 0.1270, step time: 1.5344\n",
      "87/388, train_loss: 0.2525, step time: 1.5326\n",
      "88/388, train_loss: 0.3406, step time: 1.5371\n",
      "89/388, train_loss: 0.3125, step time: 1.5442\n",
      "90/388, train_loss: 0.0853, step time: 1.5305\n",
      "91/388, train_loss: 0.1268, step time: 1.5365\n",
      "92/388, train_loss: 0.1441, step time: 1.5379\n",
      "93/388, train_loss: 0.1419, step time: 1.5446\n",
      "94/388, train_loss: 0.1588, step time: 1.5318\n",
      "95/388, train_loss: 0.0888, step time: 1.5309\n",
      "96/388, train_loss: 0.0879, step time: 1.5389\n",
      "97/388, train_loss: 0.2032, step time: 1.5491\n",
      "98/388, train_loss: 0.1835, step time: 1.5346\n",
      "99/388, train_loss: 0.1309, step time: 1.5488\n",
      "100/388, train_loss: 0.2571, step time: 1.5352\n",
      "101/388, train_loss: 0.0621, step time: 1.5423\n",
      "102/388, train_loss: 0.1830, step time: 1.5341\n",
      "103/388, train_loss: 0.0957, step time: 1.5325\n",
      "104/388, train_loss: 0.0912, step time: 1.5370\n",
      "105/388, train_loss: 0.2782, step time: 1.5492\n",
      "106/388, train_loss: 0.1090, step time: 1.5318\n",
      "107/388, train_loss: 0.1442, step time: 1.5468\n",
      "108/388, train_loss: 0.2021, step time: 1.5349\n",
      "109/388, train_loss: 0.2143, step time: 1.5443\n",
      "110/388, train_loss: 0.0851, step time: 1.5342\n",
      "111/388, train_loss: 0.1313, step time: 1.5376\n",
      "112/388, train_loss: 0.1430, step time: 1.5319\n",
      "113/388, train_loss: 0.0544, step time: 1.5427\n",
      "114/388, train_loss: 0.1558, step time: 1.5306\n",
      "115/388, train_loss: 0.1561, step time: 1.5342\n",
      "116/388, train_loss: 0.0990, step time: 1.5338\n",
      "117/388, train_loss: 0.0927, step time: 1.5486\n",
      "118/388, train_loss: 0.3245, step time: 1.5301\n",
      "119/388, train_loss: 0.2217, step time: 1.5332\n",
      "120/388, train_loss: 0.1537, step time: 1.5340\n",
      "121/388, train_loss: 0.1006, step time: 1.5500\n",
      "122/388, train_loss: 0.1921, step time: 1.5316\n",
      "123/388, train_loss: 0.2839, step time: 1.5323\n",
      "124/388, train_loss: 0.1609, step time: 1.5366\n",
      "125/388, train_loss: 0.1284, step time: 1.5489\n",
      "126/388, train_loss: 0.0975, step time: 1.5307\n",
      "127/388, train_loss: 0.4205, step time: 1.5321\n",
      "128/388, train_loss: 0.0404, step time: 1.5346\n",
      "129/388, train_loss: 0.1079, step time: 1.5464\n",
      "130/388, train_loss: 0.1027, step time: 1.5310\n",
      "131/388, train_loss: 0.1514, step time: 1.5321\n",
      "132/388, train_loss: 0.1903, step time: 1.5311\n",
      "133/388, train_loss: 0.1194, step time: 1.5463\n",
      "134/388, train_loss: 0.1735, step time: 1.5344\n",
      "135/388, train_loss: 0.0943, step time: 1.5281\n",
      "136/388, train_loss: 0.0476, step time: 1.5336\n",
      "137/388, train_loss: 0.3670, step time: 1.5508\n",
      "138/388, train_loss: 0.2862, step time: 1.5341\n",
      "139/388, train_loss: 0.1004, step time: 1.5321\n",
      "140/388, train_loss: 0.1120, step time: 1.5314\n",
      "141/388, train_loss: 0.1089, step time: 1.5455\n",
      "142/388, train_loss: 0.0885, step time: 1.5378\n",
      "143/388, train_loss: 0.2722, step time: 1.5310\n",
      "144/388, train_loss: 0.0304, step time: 1.5327\n",
      "145/388, train_loss: 0.1904, step time: 1.5443\n",
      "146/388, train_loss: 0.2863, step time: 1.5385\n",
      "147/388, train_loss: 0.0303, step time: 1.5356\n",
      "148/388, train_loss: 0.1014, step time: 1.5310\n",
      "149/388, train_loss: 0.0953, step time: 1.5435\n",
      "150/388, train_loss: 0.1407, step time: 1.5312\n",
      "151/388, train_loss: 0.2607, step time: 1.5342\n",
      "152/388, train_loss: 0.1922, step time: 1.5376\n",
      "153/388, train_loss: 0.0965, step time: 1.5487\n",
      "154/388, train_loss: 0.1982, step time: 1.5317\n",
      "155/388, train_loss: 0.1139, step time: 1.5354\n",
      "156/388, train_loss: 0.2019, step time: 1.5385\n",
      "157/388, train_loss: 0.0613, step time: 1.5471\n",
      "158/388, train_loss: 0.2549, step time: 1.5327\n",
      "159/388, train_loss: 0.0876, step time: 1.5328\n",
      "160/388, train_loss: 0.2226, step time: 1.5370\n",
      "161/388, train_loss: 0.1160, step time: 1.5451\n",
      "162/388, train_loss: 0.3808, step time: 1.5299\n",
      "163/388, train_loss: 0.1294, step time: 1.5319\n",
      "164/388, train_loss: 0.0774, step time: 1.5352\n",
      "165/388, train_loss: 0.1344, step time: 1.5491\n",
      "166/388, train_loss: 0.1089, step time: 1.5338\n",
      "167/388, train_loss: 0.2307, step time: 1.5321\n",
      "168/388, train_loss: 0.1363, step time: 1.5433\n",
      "169/388, train_loss: 0.1971, step time: 1.5470\n",
      "170/388, train_loss: 0.2841, step time: 1.5338\n",
      "171/388, train_loss: 0.1076, step time: 1.5310\n",
      "172/388, train_loss: 0.0871, step time: 1.5341\n",
      "173/388, train_loss: 0.1274, step time: 1.5476\n",
      "174/388, train_loss: 0.1039, step time: 1.5345\n",
      "175/388, train_loss: 0.2070, step time: 1.5309\n",
      "176/388, train_loss: 0.1503, step time: 1.5478\n",
      "177/388, train_loss: 0.2051, step time: 1.5500\n",
      "178/388, train_loss: 0.0606, step time: 1.5353\n",
      "179/388, train_loss: 0.3496, step time: 1.5435\n",
      "180/388, train_loss: 0.1433, step time: 1.5334\n",
      "181/388, train_loss: 0.2844, step time: 1.5461\n",
      "182/388, train_loss: 0.3835, step time: 1.5365\n",
      "183/388, train_loss: 0.1313, step time: 1.5344\n",
      "184/388, train_loss: 0.1458, step time: 1.5326\n",
      "185/388, train_loss: 0.0948, step time: 1.5442\n",
      "186/388, train_loss: 0.2587, step time: 1.5349\n",
      "187/388, train_loss: 0.2752, step time: 1.5396\n",
      "188/388, train_loss: 0.2829, step time: 1.5393\n",
      "189/388, train_loss: 0.2542, step time: 1.5483\n",
      "190/388, train_loss: 0.0907, step time: 1.5353\n",
      "191/388, train_loss: 0.0844, step time: 1.5353\n",
      "192/388, train_loss: 0.1614, step time: 1.5323\n",
      "193/388, train_loss: 0.2956, step time: 1.5465\n",
      "194/388, train_loss: 0.2084, step time: 1.5378\n",
      "195/388, train_loss: 0.1978, step time: 1.5356\n",
      "196/388, train_loss: 0.0994, step time: 1.5353\n",
      "197/388, train_loss: 0.2211, step time: 1.5460\n",
      "198/388, train_loss: 0.1353, step time: 1.5312\n",
      "199/388, train_loss: 0.2520, step time: 1.5376\n",
      "200/388, train_loss: 0.2563, step time: 1.5336\n",
      "201/388, train_loss: 0.2346, step time: 1.5421\n",
      "202/388, train_loss: 0.1735, step time: 1.5320\n",
      "203/388, train_loss: 0.3052, step time: 1.5506\n",
      "204/388, train_loss: 0.1179, step time: 1.5351\n",
      "205/388, train_loss: 0.0722, step time: 1.5473\n",
      "206/388, train_loss: 0.0999, step time: 1.5314\n",
      "207/388, train_loss: 0.1301, step time: 1.5303\n",
      "208/388, train_loss: 0.1788, step time: 1.5434\n",
      "209/388, train_loss: 0.0763, step time: 1.5459\n",
      "210/388, train_loss: 0.0672, step time: 1.5343\n",
      "211/388, train_loss: 0.1040, step time: 1.5331\n",
      "212/388, train_loss: 0.2520, step time: 1.5334\n",
      "213/388, train_loss: 0.1247, step time: 1.5464\n",
      "214/388, train_loss: 0.2105, step time: 1.5340\n",
      "215/388, train_loss: 0.2020, step time: 1.5355\n",
      "216/388, train_loss: 0.2064, step time: 1.5333\n",
      "217/388, train_loss: 0.3612, step time: 1.5504\n",
      "218/388, train_loss: 0.2000, step time: 1.5335\n",
      "219/388, train_loss: 0.1271, step time: 1.5332\n",
      "220/388, train_loss: 0.1013, step time: 1.5440\n",
      "221/388, train_loss: 0.1754, step time: 1.5397\n",
      "222/388, train_loss: 0.0970, step time: 1.5325\n",
      "223/388, train_loss: 0.4668, step time: 1.5383\n",
      "224/388, train_loss: 0.1509, step time: 1.5349\n",
      "225/388, train_loss: 0.1487, step time: 1.5469\n",
      "226/388, train_loss: 0.1457, step time: 1.5316\n",
      "227/388, train_loss: 0.2636, step time: 1.5372\n",
      "228/388, train_loss: 0.1966, step time: 1.5367\n",
      "229/388, train_loss: 0.5288, step time: 1.5579\n",
      "230/388, train_loss: 0.1017, step time: 1.5319\n",
      "231/388, train_loss: 0.0861, step time: 1.5355\n",
      "232/388, train_loss: 0.2826, step time: 1.5345\n",
      "233/388, train_loss: 0.1815, step time: 1.5445\n",
      "234/388, train_loss: 0.2424, step time: 1.5317\n",
      "235/388, train_loss: 0.0887, step time: 1.5396\n",
      "236/388, train_loss: 0.0877, step time: 1.5354\n",
      "237/388, train_loss: 0.2382, step time: 1.5457\n",
      "238/388, train_loss: 0.2190, step time: 1.5306\n",
      "239/388, train_loss: 0.1848, step time: 1.5356\n",
      "240/388, train_loss: 0.4153, step time: 1.5358\n",
      "241/388, train_loss: 0.1924, step time: 1.5425\n",
      "242/388, train_loss: 0.1245, step time: 1.5324\n",
      "243/388, train_loss: 0.0980, step time: 1.5420\n",
      "244/388, train_loss: 0.0726, step time: 1.5310\n",
      "245/388, train_loss: 0.1084, step time: 1.5453\n",
      "246/388, train_loss: 0.1785, step time: 1.5335\n",
      "247/388, train_loss: 0.1660, step time: 1.5337\n",
      "248/388, train_loss: 0.0833, step time: 1.5352\n",
      "249/388, train_loss: 0.0646, step time: 1.5563\n",
      "250/388, train_loss: 0.2664, step time: 1.5287\n",
      "251/388, train_loss: 0.0644, step time: 1.5315\n",
      "252/388, train_loss: 0.1170, step time: 1.5360\n",
      "253/388, train_loss: 0.1920, step time: 1.5483\n",
      "254/388, train_loss: 0.1255, step time: 1.5317\n",
      "255/388, train_loss: 0.0562, step time: 1.5343\n",
      "256/388, train_loss: 0.0460, step time: 1.5333\n",
      "257/388, train_loss: 0.0966, step time: 1.5501\n",
      "258/388, train_loss: 0.2051, step time: 1.5327\n",
      "259/388, train_loss: 0.0570, step time: 1.5325\n",
      "260/388, train_loss: 0.1702, step time: 1.5325\n",
      "261/388, train_loss: 0.2002, step time: 1.5491\n",
      "262/388, train_loss: 0.4810, step time: 1.5393\n",
      "263/388, train_loss: 0.2367, step time: 1.5498\n",
      "264/388, train_loss: 0.1731, step time: 1.5371\n",
      "265/388, train_loss: 0.1242, step time: 1.5439\n",
      "266/388, train_loss: 0.0969, step time: 1.5429\n",
      "267/388, train_loss: 0.1823, step time: 1.5390\n",
      "268/388, train_loss: 0.1714, step time: 1.5377\n",
      "269/388, train_loss: 0.0564, step time: 1.5444\n",
      "270/388, train_loss: 0.0986, step time: 1.5291\n",
      "271/388, train_loss: 0.2481, step time: 1.5335\n",
      "272/388, train_loss: 0.1903, step time: 1.5331\n",
      "273/388, train_loss: 0.0687, step time: 1.5487\n",
      "274/388, train_loss: 0.2292, step time: 1.5308\n",
      "275/388, train_loss: 0.0701, step time: 1.5300\n",
      "276/388, train_loss: 0.2451, step time: 1.5377\n",
      "277/388, train_loss: 0.1312, step time: 1.5487\n",
      "278/388, train_loss: 0.1414, step time: 1.5306\n",
      "279/388, train_loss: 0.0896, step time: 1.5320\n",
      "280/388, train_loss: 0.0914, step time: 1.5308\n",
      "281/388, train_loss: 0.1502, step time: 1.5481\n",
      "282/388, train_loss: 0.0796, step time: 1.5340\n",
      "283/388, train_loss: 0.1803, step time: 1.5490\n",
      "284/388, train_loss: 0.1478, step time: 1.5325\n",
      "285/388, train_loss: 0.2888, step time: 1.5479\n",
      "286/388, train_loss: 0.3591, step time: 1.5381\n",
      "287/388, train_loss: 0.0429, step time: 1.5354\n",
      "288/388, train_loss: 0.0801, step time: 1.5343\n",
      "289/388, train_loss: 0.0935, step time: 1.5417\n",
      "290/388, train_loss: 0.2564, step time: 1.5313\n",
      "291/388, train_loss: 0.0866, step time: 1.5417\n",
      "292/388, train_loss: 0.1387, step time: 1.5408\n",
      "293/388, train_loss: 0.0925, step time: 1.5458\n",
      "294/388, train_loss: 0.1257, step time: 1.5304\n",
      "295/388, train_loss: 0.1925, step time: 1.5336\n",
      "296/388, train_loss: 0.2080, step time: 1.5363\n",
      "297/388, train_loss: 0.1887, step time: 1.5484\n",
      "298/388, train_loss: 0.1199, step time: 1.5314\n",
      "299/388, train_loss: 0.2464, step time: 1.5333\n",
      "300/388, train_loss: 0.3269, step time: 1.5333\n",
      "301/388, train_loss: 0.0494, step time: 1.5472\n",
      "302/388, train_loss: 0.0641, step time: 1.5315\n",
      "303/388, train_loss: 0.1724, step time: 1.5333\n",
      "304/388, train_loss: 0.0677, step time: 1.5354\n",
      "305/388, train_loss: 0.0719, step time: 1.5497\n",
      "306/388, train_loss: 0.2331, step time: 1.5297\n",
      "307/388, train_loss: 0.0641, step time: 1.5311\n",
      "308/388, train_loss: 0.1786, step time: 1.5371\n",
      "309/388, train_loss: 0.1189, step time: 1.5488\n",
      "310/388, train_loss: 0.0775, step time: 1.5307\n",
      "311/388, train_loss: 0.0820, step time: 1.5338\n",
      "312/388, train_loss: 0.0452, step time: 1.5325\n",
      "313/388, train_loss: 0.2034, step time: 1.5463\n",
      "314/388, train_loss: 0.1523, step time: 1.5292\n",
      "315/388, train_loss: 0.1999, step time: 1.5289\n",
      "316/388, train_loss: 0.2556, step time: 1.5316\n",
      "317/388, train_loss: 0.2403, step time: 1.5490\n",
      "318/388, train_loss: 0.1307, step time: 1.5347\n",
      "319/388, train_loss: 0.0964, step time: 1.5304\n",
      "320/388, train_loss: 0.0997, step time: 1.5332\n",
      "321/388, train_loss: 0.0989, step time: 1.5442\n",
      "322/388, train_loss: 0.1759, step time: 1.5311\n",
      "323/388, train_loss: 0.0937, step time: 1.5341\n",
      "324/388, train_loss: 0.0749, step time: 1.5360\n",
      "325/388, train_loss: 0.4507, step time: 1.5459\n",
      "326/388, train_loss: 0.1165, step time: 1.5313\n",
      "327/388, train_loss: 0.0920, step time: 1.5320\n",
      "328/388, train_loss: 0.3048, step time: 1.5350\n",
      "329/388, train_loss: 0.0967, step time: 1.5468\n",
      "330/388, train_loss: 0.2192, step time: 1.5311\n",
      "331/388, train_loss: 0.1047, step time: 1.5301\n",
      "332/388, train_loss: 0.1797, step time: 1.5301\n",
      "333/388, train_loss: 0.0956, step time: 1.5486\n",
      "334/388, train_loss: 0.1562, step time: 1.5329\n",
      "335/388, train_loss: 0.1981, step time: 1.5294\n",
      "336/388, train_loss: 0.1317, step time: 1.5314\n",
      "337/388, train_loss: 0.1412, step time: 1.5453\n",
      "338/388, train_loss: 0.1587, step time: 1.5349\n",
      "339/388, train_loss: 0.1650, step time: 1.5295\n",
      "340/388, train_loss: 0.1338, step time: 1.5325\n",
      "341/388, train_loss: 0.2070, step time: 1.5431\n",
      "342/388, train_loss: 0.1773, step time: 1.5350\n",
      "343/388, train_loss: 0.1718, step time: 1.5325\n",
      "344/388, train_loss: 0.1244, step time: 1.5362\n",
      "345/388, train_loss: 0.3075, step time: 1.5445\n",
      "346/388, train_loss: 0.1866, step time: 1.5316\n",
      "347/388, train_loss: 0.1707, step time: 1.5297\n",
      "348/388, train_loss: 0.0997, step time: 1.5338\n",
      "349/388, train_loss: 0.0630, step time: 1.5480\n",
      "350/388, train_loss: 0.1110, step time: 1.5318\n",
      "351/388, train_loss: 0.1164, step time: 1.5302\n",
      "352/388, train_loss: 0.0700, step time: 1.5314\n",
      "353/388, train_loss: 0.1595, step time: 1.5460\n",
      "354/388, train_loss: 0.0315, step time: 1.5408\n",
      "355/388, train_loss: 0.1489, step time: 1.5309\n",
      "356/388, train_loss: 0.1362, step time: 1.5300\n",
      "357/388, train_loss: 0.1428, step time: 1.5447\n",
      "358/388, train_loss: 0.0504, step time: 1.5339\n",
      "359/388, train_loss: 0.0864, step time: 1.5361\n",
      "360/388, train_loss: 0.0833, step time: 1.5346\n",
      "361/388, train_loss: 0.0899, step time: 1.5425\n",
      "362/388, train_loss: 0.2486, step time: 1.5318\n",
      "363/388, train_loss: 0.1234, step time: 1.5331\n",
      "364/388, train_loss: 0.0724, step time: 1.5340\n",
      "365/388, train_loss: 0.2233, step time: 1.5468\n",
      "366/388, train_loss: 0.2124, step time: 1.5297\n",
      "367/388, train_loss: 0.2174, step time: 1.5307\n",
      "368/388, train_loss: 0.1594, step time: 1.5340\n",
      "369/388, train_loss: 0.1108, step time: 1.5499\n",
      "370/388, train_loss: 0.1138, step time: 1.5306\n",
      "371/388, train_loss: 0.2916, step time: 1.5498\n",
      "372/388, train_loss: 0.1588, step time: 1.5332\n",
      "373/388, train_loss: 0.0881, step time: 1.5530\n",
      "374/388, train_loss: 0.1305, step time: 1.5280\n",
      "375/388, train_loss: 0.0683, step time: 1.5273\n",
      "376/388, train_loss: 0.2793, step time: 1.5309\n",
      "377/388, train_loss: 0.1477, step time: 1.5405\n",
      "378/388, train_loss: 0.0878, step time: 1.5330\n",
      "379/388, train_loss: 0.0685, step time: 1.5351\n",
      "380/388, train_loss: 0.2000, step time: 1.5374\n",
      "381/388, train_loss: 0.4367, step time: 1.5451\n",
      "382/388, train_loss: 0.1022, step time: 1.5304\n",
      "383/388, train_loss: 0.2002, step time: 1.5295\n",
      "384/388, train_loss: 0.1673, step time: 1.5335\n",
      "385/388, train_loss: 0.2009, step time: 1.5456\n",
      "386/388, train_loss: 0.0729, step time: 1.5327\n",
      "387/388, train_loss: 0.0875, step time: 1.5311\n",
      "388/388, train_loss: 0.0836, step time: 1.5322\n",
      "epoch 78 average loss: 0.1629\n",
      "current epoch: 78 current mean dice: 0.7745 tc: 0.8202 wt: 0.9063 et: 0.5969\n",
      "best mean dice: 0.7794 at epoch: 76\n",
      "time consuming of epoch 78 is: 707.0583\n",
      "----------\n",
      "epoch 79/100\n",
      "1/388, train_loss: 0.0651, step time: 1.5516\n",
      "2/388, train_loss: 0.3769, step time: 1.5364\n",
      "3/388, train_loss: 0.1296, step time: 1.5326\n",
      "4/388, train_loss: 0.1571, step time: 1.5320\n",
      "5/388, train_loss: 0.0863, step time: 1.5335\n",
      "6/388, train_loss: 0.1137, step time: 1.5328\n",
      "7/388, train_loss: 0.2375, step time: 1.5313\n",
      "8/388, train_loss: 0.2368, step time: 1.5346\n",
      "9/388, train_loss: 0.1101, step time: 1.5322\n",
      "10/388, train_loss: 0.0877, step time: 1.5336\n",
      "11/388, train_loss: 0.4325, step time: 1.5340\n",
      "12/388, train_loss: 0.0828, step time: 1.5357\n",
      "13/388, train_loss: 0.1628, step time: 1.5347\n",
      "14/388, train_loss: 0.0572, step time: 1.5308\n",
      "15/388, train_loss: 0.1246, step time: 1.5329\n",
      "16/388, train_loss: 0.0268, step time: 1.5338\n",
      "17/388, train_loss: 0.4118, step time: 1.5400\n",
      "18/388, train_loss: 0.1855, step time: 1.5380\n",
      "19/388, train_loss: 0.0540, step time: 1.5339\n",
      "20/388, train_loss: 0.1867, step time: 1.5344\n",
      "21/388, train_loss: 0.0804, step time: 1.5299\n",
      "22/388, train_loss: 0.0660, step time: 1.5327\n",
      "23/388, train_loss: 0.1280, step time: 1.5349\n",
      "24/388, train_loss: 0.1258, step time: 1.5343\n",
      "25/388, train_loss: 0.1680, step time: 1.5324\n",
      "26/388, train_loss: 0.1013, step time: 1.5305\n",
      "27/388, train_loss: 0.1239, step time: 1.5310\n",
      "28/388, train_loss: 0.2122, step time: 1.5308\n",
      "29/388, train_loss: 0.1928, step time: 1.5305\n",
      "30/388, train_loss: 0.3775, step time: 1.5361\n",
      "31/388, train_loss: 0.1523, step time: 1.5360\n",
      "32/388, train_loss: 0.0794, step time: 1.5350\n",
      "33/388, train_loss: 0.3093, step time: 1.5321\n",
      "34/388, train_loss: 0.4181, step time: 1.5330\n",
      "35/388, train_loss: 0.3546, step time: 1.5334\n",
      "36/388, train_loss: 0.1119, step time: 1.5366\n",
      "37/388, train_loss: 0.0923, step time: 1.5331\n",
      "38/388, train_loss: 0.0440, step time: 1.5341\n",
      "39/388, train_loss: 0.0931, step time: 1.5324\n",
      "40/388, train_loss: 0.0770, step time: 1.5419\n",
      "41/388, train_loss: 0.0720, step time: 1.5437\n",
      "42/388, train_loss: 0.1563, step time: 1.5321\n",
      "43/388, train_loss: 0.1996, step time: 1.5334\n",
      "44/388, train_loss: 0.1313, step time: 1.5331\n",
      "45/388, train_loss: 0.0842, step time: 1.5343\n",
      "46/388, train_loss: 0.1184, step time: 1.5344\n",
      "47/388, train_loss: 0.1877, step time: 1.5344\n",
      "48/388, train_loss: 0.1354, step time: 1.5329\n",
      "49/388, train_loss: 0.2957, step time: 1.5352\n",
      "50/388, train_loss: 0.1748, step time: 1.5308\n",
      "51/388, train_loss: 0.0707, step time: 1.5305\n",
      "52/388, train_loss: 0.2481, step time: 1.5351\n",
      "53/388, train_loss: 0.1321, step time: 1.5312\n",
      "54/388, train_loss: 0.1692, step time: 1.5351\n",
      "55/388, train_loss: 0.2708, step time: 1.5338\n",
      "56/388, train_loss: 0.1014, step time: 1.5343\n",
      "57/388, train_loss: 0.1730, step time: 1.5347\n",
      "58/388, train_loss: 0.0728, step time: 1.5327\n",
      "59/388, train_loss: 0.1931, step time: 1.5315\n",
      "60/388, train_loss: 0.1186, step time: 1.5286\n",
      "61/388, train_loss: 0.1714, step time: 1.5306\n",
      "62/388, train_loss: 0.1020, step time: 1.5332\n",
      "63/388, train_loss: 0.1515, step time: 1.5461\n",
      "64/388, train_loss: 0.0978, step time: 1.5318\n",
      "65/388, train_loss: 0.1804, step time: 1.5307\n",
      "66/388, train_loss: 0.1468, step time: 1.5337\n",
      "67/388, train_loss: 0.1801, step time: 1.5307\n",
      "68/388, train_loss: 0.4837, step time: 1.5358\n",
      "69/388, train_loss: 0.1409, step time: 1.5335\n",
      "70/388, train_loss: 0.2843, step time: 1.5295\n",
      "71/388, train_loss: 0.0797, step time: 1.5312\n",
      "72/388, train_loss: 0.2012, step time: 1.5311\n",
      "73/388, train_loss: 0.0982, step time: 1.5311\n",
      "74/388, train_loss: 0.1482, step time: 1.5334\n",
      "75/388, train_loss: 0.3550, step time: 1.5342\n",
      "76/388, train_loss: 0.1575, step time: 1.5319\n",
      "77/388, train_loss: 0.0796, step time: 1.5305\n",
      "78/388, train_loss: 0.2344, step time: 1.5324\n",
      "79/388, train_loss: 0.0782, step time: 1.5296\n",
      "80/388, train_loss: 0.2400, step time: 1.5283\n",
      "81/388, train_loss: 0.1042, step time: 1.5304\n",
      "82/388, train_loss: 0.1861, step time: 1.5331\n",
      "83/388, train_loss: 0.1398, step time: 1.5359\n",
      "84/388, train_loss: 0.1698, step time: 1.5342\n",
      "85/388, train_loss: 0.1389, step time: 1.5312\n",
      "86/388, train_loss: 0.1105, step time: 1.5291\n",
      "87/388, train_loss: 0.1358, step time: 1.5418\n",
      "88/388, train_loss: 0.0509, step time: 1.5328\n",
      "89/388, train_loss: 0.1323, step time: 1.5310\n",
      "90/388, train_loss: 0.0838, step time: 1.5382\n",
      "91/388, train_loss: 0.0857, step time: 1.5305\n",
      "92/388, train_loss: 0.0705, step time: 1.5328\n",
      "93/388, train_loss: 0.1329, step time: 1.5303\n",
      "94/388, train_loss: 0.2920, step time: 1.5335\n",
      "95/388, train_loss: 0.2038, step time: 1.5355\n",
      "96/388, train_loss: 0.1800, step time: 1.5313\n",
      "97/388, train_loss: 0.1857, step time: 1.5301\n",
      "98/388, train_loss: 0.2008, step time: 1.5301\n",
      "99/388, train_loss: 0.0918, step time: 1.5307\n",
      "100/388, train_loss: 0.1044, step time: 1.5308\n",
      "101/388, train_loss: 0.0990, step time: 1.5366\n",
      "102/388, train_loss: 0.2536, step time: 1.5338\n",
      "103/388, train_loss: 0.1530, step time: 1.5302\n",
      "104/388, train_loss: 0.0882, step time: 1.5316\n",
      "105/388, train_loss: 0.2789, step time: 1.5373\n",
      "106/388, train_loss: 0.1216, step time: 1.5353\n",
      "107/388, train_loss: 0.1991, step time: 1.5344\n",
      "108/388, train_loss: 0.1000, step time: 1.5342\n",
      "109/388, train_loss: 0.1183, step time: 1.5329\n",
      "110/388, train_loss: 0.1758, step time: 1.5315\n",
      "111/388, train_loss: 0.2673, step time: 1.5414\n",
      "112/388, train_loss: 0.4235, step time: 1.5309\n",
      "113/388, train_loss: 0.1230, step time: 1.5311\n",
      "114/388, train_loss: 0.1922, step time: 1.5609\n",
      "115/388, train_loss: 0.1104, step time: 1.5371\n",
      "116/388, train_loss: 0.1714, step time: 1.5293\n",
      "117/388, train_loss: 0.1439, step time: 1.5293\n",
      "118/388, train_loss: 0.2194, step time: 1.5332\n",
      "119/388, train_loss: 0.0593, step time: 1.5350\n",
      "120/388, train_loss: 0.2400, step time: 1.5320\n",
      "121/388, train_loss: 0.0925, step time: 1.5309\n",
      "122/388, train_loss: 0.1676, step time: 1.5285\n",
      "123/388, train_loss: 0.2173, step time: 1.5309\n",
      "124/388, train_loss: 0.0915, step time: 1.5343\n",
      "125/388, train_loss: 0.2057, step time: 1.5328\n",
      "126/388, train_loss: 0.0900, step time: 1.5368\n",
      "127/388, train_loss: 0.1291, step time: 1.5357\n",
      "128/388, train_loss: 0.0289, step time: 1.5319\n",
      "129/388, train_loss: 0.1796, step time: 1.5299\n",
      "130/388, train_loss: 0.1707, step time: 1.5296\n",
      "131/388, train_loss: 0.2431, step time: 1.5287\n",
      "132/388, train_loss: 0.0672, step time: 1.5303\n",
      "133/388, train_loss: 0.0571, step time: 1.5329\n",
      "134/388, train_loss: 0.2288, step time: 1.5338\n",
      "135/388, train_loss: 0.1282, step time: 1.5327\n",
      "136/388, train_loss: 0.1189, step time: 1.5317\n",
      "137/388, train_loss: 0.2495, step time: 1.5315\n",
      "138/388, train_loss: 0.1010, step time: 1.5313\n",
      "139/388, train_loss: 0.1342, step time: 1.5305\n",
      "140/388, train_loss: 0.0835, step time: 1.5316\n",
      "141/388, train_loss: 0.2107, step time: 1.5328\n",
      "142/388, train_loss: 0.1496, step time: 1.5514\n",
      "143/388, train_loss: 0.0534, step time: 1.5286\n",
      "144/388, train_loss: 0.0729, step time: 1.5313\n",
      "145/388, train_loss: 0.0988, step time: 1.5298\n",
      "146/388, train_loss: 0.0740, step time: 1.5397\n",
      "147/388, train_loss: 0.0935, step time: 1.5305\n",
      "148/388, train_loss: 0.0571, step time: 1.5364\n",
      "149/388, train_loss: 0.1526, step time: 1.5302\n",
      "150/388, train_loss: 0.1134, step time: 1.5321\n",
      "151/388, train_loss: 0.2838, step time: 1.5323\n",
      "152/388, train_loss: 0.0686, step time: 1.5296\n",
      "153/388, train_loss: 0.1398, step time: 1.5318\n",
      "154/388, train_loss: 0.1203, step time: 1.5333\n",
      "155/388, train_loss: 0.2127, step time: 1.5355\n",
      "156/388, train_loss: 0.1896, step time: 1.5359\n",
      "157/388, train_loss: 0.2632, step time: 1.5335\n",
      "158/388, train_loss: 0.0645, step time: 1.5343\n",
      "159/388, train_loss: 0.0723, step time: 1.5302\n",
      "160/388, train_loss: 0.0822, step time: 1.5276\n",
      "161/388, train_loss: 0.1554, step time: 1.5363\n",
      "162/388, train_loss: 0.1256, step time: 1.5339\n",
      "163/388, train_loss: 0.0924, step time: 1.5549\n",
      "164/388, train_loss: 0.1747, step time: 1.5420\n",
      "165/388, train_loss: 0.0540, step time: 1.5368\n",
      "166/388, train_loss: 0.0757, step time: 1.5315\n",
      "167/388, train_loss: 0.2037, step time: 1.5322\n",
      "168/388, train_loss: 0.0878, step time: 1.5376\n",
      "169/388, train_loss: 0.3625, step time: 1.5365\n",
      "170/388, train_loss: 0.1439, step time: 1.5325\n",
      "171/388, train_loss: 0.3243, step time: 1.5320\n",
      "172/388, train_loss: 0.1150, step time: 1.5297\n",
      "173/388, train_loss: 0.2216, step time: 1.5332\n",
      "174/388, train_loss: 0.1844, step time: 1.5370\n",
      "175/388, train_loss: 0.1216, step time: 1.5334\n",
      "176/388, train_loss: 0.3450, step time: 1.5310\n",
      "177/388, train_loss: 0.1521, step time: 1.5304\n",
      "178/388, train_loss: 0.1564, step time: 1.5309\n",
      "179/388, train_loss: 0.1884, step time: 1.5318\n",
      "180/388, train_loss: 0.1171, step time: 1.5348\n",
      "181/388, train_loss: 0.1932, step time: 1.5349\n",
      "182/388, train_loss: 0.2215, step time: 1.5329\n",
      "183/388, train_loss: 0.2626, step time: 1.5307\n",
      "184/388, train_loss: 0.0786, step time: 1.5275\n",
      "185/388, train_loss: 0.0574, step time: 1.5316\n",
      "186/388, train_loss: 0.2041, step time: 1.5311\n",
      "187/388, train_loss: 0.0742, step time: 1.5332\n",
      "188/388, train_loss: 0.0912, step time: 1.5417\n",
      "189/388, train_loss: 0.1046, step time: 1.5350\n",
      "190/388, train_loss: 0.2695, step time: 1.5312\n",
      "191/388, train_loss: 0.2121, step time: 1.5364\n",
      "192/388, train_loss: 0.1700, step time: 1.5507\n",
      "193/388, train_loss: 0.0931, step time: 1.5282\n",
      "194/388, train_loss: 0.1128, step time: 1.5338\n",
      "195/388, train_loss: 0.1981, step time: 1.5351\n",
      "196/388, train_loss: 0.1008, step time: 1.5461\n",
      "197/388, train_loss: 0.1101, step time: 1.5318\n",
      "198/388, train_loss: 0.1632, step time: 1.5306\n",
      "199/388, train_loss: 0.2107, step time: 1.5292\n",
      "200/388, train_loss: 0.2760, step time: 1.5476\n",
      "201/388, train_loss: 0.1317, step time: 1.5390\n",
      "202/388, train_loss: 0.1868, step time: 1.5347\n",
      "203/388, train_loss: 0.0918, step time: 1.5327\n",
      "204/388, train_loss: 0.0893, step time: 1.5497\n",
      "205/388, train_loss: 0.1568, step time: 1.5337\n",
      "206/388, train_loss: 0.1807, step time: 1.5308\n",
      "207/388, train_loss: 0.0641, step time: 1.5303\n",
      "208/388, train_loss: 0.2948, step time: 1.5308\n",
      "209/388, train_loss: 0.0955, step time: 1.5282\n",
      "210/388, train_loss: 0.1399, step time: 1.5337\n",
      "211/388, train_loss: 0.0644, step time: 1.5311\n",
      "212/388, train_loss: 0.0949, step time: 1.5313\n",
      "213/388, train_loss: 0.2058, step time: 1.5313\n",
      "214/388, train_loss: 0.2073, step time: 1.5327\n",
      "215/388, train_loss: 0.2490, step time: 1.5332\n",
      "216/388, train_loss: 0.1097, step time: 1.5350\n",
      "217/388, train_loss: 0.3004, step time: 1.5343\n",
      "218/388, train_loss: 0.3016, step time: 1.5297\n",
      "219/388, train_loss: 0.3776, step time: 1.5336\n",
      "220/388, train_loss: 0.2194, step time: 1.5339\n",
      "221/388, train_loss: 0.1714, step time: 1.5333\n",
      "222/388, train_loss: 0.1891, step time: 1.5368\n",
      "223/388, train_loss: 0.0882, step time: 1.5349\n",
      "224/388, train_loss: 0.0822, step time: 1.5349\n",
      "225/388, train_loss: 0.1552, step time: 1.5303\n",
      "226/388, train_loss: 0.1745, step time: 1.5321\n",
      "227/388, train_loss: 0.1436, step time: 1.5334\n",
      "228/388, train_loss: 0.1290, step time: 1.5381\n",
      "229/388, train_loss: 0.0795, step time: 1.5372\n",
      "230/388, train_loss: 0.1598, step time: 1.5321\n",
      "231/388, train_loss: 0.1637, step time: 1.5301\n",
      "232/388, train_loss: 0.1967, step time: 1.5304\n",
      "233/388, train_loss: 0.0333, step time: 1.5388\n",
      "234/388, train_loss: 0.1302, step time: 1.5325\n",
      "235/388, train_loss: 0.2131, step time: 1.5341\n",
      "236/388, train_loss: 0.4248, step time: 1.5348\n",
      "237/388, train_loss: 0.1852, step time: 1.5456\n",
      "238/388, train_loss: 0.1312, step time: 1.5350\n",
      "239/388, train_loss: 0.2353, step time: 1.5292\n",
      "240/388, train_loss: 0.2152, step time: 1.5320\n",
      "241/388, train_loss: 0.0746, step time: 1.5308\n",
      "242/388, train_loss: 0.2206, step time: 1.5365\n",
      "243/388, train_loss: 0.1238, step time: 1.5338\n",
      "244/388, train_loss: 0.1758, step time: 1.5346\n",
      "245/388, train_loss: 0.2732, step time: 1.5374\n",
      "246/388, train_loss: 0.2811, step time: 1.5286\n",
      "247/388, train_loss: 0.0938, step time: 1.5305\n",
      "248/388, train_loss: 0.1150, step time: 1.5326\n",
      "249/388, train_loss: 0.1037, step time: 1.5329\n",
      "250/388, train_loss: 0.1080, step time: 1.5490\n",
      "251/388, train_loss: 0.0535, step time: 1.5301\n",
      "252/388, train_loss: 0.1726, step time: 1.5351\n",
      "253/388, train_loss: 0.0822, step time: 1.5313\n",
      "254/388, train_loss: 0.1142, step time: 1.5322\n",
      "255/388, train_loss: 0.0957, step time: 1.5340\n",
      "256/388, train_loss: 0.0425, step time: 1.5304\n",
      "257/388, train_loss: 0.1390, step time: 1.5271\n",
      "258/388, train_loss: 0.2084, step time: 1.5295\n",
      "259/388, train_loss: 0.2465, step time: 1.5314\n",
      "260/388, train_loss: 0.1068, step time: 1.5306\n",
      "261/388, train_loss: 0.1474, step time: 1.5321\n",
      "262/388, train_loss: 0.2484, step time: 1.5386\n",
      "263/388, train_loss: 0.0803, step time: 1.5346\n",
      "264/388, train_loss: 0.2282, step time: 1.5433\n",
      "265/388, train_loss: 0.1645, step time: 1.5328\n",
      "266/388, train_loss: 0.1654, step time: 1.5338\n",
      "267/388, train_loss: 0.2575, step time: 1.5359\n",
      "268/388, train_loss: 0.0805, step time: 1.5288\n",
      "269/388, train_loss: 0.1607, step time: 1.5348\n",
      "270/388, train_loss: 0.2448, step time: 1.5322\n",
      "271/388, train_loss: 0.1382, step time: 1.5320\n",
      "272/388, train_loss: 0.2576, step time: 1.5342\n",
      "273/388, train_loss: 0.1849, step time: 1.5332\n",
      "274/388, train_loss: 0.0665, step time: 1.5320\n",
      "275/388, train_loss: 0.2099, step time: 1.5275\n",
      "276/388, train_loss: 0.0639, step time: 1.5312\n",
      "277/388, train_loss: 0.1477, step time: 1.5326\n",
      "278/388, train_loss: 0.1356, step time: 1.5328\n",
      "279/388, train_loss: 0.1610, step time: 1.5354\n",
      "280/388, train_loss: 0.0730, step time: 1.5340\n",
      "281/388, train_loss: 0.3602, step time: 1.5372\n",
      "282/388, train_loss: 0.2121, step time: 1.5353\n",
      "283/388, train_loss: 0.3703, step time: 1.5327\n",
      "284/388, train_loss: 0.0402, step time: 1.5328\n",
      "285/388, train_loss: 0.1215, step time: 1.5369\n",
      "286/388, train_loss: 0.1837, step time: 1.5341\n",
      "287/388, train_loss: 0.2535, step time: 1.5369\n",
      "288/388, train_loss: 0.2530, step time: 1.5343\n",
      "289/388, train_loss: 0.1065, step time: 1.5299\n",
      "290/388, train_loss: 0.1022, step time: 1.5463\n",
      "291/388, train_loss: 0.2958, step time: 1.5293\n",
      "292/388, train_loss: 0.2014, step time: 1.5334\n",
      "293/388, train_loss: 0.1209, step time: 1.5339\n",
      "294/388, train_loss: 0.2323, step time: 1.5321\n",
      "295/388, train_loss: 0.1685, step time: 1.5355\n",
      "296/388, train_loss: 0.0832, step time: 1.5329\n",
      "297/388, train_loss: 0.2131, step time: 1.5311\n",
      "298/388, train_loss: 0.0934, step time: 1.5391\n",
      "299/388, train_loss: 0.0867, step time: 1.5350\n",
      "300/388, train_loss: 0.1232, step time: 1.5318\n",
      "301/388, train_loss: 0.0373, step time: 1.5326\n",
      "302/388, train_loss: 0.0954, step time: 1.5335\n",
      "303/388, train_loss: 0.1064, step time: 1.5336\n",
      "304/388, train_loss: 0.2419, step time: 1.5330\n",
      "305/388, train_loss: 0.0752, step time: 1.5356\n",
      "306/388, train_loss: 0.0567, step time: 1.5308\n",
      "307/388, train_loss: 0.3253, step time: 1.5298\n",
      "308/388, train_loss: 0.0979, step time: 1.5337\n",
      "309/388, train_loss: 0.1130, step time: 1.5359\n",
      "310/388, train_loss: 0.1819, step time: 1.5384\n",
      "311/388, train_loss: 0.1619, step time: 1.5345\n",
      "312/388, train_loss: 0.1833, step time: 1.5348\n",
      "313/388, train_loss: 0.1601, step time: 1.5292\n",
      "314/388, train_loss: 0.2546, step time: 1.5340\n",
      "315/388, train_loss: 0.0837, step time: 1.5355\n",
      "316/388, train_loss: 0.1176, step time: 1.5370\n",
      "317/388, train_loss: 0.1980, step time: 1.5350\n",
      "318/388, train_loss: 0.0983, step time: 1.5425\n",
      "319/388, train_loss: 0.0970, step time: 1.5321\n",
      "320/388, train_loss: 0.1033, step time: 1.5385\n",
      "321/388, train_loss: 0.2426, step time: 1.5354\n",
      "322/388, train_loss: 0.0934, step time: 1.5319\n",
      "323/388, train_loss: 0.2671, step time: 1.5325\n",
      "324/388, train_loss: 0.1765, step time: 1.5329\n",
      "325/388, train_loss: 0.0698, step time: 1.5388\n",
      "326/388, train_loss: 0.2420, step time: 1.5376\n",
      "327/388, train_loss: 0.0855, step time: 1.5333\n",
      "328/388, train_loss: 0.1871, step time: 1.5340\n",
      "329/388, train_loss: 0.1409, step time: 1.5334\n",
      "330/388, train_loss: 0.0993, step time: 1.5417\n",
      "331/388, train_loss: 0.0856, step time: 1.5344\n",
      "332/388, train_loss: 0.2607, step time: 1.5325\n",
      "333/388, train_loss: 0.1853, step time: 1.5338\n",
      "334/388, train_loss: 0.0748, step time: 1.5477\n",
      "335/388, train_loss: 0.2402, step time: 1.5381\n",
      "336/388, train_loss: 0.0455, step time: 1.5349\n",
      "337/388, train_loss: 0.1420, step time: 1.5311\n",
      "338/388, train_loss: 0.2163, step time: 1.5330\n",
      "339/388, train_loss: 0.4809, step time: 1.5345\n",
      "340/388, train_loss: 0.3067, step time: 1.5403\n",
      "341/388, train_loss: 0.1441, step time: 1.5319\n",
      "342/388, train_loss: 0.0941, step time: 1.5342\n",
      "343/388, train_loss: 0.3190, step time: 1.5352\n",
      "344/388, train_loss: 0.2044, step time: 1.5390\n",
      "345/388, train_loss: 0.1236, step time: 1.5344\n",
      "346/388, train_loss: 0.0949, step time: 1.5347\n",
      "347/388, train_loss: 0.0610, step time: 1.5318\n",
      "348/388, train_loss: 0.2449, step time: 1.5310\n",
      "349/388, train_loss: 0.1657, step time: 1.5338\n",
      "350/388, train_loss: 0.0845, step time: 1.5338\n",
      "351/388, train_loss: 0.0990, step time: 1.5336\n",
      "352/388, train_loss: 0.3439, step time: 1.5348\n",
      "353/388, train_loss: 0.1382, step time: 1.5337\n",
      "354/388, train_loss: 0.1520, step time: 1.5320\n",
      "355/388, train_loss: 0.2126, step time: 1.5333\n",
      "356/388, train_loss: 0.1949, step time: 1.5465\n",
      "357/388, train_loss: 0.1413, step time: 1.5384\n",
      "358/388, train_loss: 0.1010, step time: 1.5351\n",
      "359/388, train_loss: 0.2831, step time: 1.5356\n",
      "360/388, train_loss: 0.0738, step time: 1.5350\n",
      "361/388, train_loss: 0.0463, step time: 1.5366\n",
      "362/388, train_loss: 0.0799, step time: 1.5364\n",
      "363/388, train_loss: 0.1096, step time: 1.5327\n",
      "364/388, train_loss: 0.1334, step time: 1.5332\n",
      "365/388, train_loss: 0.0953, step time: 1.5315\n",
      "366/388, train_loss: 0.2215, step time: 1.5332\n",
      "367/388, train_loss: 0.2708, step time: 1.5607\n",
      "368/388, train_loss: 0.0908, step time: 1.5322\n",
      "369/388, train_loss: 0.1036, step time: 1.5321\n",
      "370/388, train_loss: 0.2603, step time: 1.5342\n",
      "371/388, train_loss: 0.2879, step time: 1.5338\n",
      "372/388, train_loss: 0.1320, step time: 1.5466\n",
      "373/388, train_loss: 0.1195, step time: 1.5335\n",
      "374/388, train_loss: 0.2827, step time: 1.5468\n",
      "375/388, train_loss: 0.2966, step time: 1.5390\n",
      "376/388, train_loss: 0.0953, step time: 1.5375\n",
      "377/388, train_loss: 0.1877, step time: 1.5354\n",
      "378/388, train_loss: 0.1672, step time: 1.5336\n",
      "379/388, train_loss: 0.0873, step time: 1.5358\n",
      "380/388, train_loss: 0.1305, step time: 1.5356\n",
      "381/388, train_loss: 0.4906, step time: 1.5376\n",
      "382/388, train_loss: 0.2070, step time: 1.5389\n",
      "383/388, train_loss: 0.1115, step time: 1.5315\n",
      "384/388, train_loss: 0.1331, step time: 1.5325\n",
      "385/388, train_loss: 0.1515, step time: 1.5331\n",
      "386/388, train_loss: 0.1234, step time: 1.5321\n",
      "387/388, train_loss: 0.5232, step time: 1.5340\n",
      "388/388, train_loss: 0.0478, step time: 1.5367\n",
      "epoch 79 average loss: 0.1620\n",
      "saved new best metric model\n",
      "current epoch: 79 current mean dice: 0.7802 tc: 0.8275 wt: 0.9062 et: 0.6068\n",
      "best mean dice: 0.7802 at epoch: 79\n",
      "time consuming of epoch 79 is: 705.0602\n",
      "----------\n",
      "epoch 80/100\n",
      "1/388, train_loss: 0.1552, step time: 1.5506\n",
      "2/388, train_loss: 0.0951, step time: 1.5370\n",
      "3/388, train_loss: 0.0998, step time: 1.5366\n",
      "4/388, train_loss: 0.3071, step time: 1.5362\n",
      "5/388, train_loss: 0.0920, step time: 1.5353\n",
      "6/388, train_loss: 0.1890, step time: 1.5327\n",
      "7/388, train_loss: 0.0963, step time: 1.5322\n",
      "8/388, train_loss: 0.1079, step time: 1.5317\n",
      "9/388, train_loss: 0.1245, step time: 1.5326\n",
      "10/388, train_loss: 0.1188, step time: 1.5337\n",
      "11/388, train_loss: 0.1684, step time: 1.5335\n",
      "12/388, train_loss: 0.5162, step time: 1.5336\n",
      "13/388, train_loss: 0.1138, step time: 1.5360\n",
      "14/388, train_loss: 0.1012, step time: 1.5375\n",
      "15/388, train_loss: 0.0856, step time: 1.5625\n",
      "16/388, train_loss: 0.0864, step time: 1.5340\n",
      "17/388, train_loss: 0.1593, step time: 1.5619\n",
      "18/388, train_loss: 0.1619, step time: 1.5626\n",
      "19/388, train_loss: 0.2694, step time: 1.5382\n",
      "20/388, train_loss: 0.1607, step time: 1.5470\n",
      "21/388, train_loss: 0.1370, step time: 1.5423\n",
      "22/388, train_loss: 0.1090, step time: 1.5319\n",
      "23/388, train_loss: 0.0723, step time: 1.5444\n",
      "24/388, train_loss: 0.2070, step time: 1.5353\n",
      "25/388, train_loss: 0.2868, step time: 1.5359\n",
      "26/388, train_loss: 0.1193, step time: 1.5496\n",
      "27/388, train_loss: 0.0850, step time: 1.5383\n",
      "28/388, train_loss: 0.1877, step time: 1.5359\n",
      "29/388, train_loss: 0.0755, step time: 1.5316\n",
      "30/388, train_loss: 0.0868, step time: 1.5324\n",
      "31/388, train_loss: 0.4825, step time: 1.5332\n",
      "32/388, train_loss: 0.0891, step time: 1.5364\n",
      "33/388, train_loss: 0.0696, step time: 1.5386\n",
      "34/388, train_loss: 0.1526, step time: 1.5351\n",
      "35/388, train_loss: 0.4172, step time: 1.5320\n",
      "36/388, train_loss: 0.1380, step time: 1.5342\n",
      "37/388, train_loss: 0.2371, step time: 1.5327\n",
      "38/388, train_loss: 0.3561, step time: 1.5365\n",
      "39/388, train_loss: 0.2180, step time: 1.5356\n",
      "40/388, train_loss: 0.1008, step time: 1.5339\n",
      "41/388, train_loss: 0.0537, step time: 1.5316\n",
      "42/388, train_loss: 0.1186, step time: 1.5344\n",
      "43/388, train_loss: 0.0980, step time: 1.5383\n",
      "44/388, train_loss: 0.0858, step time: 1.5536\n",
      "45/388, train_loss: 0.1542, step time: 1.5413\n",
      "46/388, train_loss: 0.0825, step time: 1.5439\n",
      "47/388, train_loss: 0.0572, step time: 1.5321\n",
      "48/388, train_loss: 0.1727, step time: 1.5336\n",
      "49/388, train_loss: 0.1258, step time: 1.5421\n",
      "50/388, train_loss: 0.0806, step time: 1.5334\n",
      "51/388, train_loss: 0.1178, step time: 1.5351\n",
      "52/388, train_loss: 0.0778, step time: 1.5459\n",
      "53/388, train_loss: 0.2207, step time: 1.5317\n",
      "54/388, train_loss: 0.1493, step time: 1.5357\n",
      "55/388, train_loss: 0.1545, step time: 1.5370\n",
      "56/388, train_loss: 0.2869, step time: 1.5399\n",
      "57/388, train_loss: 0.4059, step time: 1.5342\n",
      "58/388, train_loss: 0.3839, step time: 1.5317\n",
      "59/388, train_loss: 0.1446, step time: 1.5333\n",
      "60/388, train_loss: 0.2484, step time: 1.5343\n",
      "61/388, train_loss: 0.0777, step time: 1.5382\n",
      "62/388, train_loss: 0.2224, step time: 1.5345\n",
      "63/388, train_loss: 0.2452, step time: 1.5336\n",
      "64/388, train_loss: 0.1887, step time: 1.5349\n",
      "65/388, train_loss: 0.0944, step time: 1.5349\n",
      "66/388, train_loss: 0.1012, step time: 1.5324\n",
      "67/388, train_loss: 0.2628, step time: 1.5364\n",
      "68/388, train_loss: 0.1408, step time: 1.5445\n",
      "69/388, train_loss: 0.1030, step time: 1.5326\n",
      "70/388, train_loss: 0.2015, step time: 1.5402\n",
      "71/388, train_loss: 0.0532, step time: 1.5458\n",
      "72/388, train_loss: 0.2127, step time: 1.5465\n",
      "73/388, train_loss: 0.1350, step time: 1.5323\n",
      "74/388, train_loss: 0.1599, step time: 1.5422\n",
      "75/388, train_loss: 0.1674, step time: 1.5350\n",
      "76/388, train_loss: 0.0894, step time: 1.5467\n",
      "77/388, train_loss: 0.1402, step time: 1.5375\n",
      "78/388, train_loss: 0.1306, step time: 1.5420\n",
      "79/388, train_loss: 0.0458, step time: 1.5347\n",
      "80/388, train_loss: 0.1050, step time: 1.5384\n",
      "81/388, train_loss: 0.1875, step time: 1.5321\n",
      "82/388, train_loss: 0.1478, step time: 1.5445\n",
      "83/388, train_loss: 0.1201, step time: 1.5354\n",
      "84/388, train_loss: 0.1889, step time: 1.5341\n",
      "85/388, train_loss: 0.1475, step time: 1.5351\n",
      "86/388, train_loss: 0.1188, step time: 1.5444\n",
      "87/388, train_loss: 0.0707, step time: 1.5351\n",
      "88/388, train_loss: 0.3965, step time: 1.5348\n",
      "89/388, train_loss: 0.0784, step time: 1.5359\n",
      "90/388, train_loss: 0.1148, step time: 1.5453\n",
      "91/388, train_loss: 0.1671, step time: 1.5310\n",
      "92/388, train_loss: 0.0815, step time: 1.5340\n",
      "93/388, train_loss: 0.2772, step time: 1.5386\n",
      "94/388, train_loss: 0.1700, step time: 1.5453\n",
      "95/388, train_loss: 0.0936, step time: 1.5418\n",
      "96/388, train_loss: 0.2277, step time: 1.5347\n",
      "97/388, train_loss: 0.1901, step time: 1.5383\n",
      "98/388, train_loss: 0.2989, step time: 1.5478\n",
      "99/388, train_loss: 0.0677, step time: 1.5344\n",
      "100/388, train_loss: 0.0790, step time: 1.5298\n",
      "101/388, train_loss: 0.1675, step time: 1.5373\n",
      "102/388, train_loss: 0.2447, step time: 1.5485\n",
      "103/388, train_loss: 0.2032, step time: 1.5317\n",
      "104/388, train_loss: 0.1555, step time: 1.5351\n",
      "105/388, train_loss: 0.1371, step time: 1.5404\n",
      "106/388, train_loss: 0.1092, step time: 1.5467\n",
      "107/388, train_loss: 0.0638, step time: 1.5335\n",
      "108/388, train_loss: 0.1005, step time: 1.5324\n",
      "109/388, train_loss: 0.1393, step time: 1.5304\n",
      "110/388, train_loss: 0.1598, step time: 1.5483\n",
      "111/388, train_loss: 0.2167, step time: 1.5338\n",
      "112/388, train_loss: 0.0697, step time: 1.5338\n",
      "113/388, train_loss: 0.1110, step time: 1.5306\n",
      "114/388, train_loss: 0.2805, step time: 1.5482\n",
      "115/388, train_loss: 0.1885, step time: 1.5337\n",
      "116/388, train_loss: 0.0856, step time: 1.5358\n",
      "117/388, train_loss: 0.1683, step time: 1.5351\n",
      "118/388, train_loss: 0.1584, step time: 1.5435\n",
      "119/388, train_loss: 0.1944, step time: 1.5358\n",
      "120/388, train_loss: 0.4202, step time: 1.5382\n",
      "121/388, train_loss: 0.0774, step time: 1.5347\n",
      "122/388, train_loss: 0.2110, step time: 1.5429\n",
      "123/388, train_loss: 0.0946, step time: 1.5327\n",
      "124/388, train_loss: 0.1426, step time: 1.5344\n",
      "125/388, train_loss: 0.3848, step time: 1.5372\n",
      "126/388, train_loss: 0.0292, step time: 1.5427\n",
      "127/388, train_loss: 0.2728, step time: 1.5418\n",
      "128/388, train_loss: 0.1170, step time: 1.5467\n",
      "129/388, train_loss: 0.1013, step time: 1.5341\n",
      "130/388, train_loss: 0.2223, step time: 1.5442\n",
      "131/388, train_loss: 0.1025, step time: 1.5338\n",
      "132/388, train_loss: 0.1781, step time: 1.5367\n",
      "133/388, train_loss: 0.1662, step time: 1.5371\n",
      "134/388, train_loss: 0.1268, step time: 1.5406\n",
      "135/388, train_loss: 0.1955, step time: 1.5340\n",
      "136/388, train_loss: 0.1278, step time: 1.5373\n",
      "137/388, train_loss: 0.0988, step time: 1.5355\n",
      "138/388, train_loss: 0.1297, step time: 1.5423\n",
      "139/388, train_loss: 0.1575, step time: 1.5316\n",
      "140/388, train_loss: 0.2117, step time: 1.5324\n",
      "141/388, train_loss: 0.1419, step time: 1.5391\n",
      "142/388, train_loss: 0.3162, step time: 1.5491\n",
      "143/388, train_loss: 0.0982, step time: 1.5321\n",
      "144/388, train_loss: 0.0648, step time: 1.5322\n",
      "145/388, train_loss: 0.2091, step time: 1.5332\n",
      "146/388, train_loss: 0.0859, step time: 1.5456\n",
      "147/388, train_loss: 0.0933, step time: 1.5336\n",
      "148/388, train_loss: 0.2035, step time: 1.5321\n",
      "149/388, train_loss: 0.0426, step time: 1.5338\n",
      "150/388, train_loss: 0.0681, step time: 1.5474\n",
      "151/388, train_loss: 0.1183, step time: 1.5345\n",
      "152/388, train_loss: 0.1014, step time: 1.5368\n",
      "153/388, train_loss: 0.0752, step time: 1.5316\n",
      "154/388, train_loss: 0.0876, step time: 1.5405\n",
      "155/388, train_loss: 0.1930, step time: 1.5348\n",
      "156/388, train_loss: 0.0606, step time: 1.5356\n",
      "157/388, train_loss: 0.1011, step time: 1.5351\n",
      "158/388, train_loss: 0.0296, step time: 1.5457\n",
      "159/388, train_loss: 0.2096, step time: 1.5309\n",
      "160/388, train_loss: 0.1593, step time: 1.5335\n",
      "161/388, train_loss: 0.0971, step time: 1.5319\n",
      "162/388, train_loss: 0.0988, step time: 1.5493\n",
      "163/388, train_loss: 0.4063, step time: 1.5435\n",
      "164/388, train_loss: 0.2603, step time: 1.5322\n",
      "165/388, train_loss: 0.3927, step time: 1.5321\n",
      "166/388, train_loss: 0.0772, step time: 1.5465\n",
      "167/388, train_loss: 0.0552, step time: 1.5351\n",
      "168/388, train_loss: 0.1291, step time: 1.5323\n",
      "169/388, train_loss: 0.0999, step time: 1.5313\n",
      "170/388, train_loss: 0.1872, step time: 1.5407\n",
      "171/388, train_loss: 0.1619, step time: 1.5349\n",
      "172/388, train_loss: 0.1207, step time: 1.5378\n",
      "173/388, train_loss: 0.1312, step time: 1.5573\n",
      "174/388, train_loss: 0.2395, step time: 1.5535\n",
      "175/388, train_loss: 0.2149, step time: 1.5420\n",
      "176/388, train_loss: 0.0888, step time: 1.5316\n",
      "177/388, train_loss: 0.2801, step time: 1.5358\n",
      "178/388, train_loss: 0.2299, step time: 1.5484\n",
      "179/388, train_loss: 0.1555, step time: 1.5353\n",
      "180/388, train_loss: 0.1108, step time: 1.5407\n",
      "181/388, train_loss: 0.0722, step time: 1.5347\n",
      "182/388, train_loss: 0.1633, step time: 1.5473\n",
      "183/388, train_loss: 0.1263, step time: 1.5384\n",
      "184/388, train_loss: 0.1531, step time: 1.5357\n",
      "185/388, train_loss: 0.0806, step time: 1.5331\n",
      "186/388, train_loss: 0.1814, step time: 1.5430\n",
      "187/388, train_loss: 0.2435, step time: 1.5354\n",
      "188/388, train_loss: 0.4252, step time: 1.5359\n",
      "189/388, train_loss: 0.0421, step time: 1.5355\n",
      "190/388, train_loss: 0.3810, step time: 1.5449\n",
      "191/388, train_loss: 0.2110, step time: 1.5386\n",
      "192/388, train_loss: 0.1557, step time: 1.5322\n",
      "193/388, train_loss: 0.0515, step time: 1.5343\n",
      "194/388, train_loss: 0.1956, step time: 1.5430\n",
      "195/388, train_loss: 0.2591, step time: 1.5500\n",
      "196/388, train_loss: 0.0936, step time: 1.5378\n",
      "197/388, train_loss: 0.1195, step time: 1.5349\n",
      "198/388, train_loss: 0.0786, step time: 1.5497\n",
      "199/388, train_loss: 0.2944, step time: 1.5342\n",
      "200/388, train_loss: 0.1543, step time: 1.5318\n",
      "201/388, train_loss: 0.3244, step time: 1.5370\n",
      "202/388, train_loss: 0.2185, step time: 1.5490\n",
      "203/388, train_loss: 0.1492, step time: 1.5325\n",
      "204/388, train_loss: 0.0435, step time: 1.5351\n",
      "205/388, train_loss: 0.1132, step time: 1.5328\n",
      "206/388, train_loss: 0.2496, step time: 1.5468\n",
      "207/388, train_loss: 0.2678, step time: 1.5307\n",
      "208/388, train_loss: 0.1713, step time: 1.5305\n",
      "209/388, train_loss: 0.3521, step time: 1.5342\n",
      "210/388, train_loss: 0.1389, step time: 1.5462\n",
      "211/388, train_loss: 0.1855, step time: 1.5376\n",
      "212/388, train_loss: 0.3452, step time: 1.5350\n",
      "213/388, train_loss: 0.0981, step time: 1.5333\n",
      "214/388, train_loss: 0.0708, step time: 1.5403\n",
      "215/388, train_loss: 0.0625, step time: 1.5312\n",
      "216/388, train_loss: 0.1055, step time: 1.5360\n",
      "217/388, train_loss: 0.1307, step time: 1.5369\n",
      "218/388, train_loss: 0.3679, step time: 1.5478\n",
      "219/388, train_loss: 0.0971, step time: 1.5322\n",
      "220/388, train_loss: 0.2958, step time: 1.5307\n",
      "221/388, train_loss: 0.1880, step time: 1.5339\n",
      "222/388, train_loss: 0.1997, step time: 1.5425\n",
      "223/388, train_loss: 0.2135, step time: 1.5375\n",
      "224/388, train_loss: 0.0935, step time: 1.5374\n",
      "225/388, train_loss: 0.0880, step time: 1.5349\n",
      "226/388, train_loss: 0.2909, step time: 1.5461\n",
      "227/388, train_loss: 0.2740, step time: 1.5417\n",
      "228/388, train_loss: 0.0894, step time: 1.5337\n",
      "229/388, train_loss: 0.1334, step time: 1.5411\n",
      "230/388, train_loss: 0.1551, step time: 1.5467\n",
      "231/388, train_loss: 0.0511, step time: 1.5323\n",
      "232/388, train_loss: 0.2035, step time: 1.5302\n",
      "233/388, train_loss: 0.2520, step time: 1.5520\n",
      "234/388, train_loss: 0.2226, step time: 1.5516\n",
      "235/388, train_loss: 0.1100, step time: 1.5373\n",
      "236/388, train_loss: 0.1157, step time: 1.5366\n",
      "237/388, train_loss: 0.1111, step time: 1.5325\n",
      "238/388, train_loss: 0.1503, step time: 1.5444\n",
      "239/388, train_loss: 0.1802, step time: 1.5325\n",
      "240/388, train_loss: 0.2114, step time: 1.5360\n",
      "241/388, train_loss: 0.1175, step time: 1.5378\n",
      "242/388, train_loss: 0.1497, step time: 1.5585\n",
      "243/388, train_loss: 0.0790, step time: 1.5319\n",
      "244/388, train_loss: 0.0318, step time: 1.5351\n",
      "245/388, train_loss: 0.1521, step time: 1.5388\n",
      "246/388, train_loss: 0.2852, step time: 1.5465\n",
      "247/388, train_loss: 0.3636, step time: 1.5321\n",
      "248/388, train_loss: 0.1889, step time: 1.5386\n",
      "249/388, train_loss: 0.1434, step time: 1.5326\n",
      "250/388, train_loss: 0.2723, step time: 1.5432\n",
      "251/388, train_loss: 0.1541, step time: 1.5364\n",
      "252/388, train_loss: 0.2041, step time: 1.5343\n",
      "253/388, train_loss: 0.1348, step time: 1.5331\n",
      "254/388, train_loss: 0.1544, step time: 1.5473\n",
      "255/388, train_loss: 0.2278, step time: 1.5377\n",
      "256/388, train_loss: 0.0857, step time: 1.5321\n",
      "257/388, train_loss: 0.0807, step time: 1.5320\n",
      "258/388, train_loss: 0.0477, step time: 1.5489\n",
      "259/388, train_loss: 0.1103, step time: 1.5445\n",
      "260/388, train_loss: 0.2090, step time: 1.5337\n",
      "261/388, train_loss: 0.1204, step time: 1.5307\n",
      "262/388, train_loss: 0.1065, step time: 1.5452\n",
      "263/388, train_loss: 0.1581, step time: 1.5325\n",
      "264/388, train_loss: 0.1647, step time: 1.5373\n",
      "265/388, train_loss: 0.2271, step time: 1.5305\n",
      "266/388, train_loss: 0.1027, step time: 1.5422\n",
      "267/388, train_loss: 0.3196, step time: 1.5429\n",
      "268/388, train_loss: 0.1669, step time: 1.5354\n",
      "269/388, train_loss: 0.1862, step time: 1.5340\n",
      "270/388, train_loss: 0.2376, step time: 1.5414\n",
      "271/388, train_loss: 0.0755, step time: 1.5330\n",
      "272/388, train_loss: 0.1732, step time: 1.5424\n",
      "273/388, train_loss: 0.1015, step time: 1.5340\n",
      "274/388, train_loss: 0.1765, step time: 1.5426\n",
      "275/388, train_loss: 0.0622, step time: 1.5319\n",
      "276/388, train_loss: 0.1131, step time: 1.5332\n",
      "277/388, train_loss: 0.1859, step time: 1.5336\n",
      "278/388, train_loss: 0.0635, step time: 1.5450\n",
      "279/388, train_loss: 0.2285, step time: 1.5336\n",
      "280/388, train_loss: 0.0624, step time: 1.5331\n",
      "281/388, train_loss: 0.0863, step time: 1.5324\n",
      "282/388, train_loss: 0.0687, step time: 1.5598\n",
      "283/388, train_loss: 0.1767, step time: 1.5357\n",
      "284/388, train_loss: 0.1617, step time: 1.5335\n",
      "285/388, train_loss: 0.1615, step time: 1.5336\n",
      "286/388, train_loss: 0.1270, step time: 1.5448\n",
      "287/388, train_loss: 0.1232, step time: 1.5375\n",
      "288/388, train_loss: 0.1025, step time: 1.5434\n",
      "289/388, train_loss: 0.0485, step time: 1.5327\n",
      "290/388, train_loss: 0.0912, step time: 1.5417\n",
      "291/388, train_loss: 0.0834, step time: 1.5389\n",
      "292/388, train_loss: 0.1930, step time: 1.5362\n",
      "293/388, train_loss: 0.1333, step time: 1.5358\n",
      "294/388, train_loss: 0.0927, step time: 1.5479\n",
      "295/388, train_loss: 0.1530, step time: 1.5323\n",
      "296/388, train_loss: 0.1339, step time: 1.5327\n",
      "297/388, train_loss: 0.1114, step time: 1.5367\n",
      "298/388, train_loss: 0.1308, step time: 1.5446\n",
      "299/388, train_loss: 0.0889, step time: 1.5340\n",
      "300/388, train_loss: 0.0713, step time: 1.5365\n",
      "301/388, train_loss: 0.1787, step time: 1.5374\n",
      "302/388, train_loss: 0.2402, step time: 1.5410\n",
      "303/388, train_loss: 0.1301, step time: 1.5454\n",
      "304/388, train_loss: 0.2480, step time: 1.5361\n",
      "305/388, train_loss: 0.1187, step time: 1.5315\n",
      "306/388, train_loss: 0.1580, step time: 1.5423\n",
      "307/388, train_loss: 0.2334, step time: 1.5362\n",
      "308/388, train_loss: 0.2218, step time: 1.5359\n",
      "309/388, train_loss: 0.0893, step time: 1.5333\n",
      "310/388, train_loss: 0.0527, step time: 1.5457\n",
      "311/388, train_loss: 0.1079, step time: 1.5340\n",
      "312/388, train_loss: 0.0888, step time: 1.5345\n",
      "313/388, train_loss: 0.1402, step time: 1.5380\n",
      "314/388, train_loss: 0.1567, step time: 1.5473\n",
      "315/388, train_loss: 0.1135, step time: 1.5318\n",
      "316/388, train_loss: 0.0788, step time: 1.5320\n",
      "317/388, train_loss: 0.3793, step time: 1.5330\n",
      "318/388, train_loss: 0.1434, step time: 1.5502\n",
      "319/388, train_loss: 0.1888, step time: 1.5359\n",
      "320/388, train_loss: 0.1027, step time: 1.5357\n",
      "321/388, train_loss: 0.1917, step time: 1.5311\n",
      "322/388, train_loss: 0.1204, step time: 1.5436\n",
      "323/388, train_loss: 0.1022, step time: 1.5356\n",
      "324/388, train_loss: 0.1886, step time: 1.5362\n",
      "325/388, train_loss: 0.0901, step time: 1.5321\n",
      "326/388, train_loss: 0.0870, step time: 1.5414\n",
      "327/388, train_loss: 0.0986, step time: 1.5352\n",
      "328/388, train_loss: 0.2201, step time: 1.5375\n",
      "329/388, train_loss: 0.2793, step time: 1.5332\n",
      "330/388, train_loss: 0.1629, step time: 1.5460\n",
      "331/388, train_loss: 0.1684, step time: 1.5293\n",
      "332/388, train_loss: 0.1452, step time: 1.5323\n",
      "333/388, train_loss: 0.2082, step time: 1.5360\n",
      "334/388, train_loss: 0.2134, step time: 1.5470\n",
      "335/388, train_loss: 0.1289, step time: 1.5308\n",
      "336/388, train_loss: 0.3105, step time: 1.5332\n",
      "337/388, train_loss: 0.0815, step time: 1.5398\n",
      "338/388, train_loss: 0.2118, step time: 1.5510\n",
      "339/388, train_loss: 0.4289, step time: 1.5339\n",
      "340/388, train_loss: 0.3439, step time: 1.5299\n",
      "341/388, train_loss: 0.0891, step time: 1.5310\n",
      "342/388, train_loss: 0.0588, step time: 1.5558\n",
      "343/388, train_loss: 0.2591, step time: 1.5351\n",
      "344/388, train_loss: 0.0699, step time: 1.5348\n",
      "345/388, train_loss: 0.2479, step time: 1.5326\n",
      "346/388, train_loss: 0.0990, step time: 1.5422\n",
      "347/388, train_loss: 0.1663, step time: 1.5389\n",
      "348/388, train_loss: 0.1945, step time: 1.5367\n",
      "349/388, train_loss: 0.5197, step time: 1.5348\n",
      "350/388, train_loss: 0.3210, step time: 1.5416\n",
      "351/388, train_loss: 0.2080, step time: 1.5335\n",
      "352/388, train_loss: 0.0930, step time: 1.5336\n",
      "353/388, train_loss: 0.1852, step time: 1.5373\n",
      "354/388, train_loss: 0.2288, step time: 1.5435\n",
      "355/388, train_loss: 0.1183, step time: 1.5325\n",
      "356/388, train_loss: 0.0858, step time: 1.5325\n",
      "357/388, train_loss: 0.1827, step time: 1.5513\n",
      "358/388, train_loss: 0.1132, step time: 1.5444\n",
      "359/388, train_loss: 0.2887, step time: 1.5296\n",
      "360/388, train_loss: 0.0734, step time: 1.5352\n",
      "361/388, train_loss: 0.1813, step time: 1.5372\n",
      "362/388, train_loss: 0.0875, step time: 1.5472\n",
      "363/388, train_loss: 0.0676, step time: 1.5317\n",
      "364/388, train_loss: 0.1947, step time: 1.5325\n",
      "365/388, train_loss: 0.1944, step time: 1.5300\n",
      "366/388, train_loss: 0.2214, step time: 1.5457\n",
      "367/388, train_loss: 0.0709, step time: 1.5377\n",
      "368/388, train_loss: 0.0664, step time: 1.5348\n",
      "369/388, train_loss: 0.2591, step time: 1.5333\n",
      "370/388, train_loss: 0.1415, step time: 1.5453\n",
      "371/388, train_loss: 0.2775, step time: 1.5348\n",
      "372/388, train_loss: 0.2119, step time: 1.5333\n",
      "373/388, train_loss: 0.1922, step time: 1.5381\n",
      "374/388, train_loss: 0.1913, step time: 1.5476\n",
      "375/388, train_loss: 0.0604, step time: 1.5360\n",
      "376/388, train_loss: 0.3095, step time: 1.5354\n",
      "377/388, train_loss: 0.3166, step time: 1.5348\n",
      "378/388, train_loss: 0.1167, step time: 1.5406\n",
      "379/388, train_loss: 0.1284, step time: 1.5344\n",
      "380/388, train_loss: 0.2666, step time: 1.5347\n",
      "381/388, train_loss: 0.0824, step time: 1.5336\n",
      "382/388, train_loss: 0.1811, step time: 1.5451\n",
      "383/388, train_loss: 0.1375, step time: 1.5324\n",
      "384/388, train_loss: 0.2885, step time: 1.5351\n",
      "385/388, train_loss: 0.1557, step time: 1.5422\n",
      "386/388, train_loss: 0.1439, step time: 1.5461\n",
      "387/388, train_loss: 0.1987, step time: 1.5326\n",
      "388/388, train_loss: 0.1139, step time: 1.5298\n",
      "epoch 80 average loss: 0.1630\n",
      "current epoch: 80 current mean dice: 0.7788 tc: 0.8268 wt: 0.9063 et: 0.6033\n",
      "best mean dice: 0.7802 at epoch: 79\n",
      "time consuming of epoch 80 is: 706.2806\n",
      "----------\n",
      "epoch 81/100\n",
      "1/388, train_loss: 0.2339, step time: 1.5611\n",
      "2/388, train_loss: 0.1295, step time: 1.5666\n",
      "3/388, train_loss: 0.2596, step time: 1.5597\n",
      "4/388, train_loss: 0.0851, step time: 1.5632\n",
      "5/388, train_loss: 0.0658, step time: 1.5380\n",
      "6/388, train_loss: 0.2639, step time: 1.5360\n",
      "7/388, train_loss: 0.5094, step time: 1.5379\n",
      "8/388, train_loss: 0.0688, step time: 1.5348\n",
      "9/388, train_loss: 0.0607, step time: 1.5376\n",
      "10/388, train_loss: 0.1470, step time: 1.5412\n",
      "11/388, train_loss: 0.1725, step time: 1.5345\n",
      "12/388, train_loss: 0.1270, step time: 1.5370\n",
      "13/388, train_loss: 0.0937, step time: 1.5406\n",
      "14/388, train_loss: 0.0847, step time: 1.5397\n",
      "15/388, train_loss: 0.1431, step time: 1.5339\n",
      "16/388, train_loss: 0.1193, step time: 1.5368\n",
      "17/388, train_loss: 0.0688, step time: 1.5368\n",
      "18/388, train_loss: 0.2429, step time: 1.5360\n",
      "19/388, train_loss: 0.0782, step time: 1.5404\n",
      "20/388, train_loss: 0.2265, step time: 1.5370\n",
      "21/388, train_loss: 0.2573, step time: 1.5318\n",
      "22/388, train_loss: 0.2030, step time: 1.5306\n",
      "23/388, train_loss: 0.1932, step time: 1.5361\n",
      "24/388, train_loss: 0.1360, step time: 1.5377\n",
      "25/388, train_loss: 0.3477, step time: 1.5358\n",
      "26/388, train_loss: 0.1771, step time: 1.5671\n",
      "27/388, train_loss: 0.1161, step time: 1.5355\n",
      "28/388, train_loss: 0.1495, step time: 1.5407\n",
      "29/388, train_loss: 0.0768, step time: 1.5336\n",
      "30/388, train_loss: 0.1022, step time: 1.5327\n",
      "31/388, train_loss: 0.1658, step time: 1.5364\n",
      "32/388, train_loss: 0.2632, step time: 1.5330\n",
      "33/388, train_loss: 0.0862, step time: 1.5334\n",
      "34/388, train_loss: 0.1208, step time: 1.5375\n",
      "35/388, train_loss: 0.2125, step time: 1.5385\n",
      "36/388, train_loss: 0.0911, step time: 1.5309\n",
      "37/388, train_loss: 0.1172, step time: 1.5303\n",
      "38/388, train_loss: 0.2244, step time: 1.5293\n",
      "39/388, train_loss: 0.1424, step time: 1.5334\n",
      "40/388, train_loss: 0.0897, step time: 1.5361\n",
      "41/388, train_loss: 0.0927, step time: 1.5358\n",
      "42/388, train_loss: 0.1194, step time: 1.5367\n",
      "43/388, train_loss: 0.1960, step time: 1.5345\n",
      "44/388, train_loss: 0.3910, step time: 1.5305\n",
      "45/388, train_loss: 0.3045, step time: 1.5404\n",
      "46/388, train_loss: 0.2386, step time: 1.5377\n",
      "47/388, train_loss: 0.2336, step time: 1.5353\n",
      "48/388, train_loss: 0.1011, step time: 1.5354\n",
      "49/388, train_loss: 0.0602, step time: 1.5323\n",
      "50/388, train_loss: 0.0932, step time: 1.5326\n",
      "51/388, train_loss: 0.1860, step time: 1.5299\n",
      "52/388, train_loss: 0.0446, step time: 1.5324\n",
      "53/388, train_loss: 0.1091, step time: 1.5445\n",
      "54/388, train_loss: 0.2350, step time: 1.5357\n",
      "55/388, train_loss: 0.1965, step time: 1.5365\n",
      "56/388, train_loss: 0.1599, step time: 1.5315\n",
      "57/388, train_loss: 0.1613, step time: 1.5309\n",
      "58/388, train_loss: 0.2815, step time: 1.5317\n",
      "59/388, train_loss: 0.1292, step time: 1.5340\n",
      "60/388, train_loss: 0.0884, step time: 1.5358\n",
      "61/388, train_loss: 0.0658, step time: 1.5353\n",
      "62/388, train_loss: 0.1562, step time: 1.5364\n",
      "63/388, train_loss: 0.1599, step time: 1.5316\n",
      "64/388, train_loss: 0.2133, step time: 1.5329\n",
      "65/388, train_loss: 0.1329, step time: 1.5319\n",
      "66/388, train_loss: 0.2087, step time: 1.5349\n",
      "67/388, train_loss: 0.1522, step time: 1.5333\n",
      "68/388, train_loss: 0.1861, step time: 1.5348\n",
      "69/388, train_loss: 0.0478, step time: 1.5348\n",
      "70/388, train_loss: 0.2484, step time: 1.5301\n",
      "71/388, train_loss: 0.0503, step time: 1.5344\n",
      "72/388, train_loss: 0.0905, step time: 1.5313\n",
      "73/388, train_loss: 0.1460, step time: 1.5367\n",
      "74/388, train_loss: 0.1822, step time: 1.5374\n",
      "75/388, train_loss: 0.0897, step time: 1.5352\n",
      "76/388, train_loss: 0.1811, step time: 1.5319\n",
      "77/388, train_loss: 0.1011, step time: 1.5324\n",
      "78/388, train_loss: 0.2017, step time: 1.5333\n",
      "79/388, train_loss: 0.4067, step time: 1.5336\n",
      "80/388, train_loss: 0.0932, step time: 1.5354\n",
      "81/388, train_loss: 0.3458, step time: 1.5324\n",
      "82/388, train_loss: 0.0869, step time: 1.5320\n",
      "83/388, train_loss: 0.1641, step time: 1.5346\n",
      "84/388, train_loss: 0.2300, step time: 1.5361\n",
      "85/388, train_loss: 0.0982, step time: 1.5361\n",
      "86/388, train_loss: 0.0843, step time: 1.5374\n",
      "87/388, train_loss: 0.1076, step time: 1.5332\n",
      "88/388, train_loss: 0.1806, step time: 1.5327\n",
      "89/388, train_loss: 0.0775, step time: 1.5328\n",
      "90/388, train_loss: 0.1125, step time: 1.5382\n",
      "91/388, train_loss: 0.0900, step time: 1.5349\n",
      "92/388, train_loss: 0.2408, step time: 1.5359\n",
      "93/388, train_loss: 0.3764, step time: 1.5336\n",
      "94/388, train_loss: 0.1298, step time: 1.5361\n",
      "95/388, train_loss: 0.1454, step time: 1.5343\n",
      "96/388, train_loss: 0.0866, step time: 1.5373\n",
      "97/388, train_loss: 0.1348, step time: 1.5360\n",
      "98/388, train_loss: 0.0892, step time: 1.5383\n",
      "99/388, train_loss: 0.1414, step time: 1.5316\n",
      "100/388, train_loss: 0.2088, step time: 1.5343\n",
      "101/388, train_loss: 0.0998, step time: 1.5334\n",
      "102/388, train_loss: 0.0886, step time: 1.5371\n",
      "103/388, train_loss: 0.0897, step time: 1.5353\n",
      "104/388, train_loss: 0.2699, step time: 1.5401\n",
      "105/388, train_loss: 0.1048, step time: 1.5330\n",
      "106/388, train_loss: 0.1970, step time: 1.5321\n",
      "107/388, train_loss: 0.1497, step time: 1.5331\n",
      "108/388, train_loss: 0.0983, step time: 1.5308\n",
      "109/388, train_loss: 0.2165, step time: 1.5378\n",
      "110/388, train_loss: 0.1198, step time: 1.5382\n",
      "111/388, train_loss: 0.1500, step time: 1.5332\n",
      "112/388, train_loss: 0.1615, step time: 1.5328\n",
      "113/388, train_loss: 0.0929, step time: 1.5427\n",
      "114/388, train_loss: 0.0581, step time: 1.5343\n",
      "115/388, train_loss: 0.0531, step time: 1.5360\n",
      "116/388, train_loss: 0.2349, step time: 1.5355\n",
      "117/388, train_loss: 0.0806, step time: 1.5350\n",
      "118/388, train_loss: 0.0991, step time: 1.5341\n",
      "119/388, train_loss: 0.1337, step time: 1.5316\n",
      "120/388, train_loss: 0.1967, step time: 1.5295\n",
      "121/388, train_loss: 0.1270, step time: 1.5328\n",
      "122/388, train_loss: 0.1218, step time: 1.5494\n",
      "123/388, train_loss: 0.0451, step time: 1.5371\n",
      "124/388, train_loss: 0.1470, step time: 1.5347\n",
      "125/388, train_loss: 0.0709, step time: 1.5321\n",
      "126/388, train_loss: 0.1550, step time: 1.5297\n",
      "127/388, train_loss: 0.1596, step time: 1.5333\n",
      "128/388, train_loss: 0.0966, step time: 1.5362\n",
      "129/388, train_loss: 0.2267, step time: 1.5383\n",
      "130/388, train_loss: 0.1939, step time: 1.5371\n",
      "131/388, train_loss: 0.0979, step time: 1.5341\n",
      "132/388, train_loss: 0.0939, step time: 1.5333\n",
      "133/388, train_loss: 0.1546, step time: 1.5337\n",
      "134/388, train_loss: 0.1192, step time: 1.5322\n",
      "135/388, train_loss: 0.0983, step time: 1.5378\n",
      "136/388, train_loss: 0.1664, step time: 1.5635\n",
      "137/388, train_loss: 0.2307, step time: 1.5319\n",
      "138/388, train_loss: 0.0893, step time: 1.5470\n",
      "139/388, train_loss: 0.2965, step time: 1.5358\n",
      "140/388, train_loss: 0.5364, step time: 1.5358\n",
      "141/388, train_loss: 0.1851, step time: 1.5334\n",
      "142/388, train_loss: 0.2449, step time: 1.5356\n",
      "143/388, train_loss: 0.1991, step time: 1.5377\n",
      "144/388, train_loss: 0.1461, step time: 1.5388\n",
      "145/388, train_loss: 0.2757, step time: 1.5450\n",
      "146/388, train_loss: 0.0950, step time: 1.5320\n",
      "147/388, train_loss: 0.2022, step time: 1.5351\n",
      "148/388, train_loss: 0.4075, step time: 1.5388\n",
      "149/388, train_loss: 0.2991, step time: 1.5356\n",
      "150/388, train_loss: 0.1948, step time: 1.5363\n",
      "151/388, train_loss: 0.3788, step time: 1.5349\n",
      "152/388, train_loss: 0.0978, step time: 1.5347\n",
      "153/388, train_loss: 0.1423, step time: 1.5319\n",
      "154/388, train_loss: 0.2139, step time: 1.5380\n",
      "155/388, train_loss: 0.0795, step time: 1.5358\n",
      "156/388, train_loss: 0.3836, step time: 1.5338\n",
      "157/388, train_loss: 0.1096, step time: 1.5330\n",
      "158/388, train_loss: 0.1000, step time: 1.5331\n",
      "159/388, train_loss: 0.0630, step time: 1.5371\n",
      "160/388, train_loss: 0.0893, step time: 1.5358\n",
      "161/388, train_loss: 0.1649, step time: 1.5373\n",
      "162/388, train_loss: 0.0864, step time: 1.5359\n",
      "163/388, train_loss: 0.2350, step time: 1.5322\n",
      "164/388, train_loss: 0.2098, step time: 1.5334\n",
      "165/388, train_loss: 0.0993, step time: 1.5420\n",
      "166/388, train_loss: 0.2547, step time: 1.5417\n",
      "167/388, train_loss: 0.1400, step time: 1.5432\n",
      "168/388, train_loss: 0.1332, step time: 1.5317\n",
      "169/388, train_loss: 0.2279, step time: 1.5335\n",
      "170/388, train_loss: 0.3173, step time: 1.5369\n",
      "171/388, train_loss: 0.1541, step time: 1.5370\n",
      "172/388, train_loss: 0.3658, step time: 1.5360\n",
      "173/388, train_loss: 0.3250, step time: 1.5315\n",
      "174/388, train_loss: 0.1596, step time: 1.5415\n",
      "175/388, train_loss: 0.1015, step time: 1.5465\n",
      "176/388, train_loss: 0.1335, step time: 1.5369\n",
      "177/388, train_loss: 0.1693, step time: 1.5362\n",
      "178/388, train_loss: 0.1587, step time: 1.5330\n",
      "179/388, train_loss: 0.0421, step time: 1.5325\n",
      "180/388, train_loss: 0.2177, step time: 1.5375\n",
      "181/388, train_loss: 0.0947, step time: 1.5388\n",
      "182/388, train_loss: 0.1112, step time: 1.5443\n",
      "183/388, train_loss: 0.1256, step time: 1.5337\n",
      "184/388, train_loss: 0.1678, step time: 1.5340\n",
      "185/388, train_loss: 0.2785, step time: 1.5326\n",
      "186/388, train_loss: 0.1239, step time: 1.5371\n",
      "187/388, train_loss: 0.0552, step time: 1.5352\n",
      "188/388, train_loss: 0.1748, step time: 1.5346\n",
      "189/388, train_loss: 0.1170, step time: 1.5361\n",
      "190/388, train_loss: 0.1414, step time: 1.5305\n",
      "191/388, train_loss: 0.4949, step time: 1.5326\n",
      "192/388, train_loss: 0.2180, step time: 1.5381\n",
      "193/388, train_loss: 0.1070, step time: 1.5354\n",
      "194/388, train_loss: 0.1740, step time: 1.5383\n",
      "195/388, train_loss: 0.1570, step time: 1.5355\n",
      "196/388, train_loss: 0.1650, step time: 1.5342\n",
      "197/388, train_loss: 0.2135, step time: 1.5356\n",
      "198/388, train_loss: 0.1019, step time: 1.5378\n",
      "199/388, train_loss: 0.1735, step time: 1.5365\n",
      "200/388, train_loss: 0.2130, step time: 1.5334\n",
      "201/388, train_loss: 0.1567, step time: 1.5446\n",
      "202/388, train_loss: 0.5396, step time: 1.5363\n",
      "203/388, train_loss: 0.1586, step time: 1.5359\n",
      "204/388, train_loss: 0.2338, step time: 1.5361\n",
      "205/388, train_loss: 0.0768, step time: 1.5325\n",
      "206/388, train_loss: 0.1490, step time: 1.5319\n",
      "207/388, train_loss: 0.2474, step time: 1.5322\n",
      "208/388, train_loss: 0.1045, step time: 1.5339\n",
      "209/388, train_loss: 0.1549, step time: 1.5366\n",
      "210/388, train_loss: 0.1657, step time: 1.5343\n",
      "211/388, train_loss: 0.2142, step time: 1.5366\n",
      "212/388, train_loss: 0.2733, step time: 1.5423\n",
      "213/388, train_loss: 0.1972, step time: 1.5370\n",
      "214/388, train_loss: 0.1326, step time: 1.5360\n",
      "215/388, train_loss: 0.0998, step time: 1.5362\n",
      "216/388, train_loss: 0.1855, step time: 1.5361\n",
      "217/388, train_loss: 0.2467, step time: 1.5378\n",
      "218/388, train_loss: 0.0831, step time: 1.5371\n",
      "219/388, train_loss: 0.2462, step time: 1.5375\n",
      "220/388, train_loss: 0.1453, step time: 1.5473\n",
      "221/388, train_loss: 0.1103, step time: 1.5387\n",
      "222/388, train_loss: 0.1435, step time: 1.5331\n",
      "223/388, train_loss: 0.1713, step time: 1.5329\n",
      "224/388, train_loss: 0.1065, step time: 1.5356\n",
      "225/388, train_loss: 0.0617, step time: 1.5347\n",
      "226/388, train_loss: 0.1836, step time: 1.5332\n",
      "227/388, train_loss: 0.2569, step time: 1.5348\n",
      "228/388, train_loss: 0.1014, step time: 1.5347\n",
      "229/388, train_loss: 0.0684, step time: 1.5320\n",
      "230/388, train_loss: 0.3320, step time: 1.5363\n",
      "231/388, train_loss: 0.1215, step time: 1.5376\n",
      "232/388, train_loss: 0.1122, step time: 1.5382\n",
      "233/388, train_loss: 0.1860, step time: 1.5357\n",
      "234/388, train_loss: 0.1346, step time: 1.5350\n",
      "235/388, train_loss: 0.2225, step time: 1.5335\n",
      "236/388, train_loss: 0.1224, step time: 1.5337\n",
      "237/388, train_loss: 0.0885, step time: 1.5394\n",
      "238/388, train_loss: 0.0628, step time: 1.5372\n",
      "239/388, train_loss: 0.3281, step time: 1.5356\n",
      "240/388, train_loss: 0.0808, step time: 1.5324\n",
      "241/388, train_loss: 0.0864, step time: 1.5329\n",
      "242/388, train_loss: 0.0273, step time: 1.5336\n",
      "243/388, train_loss: 0.1106, step time: 1.5359\n",
      "244/388, train_loss: 0.2206, step time: 1.5352\n",
      "245/388, train_loss: 0.1498, step time: 1.5327\n",
      "246/388, train_loss: 0.1913, step time: 1.5327\n",
      "247/388, train_loss: 0.1951, step time: 1.5323\n",
      "248/388, train_loss: 0.1820, step time: 1.5329\n",
      "249/388, train_loss: 0.1289, step time: 1.5332\n",
      "250/388, train_loss: 0.2285, step time: 1.5375\n",
      "251/388, train_loss: 0.0743, step time: 1.5477\n",
      "252/388, train_loss: 0.1889, step time: 1.5299\n",
      "253/388, train_loss: 0.1066, step time: 1.5320\n",
      "254/388, train_loss: 0.1411, step time: 1.5317\n",
      "255/388, train_loss: 0.1819, step time: 1.5352\n",
      "256/388, train_loss: 0.1236, step time: 1.5379\n",
      "257/388, train_loss: 0.4196, step time: 1.5348\n",
      "258/388, train_loss: 0.0855, step time: 1.5341\n",
      "259/388, train_loss: 0.2149, step time: 1.5332\n",
      "260/388, train_loss: 0.1197, step time: 1.5318\n",
      "261/388, train_loss: 0.0918, step time: 1.5379\n",
      "262/388, train_loss: 0.2026, step time: 1.5366\n",
      "263/388, train_loss: 0.0617, step time: 1.5366\n",
      "264/388, train_loss: 0.1223, step time: 1.5365\n",
      "265/388, train_loss: 0.0401, step time: 1.5335\n",
      "266/388, train_loss: 0.0765, step time: 1.5379\n",
      "267/388, train_loss: 0.3551, step time: 1.5361\n",
      "268/388, train_loss: 0.0484, step time: 1.5327\n",
      "269/388, train_loss: 0.1137, step time: 1.5333\n",
      "270/388, train_loss: 0.0960, step time: 1.5328\n",
      "271/388, train_loss: 0.1381, step time: 1.5360\n",
      "272/388, train_loss: 0.0879, step time: 1.5379\n",
      "273/388, train_loss: 0.3278, step time: 1.5354\n",
      "274/388, train_loss: 0.1836, step time: 1.5338\n",
      "275/388, train_loss: 0.0915, step time: 1.5314\n",
      "276/388, train_loss: 0.0656, step time: 1.5337\n",
      "277/388, train_loss: 0.1977, step time: 1.5403\n",
      "278/388, train_loss: 0.1077, step time: 1.5391\n",
      "279/388, train_loss: 0.0626, step time: 1.5317\n",
      "280/388, train_loss: 0.1808, step time: 1.5316\n",
      "281/388, train_loss: 0.0763, step time: 1.5354\n",
      "282/388, train_loss: 0.0494, step time: 1.5343\n",
      "283/388, train_loss: 0.0322, step time: 1.5389\n",
      "284/388, train_loss: 0.0936, step time: 1.5335\n",
      "285/388, train_loss: 0.1869, step time: 1.5330\n",
      "286/388, train_loss: 0.1564, step time: 1.5334\n",
      "287/388, train_loss: 0.0638, step time: 1.5404\n",
      "288/388, train_loss: 0.2140, step time: 1.5357\n",
      "289/388, train_loss: 0.4853, step time: 1.5354\n",
      "290/388, train_loss: 0.1880, step time: 1.5338\n",
      "291/388, train_loss: 0.1224, step time: 1.5315\n",
      "292/388, train_loss: 0.0665, step time: 1.5320\n",
      "293/388, train_loss: 0.0398, step time: 1.5351\n",
      "294/388, train_loss: 0.1329, step time: 1.5354\n",
      "295/388, train_loss: 0.1815, step time: 1.5386\n",
      "296/388, train_loss: 0.1116, step time: 1.5302\n",
      "297/388, train_loss: 0.0585, step time: 1.5333\n",
      "298/388, train_loss: 0.1571, step time: 1.5399\n",
      "299/388, train_loss: 0.0314, step time: 1.5359\n",
      "300/388, train_loss: 0.1081, step time: 1.5367\n",
      "301/388, train_loss: 0.0694, step time: 1.5349\n",
      "302/388, train_loss: 0.2566, step time: 1.5302\n",
      "303/388, train_loss: 0.1165, step time: 1.5340\n",
      "304/388, train_loss: 0.0997, step time: 1.5338\n",
      "305/388, train_loss: 0.0769, step time: 1.5345\n",
      "306/388, train_loss: 0.1927, step time: 1.5365\n",
      "307/388, train_loss: 0.0838, step time: 1.5359\n",
      "308/388, train_loss: 0.1763, step time: 1.5327\n",
      "309/388, train_loss: 0.0709, step time: 1.5323\n",
      "310/388, train_loss: 0.1014, step time: 1.5388\n",
      "311/388, train_loss: 0.1628, step time: 1.5376\n",
      "312/388, train_loss: 0.0676, step time: 1.5399\n",
      "313/388, train_loss: 0.1810, step time: 1.5367\n",
      "314/388, train_loss: 0.1046, step time: 1.5350\n",
      "315/388, train_loss: 0.1830, step time: 1.5332\n",
      "316/388, train_loss: 0.0785, step time: 1.5352\n",
      "317/388, train_loss: 0.0864, step time: 1.5395\n",
      "318/388, train_loss: 0.3801, step time: 1.5370\n",
      "319/388, train_loss: 0.1827, step time: 1.5383\n",
      "320/388, train_loss: 0.1083, step time: 1.5365\n",
      "321/388, train_loss: 0.0993, step time: 1.5360\n",
      "322/388, train_loss: 0.0881, step time: 1.5367\n",
      "323/388, train_loss: 0.1087, step time: 1.5359\n",
      "324/388, train_loss: 0.3761, step time: 1.5305\n",
      "325/388, train_loss: 0.2727, step time: 1.5343\n",
      "326/388, train_loss: 0.1571, step time: 1.5308\n",
      "327/388, train_loss: 0.0588, step time: 1.5353\n",
      "328/388, train_loss: 0.2161, step time: 1.5357\n",
      "329/388, train_loss: 0.1188, step time: 1.5364\n",
      "330/388, train_loss: 0.1020, step time: 1.5334\n",
      "331/388, train_loss: 0.1760, step time: 1.5329\n",
      "332/388, train_loss: 0.2706, step time: 1.5361\n",
      "333/388, train_loss: 0.1462, step time: 1.5353\n",
      "334/388, train_loss: 0.2246, step time: 1.5333\n",
      "335/388, train_loss: 0.2516, step time: 1.5348\n",
      "336/388, train_loss: 0.1373, step time: 1.5348\n",
      "337/388, train_loss: 0.1849, step time: 1.5346\n",
      "338/388, train_loss: 0.0895, step time: 1.5390\n",
      "339/388, train_loss: 0.1920, step time: 1.5387\n",
      "340/388, train_loss: 0.1742, step time: 1.5359\n",
      "341/388, train_loss: 0.2105, step time: 1.5353\n",
      "342/388, train_loss: 0.1618, step time: 1.5320\n",
      "343/388, train_loss: 0.1148, step time: 1.5337\n",
      "344/388, train_loss: 0.0347, step time: 1.5354\n",
      "345/388, train_loss: 0.0763, step time: 1.5339\n",
      "346/388, train_loss: 0.1877, step time: 1.5363\n",
      "347/388, train_loss: 0.1121, step time: 1.5321\n",
      "348/388, train_loss: 0.1313, step time: 1.5329\n",
      "349/388, train_loss: 0.2940, step time: 1.5349\n",
      "350/388, train_loss: 0.0802, step time: 1.5392\n",
      "351/388, train_loss: 0.1328, step time: 1.5351\n",
      "352/388, train_loss: 0.2057, step time: 1.5374\n",
      "353/388, train_loss: 0.0835, step time: 1.5328\n",
      "354/388, train_loss: 0.1314, step time: 1.5299\n",
      "355/388, train_loss: 0.0968, step time: 1.5337\n",
      "356/388, train_loss: 0.1413, step time: 1.5392\n",
      "357/388, train_loss: 0.2999, step time: 1.5315\n",
      "358/388, train_loss: 0.1691, step time: 1.5319\n",
      "359/388, train_loss: 0.1274, step time: 1.5336\n",
      "360/388, train_loss: 0.1844, step time: 1.5325\n",
      "361/388, train_loss: 0.2008, step time: 1.5365\n",
      "362/388, train_loss: 0.1493, step time: 1.5362\n",
      "363/388, train_loss: 0.2562, step time: 1.5344\n",
      "364/388, train_loss: 0.0875, step time: 1.5345\n",
      "365/388, train_loss: 0.1312, step time: 1.5340\n",
      "366/388, train_loss: 0.3329, step time: 1.5351\n",
      "367/388, train_loss: 0.1560, step time: 1.5335\n",
      "368/388, train_loss: 0.1525, step time: 1.5370\n",
      "369/388, train_loss: 0.2265, step time: 1.5386\n",
      "370/388, train_loss: 0.1240, step time: 1.5349\n",
      "371/388, train_loss: 0.2064, step time: 1.5335\n",
      "372/388, train_loss: 0.1383, step time: 1.5347\n",
      "373/388, train_loss: 0.2525, step time: 1.5374\n",
      "374/388, train_loss: 0.0810, step time: 1.5382\n",
      "375/388, train_loss: 0.3177, step time: 1.5354\n",
      "376/388, train_loss: 0.0868, step time: 1.5303\n",
      "377/388, train_loss: 0.0923, step time: 1.5335\n",
      "378/388, train_loss: 0.1838, step time: 1.5322\n",
      "379/388, train_loss: 0.0545, step time: 1.5373\n",
      "380/388, train_loss: 0.4359, step time: 1.5402\n",
      "381/388, train_loss: 0.1155, step time: 1.5357\n",
      "382/388, train_loss: 0.0849, step time: 1.5361\n",
      "383/388, train_loss: 0.1252, step time: 1.5358\n",
      "384/388, train_loss: 0.0996, step time: 1.5355\n",
      "385/388, train_loss: 0.1412, step time: 1.5347\n",
      "386/388, train_loss: 0.2606, step time: 1.5338\n",
      "387/388, train_loss: 0.1165, step time: 1.5364\n",
      "388/388, train_loss: 0.1319, step time: 1.5407\n",
      "epoch 81 average loss: 0.1603\n",
      "current epoch: 81 current mean dice: 0.7740 tc: 0.8224 wt: 0.9061 et: 0.5936\n",
      "best mean dice: 0.7802 at epoch: 79\n",
      "time consuming of epoch 81 is: 701.0605\n",
      "----------\n",
      "epoch 82/100\n",
      "1/388, train_loss: 0.0438, step time: 1.5459\n",
      "2/388, train_loss: 0.0820, step time: 1.5342\n",
      "3/388, train_loss: 0.0569, step time: 1.5335\n",
      "4/388, train_loss: 0.0848, step time: 1.5315\n",
      "5/388, train_loss: 0.1111, step time: 1.5331\n",
      "6/388, train_loss: 0.0901, step time: 1.5378\n",
      "7/388, train_loss: 0.3313, step time: 1.5309\n",
      "8/388, train_loss: 0.1345, step time: 1.5351\n",
      "9/388, train_loss: 0.0599, step time: 1.5332\n",
      "10/388, train_loss: 0.0628, step time: 1.5345\n",
      "11/388, train_loss: 0.1074, step time: 1.5303\n",
      "12/388, train_loss: 0.1075, step time: 1.5324\n",
      "13/388, train_loss: 0.0922, step time: 1.5312\n",
      "14/388, train_loss: 0.2009, step time: 1.5372\n",
      "15/388, train_loss: 0.2295, step time: 1.5366\n",
      "16/388, train_loss: 0.1063, step time: 1.5332\n",
      "17/388, train_loss: 0.1769, step time: 1.5335\n",
      "18/388, train_loss: 0.2105, step time: 1.5320\n",
      "19/388, train_loss: 0.0695, step time: 1.5331\n",
      "20/388, train_loss: 0.0473, step time: 1.5388\n",
      "21/388, train_loss: 0.0985, step time: 1.5384\n",
      "22/388, train_loss: 0.0778, step time: 1.5339\n",
      "23/388, train_loss: 0.2017, step time: 1.5336\n",
      "24/388, train_loss: 0.1923, step time: 1.5379\n",
      "25/388, train_loss: 0.3450, step time: 1.5367\n",
      "26/388, train_loss: 0.2269, step time: 1.5391\n",
      "27/388, train_loss: 0.1209, step time: 1.5336\n",
      "28/388, train_loss: 0.0655, step time: 1.5333\n",
      "29/388, train_loss: 0.1883, step time: 1.5313\n",
      "30/388, train_loss: 0.1219, step time: 1.5330\n",
      "31/388, train_loss: 0.1292, step time: 1.5358\n",
      "32/388, train_loss: 0.1686, step time: 1.5352\n",
      "33/388, train_loss: 0.3064, step time: 1.5351\n",
      "34/388, train_loss: 0.1032, step time: 1.5341\n",
      "35/388, train_loss: 0.1024, step time: 1.5327\n",
      "36/388, train_loss: 0.0888, step time: 1.5391\n",
      "37/388, train_loss: 0.2004, step time: 1.5351\n",
      "38/388, train_loss: 0.0764, step time: 1.5338\n",
      "39/388, train_loss: 0.1019, step time: 1.5358\n",
      "40/388, train_loss: 0.1777, step time: 1.5342\n",
      "41/388, train_loss: 0.2830, step time: 1.5294\n",
      "42/388, train_loss: 0.2221, step time: 1.5302\n",
      "43/388, train_loss: 0.1236, step time: 1.5299\n",
      "44/388, train_loss: 0.2146, step time: 1.5316\n",
      "45/388, train_loss: 0.0876, step time: 1.5355\n",
      "46/388, train_loss: 0.1033, step time: 1.5359\n",
      "47/388, train_loss: 0.1905, step time: 1.5380\n",
      "48/388, train_loss: 0.2219, step time: 1.5351\n",
      "49/388, train_loss: 0.1813, step time: 1.5328\n",
      "50/388, train_loss: 0.0905, step time: 1.5356\n",
      "51/388, train_loss: 0.0527, step time: 1.5333\n",
      "52/388, train_loss: 0.1915, step time: 1.5371\n",
      "53/388, train_loss: 0.0647, step time: 1.5367\n",
      "54/388, train_loss: 0.1213, step time: 1.5356\n",
      "55/388, train_loss: 0.1750, step time: 1.5329\n",
      "56/388, train_loss: 0.1586, step time: 1.5325\n",
      "57/388, train_loss: 0.1227, step time: 1.5343\n",
      "58/388, train_loss: 0.0943, step time: 1.5348\n",
      "59/388, train_loss: 0.3897, step time: 1.5353\n",
      "60/388, train_loss: 0.0891, step time: 1.5399\n",
      "61/388, train_loss: 0.1541, step time: 1.5293\n",
      "62/388, train_loss: 0.1790, step time: 1.5336\n",
      "63/388, train_loss: 0.0537, step time: 1.5319\n",
      "64/388, train_loss: 0.0468, step time: 1.5422\n",
      "65/388, train_loss: 0.0601, step time: 1.5351\n",
      "66/388, train_loss: 0.2367, step time: 1.5368\n",
      "67/388, train_loss: 0.2516, step time: 1.5326\n",
      "68/388, train_loss: 0.0959, step time: 1.5322\n",
      "69/388, train_loss: 0.2589, step time: 1.5319\n",
      "70/388, train_loss: 0.1406, step time: 1.5351\n",
      "71/388, train_loss: 0.0954, step time: 1.5337\n",
      "72/388, train_loss: 0.1542, step time: 1.5355\n",
      "73/388, train_loss: 0.1490, step time: 1.5329\n",
      "74/388, train_loss: 0.0836, step time: 1.5323\n",
      "75/388, train_loss: 0.3052, step time: 1.5301\n",
      "76/388, train_loss: 0.0896, step time: 1.5315\n",
      "77/388, train_loss: 0.1254, step time: 1.5388\n",
      "78/388, train_loss: 0.0936, step time: 1.5362\n",
      "79/388, train_loss: 0.0986, step time: 1.5369\n",
      "80/388, train_loss: 0.1160, step time: 1.5360\n",
      "81/388, train_loss: 0.2808, step time: 1.5321\n",
      "82/388, train_loss: 0.2580, step time: 1.5337\n",
      "83/388, train_loss: 0.1799, step time: 1.5341\n",
      "84/388, train_loss: 0.1422, step time: 1.5374\n",
      "85/388, train_loss: 0.1608, step time: 1.5416\n",
      "86/388, train_loss: 0.2889, step time: 1.5302\n",
      "87/388, train_loss: 0.2877, step time: 1.5318\n",
      "88/388, train_loss: 0.0899, step time: 1.5305\n",
      "89/388, train_loss: 0.2838, step time: 1.5372\n",
      "90/388, train_loss: 0.1625, step time: 1.5363\n",
      "91/388, train_loss: 0.1403, step time: 1.5366\n",
      "92/388, train_loss: 0.1038, step time: 1.5358\n",
      "93/388, train_loss: 0.1178, step time: 1.5432\n",
      "94/388, train_loss: 0.1173, step time: 1.5348\n",
      "95/388, train_loss: 0.2651, step time: 1.5357\n",
      "96/388, train_loss: 0.5314, step time: 1.5370\n",
      "97/388, train_loss: 0.1403, step time: 1.5392\n",
      "98/388, train_loss: 0.1900, step time: 1.5333\n",
      "99/388, train_loss: 0.0327, step time: 1.5348\n",
      "100/388, train_loss: 0.1660, step time: 1.5349\n",
      "101/388, train_loss: 0.0867, step time: 1.5365\n",
      "102/388, train_loss: 0.0914, step time: 1.5379\n",
      "103/388, train_loss: 0.0832, step time: 1.5336\n",
      "104/388, train_loss: 0.1024, step time: 1.5334\n",
      "105/388, train_loss: 0.0858, step time: 1.5323\n",
      "106/388, train_loss: 0.2896, step time: 1.5387\n",
      "107/388, train_loss: 0.2220, step time: 1.5353\n",
      "108/388, train_loss: 0.1368, step time: 1.5351\n",
      "109/388, train_loss: 0.1188, step time: 1.5351\n",
      "110/388, train_loss: 0.1479, step time: 1.5313\n",
      "111/388, train_loss: 0.2142, step time: 1.5314\n",
      "112/388, train_loss: 0.0893, step time: 1.5337\n",
      "113/388, train_loss: 0.1298, step time: 1.5347\n",
      "114/388, train_loss: 0.1116, step time: 1.5363\n",
      "115/388, train_loss: 0.0413, step time: 1.5368\n",
      "116/388, train_loss: 0.1464, step time: 1.5336\n",
      "117/388, train_loss: 0.1803, step time: 1.5334\n",
      "118/388, train_loss: 0.1780, step time: 1.5365\n",
      "119/388, train_loss: 0.0964, step time: 1.5438\n",
      "120/388, train_loss: 0.0672, step time: 1.5388\n",
      "121/388, train_loss: 0.1335, step time: 1.5388\n",
      "122/388, train_loss: 0.0797, step time: 1.5334\n",
      "123/388, train_loss: 0.1080, step time: 1.5322\n",
      "124/388, train_loss: 0.0874, step time: 1.5322\n",
      "125/388, train_loss: 0.1905, step time: 1.5317\n",
      "126/388, train_loss: 0.0979, step time: 1.5342\n",
      "127/388, train_loss: 0.2077, step time: 1.5376\n",
      "128/388, train_loss: 0.2340, step time: 1.5359\n",
      "129/388, train_loss: 0.1863, step time: 1.5331\n",
      "130/388, train_loss: 0.1771, step time: 1.5312\n",
      "131/388, train_loss: 0.1004, step time: 1.5310\n",
      "132/388, train_loss: 0.1415, step time: 1.5344\n",
      "133/388, train_loss: 0.1111, step time: 1.5382\n",
      "134/388, train_loss: 0.2245, step time: 1.5392\n",
      "135/388, train_loss: 0.3137, step time: 1.5326\n",
      "136/388, train_loss: 0.1948, step time: 1.5325\n",
      "137/388, train_loss: 0.1984, step time: 1.5351\n",
      "138/388, train_loss: 0.1277, step time: 1.5335\n",
      "139/388, train_loss: 0.1557, step time: 1.5366\n",
      "140/388, train_loss: 0.2090, step time: 1.5392\n",
      "141/388, train_loss: 0.1506, step time: 1.5338\n",
      "142/388, train_loss: 0.1497, step time: 1.5357\n",
      "143/388, train_loss: 0.1050, step time: 1.5324\n",
      "144/388, train_loss: 0.1520, step time: 1.5334\n",
      "145/388, train_loss: 0.0713, step time: 1.5318\n",
      "146/388, train_loss: 0.1959, step time: 1.5366\n",
      "147/388, train_loss: 0.0933, step time: 1.5373\n",
      "148/388, train_loss: 0.0967, step time: 1.5387\n",
      "149/388, train_loss: 0.2288, step time: 1.5312\n",
      "150/388, train_loss: 0.2390, step time: 1.5336\n",
      "151/388, train_loss: 0.0766, step time: 1.5345\n",
      "152/388, train_loss: 0.1608, step time: 1.5349\n",
      "153/388, train_loss: 0.1885, step time: 1.5463\n",
      "154/388, train_loss: 0.0959, step time: 1.5345\n",
      "155/388, train_loss: 0.2067, step time: 1.5343\n",
      "156/388, train_loss: 0.0739, step time: 1.5359\n",
      "157/388, train_loss: 0.1359, step time: 1.5420\n",
      "158/388, train_loss: 0.0908, step time: 1.5364\n",
      "159/388, train_loss: 0.3673, step time: 1.5357\n",
      "160/388, train_loss: 0.0572, step time: 1.5341\n",
      "161/388, train_loss: 0.4790, step time: 1.5332\n",
      "162/388, train_loss: 0.1823, step time: 1.5341\n",
      "163/388, train_loss: 0.1354, step time: 1.5490\n",
      "164/388, train_loss: 0.0958, step time: 1.5340\n",
      "165/388, train_loss: 0.1589, step time: 1.5444\n",
      "166/388, train_loss: 0.1661, step time: 1.5350\n",
      "167/388, train_loss: 0.1908, step time: 1.5361\n",
      "168/388, train_loss: 0.0299, step time: 1.5363\n",
      "169/388, train_loss: 0.2291, step time: 1.5358\n",
      "170/388, train_loss: 0.2195, step time: 1.5360\n",
      "171/388, train_loss: 0.2906, step time: 1.5315\n",
      "172/388, train_loss: 0.3147, step time: 1.5432\n",
      "173/388, train_loss: 0.1480, step time: 1.5360\n",
      "174/388, train_loss: 0.1549, step time: 1.5410\n",
      "175/388, train_loss: 0.0604, step time: 1.5332\n",
      "176/388, train_loss: 0.2847, step time: 1.5353\n",
      "177/388, train_loss: 0.2504, step time: 1.5336\n",
      "178/388, train_loss: 0.1341, step time: 1.5335\n",
      "179/388, train_loss: 0.1211, step time: 1.5364\n",
      "180/388, train_loss: 0.2059, step time: 1.5331\n",
      "181/388, train_loss: 0.0890, step time: 1.5355\n",
      "182/388, train_loss: 0.1620, step time: 1.5296\n",
      "183/388, train_loss: 0.1357, step time: 1.5461\n",
      "184/388, train_loss: 0.1578, step time: 1.5357\n",
      "185/388, train_loss: 0.1649, step time: 1.5341\n",
      "186/388, train_loss: 0.0850, step time: 1.5327\n",
      "187/388, train_loss: 0.1416, step time: 1.5351\n",
      "188/388, train_loss: 0.2218, step time: 1.5340\n",
      "189/388, train_loss: 0.0857, step time: 1.5367\n",
      "190/388, train_loss: 0.1699, step time: 1.5364\n",
      "191/388, train_loss: 0.1956, step time: 1.5361\n",
      "192/388, train_loss: 0.1012, step time: 1.5331\n",
      "193/388, train_loss: 0.1727, step time: 1.5341\n",
      "194/388, train_loss: 0.0481, step time: 1.5337\n",
      "195/388, train_loss: 0.2029, step time: 1.5350\n",
      "196/388, train_loss: 0.0818, step time: 1.5373\n",
      "197/388, train_loss: 0.1251, step time: 1.5369\n",
      "198/388, train_loss: 0.2332, step time: 1.5356\n",
      "199/388, train_loss: 0.1314, step time: 1.5373\n",
      "200/388, train_loss: 0.0663, step time: 1.5350\n",
      "201/388, train_loss: 0.3075, step time: 1.5326\n",
      "202/388, train_loss: 0.1154, step time: 1.5340\n",
      "203/388, train_loss: 0.1238, step time: 1.5350\n",
      "204/388, train_loss: 0.2588, step time: 1.5342\n",
      "205/388, train_loss: 0.2013, step time: 1.5335\n",
      "206/388, train_loss: 0.2063, step time: 1.5321\n",
      "207/388, train_loss: 0.0412, step time: 1.5324\n",
      "208/388, train_loss: 0.2511, step time: 1.5368\n",
      "209/388, train_loss: 0.2714, step time: 1.5399\n",
      "210/388, train_loss: 0.1187, step time: 1.5355\n",
      "211/388, train_loss: 0.2042, step time: 1.5468\n",
      "212/388, train_loss: 0.0819, step time: 1.5354\n",
      "213/388, train_loss: 0.1526, step time: 1.5366\n",
      "214/388, train_loss: 0.1018, step time: 1.5375\n",
      "215/388, train_loss: 0.1433, step time: 1.5398\n",
      "216/388, train_loss: 0.0911, step time: 1.5361\n",
      "217/388, train_loss: 0.0538, step time: 1.5322\n",
      "218/388, train_loss: 0.3252, step time: 1.5330\n",
      "219/388, train_loss: 0.1000, step time: 1.5392\n",
      "220/388, train_loss: 0.0909, step time: 1.5342\n",
      "221/388, train_loss: 0.1520, step time: 1.5379\n",
      "222/388, train_loss: 0.2004, step time: 1.5379\n",
      "223/388, train_loss: 0.0989, step time: 1.5420\n",
      "224/388, train_loss: 0.1066, step time: 1.5334\n",
      "225/388, train_loss: 0.2111, step time: 1.5314\n",
      "226/388, train_loss: 0.1234, step time: 1.5377\n",
      "227/388, train_loss: 0.1099, step time: 1.5356\n",
      "228/388, train_loss: 0.0815, step time: 1.5333\n",
      "229/388, train_loss: 0.1366, step time: 1.5318\n",
      "230/388, train_loss: 0.1213, step time: 1.5321\n",
      "231/388, train_loss: 0.1327, step time: 1.5361\n",
      "232/388, train_loss: 0.1440, step time: 1.5352\n",
      "233/388, train_loss: 0.1868, step time: 1.5383\n",
      "234/388, train_loss: 0.1003, step time: 1.5355\n",
      "235/388, train_loss: 0.1786, step time: 1.5323\n",
      "236/388, train_loss: 0.1170, step time: 1.5317\n",
      "237/388, train_loss: 0.0952, step time: 1.5344\n",
      "238/388, train_loss: 0.1792, step time: 1.5465\n",
      "239/388, train_loss: 0.0695, step time: 1.5359\n",
      "240/388, train_loss: 0.3804, step time: 1.5345\n",
      "241/388, train_loss: 0.0929, step time: 1.5411\n",
      "242/388, train_loss: 0.0702, step time: 1.5392\n",
      "243/388, train_loss: 0.1826, step time: 1.5318\n",
      "244/388, train_loss: 0.1846, step time: 1.5349\n",
      "245/388, train_loss: 0.1643, step time: 1.5329\n",
      "246/388, train_loss: 0.1055, step time: 1.5383\n",
      "247/388, train_loss: 0.2409, step time: 1.5361\n",
      "248/388, train_loss: 0.4286, step time: 1.5335\n",
      "249/388, train_loss: 0.2659, step time: 1.5294\n",
      "250/388, train_loss: 0.0809, step time: 1.5301\n",
      "251/388, train_loss: 0.1221, step time: 1.5337\n",
      "252/388, train_loss: 0.1354, step time: 1.5341\n",
      "253/388, train_loss: 0.0686, step time: 1.5348\n",
      "254/388, train_loss: 0.1678, step time: 1.5361\n",
      "255/388, train_loss: 0.1284, step time: 1.5359\n",
      "256/388, train_loss: 0.0792, step time: 1.5302\n",
      "257/388, train_loss: 0.1425, step time: 1.5318\n",
      "258/388, train_loss: 0.1398, step time: 1.5362\n",
      "259/388, train_loss: 0.1748, step time: 1.5355\n",
      "260/388, train_loss: 0.1577, step time: 1.5489\n",
      "261/388, train_loss: 0.2861, step time: 1.5323\n",
      "262/388, train_loss: 0.1271, step time: 1.5388\n",
      "263/388, train_loss: 0.2400, step time: 1.5312\n",
      "264/388, train_loss: 0.1005, step time: 1.5477\n",
      "265/388, train_loss: 0.1925, step time: 1.5357\n",
      "266/388, train_loss: 0.2063, step time: 1.5334\n",
      "267/388, train_loss: 0.1485, step time: 1.5324\n",
      "268/388, train_loss: 0.0943, step time: 1.5424\n",
      "269/388, train_loss: 0.1076, step time: 1.5369\n",
      "270/388, train_loss: 0.1533, step time: 1.5340\n",
      "271/388, train_loss: 0.4142, step time: 1.5373\n",
      "272/388, train_loss: 0.2663, step time: 1.5438\n",
      "273/388, train_loss: 0.1865, step time: 1.5333\n",
      "274/388, train_loss: 0.2184, step time: 1.5352\n",
      "275/388, train_loss: 0.0742, step time: 1.5351\n",
      "276/388, train_loss: 0.0964, step time: 1.5447\n",
      "277/388, train_loss: 0.0673, step time: 1.5382\n",
      "278/388, train_loss: 0.2457, step time: 1.5372\n",
      "279/388, train_loss: 0.2628, step time: 1.5369\n",
      "280/388, train_loss: 0.1873, step time: 1.5415\n",
      "281/388, train_loss: 0.4075, step time: 1.5339\n",
      "282/388, train_loss: 0.0850, step time: 1.5362\n",
      "283/388, train_loss: 0.2260, step time: 1.5348\n",
      "284/388, train_loss: 0.2241, step time: 1.5455\n",
      "285/388, train_loss: 0.1209, step time: 1.5302\n",
      "286/388, train_loss: 0.1094, step time: 1.5351\n",
      "287/388, train_loss: 0.1119, step time: 1.5369\n",
      "288/388, train_loss: 0.1414, step time: 1.5485\n",
      "289/388, train_loss: 0.1469, step time: 1.5332\n",
      "290/388, train_loss: 0.1203, step time: 1.5347\n",
      "291/388, train_loss: 0.2071, step time: 1.5326\n",
      "292/388, train_loss: 0.0618, step time: 1.5454\n",
      "293/388, train_loss: 0.3285, step time: 1.5328\n",
      "294/388, train_loss: 0.0992, step time: 1.5362\n",
      "295/388, train_loss: 0.2158, step time: 1.5330\n",
      "296/388, train_loss: 0.3040, step time: 1.5413\n",
      "297/388, train_loss: 0.1463, step time: 1.5336\n",
      "298/388, train_loss: 0.1724, step time: 1.5371\n",
      "299/388, train_loss: 0.2532, step time: 1.5386\n",
      "300/388, train_loss: 0.1163, step time: 1.5444\n",
      "301/388, train_loss: 0.1021, step time: 1.5351\n",
      "302/388, train_loss: 0.0687, step time: 1.5379\n",
      "303/388, train_loss: 0.0886, step time: 1.5356\n",
      "304/388, train_loss: 0.1373, step time: 1.5484\n",
      "305/388, train_loss: 0.1108, step time: 1.5334\n",
      "306/388, train_loss: 0.2470, step time: 1.5337\n",
      "307/388, train_loss: 0.1279, step time: 1.5349\n",
      "308/388, train_loss: 0.0796, step time: 1.5566\n",
      "309/388, train_loss: 0.1220, step time: 1.5310\n",
      "310/388, train_loss: 0.1431, step time: 1.5304\n",
      "311/388, train_loss: 0.1764, step time: 1.5321\n",
      "312/388, train_loss: 0.0523, step time: 1.5516\n",
      "313/388, train_loss: 0.2088, step time: 1.5550\n",
      "314/388, train_loss: 0.1339, step time: 1.5393\n",
      "315/388, train_loss: 0.1084, step time: 1.5342\n",
      "316/388, train_loss: 0.2926, step time: 1.5511\n",
      "317/388, train_loss: 0.1955, step time: 1.5360\n",
      "318/388, train_loss: 0.2139, step time: 1.5436\n",
      "319/388, train_loss: 0.2778, step time: 1.5350\n",
      "320/388, train_loss: 0.0881, step time: 1.5509\n",
      "321/388, train_loss: 0.1596, step time: 1.5377\n",
      "322/388, train_loss: 0.0861, step time: 1.5372\n",
      "323/388, train_loss: 0.0881, step time: 1.5335\n",
      "324/388, train_loss: 0.0890, step time: 1.5450\n",
      "325/388, train_loss: 0.2280, step time: 1.5350\n",
      "326/388, train_loss: 0.2364, step time: 1.5374\n",
      "327/388, train_loss: 0.1014, step time: 1.5377\n",
      "328/388, train_loss: 0.2354, step time: 1.5464\n",
      "329/388, train_loss: 0.0980, step time: 1.5404\n",
      "330/388, train_loss: 0.0725, step time: 1.5384\n",
      "331/388, train_loss: 0.1203, step time: 1.5366\n",
      "332/388, train_loss: 0.1087, step time: 1.5472\n",
      "333/388, train_loss: 0.3854, step time: 1.5372\n",
      "334/388, train_loss: 0.2729, step time: 1.5365\n",
      "335/388, train_loss: 0.1470, step time: 1.5356\n",
      "336/388, train_loss: 0.1166, step time: 1.5438\n",
      "337/388, train_loss: 0.1783, step time: 1.5342\n",
      "338/388, train_loss: 0.0301, step time: 1.5359\n",
      "339/388, train_loss: 0.1630, step time: 1.5382\n",
      "340/388, train_loss: 0.0456, step time: 1.5433\n",
      "341/388, train_loss: 0.1471, step time: 1.5337\n",
      "342/388, train_loss: 0.4653, step time: 1.5385\n",
      "343/388, train_loss: 0.0827, step time: 1.5382\n",
      "344/388, train_loss: 0.1580, step time: 1.5470\n",
      "345/388, train_loss: 0.2004, step time: 1.5331\n",
      "346/388, train_loss: 0.2302, step time: 1.5362\n",
      "347/388, train_loss: 0.0553, step time: 1.5362\n",
      "348/388, train_loss: 0.0768, step time: 1.5496\n",
      "349/388, train_loss: 0.1497, step time: 1.5349\n",
      "350/388, train_loss: 0.2128, step time: 1.5329\n",
      "351/388, train_loss: 0.4028, step time: 1.5374\n",
      "352/388, train_loss: 0.4194, step time: 1.5461\n",
      "353/388, train_loss: 0.0353, step time: 1.5311\n",
      "354/388, train_loss: 0.1336, step time: 1.5337\n",
      "355/388, train_loss: 0.0735, step time: 1.5322\n",
      "356/388, train_loss: 0.0636, step time: 1.5407\n",
      "357/388, train_loss: 0.2524, step time: 1.5378\n",
      "358/388, train_loss: 0.3331, step time: 1.5381\n",
      "359/388, train_loss: 0.2322, step time: 1.5333\n",
      "360/388, train_loss: 0.2481, step time: 1.5457\n",
      "361/388, train_loss: 0.1598, step time: 1.5370\n",
      "362/388, train_loss: 0.2103, step time: 1.5347\n",
      "363/388, train_loss: 0.0535, step time: 1.5305\n",
      "364/388, train_loss: 0.1891, step time: 1.5436\n",
      "365/388, train_loss: 0.2967, step time: 1.5426\n",
      "366/388, train_loss: 0.1224, step time: 1.5373\n",
      "367/388, train_loss: 0.0751, step time: 1.5340\n",
      "368/388, train_loss: 0.1864, step time: 1.5462\n",
      "369/388, train_loss: 0.1467, step time: 1.5415\n",
      "370/388, train_loss: 0.1824, step time: 1.5378\n",
      "371/388, train_loss: 0.1939, step time: 1.5349\n",
      "372/388, train_loss: 0.1678, step time: 1.5425\n",
      "373/388, train_loss: 0.1378, step time: 1.5433\n",
      "374/388, train_loss: 0.1853, step time: 1.5459\n",
      "375/388, train_loss: 0.0869, step time: 1.5360\n",
      "376/388, train_loss: 0.1166, step time: 1.5433\n",
      "377/388, train_loss: 0.1649, step time: 1.5487\n",
      "378/388, train_loss: 0.0633, step time: 1.5386\n",
      "379/388, train_loss: 0.0973, step time: 1.5346\n",
      "380/388, train_loss: 0.1216, step time: 1.5438\n",
      "381/388, train_loss: 0.5262, step time: 1.5320\n",
      "382/388, train_loss: 0.1738, step time: 1.5377\n",
      "383/388, train_loss: 0.2343, step time: 1.5406\n",
      "384/388, train_loss: 0.1268, step time: 1.5466\n",
      "385/388, train_loss: 0.0897, step time: 1.5323\n",
      "386/388, train_loss: 0.2362, step time: 1.5341\n",
      "387/388, train_loss: 0.2318, step time: 1.5317\n",
      "388/388, train_loss: 0.1209, step time: 1.5469\n",
      "epoch 82 average loss: 0.1599\n",
      "current epoch: 82 current mean dice: 0.7781 tc: 0.8251 wt: 0.9066 et: 0.6027\n",
      "best mean dice: 0.7802 at epoch: 79\n",
      "time consuming of epoch 82 is: 704.4333\n",
      "----------\n",
      "epoch 83/100\n",
      "1/388, train_loss: 0.0900, step time: 1.5535\n",
      "2/388, train_loss: 0.1826, step time: 1.5475\n",
      "3/388, train_loss: 0.0460, step time: 1.5339\n",
      "4/388, train_loss: 0.0844, step time: 1.5337\n",
      "5/388, train_loss: 0.1760, step time: 1.5304\n",
      "6/388, train_loss: 0.1475, step time: 1.5559\n",
      "7/388, train_loss: 0.1160, step time: 1.5365\n",
      "8/388, train_loss: 0.2686, step time: 1.5317\n",
      "9/388, train_loss: 0.1882, step time: 1.5372\n",
      "10/388, train_loss: 0.0848, step time: 1.5490\n",
      "11/388, train_loss: 0.0667, step time: 1.5323\n",
      "12/388, train_loss: 0.1867, step time: 1.5359\n",
      "13/388, train_loss: 0.0844, step time: 1.5353\n",
      "14/388, train_loss: 0.1409, step time: 1.5433\n",
      "15/388, train_loss: 0.3225, step time: 1.5321\n",
      "16/388, train_loss: 0.1684, step time: 1.5456\n",
      "17/388, train_loss: 0.1722, step time: 1.5347\n",
      "18/388, train_loss: 0.4013, step time: 1.5557\n",
      "19/388, train_loss: 0.1945, step time: 1.5321\n",
      "20/388, train_loss: 0.3780, step time: 1.5399\n",
      "21/388, train_loss: 0.1924, step time: 1.5385\n",
      "22/388, train_loss: 0.1005, step time: 1.5426\n",
      "23/388, train_loss: 0.1044, step time: 1.5332\n",
      "24/388, train_loss: 0.2012, step time: 1.5360\n",
      "25/388, train_loss: 0.1999, step time: 1.5355\n",
      "26/388, train_loss: 0.1736, step time: 1.5456\n",
      "27/388, train_loss: 0.0488, step time: 1.5304\n",
      "28/388, train_loss: 0.1110, step time: 1.5368\n",
      "29/388, train_loss: 0.1779, step time: 1.5335\n",
      "30/388, train_loss: 0.1846, step time: 1.5445\n",
      "31/388, train_loss: 0.0811, step time: 1.5329\n",
      "32/388, train_loss: 0.3042, step time: 1.5366\n",
      "33/388, train_loss: 0.0866, step time: 1.5360\n",
      "34/388, train_loss: 0.0936, step time: 1.5467\n",
      "35/388, train_loss: 0.1644, step time: 1.5382\n",
      "36/388, train_loss: 0.1097, step time: 1.5361\n",
      "37/388, train_loss: 0.2824, step time: 1.5350\n",
      "38/388, train_loss: 0.1157, step time: 1.5449\n",
      "39/388, train_loss: 0.0986, step time: 1.5343\n",
      "40/388, train_loss: 0.2097, step time: 1.5367\n",
      "41/388, train_loss: 0.2044, step time: 1.5419\n",
      "42/388, train_loss: 0.1811, step time: 1.5406\n",
      "43/388, train_loss: 0.1425, step time: 1.5347\n",
      "44/388, train_loss: 0.0937, step time: 1.5360\n",
      "45/388, train_loss: 0.1505, step time: 1.5361\n",
      "46/388, train_loss: 0.2152, step time: 1.5458\n",
      "47/388, train_loss: 0.1846, step time: 1.5322\n",
      "48/388, train_loss: 0.4105, step time: 1.5339\n",
      "49/388, train_loss: 0.1574, step time: 1.5340\n",
      "50/388, train_loss: 0.1414, step time: 1.5600\n",
      "51/388, train_loss: 0.1081, step time: 1.5318\n",
      "52/388, train_loss: 0.2245, step time: 1.5368\n",
      "53/388, train_loss: 0.2371, step time: 1.5403\n",
      "54/388, train_loss: 0.1244, step time: 1.5494\n",
      "55/388, train_loss: 0.1433, step time: 1.5340\n",
      "56/388, train_loss: 0.0669, step time: 1.5336\n",
      "57/388, train_loss: 0.2421, step time: 1.5355\n",
      "58/388, train_loss: 0.1453, step time: 1.5550\n",
      "59/388, train_loss: 0.1778, step time: 1.5337\n",
      "60/388, train_loss: 0.1656, step time: 1.5356\n",
      "61/388, train_loss: 0.2866, step time: 1.5344\n",
      "62/388, train_loss: 0.1425, step time: 1.5580\n",
      "63/388, train_loss: 0.0664, step time: 1.5309\n",
      "64/388, train_loss: 0.1640, step time: 1.5325\n",
      "65/388, train_loss: 0.0625, step time: 1.5367\n",
      "66/388, train_loss: 0.1746, step time: 1.5479\n",
      "67/388, train_loss: 0.1761, step time: 1.5346\n",
      "68/388, train_loss: 0.1100, step time: 1.5342\n",
      "69/388, train_loss: 0.0618, step time: 1.5392\n",
      "70/388, train_loss: 0.2352, step time: 1.5448\n",
      "71/388, train_loss: 0.1633, step time: 1.5305\n",
      "72/388, train_loss: 0.1473, step time: 1.5629\n",
      "73/388, train_loss: 0.1607, step time: 1.5396\n",
      "74/388, train_loss: 0.2131, step time: 1.5546\n",
      "75/388, train_loss: 0.2013, step time: 1.5369\n",
      "76/388, train_loss: 0.0976, step time: 1.5348\n",
      "77/388, train_loss: 0.0461, step time: 1.5336\n",
      "78/388, train_loss: 0.1525, step time: 1.5427\n",
      "79/388, train_loss: 0.1405, step time: 1.5339\n",
      "80/388, train_loss: 0.1467, step time: 1.5323\n",
      "81/388, train_loss: 0.1370, step time: 1.5354\n",
      "82/388, train_loss: 0.1082, step time: 1.5588\n",
      "83/388, train_loss: 0.5434, step time: 1.5337\n",
      "84/388, train_loss: 0.0950, step time: 1.5365\n",
      "85/388, train_loss: 0.2359, step time: 1.5348\n",
      "86/388, train_loss: 0.0673, step time: 1.5468\n",
      "87/388, train_loss: 0.1181, step time: 1.5332\n",
      "88/388, train_loss: 0.1171, step time: 1.5333\n",
      "89/388, train_loss: 0.3763, step time: 1.5378\n",
      "90/388, train_loss: 0.0355, step time: 1.5560\n",
      "91/388, train_loss: 0.0642, step time: 1.5364\n",
      "92/388, train_loss: 0.0625, step time: 1.5335\n",
      "93/388, train_loss: 0.1047, step time: 1.5331\n",
      "94/388, train_loss: 0.0941, step time: 1.5421\n",
      "95/388, train_loss: 0.1125, step time: 1.5461\n",
      "96/388, train_loss: 0.0845, step time: 1.5330\n",
      "97/388, train_loss: 0.1153, step time: 1.5346\n",
      "98/388, train_loss: 0.1347, step time: 1.5498\n",
      "99/388, train_loss: 0.2546, step time: 1.5337\n",
      "100/388, train_loss: 0.1484, step time: 1.5357\n",
      "101/388, train_loss: 0.2887, step time: 1.5354\n",
      "102/388, train_loss: 0.0805, step time: 1.5536\n",
      "103/388, train_loss: 0.2104, step time: 1.5362\n",
      "104/388, train_loss: 0.1188, step time: 1.5395\n",
      "105/388, train_loss: 0.2567, step time: 1.5363\n",
      "106/388, train_loss: 0.3852, step time: 1.5571\n",
      "107/388, train_loss: 0.1698, step time: 1.5349\n",
      "108/388, train_loss: 0.1585, step time: 1.5344\n",
      "109/388, train_loss: 0.1743, step time: 1.5365\n",
      "110/388, train_loss: 0.0664, step time: 1.5504\n",
      "111/388, train_loss: 0.2654, step time: 1.5351\n",
      "112/388, train_loss: 0.2165, step time: 1.5330\n",
      "113/388, train_loss: 0.1537, step time: 1.5316\n",
      "114/388, train_loss: 0.0792, step time: 1.5535\n",
      "115/388, train_loss: 0.0938, step time: 1.5329\n",
      "116/388, train_loss: 0.1165, step time: 1.5305\n",
      "117/388, train_loss: 0.2146, step time: 1.5401\n",
      "118/388, train_loss: 0.1467, step time: 1.5454\n",
      "119/388, train_loss: 0.1031, step time: 1.5325\n",
      "120/388, train_loss: 0.6133, step time: 1.5346\n",
      "121/388, train_loss: 0.2611, step time: 1.5350\n",
      "122/388, train_loss: 0.1454, step time: 1.5482\n",
      "123/388, train_loss: 0.1195, step time: 1.5296\n",
      "124/388, train_loss: 0.1154, step time: 1.5324\n",
      "125/388, train_loss: 0.1337, step time: 1.5323\n",
      "126/388, train_loss: 0.1268, step time: 1.5457\n",
      "127/388, train_loss: 0.1877, step time: 1.5344\n",
      "128/388, train_loss: 0.0920, step time: 1.5344\n",
      "129/388, train_loss: 0.2223, step time: 1.5293\n",
      "130/388, train_loss: 0.3354, step time: 1.5473\n",
      "131/388, train_loss: 0.0354, step time: 1.5393\n",
      "132/388, train_loss: 0.3707, step time: 1.5370\n",
      "133/388, train_loss: 0.2018, step time: 1.5318\n",
      "134/388, train_loss: 0.0876, step time: 1.5419\n",
      "135/388, train_loss: 0.4021, step time: 1.5437\n",
      "136/388, train_loss: 0.0779, step time: 1.5370\n",
      "137/388, train_loss: 0.2397, step time: 1.5320\n",
      "138/388, train_loss: 0.1532, step time: 1.5525\n",
      "139/388, train_loss: 0.1173, step time: 1.5389\n",
      "140/388, train_loss: 0.1957, step time: 1.5377\n",
      "141/388, train_loss: 0.2281, step time: 1.5315\n",
      "142/388, train_loss: 0.3911, step time: 1.5446\n",
      "143/388, train_loss: 0.1321, step time: 1.5437\n",
      "144/388, train_loss: 0.1767, step time: 1.5377\n",
      "145/388, train_loss: 0.1830, step time: 1.5333\n",
      "146/388, train_loss: 0.2086, step time: 1.5531\n",
      "147/388, train_loss: 0.2744, step time: 1.5374\n",
      "148/388, train_loss: 0.1465, step time: 1.5391\n",
      "149/388, train_loss: 0.1050, step time: 1.5334\n",
      "150/388, train_loss: 0.2274, step time: 1.5559\n",
      "151/388, train_loss: 0.1984, step time: 1.5366\n",
      "152/388, train_loss: 0.2700, step time: 1.5353\n",
      "153/388, train_loss: 0.1686, step time: 1.5332\n",
      "154/388, train_loss: 0.1339, step time: 1.5547\n",
      "155/388, train_loss: 0.2490, step time: 1.5696\n",
      "156/388, train_loss: 0.1047, step time: 1.5608\n",
      "157/388, train_loss: 0.2500, step time: 1.5302\n",
      "158/388, train_loss: 0.2276, step time: 1.5526\n",
      "159/388, train_loss: 0.2340, step time: 1.5323\n",
      "160/388, train_loss: 0.1533, step time: 1.5354\n",
      "161/388, train_loss: 0.1334, step time: 1.5342\n",
      "162/388, train_loss: 0.2468, step time: 1.5560\n",
      "163/388, train_loss: 0.1390, step time: 1.5395\n",
      "164/388, train_loss: 0.1621, step time: 1.5344\n",
      "165/388, train_loss: 0.0457, step time: 1.5316\n",
      "166/388, train_loss: 0.1703, step time: 1.5554\n",
      "167/388, train_loss: 0.1763, step time: 1.5346\n",
      "168/388, train_loss: 0.1055, step time: 1.5355\n",
      "169/388, train_loss: 0.5132, step time: 1.5392\n",
      "170/388, train_loss: 0.1360, step time: 1.5433\n",
      "171/388, train_loss: 0.0555, step time: 1.5375\n",
      "172/388, train_loss: 0.2312, step time: 1.5340\n",
      "173/388, train_loss: 0.0254, step time: 1.5353\n",
      "174/388, train_loss: 0.2763, step time: 1.5526\n",
      "175/388, train_loss: 0.1193, step time: 1.5357\n",
      "176/388, train_loss: 0.0650, step time: 1.5342\n",
      "177/388, train_loss: 0.1010, step time: 1.5374\n",
      "178/388, train_loss: 0.1245, step time: 1.5413\n",
      "179/388, train_loss: 0.1919, step time: 1.5318\n",
      "180/388, train_loss: 0.1889, step time: 1.5485\n",
      "181/388, train_loss: 0.0606, step time: 1.5363\n",
      "182/388, train_loss: 0.0852, step time: 1.5430\n",
      "183/388, train_loss: 0.0774, step time: 1.5321\n",
      "184/388, train_loss: 0.0879, step time: 1.5340\n",
      "185/388, train_loss: 0.0902, step time: 1.5375\n",
      "186/388, train_loss: 0.0866, step time: 1.5436\n",
      "187/388, train_loss: 0.3396, step time: 1.5304\n",
      "188/388, train_loss: 0.1603, step time: 1.5330\n",
      "189/388, train_loss: 0.0566, step time: 1.5354\n",
      "190/388, train_loss: 0.1233, step time: 1.5532\n",
      "191/388, train_loss: 0.0904, step time: 1.5344\n",
      "192/388, train_loss: 0.1629, step time: 1.5364\n",
      "193/388, train_loss: 0.1920, step time: 1.5366\n",
      "194/388, train_loss: 0.0424, step time: 1.5509\n",
      "195/388, train_loss: 0.0881, step time: 1.5328\n",
      "196/388, train_loss: 0.1760, step time: 1.5335\n",
      "197/388, train_loss: 0.1528, step time: 1.5469\n",
      "198/388, train_loss: 0.2000, step time: 1.5396\n",
      "199/388, train_loss: 0.0626, step time: 1.5321\n",
      "200/388, train_loss: 0.1924, step time: 1.5370\n",
      "201/388, train_loss: 0.0706, step time: 1.5334\n",
      "202/388, train_loss: 0.0688, step time: 1.5439\n",
      "203/388, train_loss: 0.4284, step time: 1.5377\n",
      "204/388, train_loss: 0.4149, step time: 1.5374\n",
      "205/388, train_loss: 0.1484, step time: 1.5314\n",
      "206/388, train_loss: 0.0978, step time: 1.5426\n",
      "207/388, train_loss: 0.2050, step time: 1.5374\n",
      "208/388, train_loss: 0.1016, step time: 1.5371\n",
      "209/388, train_loss: 0.1762, step time: 1.5310\n",
      "210/388, train_loss: 0.1174, step time: 1.5568\n",
      "211/388, train_loss: 0.3010, step time: 1.5323\n",
      "212/388, train_loss: 0.1422, step time: 1.5374\n",
      "213/388, train_loss: 0.1797, step time: 1.5366\n",
      "214/388, train_loss: 0.0907, step time: 1.5449\n",
      "215/388, train_loss: 0.0839, step time: 1.5353\n",
      "216/388, train_loss: 0.0790, step time: 1.5350\n",
      "217/388, train_loss: 0.2156, step time: 1.5331\n",
      "218/388, train_loss: 0.1832, step time: 1.5438\n",
      "219/388, train_loss: 0.2801, step time: 1.5331\n",
      "220/388, train_loss: 0.0813, step time: 1.5308\n",
      "221/388, train_loss: 0.1880, step time: 1.5369\n",
      "222/388, train_loss: 0.1554, step time: 1.5631\n",
      "223/388, train_loss: 0.0841, step time: 1.5336\n",
      "224/388, train_loss: 0.1480, step time: 1.5325\n",
      "225/388, train_loss: 0.1333, step time: 1.5378\n",
      "226/388, train_loss: 0.1917, step time: 1.5432\n",
      "227/388, train_loss: 0.0929, step time: 1.5287\n",
      "228/388, train_loss: 0.0964, step time: 1.5339\n",
      "229/388, train_loss: 0.3402, step time: 1.5343\n",
      "230/388, train_loss: 0.0927, step time: 1.5449\n",
      "231/388, train_loss: 0.1002, step time: 1.5315\n",
      "232/388, train_loss: 0.0965, step time: 1.5314\n",
      "233/388, train_loss: 0.1560, step time: 1.5328\n",
      "234/388, train_loss: 0.0530, step time: 1.5493\n",
      "235/388, train_loss: 0.1008, step time: 1.5386\n",
      "236/388, train_loss: 0.1089, step time: 1.5344\n",
      "237/388, train_loss: 0.0467, step time: 1.5324\n",
      "238/388, train_loss: 0.2007, step time: 1.5425\n",
      "239/388, train_loss: 0.0688, step time: 1.5353\n",
      "240/388, train_loss: 0.1863, step time: 1.5346\n",
      "241/388, train_loss: 0.0925, step time: 1.5320\n",
      "242/388, train_loss: 0.1565, step time: 1.5420\n",
      "243/388, train_loss: 0.1512, step time: 1.5315\n",
      "244/388, train_loss: 0.0910, step time: 1.5319\n",
      "245/388, train_loss: 0.3220, step time: 1.5356\n",
      "246/388, train_loss: 0.0824, step time: 1.5466\n",
      "247/388, train_loss: 0.2669, step time: 1.5294\n",
      "248/388, train_loss: 0.0892, step time: 1.5312\n",
      "249/388, train_loss: 0.0802, step time: 1.5348\n",
      "250/388, train_loss: 0.0893, step time: 1.5606\n",
      "251/388, train_loss: 0.0772, step time: 1.5267\n",
      "252/388, train_loss: 0.0978, step time: 1.5301\n",
      "253/388, train_loss: 0.0328, step time: 1.5342\n",
      "254/388, train_loss: 0.0778, step time: 1.5482\n",
      "255/388, train_loss: 0.0999, step time: 1.5297\n",
      "256/388, train_loss: 0.1863, step time: 1.5318\n",
      "257/388, train_loss: 0.1068, step time: 1.5319\n",
      "258/388, train_loss: 0.1602, step time: 1.5501\n",
      "259/388, train_loss: 0.0721, step time: 1.5330\n",
      "260/388, train_loss: 0.1717, step time: 1.5328\n",
      "261/388, train_loss: 0.1322, step time: 1.5318\n",
      "262/388, train_loss: 0.0901, step time: 1.5467\n",
      "263/388, train_loss: 0.0739, step time: 1.5334\n",
      "264/388, train_loss: 0.1478, step time: 1.5352\n",
      "265/388, train_loss: 0.2441, step time: 1.5345\n",
      "266/388, train_loss: 0.0986, step time: 1.5393\n",
      "267/388, train_loss: 0.2153, step time: 1.5343\n",
      "268/388, train_loss: 0.2134, step time: 1.5336\n",
      "269/388, train_loss: 0.1000, step time: 1.5334\n",
      "270/388, train_loss: 0.0669, step time: 1.5507\n",
      "271/388, train_loss: 0.2023, step time: 1.5355\n",
      "272/388, train_loss: 0.2219, step time: 1.5371\n",
      "273/388, train_loss: 0.1123, step time: 1.5337\n",
      "274/388, train_loss: 0.1778, step time: 1.5431\n",
      "275/388, train_loss: 0.1402, step time: 1.5335\n",
      "276/388, train_loss: 0.2790, step time: 1.5300\n",
      "277/388, train_loss: 0.1742, step time: 1.5301\n",
      "278/388, train_loss: 0.2359, step time: 1.5465\n",
      "279/388, train_loss: 0.0691, step time: 1.5325\n",
      "280/388, train_loss: 0.1391, step time: 1.5332\n",
      "281/388, train_loss: 0.2501, step time: 1.5329\n",
      "282/388, train_loss: 0.2213, step time: 1.5419\n",
      "283/388, train_loss: 0.1015, step time: 1.5308\n",
      "284/388, train_loss: 0.0523, step time: 1.5326\n",
      "285/388, train_loss: 0.0411, step time: 1.5363\n",
      "286/388, train_loss: 0.1192, step time: 1.5458\n",
      "287/388, train_loss: 0.1955, step time: 1.5312\n",
      "288/388, train_loss: 0.1456, step time: 1.5278\n",
      "289/388, train_loss: 0.2814, step time: 1.5298\n",
      "290/388, train_loss: 0.1320, step time: 1.5468\n",
      "291/388, train_loss: 0.4368, step time: 1.5369\n",
      "292/388, train_loss: 0.1255, step time: 1.5326\n",
      "293/388, train_loss: 0.2884, step time: 1.5317\n",
      "294/388, train_loss: 0.0900, step time: 1.5591\n",
      "295/388, train_loss: 0.1250, step time: 1.5319\n",
      "296/388, train_loss: 0.2051, step time: 1.5357\n",
      "297/388, train_loss: 0.1275, step time: 1.5311\n",
      "298/388, train_loss: 0.0704, step time: 1.5542\n",
      "299/388, train_loss: 0.1322, step time: 1.5321\n",
      "300/388, train_loss: 0.0587, step time: 1.5346\n",
      "301/388, train_loss: 0.2107, step time: 1.5332\n",
      "302/388, train_loss: 0.0935, step time: 1.5445\n",
      "303/388, train_loss: 0.1325, step time: 1.5311\n",
      "304/388, train_loss: 0.0872, step time: 1.5376\n",
      "305/388, train_loss: 0.1359, step time: 1.5374\n",
      "306/388, train_loss: 0.2339, step time: 1.5442\n",
      "307/388, train_loss: 0.1502, step time: 1.5379\n",
      "308/388, train_loss: 0.1676, step time: 1.5350\n",
      "309/388, train_loss: 0.2404, step time: 1.5369\n",
      "310/388, train_loss: 0.1406, step time: 1.5426\n",
      "311/388, train_loss: 0.2600, step time: 1.5284\n",
      "312/388, train_loss: 0.1514, step time: 1.5332\n",
      "313/388, train_loss: 0.1018, step time: 1.5506\n",
      "314/388, train_loss: 0.1129, step time: 1.5565\n",
      "315/388, train_loss: 0.2472, step time: 1.5317\n",
      "316/388, train_loss: 0.1763, step time: 1.5370\n",
      "317/388, train_loss: 0.1409, step time: 1.5345\n",
      "318/388, train_loss: 0.1982, step time: 1.5463\n",
      "319/388, train_loss: 0.3190, step time: 1.5324\n",
      "320/388, train_loss: 0.0751, step time: 1.5279\n",
      "321/388, train_loss: 0.1428, step time: 1.5299\n",
      "322/388, train_loss: 0.3680, step time: 1.5514\n",
      "323/388, train_loss: 0.2449, step time: 1.5339\n",
      "324/388, train_loss: 0.1421, step time: 1.5351\n",
      "325/388, train_loss: 0.0645, step time: 1.5344\n",
      "326/388, train_loss: 0.0998, step time: 1.5425\n",
      "327/388, train_loss: 0.0719, step time: 1.5374\n",
      "328/388, train_loss: 0.0998, step time: 1.5343\n",
      "329/388, train_loss: 0.1484, step time: 1.5318\n",
      "330/388, train_loss: 0.1306, step time: 1.5441\n",
      "331/388, train_loss: 0.0833, step time: 1.5365\n",
      "332/388, train_loss: 0.1253, step time: 1.5372\n",
      "333/388, train_loss: 0.2058, step time: 1.5346\n",
      "334/388, train_loss: 0.0813, step time: 1.5538\n",
      "335/388, train_loss: 0.0689, step time: 1.5385\n",
      "336/388, train_loss: 0.1006, step time: 1.5333\n",
      "337/388, train_loss: 0.0536, step time: 1.5347\n",
      "338/388, train_loss: 0.0735, step time: 1.5425\n",
      "339/388, train_loss: 0.1089, step time: 1.5291\n",
      "340/388, train_loss: 0.2426, step time: 1.5316\n",
      "341/388, train_loss: 0.1473, step time: 1.5368\n",
      "342/388, train_loss: 0.1355, step time: 1.5465\n",
      "343/388, train_loss: 0.0875, step time: 1.5283\n",
      "344/388, train_loss: 0.1980, step time: 1.5296\n",
      "345/388, train_loss: 0.1017, step time: 1.5316\n",
      "346/388, train_loss: 0.0509, step time: 1.5577\n",
      "347/388, train_loss: 0.0969, step time: 1.5303\n",
      "348/388, train_loss: 0.2725, step time: 1.5313\n",
      "349/388, train_loss: 0.0920, step time: 1.5314\n",
      "350/388, train_loss: 0.2694, step time: 1.5513\n",
      "351/388, train_loss: 0.1604, step time: 1.5323\n",
      "352/388, train_loss: 0.1567, step time: 1.5303\n",
      "353/388, train_loss: 0.1182, step time: 1.5307\n",
      "354/388, train_loss: 0.1028, step time: 1.5458\n",
      "355/388, train_loss: 0.0848, step time: 1.5346\n",
      "356/388, train_loss: 0.2822, step time: 1.5352\n",
      "357/388, train_loss: 0.2037, step time: 1.5331\n",
      "358/388, train_loss: 0.1179, step time: 1.5466\n",
      "359/388, train_loss: 0.1374, step time: 1.5486\n",
      "360/388, train_loss: 0.2190, step time: 1.5333\n",
      "361/388, train_loss: 0.2511, step time: 1.5306\n",
      "362/388, train_loss: 0.1851, step time: 1.5478\n",
      "363/388, train_loss: 0.0587, step time: 1.5362\n",
      "364/388, train_loss: 0.1875, step time: 1.5338\n",
      "365/388, train_loss: 0.2185, step time: 1.5333\n",
      "366/388, train_loss: 0.0827, step time: 1.5630\n",
      "367/388, train_loss: 0.1182, step time: 1.5334\n",
      "368/388, train_loss: 0.1353, step time: 1.5305\n",
      "369/388, train_loss: 0.1561, step time: 1.5341\n",
      "370/388, train_loss: 0.3152, step time: 1.5391\n",
      "371/388, train_loss: 0.0468, step time: 1.5355\n",
      "372/388, train_loss: 0.1528, step time: 1.5336\n",
      "373/388, train_loss: 0.1304, step time: 1.5306\n",
      "374/388, train_loss: 0.3558, step time: 1.5446\n",
      "375/388, train_loss: 0.0875, step time: 1.5312\n",
      "376/388, train_loss: 0.3131, step time: 1.5357\n",
      "377/388, train_loss: 0.5079, step time: 1.5357\n",
      "378/388, train_loss: 0.0772, step time: 1.5445\n",
      "379/388, train_loss: 0.1559, step time: 1.5308\n",
      "380/388, train_loss: 0.1444, step time: 1.5309\n",
      "381/388, train_loss: 0.1160, step time: 1.5360\n",
      "382/388, train_loss: 0.2145, step time: 1.5451\n",
      "383/388, train_loss: 0.0719, step time: 1.5299\n",
      "384/388, train_loss: 0.1181, step time: 1.5416\n",
      "385/388, train_loss: 0.1186, step time: 1.5327\n",
      "386/388, train_loss: 0.0529, step time: 1.5469\n",
      "387/388, train_loss: 0.1132, step time: 1.5330\n",
      "388/388, train_loss: 0.1997, step time: 1.5385\n",
      "epoch 83 average loss: 0.1599\n",
      "current epoch: 83 current mean dice: 0.7794 tc: 0.8260 wt: 0.9068 et: 0.6055\n",
      "best mean dice: 0.7802 at epoch: 79\n",
      "time consuming of epoch 83 is: 704.1134\n",
      "----------\n",
      "epoch 84/100\n",
      "1/388, train_loss: 0.2318, step time: 1.5531\n",
      "2/388, train_loss: 0.2061, step time: 1.5338\n",
      "3/388, train_loss: 0.2187, step time: 1.5334\n",
      "4/388, train_loss: 0.0548, step time: 1.5314\n",
      "5/388, train_loss: 0.2008, step time: 1.5314\n",
      "6/388, train_loss: 0.0836, step time: 1.5331\n",
      "7/388, train_loss: 0.1942, step time: 1.5301\n",
      "8/388, train_loss: 0.2663, step time: 1.5358\n",
      "9/388, train_loss: 0.2522, step time: 1.5316\n",
      "10/388, train_loss: 0.1424, step time: 1.5288\n",
      "11/388, train_loss: 0.0632, step time: 1.5299\n",
      "12/388, train_loss: 0.1128, step time: 1.5454\n",
      "13/388, train_loss: 0.2352, step time: 1.5354\n",
      "14/388, train_loss: 0.2559, step time: 1.5333\n",
      "15/388, train_loss: 0.1338, step time: 1.5318\n",
      "16/388, train_loss: 0.2887, step time: 1.5353\n",
      "17/388, train_loss: 0.1452, step time: 1.5353\n",
      "18/388, train_loss: 0.1529, step time: 1.5318\n",
      "19/388, train_loss: 0.3417, step time: 1.5299\n",
      "20/388, train_loss: 0.1550, step time: 1.5304\n",
      "21/388, train_loss: 0.1859, step time: 1.5290\n",
      "22/388, train_loss: 0.3331, step time: 1.5315\n",
      "23/388, train_loss: 0.0600, step time: 1.5471\n",
      "24/388, train_loss: 0.0391, step time: 1.5326\n",
      "25/388, train_loss: 0.1812, step time: 1.5363\n",
      "26/388, train_loss: 0.2072, step time: 1.5290\n",
      "27/388, train_loss: 0.0835, step time: 1.5291\n",
      "28/388, train_loss: 0.2638, step time: 1.5327\n",
      "29/388, train_loss: 0.2838, step time: 1.5304\n",
      "30/388, train_loss: 0.1420, step time: 1.5284\n",
      "31/388, train_loss: 0.1061, step time: 1.5478\n",
      "32/388, train_loss: 0.1445, step time: 1.5345\n",
      "33/388, train_loss: 0.1072, step time: 1.5342\n",
      "34/388, train_loss: 0.1396, step time: 1.5329\n",
      "35/388, train_loss: 0.1140, step time: 1.5355\n",
      "36/388, train_loss: 0.1879, step time: 1.5309\n",
      "37/388, train_loss: 0.0959, step time: 1.5296\n",
      "38/388, train_loss: 0.1344, step time: 1.5367\n",
      "39/388, train_loss: 0.1789, step time: 1.5556\n",
      "40/388, train_loss: 0.1054, step time: 1.5289\n",
      "41/388, train_loss: 0.1086, step time: 1.5336\n",
      "42/388, train_loss: 0.1480, step time: 1.5471\n",
      "43/388, train_loss: 0.1984, step time: 1.5313\n",
      "44/388, train_loss: 0.1318, step time: 1.5320\n",
      "45/388, train_loss: 0.1171, step time: 1.5301\n",
      "46/388, train_loss: 0.1396, step time: 1.5332\n",
      "47/388, train_loss: 0.1057, step time: 1.5388\n",
      "48/388, train_loss: 0.1513, step time: 1.5310\n",
      "49/388, train_loss: 0.1290, step time: 1.5325\n",
      "50/388, train_loss: 0.1043, step time: 1.5357\n",
      "51/388, train_loss: 0.2847, step time: 1.5313\n",
      "52/388, train_loss: 0.1063, step time: 1.5308\n",
      "53/388, train_loss: 0.0382, step time: 1.5302\n",
      "54/388, train_loss: 0.4320, step time: 1.5344\n",
      "55/388, train_loss: 0.0781, step time: 1.5336\n",
      "56/388, train_loss: 0.0837, step time: 1.5373\n",
      "57/388, train_loss: 0.1507, step time: 1.5306\n",
      "58/388, train_loss: 0.0911, step time: 1.5297\n",
      "59/388, train_loss: 0.0850, step time: 1.5326\n",
      "60/388, train_loss: 0.1006, step time: 1.5272\n",
      "61/388, train_loss: 0.0959, step time: 1.5314\n",
      "62/388, train_loss: 0.2703, step time: 1.5326\n",
      "63/388, train_loss: 0.1335, step time: 1.5304\n",
      "64/388, train_loss: 0.1647, step time: 1.5356\n",
      "65/388, train_loss: 0.1051, step time: 1.5323\n",
      "66/388, train_loss: 0.1011, step time: 1.5326\n",
      "67/388, train_loss: 0.1934, step time: 1.5291\n",
      "68/388, train_loss: 0.1801, step time: 1.5337\n",
      "69/388, train_loss: 0.2250, step time: 1.5340\n",
      "70/388, train_loss: 0.2007, step time: 1.5300\n",
      "71/388, train_loss: 0.1033, step time: 1.5329\n",
      "72/388, train_loss: 0.4234, step time: 1.5309\n",
      "73/388, train_loss: 0.1247, step time: 1.5317\n",
      "74/388, train_loss: 0.0917, step time: 1.5320\n",
      "75/388, train_loss: 0.1831, step time: 1.5318\n",
      "76/388, train_loss: 0.1784, step time: 1.5342\n",
      "77/388, train_loss: 0.2313, step time: 1.5333\n",
      "78/388, train_loss: 0.1129, step time: 1.5362\n",
      "79/388, train_loss: 0.1816, step time: 1.5334\n",
      "80/388, train_loss: 0.1844, step time: 1.5292\n",
      "81/388, train_loss: 0.0866, step time: 1.5292\n",
      "82/388, train_loss: 0.0676, step time: 1.5294\n",
      "83/388, train_loss: 0.3003, step time: 1.5327\n",
      "84/388, train_loss: 0.0818, step time: 1.5293\n",
      "85/388, train_loss: 0.0704, step time: 1.5335\n",
      "86/388, train_loss: 0.1299, step time: 1.5338\n",
      "87/388, train_loss: 0.1508, step time: 1.5313\n",
      "88/388, train_loss: 0.1894, step time: 1.5359\n",
      "89/388, train_loss: 0.0806, step time: 1.5326\n",
      "90/388, train_loss: 0.1815, step time: 1.5319\n",
      "91/388, train_loss: 0.1662, step time: 1.5325\n",
      "92/388, train_loss: 0.1526, step time: 1.5298\n",
      "93/388, train_loss: 0.1588, step time: 1.5324\n",
      "94/388, train_loss: 0.2167, step time: 1.5355\n",
      "95/388, train_loss: 0.1343, step time: 1.5321\n",
      "96/388, train_loss: 0.4038, step time: 1.5327\n",
      "97/388, train_loss: 0.0856, step time: 1.5334\n",
      "98/388, train_loss: 0.0888, step time: 1.5462\n",
      "99/388, train_loss: 0.2622, step time: 1.5314\n",
      "100/388, train_loss: 0.1024, step time: 1.5301\n",
      "101/388, train_loss: 0.2393, step time: 1.5308\n",
      "102/388, train_loss: 0.0588, step time: 1.5315\n",
      "103/388, train_loss: 0.0597, step time: 1.5336\n",
      "104/388, train_loss: 0.0829, step time: 1.5323\n",
      "105/388, train_loss: 0.1160, step time: 1.5278\n",
      "106/388, train_loss: 0.2028, step time: 1.5298\n",
      "107/388, train_loss: 0.3476, step time: 1.5317\n",
      "108/388, train_loss: 0.2353, step time: 1.5388\n",
      "109/388, train_loss: 0.1841, step time: 1.5334\n",
      "110/388, train_loss: 0.0807, step time: 1.5352\n",
      "111/388, train_loss: 0.3218, step time: 1.5357\n",
      "112/388, train_loss: 0.2498, step time: 1.5298\n",
      "113/388, train_loss: 0.1002, step time: 1.5312\n",
      "114/388, train_loss: 0.1444, step time: 1.5305\n",
      "115/388, train_loss: 0.1072, step time: 1.5324\n",
      "116/388, train_loss: 0.1315, step time: 1.5313\n",
      "117/388, train_loss: 0.1128, step time: 1.5310\n",
      "118/388, train_loss: 0.4086, step time: 1.5333\n",
      "119/388, train_loss: 0.1438, step time: 1.5342\n",
      "120/388, train_loss: 0.3036, step time: 1.5343\n",
      "121/388, train_loss: 0.0331, step time: 1.5296\n",
      "122/388, train_loss: 0.0734, step time: 1.5364\n",
      "123/388, train_loss: 0.0822, step time: 1.5300\n",
      "124/388, train_loss: 0.1969, step time: 1.5337\n",
      "125/388, train_loss: 0.1448, step time: 1.5308\n",
      "126/388, train_loss: 0.1661, step time: 1.5322\n",
      "127/388, train_loss: 0.1510, step time: 1.5294\n",
      "128/388, train_loss: 0.2291, step time: 1.5308\n",
      "129/388, train_loss: 0.2258, step time: 1.5287\n",
      "130/388, train_loss: 0.3896, step time: 1.5387\n",
      "131/388, train_loss: 0.4222, step time: 1.5352\n",
      "132/388, train_loss: 0.1010, step time: 1.5446\n",
      "133/388, train_loss: 0.1127, step time: 1.5289\n",
      "134/388, train_loss: 0.1213, step time: 1.5274\n",
      "135/388, train_loss: 0.1852, step time: 1.5302\n",
      "136/388, train_loss: 0.0675, step time: 1.5331\n",
      "137/388, train_loss: 0.0544, step time: 1.5339\n",
      "138/388, train_loss: 0.0879, step time: 1.5339\n",
      "139/388, train_loss: 0.1179, step time: 1.5336\n",
      "140/388, train_loss: 0.2130, step time: 1.5307\n",
      "141/388, train_loss: 0.0968, step time: 1.5271\n",
      "142/388, train_loss: 0.1891, step time: 1.5310\n",
      "143/388, train_loss: 0.1261, step time: 1.5283\n",
      "144/388, train_loss: 0.1567, step time: 1.5306\n",
      "145/388, train_loss: 0.1211, step time: 1.5300\n",
      "146/388, train_loss: 0.0868, step time: 1.5310\n",
      "147/388, train_loss: 0.2395, step time: 1.5332\n",
      "148/388, train_loss: 0.1148, step time: 1.5443\n",
      "149/388, train_loss: 0.2312, step time: 1.5359\n",
      "150/388, train_loss: 0.2217, step time: 1.5301\n",
      "151/388, train_loss: 0.1125, step time: 1.5300\n",
      "152/388, train_loss: 0.0971, step time: 1.5295\n",
      "153/388, train_loss: 0.2154, step time: 1.5278\n",
      "154/388, train_loss: 0.2264, step time: 1.5365\n",
      "155/388, train_loss: 0.4957, step time: 1.5316\n",
      "156/388, train_loss: 0.0639, step time: 1.5345\n",
      "157/388, train_loss: 0.2671, step time: 1.5361\n",
      "158/388, train_loss: 0.0405, step time: 1.5296\n",
      "159/388, train_loss: 0.0420, step time: 1.5295\n",
      "160/388, train_loss: 0.2907, step time: 1.5349\n",
      "161/388, train_loss: 0.0680, step time: 1.5330\n",
      "162/388, train_loss: 0.2219, step time: 1.5308\n",
      "163/388, train_loss: 0.1379, step time: 1.5317\n",
      "164/388, train_loss: 0.1227, step time: 1.5295\n",
      "165/388, train_loss: 0.2980, step time: 1.5313\n",
      "166/388, train_loss: 0.2789, step time: 1.5310\n",
      "167/388, train_loss: 0.1623, step time: 1.5317\n",
      "168/388, train_loss: 0.1300, step time: 1.5314\n",
      "169/388, train_loss: 0.3438, step time: 1.5286\n",
      "170/388, train_loss: 0.2647, step time: 1.5346\n",
      "171/388, train_loss: 0.1932, step time: 1.5327\n",
      "172/388, train_loss: 0.0865, step time: 1.5330\n",
      "173/388, train_loss: 0.2187, step time: 1.5329\n",
      "174/388, train_loss: 0.1363, step time: 1.5342\n",
      "175/388, train_loss: 0.2296, step time: 1.5312\n",
      "176/388, train_loss: 0.1900, step time: 1.5311\n",
      "177/388, train_loss: 0.2108, step time: 1.5305\n",
      "178/388, train_loss: 0.1336, step time: 1.5288\n",
      "179/388, train_loss: 0.2516, step time: 1.5317\n",
      "180/388, train_loss: 0.0863, step time: 1.5316\n",
      "181/388, train_loss: 0.1439, step time: 1.5431\n",
      "182/388, train_loss: 0.1180, step time: 1.5345\n",
      "183/388, train_loss: 0.0924, step time: 1.5315\n",
      "184/388, train_loss: 0.0627, step time: 1.5315\n",
      "185/388, train_loss: 0.1469, step time: 1.5285\n",
      "186/388, train_loss: 0.1869, step time: 1.5305\n",
      "187/388, train_loss: 0.0309, step time: 1.5299\n",
      "188/388, train_loss: 0.1861, step time: 1.5316\n",
      "189/388, train_loss: 0.1977, step time: 1.5324\n",
      "190/388, train_loss: 0.1875, step time: 1.5307\n",
      "191/388, train_loss: 0.0743, step time: 1.5317\n",
      "192/388, train_loss: 0.0464, step time: 1.5342\n",
      "193/388, train_loss: 0.5813, step time: 1.5296\n",
      "194/388, train_loss: 0.0942, step time: 1.5571\n",
      "195/388, train_loss: 0.1217, step time: 1.5304\n",
      "196/388, train_loss: 0.1530, step time: 1.5421\n",
      "197/388, train_loss: 0.0966, step time: 1.5303\n",
      "198/388, train_loss: 0.1197, step time: 1.5306\n",
      "199/388, train_loss: 0.1301, step time: 1.5315\n",
      "200/388, train_loss: 0.1262, step time: 1.5340\n",
      "201/388, train_loss: 0.0600, step time: 1.5311\n",
      "202/388, train_loss: 0.3433, step time: 1.5349\n",
      "203/388, train_loss: 0.0766, step time: 1.5346\n",
      "204/388, train_loss: 0.1762, step time: 1.5337\n",
      "205/388, train_loss: 0.1091, step time: 1.5276\n",
      "206/388, train_loss: 0.2139, step time: 1.5320\n",
      "207/388, train_loss: 0.0646, step time: 1.5307\n",
      "208/388, train_loss: 0.0797, step time: 1.5294\n",
      "209/388, train_loss: 0.0641, step time: 1.5296\n",
      "210/388, train_loss: 0.1860, step time: 1.5356\n",
      "211/388, train_loss: 0.1613, step time: 1.5315\n",
      "212/388, train_loss: 0.1108, step time: 1.5361\n",
      "213/388, train_loss: 0.2621, step time: 1.5296\n",
      "214/388, train_loss: 0.0679, step time: 1.5320\n",
      "215/388, train_loss: 0.2040, step time: 1.5309\n",
      "216/388, train_loss: 0.0419, step time: 1.5293\n",
      "217/388, train_loss: 0.1907, step time: 1.5278\n",
      "218/388, train_loss: 0.1746, step time: 1.5308\n",
      "219/388, train_loss: 0.0733, step time: 1.5326\n",
      "220/388, train_loss: 0.2596, step time: 1.5346\n",
      "221/388, train_loss: 0.1890, step time: 1.5354\n",
      "222/388, train_loss: 0.1493, step time: 1.5328\n",
      "223/388, train_loss: 0.1050, step time: 1.5311\n",
      "224/388, train_loss: 0.1550, step time: 1.5328\n",
      "225/388, train_loss: 0.0831, step time: 1.5312\n",
      "226/388, train_loss: 0.1554, step time: 1.5417\n",
      "227/388, train_loss: 0.2851, step time: 1.5335\n",
      "228/388, train_loss: 0.2536, step time: 1.5369\n",
      "229/388, train_loss: 0.2481, step time: 1.5337\n",
      "230/388, train_loss: 0.1617, step time: 1.5326\n",
      "231/388, train_loss: 0.1359, step time: 1.5310\n",
      "232/388, train_loss: 0.1225, step time: 1.5316\n",
      "233/388, train_loss: 0.3135, step time: 1.5320\n",
      "234/388, train_loss: 0.0521, step time: 1.5298\n",
      "235/388, train_loss: 0.0529, step time: 1.5290\n",
      "236/388, train_loss: 0.1002, step time: 1.5301\n",
      "237/388, train_loss: 0.2727, step time: 1.5301\n",
      "238/388, train_loss: 0.1553, step time: 1.5331\n",
      "239/388, train_loss: 0.0685, step time: 1.5486\n",
      "240/388, train_loss: 0.2913, step time: 1.5285\n",
      "241/388, train_loss: 0.1814, step time: 1.5310\n",
      "242/388, train_loss: 0.0576, step time: 1.5332\n",
      "243/388, train_loss: 0.0898, step time: 1.5334\n",
      "244/388, train_loss: 0.3588, step time: 1.5348\n",
      "245/388, train_loss: 0.1276, step time: 1.5328\n",
      "246/388, train_loss: 0.1041, step time: 1.5315\n",
      "247/388, train_loss: 0.1697, step time: 1.5464\n",
      "248/388, train_loss: 0.0638, step time: 1.5296\n",
      "249/388, train_loss: 0.0469, step time: 1.5307\n",
      "250/388, train_loss: 0.0501, step time: 1.5340\n",
      "251/388, train_loss: 0.3611, step time: 1.5431\n",
      "252/388, train_loss: 0.1406, step time: 1.5289\n",
      "253/388, train_loss: 0.1212, step time: 1.5321\n",
      "254/388, train_loss: 0.1159, step time: 1.5300\n",
      "255/388, train_loss: 0.0853, step time: 1.5304\n",
      "256/388, train_loss: 0.4191, step time: 1.5339\n",
      "257/388, train_loss: 0.1073, step time: 1.5360\n",
      "258/388, train_loss: 0.0934, step time: 1.5330\n",
      "259/388, train_loss: 0.1555, step time: 1.5308\n",
      "260/388, train_loss: 0.1702, step time: 1.5335\n",
      "261/388, train_loss: 0.1240, step time: 1.5306\n",
      "262/388, train_loss: 0.0960, step time: 1.5290\n",
      "263/388, train_loss: 0.1507, step time: 1.5311\n",
      "264/388, train_loss: 0.2027, step time: 1.5331\n",
      "265/388, train_loss: 0.1027, step time: 1.5351\n",
      "266/388, train_loss: 0.1445, step time: 1.5340\n",
      "267/388, train_loss: 0.1784, step time: 1.5306\n",
      "268/388, train_loss: 0.2468, step time: 1.5300\n",
      "269/388, train_loss: 0.5146, step time: 1.5320\n",
      "270/388, train_loss: 0.0948, step time: 1.5290\n",
      "271/388, train_loss: 0.0954, step time: 1.5317\n",
      "272/388, train_loss: 0.0661, step time: 1.5342\n",
      "273/388, train_loss: 0.1259, step time: 1.5357\n",
      "274/388, train_loss: 0.1259, step time: 1.5318\n",
      "275/388, train_loss: 0.0928, step time: 1.5321\n",
      "276/388, train_loss: 0.0864, step time: 1.5285\n",
      "277/388, train_loss: 0.0785, step time: 1.5289\n",
      "278/388, train_loss: 0.1416, step time: 1.5282\n",
      "279/388, train_loss: 0.2207, step time: 1.5318\n",
      "280/388, train_loss: 0.0851, step time: 1.5296\n",
      "281/388, train_loss: 0.1116, step time: 1.5280\n",
      "282/388, train_loss: 0.1898, step time: 1.5326\n",
      "283/388, train_loss: 0.0891, step time: 1.5305\n",
      "284/388, train_loss: 0.1912, step time: 1.5336\n",
      "285/388, train_loss: 0.2107, step time: 1.5379\n",
      "286/388, train_loss: 0.2823, step time: 1.5330\n",
      "287/388, train_loss: 0.0672, step time: 1.5313\n",
      "288/388, train_loss: 0.1721, step time: 1.5317\n",
      "289/388, train_loss: 0.1693, step time: 1.5307\n",
      "290/388, train_loss: 0.0951, step time: 1.5325\n",
      "291/388, train_loss: 0.2658, step time: 1.5318\n",
      "292/388, train_loss: 0.0900, step time: 1.5348\n",
      "293/388, train_loss: 0.1275, step time: 1.5340\n",
      "294/388, train_loss: 0.0981, step time: 1.5322\n",
      "295/388, train_loss: 0.1206, step time: 1.5331\n",
      "296/388, train_loss: 0.0978, step time: 1.5325\n",
      "297/388, train_loss: 0.1121, step time: 1.5326\n",
      "298/388, train_loss: 0.2846, step time: 1.5320\n",
      "299/388, train_loss: 0.3941, step time: 1.5533\n",
      "300/388, train_loss: 0.1534, step time: 1.5319\n",
      "301/388, train_loss: 0.2233, step time: 1.5322\n",
      "302/388, train_loss: 0.1673, step time: 1.5308\n",
      "303/388, train_loss: 0.1981, step time: 1.5316\n",
      "304/388, train_loss: 0.3947, step time: 1.5317\n",
      "305/388, train_loss: 0.1550, step time: 1.5304\n",
      "306/388, train_loss: 0.0862, step time: 1.5342\n",
      "307/388, train_loss: 0.2359, step time: 1.5358\n",
      "308/388, train_loss: 0.0871, step time: 1.5334\n",
      "309/388, train_loss: 0.0261, step time: 1.5320\n",
      "310/388, train_loss: 0.3111, step time: 1.5303\n",
      "311/388, train_loss: 0.0745, step time: 1.5319\n",
      "312/388, train_loss: 0.3782, step time: 1.5295\n",
      "313/388, train_loss: 0.0906, step time: 1.5291\n",
      "314/388, train_loss: 0.1933, step time: 1.5326\n",
      "315/388, train_loss: 0.3187, step time: 1.5332\n",
      "316/388, train_loss: 0.1895, step time: 1.5359\n",
      "317/388, train_loss: 0.2697, step time: 1.5329\n",
      "318/388, train_loss: 0.1501, step time: 1.5308\n",
      "319/388, train_loss: 0.1880, step time: 1.5554\n",
      "320/388, train_loss: 0.0982, step time: 1.5364\n",
      "321/388, train_loss: 0.1429, step time: 1.5355\n",
      "322/388, train_loss: 0.1195, step time: 1.5328\n",
      "323/388, train_loss: 0.2508, step time: 1.5318\n",
      "324/388, train_loss: 0.0764, step time: 1.5340\n",
      "325/388, train_loss: 0.2045, step time: 1.5300\n",
      "326/388, train_loss: 0.1402, step time: 1.5295\n",
      "327/388, train_loss: 0.0935, step time: 1.5316\n",
      "328/388, train_loss: 0.1352, step time: 1.5342\n",
      "329/388, train_loss: 0.4313, step time: 1.5344\n",
      "330/388, train_loss: 0.0537, step time: 1.5357\n",
      "331/388, train_loss: 0.2171, step time: 1.5284\n",
      "332/388, train_loss: 0.0811, step time: 1.5364\n",
      "333/388, train_loss: 0.1656, step time: 1.5444\n",
      "334/388, train_loss: 0.0816, step time: 1.5473\n",
      "335/388, train_loss: 0.1124, step time: 1.5333\n",
      "336/388, train_loss: 0.0944, step time: 1.5321\n",
      "337/388, train_loss: 0.1843, step time: 1.5305\n",
      "338/388, train_loss: 0.1006, step time: 1.5330\n",
      "339/388, train_loss: 0.2233, step time: 1.5353\n",
      "340/388, train_loss: 0.1643, step time: 1.5479\n",
      "341/388, train_loss: 0.1493, step time: 1.5294\n",
      "342/388, train_loss: 0.1192, step time: 1.5270\n",
      "343/388, train_loss: 0.2843, step time: 1.5299\n",
      "344/388, train_loss: 0.2110, step time: 1.5331\n",
      "345/388, train_loss: 0.1697, step time: 1.5341\n",
      "346/388, train_loss: 0.1860, step time: 1.5297\n",
      "347/388, train_loss: 0.1694, step time: 1.5302\n",
      "348/388, train_loss: 0.2303, step time: 1.5302\n",
      "349/388, train_loss: 0.1026, step time: 1.5314\n",
      "350/388, train_loss: 0.1199, step time: 1.5331\n",
      "351/388, train_loss: 0.0553, step time: 1.5448\n",
      "352/388, train_loss: 0.1110, step time: 1.5286\n",
      "353/388, train_loss: 0.1640, step time: 1.5273\n",
      "354/388, train_loss: 0.1381, step time: 1.5318\n",
      "355/388, train_loss: 0.0755, step time: 1.5337\n",
      "356/388, train_loss: 0.0865, step time: 1.5394\n",
      "357/388, train_loss: 0.1543, step time: 1.5268\n",
      "358/388, train_loss: 0.1252, step time: 1.5305\n",
      "359/388, train_loss: 0.0821, step time: 1.5292\n",
      "360/388, train_loss: 0.0989, step time: 1.5282\n",
      "361/388, train_loss: 0.2418, step time: 1.5321\n",
      "362/388, train_loss: 0.1445, step time: 1.5396\n",
      "363/388, train_loss: 0.2655, step time: 1.5346\n",
      "364/388, train_loss: 0.0959, step time: 1.5315\n",
      "365/388, train_loss: 0.1006, step time: 1.5325\n",
      "366/388, train_loss: 0.0923, step time: 1.5328\n",
      "367/388, train_loss: 0.2050, step time: 1.5264\n",
      "368/388, train_loss: 0.0726, step time: 1.5301\n",
      "369/388, train_loss: 0.0768, step time: 1.5288\n",
      "370/388, train_loss: 0.0847, step time: 1.5315\n",
      "371/388, train_loss: 0.0919, step time: 1.5356\n",
      "372/388, train_loss: 0.1376, step time: 1.5329\n",
      "373/388, train_loss: 0.1149, step time: 1.5329\n",
      "374/388, train_loss: 0.0919, step time: 1.5292\n",
      "375/388, train_loss: 0.1892, step time: 1.5583\n",
      "376/388, train_loss: 0.1973, step time: 1.5335\n",
      "377/388, train_loss: 0.0893, step time: 1.5331\n",
      "378/388, train_loss: 0.0992, step time: 1.5331\n",
      "379/388, train_loss: 0.1518, step time: 1.5281\n",
      "380/388, train_loss: 0.1058, step time: 1.5275\n",
      "381/388, train_loss: 0.1017, step time: 1.5281\n",
      "382/388, train_loss: 0.1486, step time: 1.5298\n",
      "383/388, train_loss: 0.1258, step time: 1.5326\n",
      "384/388, train_loss: 0.0958, step time: 1.5478\n",
      "385/388, train_loss: 0.1577, step time: 1.5312\n",
      "386/388, train_loss: 0.1632, step time: 1.5273\n",
      "387/388, train_loss: 0.1698, step time: 1.5333\n",
      "388/388, train_loss: 0.1263, step time: 1.5304\n",
      "epoch 84 average loss: 0.1602\n",
      "saved new best metric model\n",
      "current epoch: 84 current mean dice: 0.7817 tc: 0.8294 wt: 0.9067 et: 0.6089\n",
      "best mean dice: 0.7817 at epoch: 84\n",
      "time consuming of epoch 84 is: 699.3033\n",
      "----------\n",
      "epoch 85/100\n",
      "1/388, train_loss: 0.1880, step time: 1.5501\n",
      "2/388, train_loss: 0.0711, step time: 1.5360\n",
      "3/388, train_loss: 0.0704, step time: 1.5317\n",
      "4/388, train_loss: 0.2387, step time: 1.5295\n",
      "5/388, train_loss: 0.0937, step time: 1.5307\n",
      "6/388, train_loss: 0.4429, step time: 1.5309\n",
      "7/388, train_loss: 0.1930, step time: 1.5299\n",
      "8/388, train_loss: 0.0700, step time: 1.5343\n",
      "9/388, train_loss: 0.1290, step time: 1.5379\n",
      "10/388, train_loss: 0.1262, step time: 1.5293\n",
      "11/388, train_loss: 0.2556, step time: 1.5310\n",
      "12/388, train_loss: 0.1306, step time: 1.5291\n",
      "13/388, train_loss: 0.0840, step time: 1.5308\n",
      "14/388, train_loss: 0.3316, step time: 1.5333\n",
      "15/388, train_loss: 0.1526, step time: 1.5405\n",
      "16/388, train_loss: 0.1383, step time: 1.5363\n",
      "17/388, train_loss: 0.2364, step time: 1.5337\n",
      "18/388, train_loss: 0.1692, step time: 1.5304\n",
      "19/388, train_loss: 0.0865, step time: 1.5286\n",
      "20/388, train_loss: 0.1920, step time: 1.5308\n",
      "21/388, train_loss: 0.0730, step time: 1.5310\n",
      "22/388, train_loss: 0.0980, step time: 1.5287\n",
      "23/388, train_loss: 0.2955, step time: 1.5283\n",
      "24/388, train_loss: 0.1305, step time: 1.5353\n",
      "25/388, train_loss: 0.0829, step time: 1.5328\n",
      "26/388, train_loss: 0.2666, step time: 1.5494\n",
      "27/388, train_loss: 0.0588, step time: 1.5329\n",
      "28/388, train_loss: 0.0394, step time: 1.5345\n",
      "29/388, train_loss: 0.0951, step time: 1.5423\n",
      "30/388, train_loss: 0.0957, step time: 1.5319\n",
      "31/388, train_loss: 0.0939, step time: 1.5337\n",
      "32/388, train_loss: 0.2727, step time: 1.5288\n",
      "33/388, train_loss: 0.1629, step time: 1.5319\n",
      "34/388, train_loss: 0.1195, step time: 1.5310\n",
      "35/388, train_loss: 0.0585, step time: 1.5367\n",
      "36/388, train_loss: 0.0991, step time: 1.5344\n",
      "37/388, train_loss: 0.1570, step time: 1.5324\n",
      "38/388, train_loss: 0.1076, step time: 1.5294\n",
      "39/388, train_loss: 0.0615, step time: 1.5306\n",
      "40/388, train_loss: 0.0614, step time: 1.5288\n",
      "41/388, train_loss: 0.2374, step time: 1.5284\n",
      "42/388, train_loss: 0.0985, step time: 1.5338\n",
      "43/388, train_loss: 0.1131, step time: 1.5364\n",
      "44/388, train_loss: 0.1481, step time: 1.5314\n",
      "45/388, train_loss: 0.0859, step time: 1.5315\n",
      "46/388, train_loss: 0.1009, step time: 1.5329\n",
      "47/388, train_loss: 0.0468, step time: 1.5261\n",
      "48/388, train_loss: 0.1059, step time: 1.5277\n",
      "49/388, train_loss: 0.2443, step time: 1.5298\n",
      "50/388, train_loss: 0.0865, step time: 1.5347\n",
      "51/388, train_loss: 0.1521, step time: 1.5308\n",
      "52/388, train_loss: 0.1748, step time: 1.5330\n",
      "53/388, train_loss: 0.1586, step time: 1.5372\n",
      "54/388, train_loss: 0.1070, step time: 1.5350\n",
      "55/388, train_loss: 0.1481, step time: 1.5353\n",
      "56/388, train_loss: 0.2347, step time: 1.5449\n",
      "57/388, train_loss: 0.1774, step time: 1.5302\n",
      "58/388, train_loss: 0.0444, step time: 1.5476\n",
      "59/388, train_loss: 0.1066, step time: 1.5338\n",
      "60/388, train_loss: 0.2168, step time: 1.5314\n",
      "61/388, train_loss: 0.1950, step time: 1.5264\n",
      "62/388, train_loss: 0.1826, step time: 1.5422\n",
      "63/388, train_loss: 0.1290, step time: 1.5319\n",
      "64/388, train_loss: 0.0856, step time: 1.5469\n",
      "65/388, train_loss: 0.0655, step time: 1.5286\n",
      "66/388, train_loss: 0.1314, step time: 1.5439\n",
      "67/388, train_loss: 0.0743, step time: 1.5298\n",
      "68/388, train_loss: 0.1413, step time: 1.5496\n",
      "69/388, train_loss: 0.0965, step time: 1.5334\n",
      "70/388, train_loss: 0.2651, step time: 1.5432\n",
      "71/388, train_loss: 0.1135, step time: 1.5283\n",
      "72/388, train_loss: 0.1939, step time: 1.5442\n",
      "73/388, train_loss: 0.1497, step time: 1.5286\n",
      "74/388, train_loss: 0.1719, step time: 1.5409\n",
      "75/388, train_loss: 0.0804, step time: 1.5375\n",
      "76/388, train_loss: 0.1862, step time: 1.5432\n",
      "77/388, train_loss: 0.1876, step time: 1.5305\n",
      "78/388, train_loss: 0.1883, step time: 1.5457\n",
      "79/388, train_loss: 0.3431, step time: 1.5345\n",
      "80/388, train_loss: 0.1385, step time: 1.5506\n",
      "81/388, train_loss: 0.2563, step time: 1.5293\n",
      "82/388, train_loss: 0.1048, step time: 1.5429\n",
      "83/388, train_loss: 0.1882, step time: 1.5277\n",
      "84/388, train_loss: 0.0854, step time: 1.5481\n",
      "85/388, train_loss: 0.1404, step time: 1.5443\n",
      "86/388, train_loss: 0.1222, step time: 1.5340\n",
      "87/388, train_loss: 0.1579, step time: 1.5439\n",
      "88/388, train_loss: 0.1497, step time: 1.5420\n",
      "89/388, train_loss: 0.0740, step time: 1.5337\n",
      "90/388, train_loss: 0.2048, step time: 1.5509\n",
      "91/388, train_loss: 0.0636, step time: 1.5339\n",
      "92/388, train_loss: 0.0925, step time: 1.5445\n",
      "93/388, train_loss: 0.0788, step time: 1.5410\n",
      "94/388, train_loss: 0.0845, step time: 1.5479\n",
      "95/388, train_loss: 0.0927, step time: 1.5397\n",
      "96/388, train_loss: 0.2378, step time: 1.5413\n",
      "97/388, train_loss: 0.0826, step time: 1.5334\n",
      "98/388, train_loss: 0.1269, step time: 1.5498\n",
      "99/388, train_loss: 0.3547, step time: 1.5362\n",
      "100/388, train_loss: 0.0673, step time: 1.5468\n",
      "101/388, train_loss: 0.1496, step time: 1.5386\n",
      "102/388, train_loss: 0.1521, step time: 1.5462\n",
      "103/388, train_loss: 0.2347, step time: 1.5378\n",
      "104/388, train_loss: 0.0977, step time: 1.5507\n",
      "105/388, train_loss: 0.1509, step time: 1.5410\n",
      "106/388, train_loss: 0.3011, step time: 1.5445\n",
      "107/388, train_loss: 0.2021, step time: 1.5378\n",
      "108/388, train_loss: 0.1009, step time: 1.5502\n",
      "109/388, train_loss: 0.0392, step time: 1.5408\n",
      "110/388, train_loss: 0.0790, step time: 1.5513\n",
      "111/388, train_loss: 0.1965, step time: 1.5336\n",
      "112/388, train_loss: 0.4497, step time: 1.5477\n",
      "113/388, train_loss: 0.0840, step time: 1.5556\n",
      "114/388, train_loss: 0.1242, step time: 1.5428\n",
      "115/388, train_loss: 0.1066, step time: 1.5357\n",
      "116/388, train_loss: 0.0328, step time: 1.5553\n",
      "117/388, train_loss: 0.2140, step time: 1.5463\n",
      "118/388, train_loss: 0.1379, step time: 1.5433\n",
      "119/388, train_loss: 0.0830, step time: 1.5383\n",
      "120/388, train_loss: 0.0493, step time: 1.5481\n",
      "121/388, train_loss: 0.2808, step time: 1.5371\n",
      "122/388, train_loss: 0.0932, step time: 1.5498\n",
      "123/388, train_loss: 0.1392, step time: 1.5516\n",
      "124/388, train_loss: 0.1120, step time: 1.5441\n",
      "125/388, train_loss: 0.2199, step time: 1.5464\n",
      "126/388, train_loss: 0.1517, step time: 1.5428\n",
      "127/388, train_loss: 0.3999, step time: 1.5417\n",
      "128/388, train_loss: 0.2450, step time: 1.5545\n",
      "129/388, train_loss: 0.0924, step time: 1.5407\n",
      "130/388, train_loss: 0.1849, step time: 1.5447\n",
      "131/388, train_loss: 0.0693, step time: 1.5424\n",
      "132/388, train_loss: 0.2154, step time: 1.5448\n",
      "133/388, train_loss: 0.0592, step time: 1.5429\n",
      "134/388, train_loss: 0.3704, step time: 1.5474\n",
      "135/388, train_loss: 0.1011, step time: 1.5447\n",
      "136/388, train_loss: 0.0992, step time: 1.5507\n",
      "137/388, train_loss: 0.0954, step time: 1.5414\n",
      "138/388, train_loss: 0.2366, step time: 1.5542\n",
      "139/388, train_loss: 0.1805, step time: 1.5470\n",
      "140/388, train_loss: 0.3143, step time: 1.5455\n",
      "141/388, train_loss: 0.0946, step time: 1.5359\n",
      "142/388, train_loss: 0.0271, step time: 1.5505\n",
      "143/388, train_loss: 0.3850, step time: 1.5403\n",
      "144/388, train_loss: 0.1691, step time: 1.5480\n",
      "145/388, train_loss: 0.0424, step time: 1.5502\n",
      "146/388, train_loss: 0.4689, step time: 1.5442\n",
      "147/388, train_loss: 0.1323, step time: 1.5469\n",
      "148/388, train_loss: 0.1353, step time: 1.5460\n",
      "149/388, train_loss: 0.2108, step time: 1.5329\n",
      "150/388, train_loss: 0.2231, step time: 1.5490\n",
      "151/388, train_loss: 0.0749, step time: 1.5442\n",
      "152/388, train_loss: 0.1872, step time: 1.5426\n",
      "153/388, train_loss: 0.2174, step time: 1.5469\n",
      "154/388, train_loss: 0.1900, step time: 1.5453\n",
      "155/388, train_loss: 0.1226, step time: 1.5471\n",
      "156/388, train_loss: 0.1955, step time: 1.5482\n",
      "157/388, train_loss: 0.3028, step time: 1.5372\n",
      "158/388, train_loss: 0.1658, step time: 1.5465\n",
      "159/388, train_loss: 0.0829, step time: 1.5367\n",
      "160/388, train_loss: 0.0472, step time: 1.5432\n",
      "161/388, train_loss: 0.1030, step time: 1.5453\n",
      "162/388, train_loss: 0.1214, step time: 1.5481\n",
      "163/388, train_loss: 0.1150, step time: 1.5430\n",
      "164/388, train_loss: 0.2043, step time: 1.5529\n",
      "165/388, train_loss: 0.1597, step time: 1.5442\n",
      "166/388, train_loss: 0.2264, step time: 1.5477\n",
      "167/388, train_loss: 0.4818, step time: 1.5430\n",
      "168/388, train_loss: 0.0934, step time: 1.5437\n",
      "169/388, train_loss: 0.2593, step time: 1.5483\n",
      "170/388, train_loss: 0.1255, step time: 1.5453\n",
      "171/388, train_loss: 0.0986, step time: 1.5417\n",
      "172/388, train_loss: 0.1792, step time: 1.5424\n",
      "173/388, train_loss: 0.4156, step time: 1.5471\n",
      "174/388, train_loss: 0.1829, step time: 1.5404\n",
      "175/388, train_loss: 0.1684, step time: 1.5336\n",
      "176/388, train_loss: 0.3278, step time: 1.5494\n",
      "177/388, train_loss: 0.0752, step time: 1.5397\n",
      "178/388, train_loss: 0.1012, step time: 1.5523\n",
      "179/388, train_loss: 0.1808, step time: 1.5400\n",
      "180/388, train_loss: 0.3285, step time: 1.5470\n",
      "181/388, train_loss: 0.1713, step time: 1.5357\n",
      "182/388, train_loss: 0.1179, step time: 1.5459\n",
      "183/388, train_loss: 0.0888, step time: 1.5291\n",
      "184/388, train_loss: 0.1942, step time: 1.5406\n",
      "185/388, train_loss: 0.2773, step time: 1.5492\n",
      "186/388, train_loss: 0.0904, step time: 1.5460\n",
      "187/388, train_loss: 0.1331, step time: 1.5296\n",
      "188/388, train_loss: 0.1446, step time: 1.5478\n",
      "189/388, train_loss: 0.1763, step time: 1.5388\n",
      "190/388, train_loss: 0.0738, step time: 1.5441\n",
      "191/388, train_loss: 0.2273, step time: 1.5352\n",
      "192/388, train_loss: 0.2642, step time: 1.5467\n",
      "193/388, train_loss: 0.2008, step time: 1.5425\n",
      "194/388, train_loss: 0.2441, step time: 1.5469\n",
      "195/388, train_loss: 0.1284, step time: 1.5429\n",
      "196/388, train_loss: 0.0716, step time: 1.5478\n",
      "197/388, train_loss: 0.1696, step time: 1.5484\n",
      "198/388, train_loss: 0.1168, step time: 1.5431\n",
      "199/388, train_loss: 0.0742, step time: 1.5434\n",
      "200/388, train_loss: 0.2257, step time: 1.5490\n",
      "201/388, train_loss: 0.0497, step time: 1.5428\n",
      "202/388, train_loss: 0.1281, step time: 1.5499\n",
      "203/388, train_loss: 0.0905, step time: 1.5382\n",
      "204/388, train_loss: 0.0598, step time: 1.5437\n",
      "205/388, train_loss: 0.2188, step time: 1.5511\n",
      "206/388, train_loss: 0.0632, step time: 1.5444\n",
      "207/388, train_loss: 0.1578, step time: 1.5610\n",
      "208/388, train_loss: 0.1253, step time: 1.5429\n",
      "209/388, train_loss: 0.2133, step time: 1.5473\n",
      "210/388, train_loss: 0.1985, step time: 1.5508\n",
      "211/388, train_loss: 0.1443, step time: 1.5297\n",
      "212/388, train_loss: 0.1335, step time: 1.5440\n",
      "213/388, train_loss: 0.2366, step time: 1.5474\n",
      "214/388, train_loss: 0.1111, step time: 1.5476\n",
      "215/388, train_loss: 0.1274, step time: 1.5413\n",
      "216/388, train_loss: 0.0860, step time: 1.5431\n",
      "217/388, train_loss: 0.1435, step time: 1.5507\n",
      "218/388, train_loss: 0.0968, step time: 1.5457\n",
      "219/388, train_loss: 0.1059, step time: 1.5436\n",
      "220/388, train_loss: 0.1036, step time: 1.5470\n",
      "221/388, train_loss: 0.3022, step time: 1.5440\n",
      "222/388, train_loss: 0.0961, step time: 1.5448\n",
      "223/388, train_loss: 0.1447, step time: 1.5431\n",
      "224/388, train_loss: 0.2370, step time: 1.5500\n",
      "225/388, train_loss: 0.1446, step time: 1.5507\n",
      "226/388, train_loss: 0.0962, step time: 1.5461\n",
      "227/388, train_loss: 0.1437, step time: 1.5426\n",
      "228/388, train_loss: 0.1028, step time: 1.5493\n",
      "229/388, train_loss: 0.1120, step time: 1.5465\n",
      "230/388, train_loss: 0.1137, step time: 1.5486\n",
      "231/388, train_loss: 0.1472, step time: 1.5483\n",
      "232/388, train_loss: 0.0877, step time: 1.5460\n",
      "233/388, train_loss: 0.1028, step time: 1.5499\n",
      "234/388, train_loss: 0.1321, step time: 1.5472\n",
      "235/388, train_loss: 0.0358, step time: 1.5433\n",
      "236/388, train_loss: 0.2556, step time: 1.5489\n",
      "237/388, train_loss: 0.2113, step time: 1.5450\n",
      "238/388, train_loss: 0.0922, step time: 1.5588\n",
      "239/388, train_loss: 0.0666, step time: 1.5486\n",
      "240/388, train_loss: 0.1017, step time: 1.5429\n",
      "241/388, train_loss: 0.1302, step time: 1.5479\n",
      "242/388, train_loss: 0.1004, step time: 1.5476\n",
      "243/388, train_loss: 0.0607, step time: 1.5439\n",
      "244/388, train_loss: 0.1067, step time: 1.5491\n",
      "245/388, train_loss: 0.1628, step time: 1.5487\n",
      "246/388, train_loss: 0.1535, step time: 1.5418\n",
      "247/388, train_loss: 0.0892, step time: 1.5482\n",
      "248/388, train_loss: 0.1409, step time: 1.5450\n",
      "249/388, train_loss: 0.1443, step time: 1.5428\n",
      "250/388, train_loss: 0.0933, step time: 1.5480\n",
      "251/388, train_loss: 0.4052, step time: 1.5466\n",
      "252/388, train_loss: 0.2372, step time: 1.5423\n",
      "253/388, train_loss: 0.2182, step time: 1.5479\n",
      "254/388, train_loss: 0.1976, step time: 1.5491\n",
      "255/388, train_loss: 0.0993, step time: 1.5436\n",
      "256/388, train_loss: 0.0842, step time: 1.5475\n",
      "257/388, train_loss: 0.1838, step time: 1.5464\n",
      "258/388, train_loss: 0.0529, step time: 1.5446\n",
      "259/388, train_loss: 0.0917, step time: 1.5496\n",
      "260/388, train_loss: 0.1597, step time: 1.5460\n",
      "261/388, train_loss: 0.1516, step time: 1.5513\n",
      "262/388, train_loss: 0.1622, step time: 1.5514\n",
      "263/388, train_loss: 0.1798, step time: 1.5433\n",
      "264/388, train_loss: 0.1825, step time: 1.5470\n",
      "265/388, train_loss: 0.2039, step time: 1.5490\n",
      "266/388, train_loss: 0.3235, step time: 1.5426\n",
      "267/388, train_loss: 0.1283, step time: 1.5480\n",
      "268/388, train_loss: 0.0889, step time: 1.5490\n",
      "269/388, train_loss: 0.2155, step time: 1.5462\n",
      "270/388, train_loss: 0.1294, step time: 1.5484\n",
      "271/388, train_loss: 0.1001, step time: 1.5493\n",
      "272/388, train_loss: 0.0969, step time: 1.5434\n",
      "273/388, train_loss: 0.2389, step time: 1.5510\n",
      "274/388, train_loss: 0.1561, step time: 1.5462\n",
      "275/388, train_loss: 0.2526, step time: 1.5460\n",
      "276/388, train_loss: 0.1101, step time: 1.5491\n",
      "277/388, train_loss: 0.1509, step time: 1.5443\n",
      "278/388, train_loss: 0.3422, step time: 1.5474\n",
      "279/388, train_loss: 0.1950, step time: 1.5442\n",
      "280/388, train_loss: 0.1747, step time: 1.5543\n",
      "281/388, train_loss: 0.1844, step time: 1.5433\n",
      "282/388, train_loss: 0.0788, step time: 1.5486\n",
      "283/388, train_loss: 0.1606, step time: 1.5446\n",
      "284/388, train_loss: 0.1520, step time: 1.5435\n",
      "285/388, train_loss: 0.0860, step time: 1.5470\n",
      "286/388, train_loss: 0.0863, step time: 1.5467\n",
      "287/388, train_loss: 0.1179, step time: 1.5478\n",
      "288/388, train_loss: 0.1111, step time: 1.5467\n",
      "289/388, train_loss: 0.2607, step time: 1.5426\n",
      "290/388, train_loss: 0.2038, step time: 1.5459\n",
      "291/388, train_loss: 0.2111, step time: 1.5493\n",
      "292/388, train_loss: 0.0893, step time: 1.5430\n",
      "293/388, train_loss: 0.1312, step time: 1.5443\n",
      "294/388, train_loss: 0.2659, step time: 1.5471\n",
      "295/388, train_loss: 0.0962, step time: 1.5438\n",
      "296/388, train_loss: 0.0980, step time: 1.5457\n",
      "297/388, train_loss: 0.1037, step time: 1.5517\n",
      "298/388, train_loss: 0.0746, step time: 1.5433\n",
      "299/388, train_loss: 0.1474, step time: 1.5484\n",
      "300/388, train_loss: 0.1335, step time: 1.5487\n",
      "301/388, train_loss: 0.2065, step time: 1.5442\n",
      "302/388, train_loss: 0.0987, step time: 1.5497\n",
      "303/388, train_loss: 0.1570, step time: 1.5478\n",
      "304/388, train_loss: 0.1058, step time: 1.5427\n",
      "305/388, train_loss: 0.0728, step time: 1.5463\n",
      "306/388, train_loss: 0.0665, step time: 1.5484\n",
      "307/388, train_loss: 0.2893, step time: 1.5447\n",
      "308/388, train_loss: 0.2044, step time: 1.5522\n",
      "309/388, train_loss: 0.0833, step time: 1.5465\n",
      "310/388, train_loss: 0.1871, step time: 1.5428\n",
      "311/388, train_loss: 0.1969, step time: 1.5469\n",
      "312/388, train_loss: 0.0485, step time: 1.5451\n",
      "313/388, train_loss: 0.1478, step time: 1.5449\n",
      "314/388, train_loss: 0.1048, step time: 1.5569\n",
      "315/388, train_loss: 0.2022, step time: 1.5455\n",
      "316/388, train_loss: 0.1576, step time: 1.5490\n",
      "317/388, train_loss: 0.0869, step time: 1.5503\n",
      "318/388, train_loss: 0.1893, step time: 1.5434\n",
      "319/388, train_loss: 0.3416, step time: 1.5486\n",
      "320/388, train_loss: 0.1524, step time: 1.5545\n",
      "321/388, train_loss: 0.1797, step time: 1.5416\n",
      "322/388, train_loss: 0.2486, step time: 1.5494\n",
      "323/388, train_loss: 0.5304, step time: 1.5479\n",
      "324/388, train_loss: 0.0857, step time: 1.5434\n",
      "325/388, train_loss: 0.1185, step time: 1.5471\n",
      "326/388, train_loss: 0.1094, step time: 1.5457\n",
      "327/388, train_loss: 0.0598, step time: 1.5438\n",
      "328/388, train_loss: 0.2566, step time: 1.5471\n",
      "329/388, train_loss: 0.2536, step time: 1.5438\n",
      "330/388, train_loss: 0.3624, step time: 1.5432\n",
      "331/388, train_loss: 0.1570, step time: 1.5445\n",
      "332/388, train_loss: 0.0861, step time: 1.5451\n",
      "333/388, train_loss: 0.1556, step time: 1.5456\n",
      "334/388, train_loss: 0.0940, step time: 1.5522\n",
      "335/388, train_loss: 0.3216, step time: 1.5423\n",
      "336/388, train_loss: 0.1024, step time: 1.5471\n",
      "337/388, train_loss: 0.2333, step time: 1.5456\n",
      "338/388, train_loss: 0.1353, step time: 1.5445\n",
      "339/388, train_loss: 0.2111, step time: 1.5479\n",
      "340/388, train_loss: 0.1545, step time: 1.5512\n",
      "341/388, train_loss: 0.1923, step time: 1.5427\n",
      "342/388, train_loss: 0.3264, step time: 1.5505\n",
      "343/388, train_loss: 0.0898, step time: 1.5477\n",
      "344/388, train_loss: 0.2490, step time: 1.5442\n",
      "345/388, train_loss: 0.1771, step time: 1.5470\n",
      "346/388, train_loss: 0.0549, step time: 1.5456\n",
      "347/388, train_loss: 0.0725, step time: 1.5459\n",
      "348/388, train_loss: 0.0534, step time: 1.5497\n",
      "349/388, train_loss: 0.1914, step time: 1.5411\n",
      "350/388, train_loss: 0.1189, step time: 1.5344\n",
      "351/388, train_loss: 0.1990, step time: 1.5541\n",
      "352/388, train_loss: 0.0653, step time: 1.5571\n",
      "353/388, train_loss: 0.1042, step time: 1.5480\n",
      "354/388, train_loss: 0.2676, step time: 1.5404\n",
      "355/388, train_loss: 0.1746, step time: 1.5430\n",
      "356/388, train_loss: 0.1518, step time: 1.5488\n",
      "357/388, train_loss: 0.1045, step time: 1.5430\n",
      "358/388, train_loss: 0.1262, step time: 1.5431\n",
      "359/388, train_loss: 0.1375, step time: 1.5484\n",
      "360/388, train_loss: 0.3461, step time: 1.5458\n",
      "361/388, train_loss: 0.1355, step time: 1.5441\n",
      "362/388, train_loss: 0.1935, step time: 1.5445\n",
      "363/388, train_loss: 0.1602, step time: 1.5455\n",
      "364/388, train_loss: 0.2978, step time: 1.5444\n",
      "365/388, train_loss: 0.2960, step time: 1.5471\n",
      "366/388, train_loss: 0.1584, step time: 1.5448\n",
      "367/388, train_loss: 0.0975, step time: 1.5462\n",
      "368/388, train_loss: 0.0312, step time: 1.5466\n",
      "369/388, train_loss: 0.1710, step time: 1.5458\n",
      "370/388, train_loss: 0.1555, step time: 1.5475\n",
      "371/388, train_loss: 0.0634, step time: 1.5451\n",
      "372/388, train_loss: 0.1890, step time: 1.5451\n",
      "373/388, train_loss: 0.0805, step time: 1.5447\n",
      "374/388, train_loss: 0.3166, step time: 1.5506\n",
      "375/388, train_loss: 0.1269, step time: 1.5411\n",
      "376/388, train_loss: 0.3874, step time: 1.5514\n",
      "377/388, train_loss: 0.1282, step time: 1.5484\n",
      "378/388, train_loss: 0.0931, step time: 1.5420\n",
      "379/388, train_loss: 0.2401, step time: 1.5495\n",
      "380/388, train_loss: 0.2762, step time: 1.5453\n",
      "381/388, train_loss: 0.1337, step time: 1.5416\n",
      "382/388, train_loss: 0.1152, step time: 1.5476\n",
      "383/388, train_loss: 0.1500, step time: 1.5467\n",
      "384/388, train_loss: 0.1179, step time: 1.5424\n",
      "385/388, train_loss: 0.2319, step time: 1.5503\n",
      "386/388, train_loss: 0.0971, step time: 1.5456\n",
      "387/388, train_loss: 0.0779, step time: 1.5423\n",
      "388/388, train_loss: 0.2093, step time: 1.5474\n",
      "epoch 85 average loss: 0.1579\n",
      "current epoch: 85 current mean dice: 0.7806 tc: 0.8288 wt: 0.9055 et: 0.6074\n",
      "best mean dice: 0.7817 at epoch: 84\n",
      "time consuming of epoch 85 is: 707.0975\n",
      "----------\n",
      "epoch 86/100\n",
      "1/388, train_loss: 0.0937, step time: 1.5440\n",
      "2/388, train_loss: 0.2218, step time: 1.5358\n",
      "3/388, train_loss: 0.2536, step time: 1.5575\n",
      "4/388, train_loss: 0.0960, step time: 1.5336\n",
      "5/388, train_loss: 0.1861, step time: 1.5269\n",
      "6/388, train_loss: 0.0844, step time: 1.5297\n",
      "7/388, train_loss: 0.1572, step time: 1.5280\n",
      "8/388, train_loss: 0.1977, step time: 1.5343\n",
      "9/388, train_loss: 0.1462, step time: 1.5314\n",
      "10/388, train_loss: 0.3903, step time: 1.5545\n",
      "11/388, train_loss: 0.1588, step time: 1.5389\n",
      "12/388, train_loss: 0.3648, step time: 1.5349\n",
      "13/388, train_loss: 0.1454, step time: 1.5320\n",
      "14/388, train_loss: 0.2072, step time: 1.5306\n",
      "15/388, train_loss: 0.1026, step time: 1.5316\n",
      "16/388, train_loss: 0.0742, step time: 1.5275\n",
      "17/388, train_loss: 0.1935, step time: 1.5282\n",
      "18/388, train_loss: 0.0691, step time: 1.5286\n",
      "19/388, train_loss: 0.0785, step time: 1.5329\n",
      "20/388, train_loss: 0.1771, step time: 1.5307\n",
      "21/388, train_loss: 0.1654, step time: 1.5316\n",
      "22/388, train_loss: 0.1859, step time: 1.5326\n",
      "23/388, train_loss: 0.2102, step time: 1.5307\n",
      "24/388, train_loss: 0.3550, step time: 1.5327\n",
      "25/388, train_loss: 0.2664, step time: 1.5271\n",
      "26/388, train_loss: 0.0414, step time: 1.5302\n",
      "27/388, train_loss: 0.0684, step time: 1.5302\n",
      "28/388, train_loss: 0.1306, step time: 1.5316\n",
      "29/388, train_loss: 0.0808, step time: 1.5278\n",
      "30/388, train_loss: 0.1987, step time: 1.5290\n",
      "31/388, train_loss: 0.1976, step time: 1.5365\n",
      "32/388, train_loss: 0.0855, step time: 1.5376\n",
      "33/388, train_loss: 0.0836, step time: 1.5280\n",
      "34/388, train_loss: 0.1333, step time: 1.5287\n",
      "35/388, train_loss: 0.0900, step time: 1.5280\n",
      "36/388, train_loss: 0.3394, step time: 1.5328\n",
      "37/388, train_loss: 0.2762, step time: 1.5288\n",
      "38/388, train_loss: 0.2279, step time: 1.5298\n",
      "39/388, train_loss: 0.3511, step time: 1.5323\n",
      "40/388, train_loss: 0.2693, step time: 1.5330\n",
      "41/388, train_loss: 0.0892, step time: 1.5321\n",
      "42/388, train_loss: 0.1850, step time: 1.5329\n",
      "43/388, train_loss: 0.0771, step time: 1.5282\n",
      "44/388, train_loss: 0.0834, step time: 1.5319\n",
      "45/388, train_loss: 0.0671, step time: 1.5305\n",
      "46/388, train_loss: 0.1869, step time: 1.5314\n",
      "47/388, train_loss: 0.2428, step time: 1.5287\n",
      "48/388, train_loss: 0.1044, step time: 1.5346\n",
      "49/388, train_loss: 0.0866, step time: 1.5332\n",
      "50/388, train_loss: 0.1770, step time: 1.5323\n",
      "51/388, train_loss: 0.0996, step time: 1.5329\n",
      "52/388, train_loss: 0.1182, step time: 1.5330\n",
      "53/388, train_loss: 0.1097, step time: 1.5372\n",
      "54/388, train_loss: 0.1979, step time: 1.5333\n",
      "55/388, train_loss: 0.2486, step time: 1.5344\n",
      "56/388, train_loss: 0.1691, step time: 1.5339\n",
      "57/388, train_loss: 0.1904, step time: 1.5327\n",
      "58/388, train_loss: 0.1380, step time: 1.5314\n",
      "59/388, train_loss: 0.1012, step time: 1.5296\n",
      "60/388, train_loss: 0.1053, step time: 1.5296\n",
      "61/388, train_loss: 0.1873, step time: 1.5300\n",
      "62/388, train_loss: 0.1891, step time: 1.5315\n",
      "63/388, train_loss: 0.1024, step time: 1.5299\n",
      "64/388, train_loss: 0.2914, step time: 1.5305\n",
      "65/388, train_loss: 0.1179, step time: 1.5296\n",
      "66/388, train_loss: 0.4295, step time: 1.5465\n",
      "67/388, train_loss: 0.0949, step time: 1.5302\n",
      "68/388, train_loss: 0.1270, step time: 1.5304\n",
      "69/388, train_loss: 0.2139, step time: 1.5307\n",
      "70/388, train_loss: 0.2470, step time: 1.5338\n",
      "71/388, train_loss: 0.1470, step time: 1.5328\n",
      "72/388, train_loss: 0.0867, step time: 1.5328\n",
      "73/388, train_loss: 0.4343, step time: 1.5320\n",
      "74/388, train_loss: 0.1668, step time: 1.5318\n",
      "75/388, train_loss: 0.1287, step time: 1.5414\n",
      "76/388, train_loss: 0.1074, step time: 1.5454\n",
      "77/388, train_loss: 0.2122, step time: 1.5434\n",
      "78/388, train_loss: 0.0548, step time: 1.5395\n",
      "79/388, train_loss: 0.2998, step time: 1.5383\n",
      "80/388, train_loss: 0.0816, step time: 1.5436\n",
      "81/388, train_loss: 0.1800, step time: 1.5418\n",
      "82/388, train_loss: 0.1024, step time: 1.5375\n",
      "83/388, train_loss: 0.0350, step time: 1.5386\n",
      "84/388, train_loss: 0.3592, step time: 1.5382\n",
      "85/388, train_loss: 0.1650, step time: 1.5428\n",
      "86/388, train_loss: 0.2283, step time: 1.5443\n",
      "87/388, train_loss: 0.1259, step time: 1.5354\n",
      "88/388, train_loss: 0.3030, step time: 1.5452\n",
      "89/388, train_loss: 0.0829, step time: 1.5367\n",
      "90/388, train_loss: 0.0786, step time: 1.5389\n",
      "91/388, train_loss: 0.1648, step time: 1.5422\n",
      "92/388, train_loss: 0.0851, step time: 1.5364\n",
      "93/388, train_loss: 0.1171, step time: 1.5449\n",
      "94/388, train_loss: 0.2057, step time: 1.5401\n",
      "95/388, train_loss: 0.0881, step time: 1.5428\n",
      "96/388, train_loss: 0.1127, step time: 1.5380\n",
      "97/388, train_loss: 0.1400, step time: 1.5440\n",
      "98/388, train_loss: 0.5224, step time: 1.5353\n",
      "99/388, train_loss: 0.1421, step time: 1.5351\n",
      "100/388, train_loss: 0.0646, step time: 1.5322\n",
      "101/388, train_loss: 0.2573, step time: 1.5326\n",
      "102/388, train_loss: 0.1150, step time: 1.5328\n",
      "103/388, train_loss: 0.2383, step time: 1.5342\n",
      "104/388, train_loss: 0.0288, step time: 1.5328\n",
      "105/388, train_loss: 0.1172, step time: 1.5261\n",
      "106/388, train_loss: 0.0761, step time: 1.5281\n",
      "107/388, train_loss: 0.2244, step time: 1.5282\n",
      "108/388, train_loss: 0.1742, step time: 1.5300\n",
      "109/388, train_loss: 0.0951, step time: 1.5289\n",
      "110/388, train_loss: 0.1511, step time: 1.5309\n",
      "111/388, train_loss: 0.1173, step time: 1.5362\n",
      "112/388, train_loss: 0.1227, step time: 1.5326\n",
      "113/388, train_loss: 0.1534, step time: 1.5333\n",
      "114/388, train_loss: 0.1551, step time: 1.5300\n",
      "115/388, train_loss: 0.1510, step time: 1.5323\n",
      "116/388, train_loss: 0.1051, step time: 1.5299\n",
      "117/388, train_loss: 0.1047, step time: 1.5347\n",
      "118/388, train_loss: 0.3514, step time: 1.5326\n",
      "119/388, train_loss: 0.1652, step time: 1.5349\n",
      "120/388, train_loss: 0.0521, step time: 1.5338\n",
      "121/388, train_loss: 0.0929, step time: 1.5325\n",
      "122/388, train_loss: 0.1124, step time: 1.5301\n",
      "123/388, train_loss: 0.0638, step time: 1.5277\n",
      "124/388, train_loss: 0.1509, step time: 1.5289\n",
      "125/388, train_loss: 0.1063, step time: 1.5302\n",
      "126/388, train_loss: 0.2104, step time: 1.5344\n",
      "127/388, train_loss: 0.1485, step time: 1.5344\n",
      "128/388, train_loss: 0.2634, step time: 1.5320\n",
      "129/388, train_loss: 0.0667, step time: 1.5349\n",
      "130/388, train_loss: 0.1461, step time: 1.5356\n",
      "131/388, train_loss: 0.0600, step time: 1.5267\n",
      "132/388, train_loss: 0.0751, step time: 1.5287\n",
      "133/388, train_loss: 0.1934, step time: 1.5270\n",
      "134/388, train_loss: 0.1051, step time: 1.5325\n",
      "135/388, train_loss: 0.2727, step time: 1.5279\n",
      "136/388, train_loss: 0.1425, step time: 1.5347\n",
      "137/388, train_loss: 0.2945, step time: 1.5357\n",
      "138/388, train_loss: 0.0881, step time: 1.5364\n",
      "139/388, train_loss: 0.2138, step time: 1.5330\n",
      "140/388, train_loss: 0.1580, step time: 1.5329\n",
      "141/388, train_loss: 0.2107, step time: 1.5339\n",
      "142/388, train_loss: 0.1333, step time: 1.5311\n",
      "143/388, train_loss: 0.1005, step time: 1.5286\n",
      "144/388, train_loss: 0.1477, step time: 1.5358\n",
      "145/388, train_loss: 0.1368, step time: 1.5324\n",
      "146/388, train_loss: 0.1558, step time: 1.5326\n",
      "147/388, train_loss: 0.2136, step time: 1.5340\n",
      "148/388, train_loss: 0.1130, step time: 1.5301\n",
      "149/388, train_loss: 0.2231, step time: 1.5312\n",
      "150/388, train_loss: 0.1432, step time: 1.5297\n",
      "151/388, train_loss: 0.1737, step time: 1.5361\n",
      "152/388, train_loss: 0.2304, step time: 1.5344\n",
      "153/388, train_loss: 0.3045, step time: 1.5315\n",
      "154/388, train_loss: 0.2056, step time: 1.5478\n",
      "155/388, train_loss: 0.1259, step time: 1.5313\n",
      "156/388, train_loss: 0.1933, step time: 1.5292\n",
      "157/388, train_loss: 0.0783, step time: 1.5303\n",
      "158/388, train_loss: 0.1645, step time: 1.5362\n",
      "159/388, train_loss: 0.1966, step time: 1.5342\n",
      "160/388, train_loss: 0.0515, step time: 1.5303\n",
      "161/388, train_loss: 0.1769, step time: 1.5343\n",
      "162/388, train_loss: 0.1616, step time: 1.5325\n",
      "163/388, train_loss: 0.1206, step time: 1.5354\n",
      "164/388, train_loss: 0.0887, step time: 1.5329\n",
      "165/388, train_loss: 0.1593, step time: 1.5295\n",
      "166/388, train_loss: 0.2048, step time: 1.5303\n",
      "167/388, train_loss: 0.1949, step time: 1.5296\n",
      "168/388, train_loss: 0.1999, step time: 1.5282\n",
      "169/388, train_loss: 0.1259, step time: 1.5305\n",
      "170/388, train_loss: 0.1017, step time: 1.5333\n",
      "171/388, train_loss: 0.0303, step time: 1.5471\n",
      "172/388, train_loss: 0.0964, step time: 1.5438\n",
      "173/388, train_loss: 0.0972, step time: 1.5272\n",
      "174/388, train_loss: 0.0789, step time: 1.5324\n",
      "175/388, train_loss: 0.0443, step time: 1.5361\n",
      "176/388, train_loss: 0.2090, step time: 1.5316\n",
      "177/388, train_loss: 0.0752, step time: 1.5329\n",
      "178/388, train_loss: 0.0675, step time: 1.5311\n",
      "179/388, train_loss: 0.2458, step time: 1.5314\n",
      "180/388, train_loss: 0.0937, step time: 1.5290\n",
      "181/388, train_loss: 0.1115, step time: 1.5317\n",
      "182/388, train_loss: 0.1934, step time: 1.5290\n",
      "183/388, train_loss: 0.1309, step time: 1.5305\n",
      "184/388, train_loss: 0.1671, step time: 1.5319\n",
      "185/388, train_loss: 0.1251, step time: 1.5565\n",
      "186/388, train_loss: 0.1070, step time: 1.5309\n",
      "187/388, train_loss: 0.0880, step time: 1.5289\n",
      "188/388, train_loss: 0.1580, step time: 1.5306\n",
      "189/388, train_loss: 0.0579, step time: 1.5310\n",
      "190/388, train_loss: 0.1042, step time: 1.5372\n",
      "191/388, train_loss: 0.0808, step time: 1.5309\n",
      "192/388, train_loss: 0.0906, step time: 1.5354\n",
      "193/388, train_loss: 0.2126, step time: 1.5296\n",
      "194/388, train_loss: 0.0607, step time: 1.5321\n",
      "195/388, train_loss: 0.0291, step time: 1.5302\n",
      "196/388, train_loss: 0.1275, step time: 1.5299\n",
      "197/388, train_loss: 0.1374, step time: 1.5273\n",
      "198/388, train_loss: 0.1888, step time: 1.5269\n",
      "199/388, train_loss: 0.5112, step time: 1.5310\n",
      "200/388, train_loss: 0.1484, step time: 1.5335\n",
      "201/388, train_loss: 0.0837, step time: 1.5315\n",
      "202/388, train_loss: 0.3892, step time: 1.5341\n",
      "203/388, train_loss: 0.1940, step time: 1.5309\n",
      "204/388, train_loss: 0.1002, step time: 1.5457\n",
      "205/388, train_loss: 0.1230, step time: 1.5320\n",
      "206/388, train_loss: 0.2041, step time: 1.5384\n",
      "207/388, train_loss: 0.4499, step time: 1.5306\n",
      "208/388, train_loss: 0.1140, step time: 1.5320\n",
      "209/388, train_loss: 0.0991, step time: 1.5278\n",
      "210/388, train_loss: 0.5837, step time: 1.5305\n",
      "211/388, train_loss: 0.2523, step time: 1.5321\n",
      "212/388, train_loss: 0.4113, step time: 1.5307\n",
      "213/388, train_loss: 0.1174, step time: 1.5337\n",
      "214/388, train_loss: 0.3067, step time: 1.5334\n",
      "215/388, train_loss: 0.0502, step time: 1.5290\n",
      "216/388, train_loss: 0.1666, step time: 1.5297\n",
      "217/388, train_loss: 0.0999, step time: 1.5309\n",
      "218/388, train_loss: 0.2645, step time: 1.5336\n",
      "219/388, train_loss: 0.0903, step time: 1.5363\n",
      "220/388, train_loss: 0.1480, step time: 1.5356\n",
      "221/388, train_loss: 0.1592, step time: 1.5354\n",
      "222/388, train_loss: 0.1276, step time: 1.5309\n",
      "223/388, train_loss: 0.1167, step time: 1.5332\n",
      "224/388, train_loss: 0.1014, step time: 1.5307\n",
      "225/388, train_loss: 0.0684, step time: 1.5379\n",
      "226/388, train_loss: 0.1414, step time: 1.5314\n",
      "227/388, train_loss: 0.2105, step time: 1.5338\n",
      "228/388, train_loss: 0.0869, step time: 1.5357\n",
      "229/388, train_loss: 0.0803, step time: 1.5322\n",
      "230/388, train_loss: 0.0452, step time: 1.5343\n",
      "231/388, train_loss: 0.0936, step time: 1.5300\n",
      "232/388, train_loss: 0.1097, step time: 1.5292\n",
      "233/388, train_loss: 0.0928, step time: 1.5318\n",
      "234/388, train_loss: 0.0574, step time: 1.5278\n",
      "235/388, train_loss: 0.0780, step time: 1.5385\n",
      "236/388, train_loss: 0.1878, step time: 1.5265\n",
      "237/388, train_loss: 0.0904, step time: 1.5411\n",
      "238/388, train_loss: 0.0839, step time: 1.5342\n",
      "239/388, train_loss: 0.1837, step time: 1.5391\n",
      "240/388, train_loss: 0.0999, step time: 1.5287\n",
      "241/388, train_loss: 0.2403, step time: 1.5311\n",
      "242/388, train_loss: 0.3107, step time: 1.5303\n",
      "243/388, train_loss: 0.2174, step time: 1.5283\n",
      "244/388, train_loss: 0.0643, step time: 1.5290\n",
      "245/388, train_loss: 0.4578, step time: 1.5273\n",
      "246/388, train_loss: 0.1355, step time: 1.5313\n",
      "247/388, train_loss: 0.0876, step time: 1.5305\n",
      "248/388, train_loss: 0.1216, step time: 1.5347\n",
      "249/388, train_loss: 0.0880, step time: 1.5337\n",
      "250/388, train_loss: 0.0970, step time: 1.5338\n",
      "251/388, train_loss: 0.1566, step time: 1.5330\n",
      "252/388, train_loss: 0.1469, step time: 1.5319\n",
      "253/388, train_loss: 0.3152, step time: 1.5271\n",
      "254/388, train_loss: 0.2034, step time: 1.5308\n",
      "255/388, train_loss: 0.1059, step time: 1.5294\n",
      "256/388, train_loss: 0.1383, step time: 1.5264\n",
      "257/388, train_loss: 0.0437, step time: 1.5293\n",
      "258/388, train_loss: 0.2771, step time: 1.5325\n",
      "259/388, train_loss: 0.1595, step time: 1.5331\n",
      "260/388, train_loss: 0.1594, step time: 1.5336\n",
      "261/388, train_loss: 0.0876, step time: 1.5342\n",
      "262/388, train_loss: 0.2415, step time: 1.5351\n",
      "263/388, train_loss: 0.1076, step time: 1.5331\n",
      "264/388, train_loss: 0.1719, step time: 1.5323\n",
      "265/388, train_loss: 0.1875, step time: 1.5289\n",
      "266/388, train_loss: 0.1506, step time: 1.5309\n",
      "267/388, train_loss: 0.1302, step time: 1.5444\n",
      "268/388, train_loss: 0.2541, step time: 1.5367\n",
      "269/388, train_loss: 0.2440, step time: 1.5335\n",
      "270/388, train_loss: 0.1180, step time: 1.5307\n",
      "271/388, train_loss: 0.1355, step time: 1.5302\n",
      "272/388, train_loss: 0.1649, step time: 1.5300\n",
      "273/388, train_loss: 0.1524, step time: 1.5323\n",
      "274/388, train_loss: 0.1424, step time: 1.5320\n",
      "275/388, train_loss: 0.1270, step time: 1.5280\n",
      "276/388, train_loss: 0.0870, step time: 1.5317\n",
      "277/388, train_loss: 0.1508, step time: 1.5317\n",
      "278/388, train_loss: 0.1974, step time: 1.5365\n",
      "279/388, train_loss: 0.1357, step time: 1.5330\n",
      "280/388, train_loss: 0.2683, step time: 1.5313\n",
      "281/388, train_loss: 0.1585, step time: 1.5286\n",
      "282/388, train_loss: 0.1840, step time: 1.5315\n",
      "283/388, train_loss: 0.1991, step time: 1.5274\n",
      "284/388, train_loss: 0.1908, step time: 1.5335\n",
      "285/388, train_loss: 0.2026, step time: 1.5289\n",
      "286/388, train_loss: 0.0785, step time: 1.5496\n",
      "287/388, train_loss: 0.1900, step time: 1.5360\n",
      "288/388, train_loss: 0.1019, step time: 1.5339\n",
      "289/388, train_loss: 0.1065, step time: 1.5334\n",
      "290/388, train_loss: 0.2114, step time: 1.5311\n",
      "291/388, train_loss: 0.0908, step time: 1.5297\n",
      "292/388, train_loss: 0.1480, step time: 1.5301\n",
      "293/388, train_loss: 0.2073, step time: 1.5283\n",
      "294/388, train_loss: 0.1730, step time: 1.5304\n",
      "295/388, train_loss: 0.1501, step time: 1.5346\n",
      "296/388, train_loss: 0.1867, step time: 1.5333\n",
      "297/388, train_loss: 0.0691, step time: 1.5310\n",
      "298/388, train_loss: 0.0769, step time: 1.5310\n",
      "299/388, train_loss: 0.2577, step time: 1.5339\n",
      "300/388, train_loss: 0.1283, step time: 1.5288\n",
      "301/388, train_loss: 0.1650, step time: 1.5274\n",
      "302/388, train_loss: 0.2191, step time: 1.5314\n",
      "303/388, train_loss: 0.2468, step time: 1.5291\n",
      "304/388, train_loss: 0.1087, step time: 1.5324\n",
      "305/388, train_loss: 0.2193, step time: 1.5321\n",
      "306/388, train_loss: 0.0913, step time: 1.5312\n",
      "307/388, train_loss: 0.2756, step time: 1.5364\n",
      "308/388, train_loss: 0.1165, step time: 1.5341\n",
      "309/388, train_loss: 0.2064, step time: 1.5270\n",
      "310/388, train_loss: 0.2162, step time: 1.5299\n",
      "311/388, train_loss: 0.1754, step time: 1.5285\n",
      "312/388, train_loss: 0.1878, step time: 1.5314\n",
      "313/388, train_loss: 0.1157, step time: 1.5246\n",
      "314/388, train_loss: 0.0757, step time: 1.5302\n",
      "315/388, train_loss: 0.2950, step time: 1.5370\n",
      "316/388, train_loss: 0.1856, step time: 1.5331\n",
      "317/388, train_loss: 0.0697, step time: 1.5306\n",
      "318/388, train_loss: 0.0820, step time: 1.5291\n",
      "319/388, train_loss: 0.2497, step time: 1.5283\n",
      "320/388, train_loss: 0.2807, step time: 1.5299\n",
      "321/388, train_loss: 0.0563, step time: 1.5290\n",
      "322/388, train_loss: 0.1654, step time: 1.5301\n",
      "323/388, train_loss: 0.1394, step time: 1.5337\n",
      "324/388, train_loss: 0.0662, step time: 1.5347\n",
      "325/388, train_loss: 0.1492, step time: 1.5311\n",
      "326/388, train_loss: 0.1089, step time: 1.5316\n",
      "327/388, train_loss: 0.1160, step time: 1.5334\n",
      "328/388, train_loss: 0.1514, step time: 1.5302\n",
      "329/388, train_loss: 0.0882, step time: 1.5310\n",
      "330/388, train_loss: 0.2331, step time: 1.5272\n",
      "331/388, train_loss: 0.1903, step time: 1.5314\n",
      "332/388, train_loss: 0.1855, step time: 1.5317\n",
      "333/388, train_loss: 0.0717, step time: 1.5301\n",
      "334/388, train_loss: 0.0900, step time: 1.5348\n",
      "335/388, train_loss: 0.0704, step time: 1.5314\n",
      "336/388, train_loss: 0.0876, step time: 1.5345\n",
      "337/388, train_loss: 0.3352, step time: 1.5328\n",
      "338/388, train_loss: 0.0412, step time: 1.5311\n",
      "339/388, train_loss: 0.3005, step time: 1.5297\n",
      "340/388, train_loss: 0.0686, step time: 1.5310\n",
      "341/388, train_loss: 0.1315, step time: 1.5279\n",
      "342/388, train_loss: 0.0931, step time: 1.5338\n",
      "343/388, train_loss: 0.1481, step time: 1.5286\n",
      "344/388, train_loss: 0.1838, step time: 1.5328\n",
      "345/388, train_loss: 0.2103, step time: 1.5454\n",
      "346/388, train_loss: 0.1349, step time: 1.5307\n",
      "347/388, train_loss: 0.1314, step time: 1.5312\n",
      "348/388, train_loss: 0.2338, step time: 1.5322\n",
      "349/388, train_loss: 0.0908, step time: 1.5329\n",
      "350/388, train_loss: 0.0892, step time: 1.5343\n",
      "351/388, train_loss: 0.2245, step time: 1.5371\n",
      "352/388, train_loss: 0.1809, step time: 1.5333\n",
      "353/388, train_loss: 0.2681, step time: 1.5316\n",
      "354/388, train_loss: 0.1084, step time: 1.5264\n",
      "355/388, train_loss: 0.3205, step time: 1.5290\n",
      "356/388, train_loss: 0.1047, step time: 1.5317\n",
      "357/388, train_loss: 0.0996, step time: 1.5319\n",
      "358/388, train_loss: 0.1071, step time: 1.5298\n",
      "359/388, train_loss: 0.0841, step time: 1.5318\n",
      "360/388, train_loss: 0.1486, step time: 1.5353\n",
      "361/388, train_loss: 0.0838, step time: 1.5335\n",
      "362/388, train_loss: 0.4017, step time: 1.5340\n",
      "363/388, train_loss: 0.3143, step time: 1.5289\n",
      "364/388, train_loss: 0.1599, step time: 1.5297\n",
      "365/388, train_loss: 0.2270, step time: 1.5293\n",
      "366/388, train_loss: 0.1314, step time: 1.5329\n",
      "367/388, train_loss: 0.1655, step time: 1.5301\n",
      "368/388, train_loss: 0.1650, step time: 1.5370\n",
      "369/388, train_loss: 0.2041, step time: 1.5352\n",
      "370/388, train_loss: 0.1149, step time: 1.5310\n",
      "371/388, train_loss: 0.0873, step time: 1.5344\n",
      "372/388, train_loss: 0.0986, step time: 1.5313\n",
      "373/388, train_loss: 0.0850, step time: 1.5288\n",
      "374/388, train_loss: 0.2629, step time: 1.5306\n",
      "375/388, train_loss: 0.0641, step time: 1.5295\n",
      "376/388, train_loss: 0.0916, step time: 1.5322\n",
      "377/388, train_loss: 0.1498, step time: 1.5530\n",
      "378/388, train_loss: 0.1974, step time: 1.5415\n",
      "379/388, train_loss: 0.0591, step time: 1.5332\n",
      "380/388, train_loss: 0.1796, step time: 1.5320\n",
      "381/388, train_loss: 0.0717, step time: 1.5304\n",
      "382/388, train_loss: 0.3379, step time: 1.5343\n",
      "383/388, train_loss: 0.0908, step time: 1.5337\n",
      "384/388, train_loss: 0.1223, step time: 1.5320\n",
      "385/388, train_loss: 0.0430, step time: 1.5321\n",
      "386/388, train_loss: 0.0675, step time: 1.5378\n",
      "387/388, train_loss: 0.1916, step time: 1.5316\n",
      "388/388, train_loss: 0.1763, step time: 1.5284\n",
      "epoch 86 average loss: 0.1589\n",
      "current epoch: 86 current mean dice: 0.7797 tc: 0.8270 wt: 0.9049 et: 0.6073\n",
      "best mean dice: 0.7817 at epoch: 84\n",
      "time consuming of epoch 86 is: 701.0884\n",
      "----------\n",
      "epoch 87/100\n",
      "1/388, train_loss: 0.1539, step time: 1.5804\n",
      "2/388, train_loss: 0.0953, step time: 1.5323\n",
      "3/388, train_loss: 0.1638, step time: 1.5319\n",
      "4/388, train_loss: 0.2430, step time: 1.5325\n",
      "5/388, train_loss: 0.2741, step time: 1.5312\n",
      "6/388, train_loss: 0.1129, step time: 1.5339\n",
      "7/388, train_loss: 0.1049, step time: 1.5322\n",
      "8/388, train_loss: 0.1924, step time: 1.5280\n",
      "9/388, train_loss: 0.0866, step time: 1.5370\n",
      "10/388, train_loss: 0.0848, step time: 1.5365\n",
      "11/388, train_loss: 0.0457, step time: 1.5424\n",
      "12/388, train_loss: 0.2282, step time: 1.5360\n",
      "13/388, train_loss: 0.0838, step time: 1.5289\n",
      "14/388, train_loss: 0.1439, step time: 1.5320\n",
      "15/388, train_loss: 0.1046, step time: 1.5296\n",
      "16/388, train_loss: 0.1549, step time: 1.5380\n",
      "17/388, train_loss: 0.0372, step time: 1.5400\n",
      "18/388, train_loss: 0.1083, step time: 1.5365\n",
      "19/388, train_loss: 0.0733, step time: 1.5348\n",
      "20/388, train_loss: 0.1701, step time: 1.5298\n",
      "21/388, train_loss: 0.1958, step time: 1.5338\n",
      "22/388, train_loss: 0.0867, step time: 1.5346\n",
      "23/388, train_loss: 0.1167, step time: 1.5325\n",
      "24/388, train_loss: 0.0908, step time: 1.5327\n",
      "25/388, train_loss: 0.1158, step time: 1.5367\n",
      "26/388, train_loss: 0.4294, step time: 1.5336\n",
      "27/388, train_loss: 0.1316, step time: 1.5299\n",
      "28/388, train_loss: 0.1845, step time: 1.5307\n",
      "29/388, train_loss: 0.1984, step time: 1.5329\n",
      "30/388, train_loss: 0.2140, step time: 1.5409\n",
      "31/388, train_loss: 0.0271, step time: 1.5426\n",
      "32/388, train_loss: 0.1473, step time: 1.5409\n",
      "33/388, train_loss: 0.0871, step time: 1.5360\n",
      "34/388, train_loss: 0.0952, step time: 1.5364\n",
      "35/388, train_loss: 0.2007, step time: 1.5361\n",
      "36/388, train_loss: 0.1541, step time: 1.5458\n",
      "37/388, train_loss: 0.1855, step time: 1.5297\n",
      "38/388, train_loss: 0.1468, step time: 1.5307\n",
      "39/388, train_loss: 0.2394, step time: 1.5333\n",
      "40/388, train_loss: 0.0450, step time: 1.5460\n",
      "41/388, train_loss: 0.0762, step time: 1.5294\n",
      "42/388, train_loss: 0.1070, step time: 1.5293\n",
      "43/388, train_loss: 0.0967, step time: 1.5296\n",
      "44/388, train_loss: 0.2093, step time: 1.5496\n",
      "45/388, train_loss: 0.0830, step time: 1.5330\n",
      "46/388, train_loss: 0.1693, step time: 1.5288\n",
      "47/388, train_loss: 0.1895, step time: 1.5368\n",
      "48/388, train_loss: 0.1903, step time: 1.5487\n",
      "49/388, train_loss: 0.3323, step time: 1.5356\n",
      "50/388, train_loss: 0.2406, step time: 1.5321\n",
      "51/388, train_loss: 0.0994, step time: 1.5434\n",
      "52/388, train_loss: 0.3246, step time: 1.5374\n",
      "53/388, train_loss: 0.0796, step time: 1.5324\n",
      "54/388, train_loss: 0.0825, step time: 1.5365\n",
      "55/388, train_loss: 0.1209, step time: 1.5366\n",
      "56/388, train_loss: 0.1170, step time: 1.5483\n",
      "57/388, train_loss: 0.2344, step time: 1.5307\n",
      "58/388, train_loss: 0.2869, step time: 1.5310\n",
      "59/388, train_loss: 0.5102, step time: 1.5311\n",
      "60/388, train_loss: 0.1120, step time: 1.5450\n",
      "61/388, train_loss: 0.0772, step time: 1.5338\n",
      "62/388, train_loss: 0.1901, step time: 1.5303\n",
      "63/388, train_loss: 0.1064, step time: 1.5315\n",
      "64/388, train_loss: 0.0656, step time: 1.5425\n",
      "65/388, train_loss: 0.2484, step time: 1.5282\n",
      "66/388, train_loss: 0.1311, step time: 1.5319\n",
      "67/388, train_loss: 0.4925, step time: 1.5326\n",
      "68/388, train_loss: 0.1925, step time: 1.5575\n",
      "69/388, train_loss: 0.1161, step time: 1.5310\n",
      "70/388, train_loss: 0.1260, step time: 1.5293\n",
      "71/388, train_loss: 0.1871, step time: 1.5325\n",
      "72/388, train_loss: 0.3893, step time: 1.5449\n",
      "73/388, train_loss: 0.1287, step time: 1.5293\n",
      "74/388, train_loss: 0.1319, step time: 1.5304\n",
      "75/388, train_loss: 0.0842, step time: 1.5317\n",
      "76/388, train_loss: 0.2225, step time: 1.5433\n",
      "77/388, train_loss: 0.2529, step time: 1.5324\n",
      "78/388, train_loss: 0.0738, step time: 1.5323\n",
      "79/388, train_loss: 0.1771, step time: 1.5361\n",
      "80/388, train_loss: 0.1345, step time: 1.5565\n",
      "81/388, train_loss: 0.1657, step time: 1.5289\n",
      "82/388, train_loss: 0.1828, step time: 1.5395\n",
      "83/388, train_loss: 0.1267, step time: 1.5367\n",
      "84/388, train_loss: 0.1767, step time: 1.5403\n",
      "85/388, train_loss: 0.0988, step time: 1.5420\n",
      "86/388, train_loss: 0.1546, step time: 1.5468\n",
      "87/388, train_loss: 0.1433, step time: 1.5351\n",
      "88/388, train_loss: 0.1310, step time: 1.5431\n",
      "89/388, train_loss: 0.2691, step time: 1.5310\n",
      "90/388, train_loss: 0.3019, step time: 1.5361\n",
      "91/388, train_loss: 0.0879, step time: 1.5327\n",
      "92/388, train_loss: 0.1548, step time: 1.5517\n",
      "93/388, train_loss: 0.1417, step time: 1.5485\n",
      "94/388, train_loss: 0.2428, step time: 1.5279\n",
      "95/388, train_loss: 0.0996, step time: 1.5331\n",
      "96/388, train_loss: 0.0968, step time: 1.5470\n",
      "97/388, train_loss: 0.1000, step time: 1.5299\n",
      "98/388, train_loss: 0.1052, step time: 1.5310\n",
      "99/388, train_loss: 0.3655, step time: 1.5316\n",
      "100/388, train_loss: 0.0804, step time: 1.5424\n",
      "101/388, train_loss: 0.4256, step time: 1.5375\n",
      "102/388, train_loss: 0.0567, step time: 1.5335\n",
      "103/388, train_loss: 0.1160, step time: 1.5293\n",
      "104/388, train_loss: 0.0664, step time: 1.5433\n",
      "105/388, train_loss: 0.1063, step time: 1.5273\n",
      "106/388, train_loss: 0.1750, step time: 1.5340\n",
      "107/388, train_loss: 0.1814, step time: 1.5352\n",
      "108/388, train_loss: 0.1285, step time: 1.5440\n",
      "109/388, train_loss: 0.3386, step time: 1.5301\n",
      "110/388, train_loss: 0.0766, step time: 1.5322\n",
      "111/388, train_loss: 0.2159, step time: 1.5392\n",
      "112/388, train_loss: 0.1967, step time: 1.5473\n",
      "113/388, train_loss: 0.1270, step time: 1.5301\n",
      "114/388, train_loss: 0.1459, step time: 1.5324\n",
      "115/388, train_loss: 0.2765, step time: 1.5326\n",
      "116/388, train_loss: 0.1277, step time: 1.5456\n",
      "117/388, train_loss: 0.1011, step time: 1.5321\n",
      "118/388, train_loss: 0.0912, step time: 1.5299\n",
      "119/388, train_loss: 0.1690, step time: 1.5327\n",
      "120/388, train_loss: 0.1982, step time: 1.5438\n",
      "121/388, train_loss: 0.1970, step time: 1.5366\n",
      "122/388, train_loss: 0.1580, step time: 1.5348\n",
      "123/388, train_loss: 0.1400, step time: 1.5361\n",
      "124/388, train_loss: 0.0620, step time: 1.5409\n",
      "125/388, train_loss: 0.0715, step time: 1.5399\n",
      "126/388, train_loss: 0.0990, step time: 1.5338\n",
      "127/388, train_loss: 0.2765, step time: 1.5349\n",
      "128/388, train_loss: 0.0661, step time: 1.5485\n",
      "129/388, train_loss: 0.1178, step time: 1.5301\n",
      "130/388, train_loss: 0.1760, step time: 1.5288\n",
      "131/388, train_loss: 0.1663, step time: 1.5337\n",
      "132/388, train_loss: 0.1002, step time: 1.5425\n",
      "133/388, train_loss: 0.0578, step time: 1.5371\n",
      "134/388, train_loss: 0.1373, step time: 1.5359\n",
      "135/388, train_loss: 0.0942, step time: 1.5329\n",
      "136/388, train_loss: 0.1159, step time: 1.5439\n",
      "137/388, train_loss: 0.0655, step time: 1.5301\n",
      "138/388, train_loss: 0.1451, step time: 1.5287\n",
      "139/388, train_loss: 0.1085, step time: 1.5329\n",
      "140/388, train_loss: 0.0982, step time: 1.5495\n",
      "141/388, train_loss: 0.3408, step time: 1.5298\n",
      "142/388, train_loss: 0.1927, step time: 1.5309\n",
      "143/388, train_loss: 0.0769, step time: 1.5343\n",
      "144/388, train_loss: 0.0628, step time: 1.5428\n",
      "145/388, train_loss: 0.1139, step time: 1.5461\n",
      "146/388, train_loss: 0.0830, step time: 1.5274\n",
      "147/388, train_loss: 0.0879, step time: 1.5309\n",
      "148/388, train_loss: 0.1481, step time: 1.5443\n",
      "149/388, train_loss: 0.0763, step time: 1.5332\n",
      "150/388, train_loss: 0.2404, step time: 1.5326\n",
      "151/388, train_loss: 0.1331, step time: 1.5329\n",
      "152/388, train_loss: 0.2194, step time: 1.5475\n",
      "153/388, train_loss: 0.0873, step time: 1.5377\n",
      "154/388, train_loss: 0.2780, step time: 1.5365\n",
      "155/388, train_loss: 0.1645, step time: 1.5385\n",
      "156/388, train_loss: 0.0498, step time: 1.5475\n",
      "157/388, train_loss: 0.2136, step time: 1.5412\n",
      "158/388, train_loss: 0.0937, step time: 1.5320\n",
      "159/388, train_loss: 0.1538, step time: 1.5350\n",
      "160/388, train_loss: 0.0820, step time: 1.5467\n",
      "161/388, train_loss: 0.1939, step time: 1.5279\n",
      "162/388, train_loss: 0.1310, step time: 1.5436\n",
      "163/388, train_loss: 0.0315, step time: 1.5309\n",
      "164/388, train_loss: 0.0920, step time: 1.5479\n",
      "165/388, train_loss: 0.2092, step time: 1.5300\n",
      "166/388, train_loss: 0.1278, step time: 1.5284\n",
      "167/388, train_loss: 0.1940, step time: 1.5317\n",
      "168/388, train_loss: 0.0609, step time: 1.5433\n",
      "169/388, train_loss: 0.1189, step time: 1.5363\n",
      "170/388, train_loss: 0.0674, step time: 1.5499\n",
      "171/388, train_loss: 0.1627, step time: 1.5328\n",
      "172/388, train_loss: 0.1820, step time: 1.5428\n",
      "173/388, train_loss: 0.0997, step time: 1.5345\n",
      "174/388, train_loss: 0.1345, step time: 1.5337\n",
      "175/388, train_loss: 0.4939, step time: 1.5330\n",
      "176/388, train_loss: 0.0827, step time: 1.5488\n",
      "177/388, train_loss: 0.1660, step time: 1.5320\n",
      "178/388, train_loss: 0.0667, step time: 1.5323\n",
      "179/388, train_loss: 0.1859, step time: 1.5526\n",
      "180/388, train_loss: 0.1628, step time: 1.5466\n",
      "181/388, train_loss: 0.2008, step time: 1.5288\n",
      "182/388, train_loss: 0.1677, step time: 1.5359\n",
      "183/388, train_loss: 0.1149, step time: 1.5325\n",
      "184/388, train_loss: 0.2638, step time: 1.5463\n",
      "185/388, train_loss: 0.1574, step time: 1.5331\n",
      "186/388, train_loss: 0.0883, step time: 1.5382\n",
      "187/388, train_loss: 0.1169, step time: 1.5432\n",
      "188/388, train_loss: 0.1137, step time: 1.5472\n",
      "189/388, train_loss: 0.0543, step time: 1.5416\n",
      "190/388, train_loss: 0.0367, step time: 1.5322\n",
      "191/388, train_loss: 0.2018, step time: 1.5438\n",
      "192/388, train_loss: 0.1861, step time: 1.5434\n",
      "193/388, train_loss: 0.0647, step time: 1.5408\n",
      "194/388, train_loss: 0.1506, step time: 1.5368\n",
      "195/388, train_loss: 0.1033, step time: 1.5381\n",
      "196/388, train_loss: 0.2946, step time: 1.5456\n",
      "197/388, train_loss: 0.1131, step time: 1.5521\n",
      "198/388, train_loss: 0.0988, step time: 1.5487\n",
      "199/388, train_loss: 0.1034, step time: 1.5409\n",
      "200/388, train_loss: 0.2257, step time: 1.5475\n",
      "201/388, train_loss: 0.1588, step time: 1.5501\n",
      "202/388, train_loss: 0.3742, step time: 1.5348\n",
      "203/388, train_loss: 0.1458, step time: 1.5304\n",
      "204/388, train_loss: 0.0737, step time: 1.5459\n",
      "205/388, train_loss: 0.1250, step time: 1.5443\n",
      "206/388, train_loss: 0.1377, step time: 1.5477\n",
      "207/388, train_loss: 0.0961, step time: 1.5515\n",
      "208/388, train_loss: 0.1425, step time: 1.5464\n",
      "209/388, train_loss: 0.2149, step time: 1.5412\n",
      "210/388, train_loss: 0.1555, step time: 1.5548\n",
      "211/388, train_loss: 0.2409, step time: 1.5355\n",
      "212/388, train_loss: 0.1821, step time: 1.5430\n",
      "213/388, train_loss: 0.1798, step time: 1.5570\n",
      "214/388, train_loss: 0.3974, step time: 1.5492\n",
      "215/388, train_loss: 0.2367, step time: 1.5348\n",
      "216/388, train_loss: 0.2413, step time: 1.5479\n",
      "217/388, train_loss: 0.0765, step time: 1.5490\n",
      "218/388, train_loss: 0.0871, step time: 1.5466\n",
      "219/388, train_loss: 0.2203, step time: 1.5350\n",
      "220/388, train_loss: 0.1507, step time: 1.5454\n",
      "221/388, train_loss: 0.1525, step time: 1.5430\n",
      "222/388, train_loss: 0.1248, step time: 1.5460\n",
      "223/388, train_loss: 0.3127, step time: 1.5406\n",
      "224/388, train_loss: 0.0716, step time: 1.5436\n",
      "225/388, train_loss: 0.1233, step time: 1.5500\n",
      "226/388, train_loss: 0.1945, step time: 1.5499\n",
      "227/388, train_loss: 0.0715, step time: 1.5327\n",
      "228/388, train_loss: 0.1155, step time: 1.5411\n",
      "229/388, train_loss: 0.1737, step time: 1.5453\n",
      "230/388, train_loss: 0.2991, step time: 1.5429\n",
      "231/388, train_loss: 0.1609, step time: 1.5404\n",
      "232/388, train_loss: 0.0627, step time: 1.5483\n",
      "233/388, train_loss: 0.0984, step time: 1.5426\n",
      "234/388, train_loss: 0.0668, step time: 1.5437\n",
      "235/388, train_loss: 0.2175, step time: 1.5427\n",
      "236/388, train_loss: 0.1981, step time: 1.5484\n",
      "237/388, train_loss: 0.1482, step time: 1.5377\n",
      "238/388, train_loss: 0.0758, step time: 1.5485\n",
      "239/388, train_loss: 0.0761, step time: 1.5402\n",
      "240/388, train_loss: 0.0753, step time: 1.5527\n",
      "241/388, train_loss: 0.1619, step time: 1.5464\n",
      "242/388, train_loss: 0.3019, step time: 1.5471\n",
      "243/388, train_loss: 0.1771, step time: 1.5346\n",
      "244/388, train_loss: 0.1119, step time: 1.5417\n",
      "245/388, train_loss: 0.1019, step time: 1.5469\n",
      "246/388, train_loss: 0.3183, step time: 1.5536\n",
      "247/388, train_loss: 0.2194, step time: 1.5431\n",
      "248/388, train_loss: 0.0627, step time: 1.5482\n",
      "249/388, train_loss: 0.3076, step time: 1.5453\n",
      "250/388, train_loss: 0.1908, step time: 1.5625\n",
      "251/388, train_loss: 0.1132, step time: 1.5406\n",
      "252/388, train_loss: 0.2226, step time: 1.5396\n",
      "253/388, train_loss: 0.2100, step time: 1.5395\n",
      "254/388, train_loss: 0.0861, step time: 1.5472\n",
      "255/388, train_loss: 0.2100, step time: 1.5485\n",
      "256/388, train_loss: 0.0843, step time: 1.5429\n",
      "257/388, train_loss: 0.0786, step time: 1.5477\n",
      "258/388, train_loss: 0.2113, step time: 1.5472\n",
      "259/388, train_loss: 0.2194, step time: 1.5409\n",
      "260/388, train_loss: 0.1418, step time: 1.5478\n",
      "261/388, train_loss: 0.0895, step time: 1.5484\n",
      "262/388, train_loss: 0.1268, step time: 1.5450\n",
      "263/388, train_loss: 0.1660, step time: 1.5521\n",
      "264/388, train_loss: 0.1489, step time: 1.5434\n",
      "265/388, train_loss: 0.1144, step time: 1.5467\n",
      "266/388, train_loss: 0.1418, step time: 1.5487\n",
      "267/388, train_loss: 0.2050, step time: 1.5433\n",
      "268/388, train_loss: 0.1504, step time: 1.5466\n",
      "269/388, train_loss: 0.1569, step time: 1.5458\n",
      "270/388, train_loss: 0.1584, step time: 1.5460\n",
      "271/388, train_loss: 0.1990, step time: 1.5449\n",
      "272/388, train_loss: 0.4051, step time: 1.5467\n",
      "273/388, train_loss: 0.1609, step time: 1.5426\n",
      "274/388, train_loss: 0.2971, step time: 1.5479\n",
      "275/388, train_loss: 0.0902, step time: 1.5454\n",
      "276/388, train_loss: 0.1612, step time: 1.5438\n",
      "277/388, train_loss: 0.2235, step time: 1.5450\n",
      "278/388, train_loss: 0.2598, step time: 1.5493\n",
      "279/388, train_loss: 0.2699, step time: 1.5432\n",
      "280/388, train_loss: 0.0995, step time: 1.5477\n",
      "281/388, train_loss: 0.0465, step time: 1.5502\n",
      "282/388, train_loss: 0.1177, step time: 1.5423\n",
      "283/388, train_loss: 0.1747, step time: 1.5440\n",
      "284/388, train_loss: 0.0866, step time: 1.5499\n",
      "285/388, train_loss: 0.2107, step time: 1.5453\n",
      "286/388, train_loss: 0.0769, step time: 1.5483\n",
      "287/388, train_loss: 0.1841, step time: 1.5356\n",
      "288/388, train_loss: 0.1447, step time: 1.5458\n",
      "289/388, train_loss: 0.0942, step time: 1.5443\n",
      "290/388, train_loss: 0.2101, step time: 1.5455\n",
      "291/388, train_loss: 0.1028, step time: 1.5388\n",
      "292/388, train_loss: 0.1111, step time: 1.5442\n",
      "293/388, train_loss: 0.1020, step time: 1.5466\n",
      "294/388, train_loss: 0.0454, step time: 1.5502\n",
      "295/388, train_loss: 0.2008, step time: 1.5341\n",
      "296/388, train_loss: 0.2657, step time: 1.5579\n",
      "297/388, train_loss: 0.0834, step time: 1.5368\n",
      "298/388, train_loss: 0.0913, step time: 1.5441\n",
      "299/388, train_loss: 0.1554, step time: 1.5430\n",
      "300/388, train_loss: 0.1196, step time: 1.5480\n",
      "301/388, train_loss: 0.1249, step time: 1.5459\n",
      "302/388, train_loss: 0.3094, step time: 1.5508\n",
      "303/388, train_loss: 0.1814, step time: 1.5383\n",
      "304/388, train_loss: 0.1059, step time: 1.5472\n",
      "305/388, train_loss: 0.3179, step time: 1.5493\n",
      "306/388, train_loss: 0.0732, step time: 1.5426\n",
      "307/388, train_loss: 0.0970, step time: 1.5290\n",
      "308/388, train_loss: 0.2984, step time: 1.5494\n",
      "309/388, train_loss: 0.0900, step time: 1.5457\n",
      "310/388, train_loss: 0.2671, step time: 1.5440\n",
      "311/388, train_loss: 0.4211, step time: 1.5412\n",
      "312/388, train_loss: 0.2815, step time: 1.5548\n",
      "313/388, train_loss: 0.1967, step time: 1.5431\n",
      "314/388, train_loss: 0.0508, step time: 1.5480\n",
      "315/388, train_loss: 0.0881, step time: 1.5405\n",
      "316/388, train_loss: 0.1208, step time: 1.5422\n",
      "317/388, train_loss: 0.2646, step time: 1.5475\n",
      "318/388, train_loss: 0.3309, step time: 1.5474\n",
      "319/388, train_loss: 0.2500, step time: 1.5274\n",
      "320/388, train_loss: 0.0975, step time: 1.5423\n",
      "321/388, train_loss: 0.0681, step time: 1.5487\n",
      "322/388, train_loss: 0.1778, step time: 1.5487\n",
      "323/388, train_loss: 0.0836, step time: 1.5330\n",
      "324/388, train_loss: 0.0872, step time: 1.5456\n",
      "325/388, train_loss: 0.2704, step time: 1.5470\n",
      "326/388, train_loss: 0.1883, step time: 1.5433\n",
      "327/388, train_loss: 0.0997, step time: 1.5382\n",
      "328/388, train_loss: 0.1049, step time: 1.5466\n",
      "329/388, train_loss: 0.2180, step time: 1.5416\n",
      "330/388, train_loss: 0.3527, step time: 1.5444\n",
      "331/388, train_loss: 0.1834, step time: 1.5385\n",
      "332/388, train_loss: 0.2752, step time: 1.5481\n",
      "333/388, train_loss: 0.0889, step time: 1.5422\n",
      "334/388, train_loss: 0.1633, step time: 1.5464\n",
      "335/388, train_loss: 0.2206, step time: 1.5353\n",
      "336/388, train_loss: 0.1003, step time: 1.5430\n",
      "337/388, train_loss: 0.1039, step time: 1.5475\n",
      "338/388, train_loss: 0.1776, step time: 1.5488\n",
      "339/388, train_loss: 0.2254, step time: 1.5313\n",
      "340/388, train_loss: 0.3800, step time: 1.5493\n",
      "341/388, train_loss: 0.1284, step time: 1.5451\n",
      "342/388, train_loss: 0.1367, step time: 1.5432\n",
      "343/388, train_loss: 0.1963, step time: 1.5377\n",
      "344/388, train_loss: 0.0873, step time: 1.5625\n",
      "345/388, train_loss: 0.1984, step time: 1.5417\n",
      "346/388, train_loss: 0.1586, step time: 1.5474\n",
      "347/388, train_loss: 0.0942, step time: 1.5375\n",
      "348/388, train_loss: 0.2363, step time: 1.5532\n",
      "349/388, train_loss: 0.1191, step time: 1.5476\n",
      "350/388, train_loss: 0.1416, step time: 1.5475\n",
      "351/388, train_loss: 0.1465, step time: 1.5375\n",
      "352/388, train_loss: 0.1207, step time: 1.5557\n",
      "353/388, train_loss: 0.0663, step time: 1.5458\n",
      "354/388, train_loss: 0.1838, step time: 1.5425\n",
      "355/388, train_loss: 0.1115, step time: 1.5455\n",
      "356/388, train_loss: 0.0713, step time: 1.5453\n",
      "357/388, train_loss: 0.1454, step time: 1.5460\n",
      "358/388, train_loss: 0.2133, step time: 1.5495\n",
      "359/388, train_loss: 0.1634, step time: 1.5298\n",
      "360/388, train_loss: 0.1910, step time: 1.5474\n",
      "361/388, train_loss: 0.0758, step time: 1.5481\n",
      "362/388, train_loss: 0.0619, step time: 1.5447\n",
      "363/388, train_loss: 0.0694, step time: 1.5351\n",
      "364/388, train_loss: 0.0574, step time: 1.5478\n",
      "365/388, train_loss: 0.1587, step time: 1.5464\n",
      "366/388, train_loss: 0.0332, step time: 1.5429\n",
      "367/388, train_loss: 0.1008, step time: 1.5400\n",
      "368/388, train_loss: 0.2368, step time: 1.5467\n",
      "369/388, train_loss: 0.1245, step time: 1.5439\n",
      "370/388, train_loss: 0.1469, step time: 1.5478\n",
      "371/388, train_loss: 0.1034, step time: 1.5327\n",
      "372/388, train_loss: 0.0526, step time: 1.5512\n",
      "373/388, train_loss: 0.0948, step time: 1.5408\n",
      "374/388, train_loss: 0.1686, step time: 1.5457\n",
      "375/388, train_loss: 0.1102, step time: 1.5401\n",
      "376/388, train_loss: 0.1998, step time: 1.5540\n",
      "377/388, train_loss: 0.3994, step time: 1.5479\n",
      "378/388, train_loss: 0.2873, step time: 1.5624\n",
      "379/388, train_loss: 0.0461, step time: 1.5384\n",
      "380/388, train_loss: 0.1677, step time: 1.5468\n",
      "381/388, train_loss: 0.1704, step time: 1.5403\n",
      "382/388, train_loss: 0.0911, step time: 1.5491\n",
      "383/388, train_loss: 0.1188, step time: 1.5373\n",
      "384/388, train_loss: 0.2831, step time: 1.5421\n",
      "385/388, train_loss: 0.3856, step time: 1.5483\n",
      "386/388, train_loss: 0.0750, step time: 1.5438\n",
      "387/388, train_loss: 0.2034, step time: 1.5355\n",
      "388/388, train_loss: 0.1548, step time: 1.5468\n",
      "epoch 87 average loss: 0.1586\n",
      "current epoch: 87 current mean dice: 0.7804 tc: 0.8285 wt: 0.9067 et: 0.6061\n",
      "best mean dice: 0.7817 at epoch: 84\n",
      "time consuming of epoch 87 is: 706.7143\n",
      "----------\n",
      "epoch 88/100\n",
      "1/388, train_loss: 0.0846, step time: 1.5779\n",
      "2/388, train_loss: 0.0590, step time: 1.5641\n",
      "3/388, train_loss: 0.3081, step time: 1.5451\n",
      "4/388, train_loss: 0.1796, step time: 1.5427\n",
      "5/388, train_loss: 0.1177, step time: 1.5455\n",
      "6/388, train_loss: 0.3389, step time: 1.5492\n",
      "7/388, train_loss: 0.3556, step time: 1.5618\n",
      "8/388, train_loss: 0.1623, step time: 1.5465\n",
      "9/388, train_loss: 0.0865, step time: 1.5501\n",
      "10/388, train_loss: 0.1111, step time: 1.5420\n",
      "11/388, train_loss: 0.1016, step time: 1.5533\n",
      "12/388, train_loss: 0.1423, step time: 1.5401\n",
      "13/388, train_loss: 0.1089, step time: 1.5398\n",
      "14/388, train_loss: 0.1038, step time: 1.5457\n",
      "15/388, train_loss: 0.1619, step time: 1.5415\n",
      "16/388, train_loss: 0.0709, step time: 1.5401\n",
      "17/388, train_loss: 0.2466, step time: 1.5477\n",
      "18/388, train_loss: 0.2144, step time: 1.5430\n",
      "19/388, train_loss: 0.0710, step time: 1.5388\n",
      "20/388, train_loss: 0.0995, step time: 1.5444\n",
      "21/388, train_loss: 0.5303, step time: 1.5583\n",
      "22/388, train_loss: 0.0798, step time: 1.5424\n",
      "23/388, train_loss: 0.1365, step time: 1.5490\n",
      "24/388, train_loss: 0.1156, step time: 1.5453\n",
      "25/388, train_loss: 0.3447, step time: 1.5498\n",
      "26/388, train_loss: 0.1287, step time: 1.5453\n",
      "27/388, train_loss: 0.2749, step time: 1.5583\n",
      "28/388, train_loss: 0.1363, step time: 1.5467\n",
      "29/388, train_loss: 0.1053, step time: 1.5423\n",
      "30/388, train_loss: 0.0920, step time: 1.5485\n",
      "31/388, train_loss: 0.0441, step time: 1.5487\n",
      "32/388, train_loss: 0.2846, step time: 1.5436\n",
      "33/388, train_loss: 0.1243, step time: 1.5477\n",
      "34/388, train_loss: 0.1758, step time: 1.5480\n",
      "35/388, train_loss: 0.0440, step time: 1.5407\n",
      "36/388, train_loss: 0.0693, step time: 1.5490\n",
      "37/388, train_loss: 0.1415, step time: 1.5417\n",
      "38/388, train_loss: 0.0674, step time: 1.5453\n",
      "39/388, train_loss: 0.1067, step time: 1.5468\n",
      "40/388, train_loss: 0.0855, step time: 1.5447\n",
      "41/388, train_loss: 0.1607, step time: 1.5415\n",
      "42/388, train_loss: 0.1154, step time: 1.5490\n",
      "43/388, train_loss: 0.2905, step time: 1.5623\n",
      "44/388, train_loss: 0.3325, step time: 1.5495\n",
      "45/388, train_loss: 0.1699, step time: 1.5457\n",
      "46/388, train_loss: 0.1515, step time: 1.5441\n",
      "47/388, train_loss: 0.0814, step time: 1.5647\n",
      "48/388, train_loss: 0.1844, step time: 1.5444\n",
      "49/388, train_loss: 0.1278, step time: 1.5443\n",
      "50/388, train_loss: 0.0486, step time: 1.5312\n",
      "51/388, train_loss: 0.1922, step time: 1.5533\n",
      "52/388, train_loss: 0.0777, step time: 1.5460\n",
      "53/388, train_loss: 0.0506, step time: 1.5486\n",
      "54/388, train_loss: 0.1705, step time: 1.5363\n",
      "55/388, train_loss: 0.1786, step time: 1.5479\n",
      "56/388, train_loss: 0.2222, step time: 1.5475\n",
      "57/388, train_loss: 0.1064, step time: 1.5493\n",
      "58/388, train_loss: 0.0969, step time: 1.5278\n",
      "59/388, train_loss: 0.1654, step time: 1.5603\n",
      "60/388, train_loss: 0.3984, step time: 1.5470\n",
      "61/388, train_loss: 0.1382, step time: 1.5452\n",
      "62/388, train_loss: 0.1182, step time: 1.5321\n",
      "63/388, train_loss: 0.1326, step time: 1.5465\n",
      "64/388, train_loss: 0.1960, step time: 1.5536\n",
      "65/388, train_loss: 0.1182, step time: 1.5459\n",
      "66/388, train_loss: 0.1489, step time: 1.5338\n",
      "67/388, train_loss: 0.2501, step time: 1.5387\n",
      "68/388, train_loss: 0.1362, step time: 1.5457\n",
      "69/388, train_loss: 0.3572, step time: 1.5476\n",
      "70/388, train_loss: 0.3133, step time: 1.5309\n",
      "71/388, train_loss: 0.0899, step time: 1.5559\n",
      "72/388, train_loss: 0.0932, step time: 1.5537\n",
      "73/388, train_loss: 0.2741, step time: 1.5443\n",
      "74/388, train_loss: 0.2278, step time: 1.5526\n",
      "75/388, train_loss: 0.2013, step time: 1.5564\n",
      "76/388, train_loss: 0.0829, step time: 1.5512\n",
      "77/388, train_loss: 0.1252, step time: 1.5485\n",
      "78/388, train_loss: 0.1005, step time: 1.5432\n",
      "79/388, train_loss: 0.0826, step time: 1.5527\n",
      "80/388, train_loss: 0.1884, step time: 1.5460\n",
      "81/388, train_loss: 0.1567, step time: 1.5548\n",
      "82/388, train_loss: 0.0931, step time: 1.5403\n",
      "83/388, train_loss: 0.0979, step time: 1.5457\n",
      "84/388, train_loss: 0.3113, step time: 1.5481\n",
      "85/388, train_loss: 0.1130, step time: 1.5509\n",
      "86/388, train_loss: 0.0661, step time: 1.5446\n",
      "87/388, train_loss: 0.1633, step time: 1.5425\n",
      "88/388, train_loss: 0.4143, step time: 1.5435\n",
      "89/388, train_loss: 0.1915, step time: 1.5514\n",
      "90/388, train_loss: 0.2894, step time: 1.5396\n",
      "91/388, train_loss: 0.0787, step time: 1.5404\n",
      "92/388, train_loss: 0.1980, step time: 1.5511\n",
      "93/388, train_loss: 0.1191, step time: 1.5430\n",
      "94/388, train_loss: 0.2693, step time: 1.5439\n",
      "95/388, train_loss: 0.2432, step time: 1.5437\n",
      "96/388, train_loss: 0.0667, step time: 1.5420\n",
      "97/388, train_loss: 0.1855, step time: 1.5420\n",
      "98/388, train_loss: 0.1224, step time: 1.5469\n",
      "99/388, train_loss: 0.1708, step time: 1.5373\n",
      "100/388, train_loss: 0.2368, step time: 1.5396\n",
      "101/388, train_loss: 0.2656, step time: 1.5443\n",
      "102/388, train_loss: 0.1407, step time: 1.5431\n",
      "103/388, train_loss: 0.0957, step time: 1.5439\n",
      "104/388, train_loss: 0.1173, step time: 1.5474\n",
      "105/388, train_loss: 0.1607, step time: 1.5495\n",
      "106/388, train_loss: 0.1368, step time: 1.5330\n",
      "107/388, train_loss: 0.0800, step time: 1.5487\n",
      "108/388, train_loss: 0.0875, step time: 1.5489\n",
      "109/388, train_loss: 0.2235, step time: 1.5421\n",
      "110/388, train_loss: 0.0713, step time: 1.5339\n",
      "111/388, train_loss: 0.1263, step time: 1.5487\n",
      "112/388, train_loss: 0.0790, step time: 1.5468\n",
      "113/388, train_loss: 0.0937, step time: 1.5605\n",
      "114/388, train_loss: 0.0796, step time: 1.5327\n",
      "115/388, train_loss: 0.2303, step time: 1.5405\n",
      "116/388, train_loss: 0.0971, step time: 1.5485\n",
      "117/388, train_loss: 0.1502, step time: 1.5437\n",
      "118/388, train_loss: 0.2929, step time: 1.5310\n",
      "119/388, train_loss: 0.2860, step time: 1.5480\n",
      "120/388, train_loss: 0.2129, step time: 1.5452\n",
      "121/388, train_loss: 0.1621, step time: 1.5465\n",
      "122/388, train_loss: 0.1990, step time: 1.5370\n",
      "123/388, train_loss: 0.0843, step time: 1.5475\n",
      "124/388, train_loss: 0.2185, step time: 1.5445\n",
      "125/388, train_loss: 0.1685, step time: 1.5545\n",
      "126/388, train_loss: 0.1197, step time: 1.5453\n",
      "127/388, train_loss: 0.1984, step time: 1.5458\n",
      "128/388, train_loss: 0.2591, step time: 1.5470\n",
      "129/388, train_loss: 0.1519, step time: 1.5399\n",
      "130/388, train_loss: 0.1932, step time: 1.5449\n",
      "131/388, train_loss: 0.2288, step time: 1.5460\n",
      "132/388, train_loss: 0.0957, step time: 1.5449\n",
      "133/388, train_loss: 0.2224, step time: 1.5644\n",
      "134/388, train_loss: 0.1320, step time: 1.5448\n",
      "135/388, train_loss: 0.0849, step time: 1.5454\n",
      "136/388, train_loss: 0.1752, step time: 1.5469\n",
      "137/388, train_loss: 0.2015, step time: 1.5567\n",
      "138/388, train_loss: 0.0930, step time: 1.5520\n",
      "139/388, train_loss: 0.5010, step time: 1.5456\n",
      "140/388, train_loss: 0.1561, step time: 1.5424\n",
      "141/388, train_loss: 0.1271, step time: 1.5605\n",
      "142/388, train_loss: 0.2442, step time: 1.5333\n",
      "143/388, train_loss: 0.1493, step time: 1.5434\n",
      "144/388, train_loss: 0.0584, step time: 1.5503\n",
      "145/388, train_loss: 0.1558, step time: 1.5426\n",
      "146/388, train_loss: 0.1784, step time: 1.5428\n",
      "147/388, train_loss: 0.2099, step time: 1.5496\n",
      "148/388, train_loss: 0.2740, step time: 1.5478\n",
      "149/388, train_loss: 0.1049, step time: 1.5423\n",
      "150/388, train_loss: 0.1627, step time: 1.5489\n",
      "151/388, train_loss: 0.0623, step time: 1.5464\n",
      "152/388, train_loss: 0.0624, step time: 1.5402\n",
      "153/388, train_loss: 0.1225, step time: 1.5632\n",
      "154/388, train_loss: 0.1354, step time: 1.5438\n",
      "155/388, train_loss: 0.0721, step time: 1.5443\n",
      "156/388, train_loss: 0.0521, step time: 1.5505\n",
      "157/388, train_loss: 0.0546, step time: 1.5465\n",
      "158/388, train_loss: 0.0881, step time: 1.5473\n",
      "159/388, train_loss: 0.1256, step time: 1.5456\n",
      "160/388, train_loss: 0.2046, step time: 1.5457\n",
      "161/388, train_loss: 0.2046, step time: 1.5506\n",
      "162/388, train_loss: 0.2286, step time: 1.5472\n",
      "163/388, train_loss: 0.0920, step time: 1.5443\n",
      "164/388, train_loss: 0.3954, step time: 1.5519\n",
      "165/388, train_loss: 0.2129, step time: 1.5407\n",
      "166/388, train_loss: 0.0999, step time: 1.5467\n",
      "167/388, train_loss: 0.1650, step time: 1.5430\n",
      "168/388, train_loss: 0.0990, step time: 1.5562\n",
      "169/388, train_loss: 0.0756, step time: 1.5462\n",
      "170/388, train_loss: 0.1377, step time: 1.5515\n",
      "171/388, train_loss: 0.1119, step time: 1.5496\n",
      "172/388, train_loss: 0.1967, step time: 1.5621\n",
      "173/388, train_loss: 0.0745, step time: 1.5517\n",
      "174/388, train_loss: 0.2194, step time: 1.5599\n",
      "175/388, train_loss: 0.0984, step time: 1.5531\n",
      "176/388, train_loss: 0.2270, step time: 1.5656\n",
      "177/388, train_loss: 0.3134, step time: 1.5425\n",
      "178/388, train_loss: 0.1110, step time: 1.5393\n",
      "179/388, train_loss: 0.0691, step time: 1.5463\n",
      "180/388, train_loss: 0.2053, step time: 1.5463\n",
      "181/388, train_loss: 0.1493, step time: 1.5418\n",
      "182/388, train_loss: 0.1173, step time: 1.5485\n",
      "183/388, train_loss: 0.1317, step time: 1.5469\n",
      "184/388, train_loss: 0.0885, step time: 1.5445\n",
      "185/388, train_loss: 0.1405, step time: 1.5476\n",
      "186/388, train_loss: 0.2529, step time: 1.5479\n",
      "187/388, train_loss: 0.0781, step time: 1.5648\n",
      "188/388, train_loss: 0.1843, step time: 1.5447\n",
      "189/388, train_loss: 0.2051, step time: 1.5473\n",
      "190/388, train_loss: 0.1961, step time: 1.5465\n",
      "191/388, train_loss: 0.1478, step time: 1.5427\n",
      "192/388, train_loss: 0.1365, step time: 1.5411\n",
      "193/388, train_loss: 0.1157, step time: 1.5431\n",
      "194/388, train_loss: 0.0836, step time: 1.5399\n",
      "195/388, train_loss: 0.2423, step time: 1.5423\n",
      "196/388, train_loss: 0.1364, step time: 1.5534\n",
      "197/388, train_loss: 0.0972, step time: 1.5470\n",
      "198/388, train_loss: 0.0460, step time: 1.5490\n",
      "199/388, train_loss: 0.1554, step time: 1.5547\n",
      "200/388, train_loss: 0.1929, step time: 1.5423\n",
      "201/388, train_loss: 0.0964, step time: 1.5616\n",
      "202/388, train_loss: 0.0863, step time: 1.5434\n",
      "203/388, train_loss: 0.3961, step time: 1.5481\n",
      "204/388, train_loss: 0.0787, step time: 1.5462\n",
      "205/388, train_loss: 0.0998, step time: 1.5433\n",
      "206/388, train_loss: 0.0704, step time: 1.5469\n",
      "207/388, train_loss: 0.0882, step time: 1.5415\n",
      "208/388, train_loss: 0.2070, step time: 1.5419\n",
      "209/388, train_loss: 0.0875, step time: 1.5491\n",
      "210/388, train_loss: 0.0934, step time: 1.5462\n",
      "211/388, train_loss: 0.2480, step time: 1.5482\n",
      "212/388, train_loss: 0.1472, step time: 1.5465\n",
      "213/388, train_loss: 0.0624, step time: 1.5577\n",
      "214/388, train_loss: 0.1567, step time: 1.5493\n",
      "215/388, train_loss: 0.2028, step time: 1.5508\n",
      "216/388, train_loss: 0.0729, step time: 1.5483\n",
      "217/388, train_loss: 0.1675, step time: 1.5721\n",
      "218/388, train_loss: 0.1948, step time: 1.5487\n",
      "219/388, train_loss: 0.0777, step time: 1.5490\n",
      "220/388, train_loss: 0.1437, step time: 1.5435\n",
      "221/388, train_loss: 0.1855, step time: 1.5494\n",
      "222/388, train_loss: 0.2475, step time: 1.5422\n",
      "223/388, train_loss: 0.1321, step time: 1.5536\n",
      "224/388, train_loss: 0.1007, step time: 1.5511\n",
      "225/388, train_loss: 0.1863, step time: 1.5442\n",
      "226/388, train_loss: 0.2357, step time: 1.5494\n",
      "227/388, train_loss: 0.3461, step time: 1.5538\n",
      "228/388, train_loss: 0.0977, step time: 1.5520\n",
      "229/388, train_loss: 0.0843, step time: 1.5602\n",
      "230/388, train_loss: 0.1559, step time: 1.5433\n",
      "231/388, train_loss: 0.1484, step time: 1.5532\n",
      "232/388, train_loss: 0.0982, step time: 1.5431\n",
      "233/388, train_loss: 0.1838, step time: 1.5514\n",
      "234/388, train_loss: 0.1814, step time: 1.5480\n",
      "235/388, train_loss: 0.1045, step time: 1.5613\n",
      "236/388, train_loss: 0.0651, step time: 1.5685\n",
      "237/388, train_loss: 0.2110, step time: 1.5566\n",
      "238/388, train_loss: 0.1228, step time: 1.5742\n",
      "239/388, train_loss: 0.1080, step time: 1.5585\n",
      "240/388, train_loss: 0.0885, step time: 1.5489\n",
      "241/388, train_loss: 0.1561, step time: 1.5414\n",
      "242/388, train_loss: 0.2617, step time: 1.5494\n",
      "243/388, train_loss: 0.2057, step time: 1.5417\n",
      "244/388, train_loss: 0.1829, step time: 1.5425\n",
      "245/388, train_loss: 0.0872, step time: 1.5591\n",
      "246/388, train_loss: 0.2263, step time: 1.5424\n",
      "247/388, train_loss: 0.1445, step time: 1.5487\n",
      "248/388, train_loss: 0.1955, step time: 1.5485\n",
      "249/388, train_loss: 0.0409, step time: 1.5395\n",
      "250/388, train_loss: 0.1491, step time: 1.5470\n",
      "251/388, train_loss: 0.0977, step time: 1.5505\n",
      "252/388, train_loss: 0.1306, step time: 1.5395\n",
      "253/388, train_loss: 0.1188, step time: 1.5461\n",
      "254/388, train_loss: 0.1102, step time: 1.5494\n",
      "255/388, train_loss: 0.2622, step time: 1.5406\n",
      "256/388, train_loss: 0.0860, step time: 1.5451\n",
      "257/388, train_loss: 0.1486, step time: 1.5450\n",
      "258/388, train_loss: 0.3275, step time: 1.5407\n",
      "259/388, train_loss: 0.0805, step time: 1.5440\n",
      "260/388, train_loss: 0.3736, step time: 1.5418\n",
      "261/388, train_loss: 0.1887, step time: 1.5538\n",
      "262/388, train_loss: 0.1193, step time: 1.5477\n",
      "263/388, train_loss: 0.0737, step time: 1.5423\n",
      "264/388, train_loss: 0.1765, step time: 1.5514\n",
      "265/388, train_loss: 0.1023, step time: 1.5467\n",
      "266/388, train_loss: 0.1002, step time: 1.5488\n",
      "267/388, train_loss: 0.2341, step time: 1.5452\n",
      "268/388, train_loss: 0.2123, step time: 1.5425\n",
      "269/388, train_loss: 0.2282, step time: 1.5558\n",
      "270/388, train_loss: 0.2398, step time: 1.5433\n",
      "271/388, train_loss: 0.0851, step time: 1.5458\n",
      "272/388, train_loss: 0.1662, step time: 1.5495\n",
      "273/388, train_loss: 0.1342, step time: 1.5460\n",
      "274/388, train_loss: 0.1342, step time: 1.5487\n",
      "275/388, train_loss: 0.2434, step time: 1.5536\n",
      "276/388, train_loss: 0.2341, step time: 1.5573\n",
      "277/388, train_loss: 0.0886, step time: 1.5457\n",
      "278/388, train_loss: 0.0654, step time: 1.5471\n",
      "279/388, train_loss: 0.1594, step time: 1.5536\n",
      "280/388, train_loss: 0.4425, step time: 1.5409\n",
      "281/388, train_loss: 0.3413, step time: 1.5554\n",
      "282/388, train_loss: 0.0642, step time: 1.5476\n",
      "283/388, train_loss: 0.0764, step time: 1.5524\n",
      "284/388, train_loss: 0.0393, step time: 1.5470\n",
      "285/388, train_loss: 0.1655, step time: 1.5443\n",
      "286/388, train_loss: 0.1788, step time: 1.5456\n",
      "287/388, train_loss: 0.1507, step time: 1.5478\n",
      "288/388, train_loss: 0.2769, step time: 1.5444\n",
      "289/388, train_loss: 0.1676, step time: 1.5497\n",
      "290/388, train_loss: 0.1450, step time: 1.5486\n",
      "291/388, train_loss: 0.0977, step time: 1.5462\n",
      "292/388, train_loss: 0.1877, step time: 1.5478\n",
      "293/388, train_loss: 0.0846, step time: 1.5435\n",
      "294/388, train_loss: 0.1143, step time: 1.5490\n",
      "295/388, train_loss: 0.0765, step time: 1.5501\n",
      "296/388, train_loss: 0.1881, step time: 1.5458\n",
      "297/388, train_loss: 0.0575, step time: 1.5570\n",
      "298/388, train_loss: 0.2458, step time: 1.5444\n",
      "299/388, train_loss: 0.0897, step time: 1.5483\n",
      "300/388, train_loss: 0.1530, step time: 1.5500\n",
      "301/388, train_loss: 0.1913, step time: 1.5445\n",
      "302/388, train_loss: 0.1604, step time: 1.5487\n",
      "303/388, train_loss: 0.0636, step time: 1.5441\n",
      "304/388, train_loss: 0.1292, step time: 1.5482\n",
      "305/388, train_loss: 0.1087, step time: 1.5450\n",
      "306/388, train_loss: 0.1405, step time: 1.5464\n",
      "307/388, train_loss: 0.2030, step time: 1.5460\n",
      "308/388, train_loss: 0.0895, step time: 1.5432\n",
      "309/388, train_loss: 0.4105, step time: 1.5474\n",
      "310/388, train_loss: 0.1185, step time: 1.5456\n",
      "311/388, train_loss: 0.1224, step time: 1.5460\n",
      "312/388, train_loss: 0.1851, step time: 1.5469\n",
      "313/388, train_loss: 0.2356, step time: 1.5412\n",
      "314/388, train_loss: 0.1506, step time: 1.5441\n",
      "315/388, train_loss: 0.1187, step time: 1.5560\n",
      "316/388, train_loss: 0.2768, step time: 1.5432\n",
      "317/388, train_loss: 0.3068, step time: 1.5481\n",
      "318/388, train_loss: 0.0914, step time: 1.5486\n",
      "319/388, train_loss: 0.0398, step time: 1.5444\n",
      "320/388, train_loss: 0.1687, step time: 1.5489\n",
      "321/388, train_loss: 0.2476, step time: 1.5575\n",
      "322/388, train_loss: 0.3244, step time: 1.5443\n",
      "323/388, train_loss: 0.2089, step time: 1.5573\n",
      "324/388, train_loss: 0.1572, step time: 1.5461\n",
      "325/388, train_loss: 0.3452, step time: 1.5469\n",
      "326/388, train_loss: 0.0922, step time: 1.5492\n",
      "327/388, train_loss: 0.0255, step time: 1.5452\n",
      "328/388, train_loss: 0.0356, step time: 1.5485\n",
      "329/388, train_loss: 0.0920, step time: 1.5444\n",
      "330/388, train_loss: 0.0888, step time: 1.5523\n",
      "331/388, train_loss: 0.1395, step time: 1.5700\n",
      "332/388, train_loss: 0.0949, step time: 1.5465\n",
      "333/388, train_loss: 0.3370, step time: 1.5486\n",
      "334/388, train_loss: 0.0886, step time: 1.5446\n",
      "335/388, train_loss: 0.1848, step time: 1.5590\n",
      "336/388, train_loss: 0.0913, step time: 1.5580\n",
      "337/388, train_loss: 0.0970, step time: 1.5452\n",
      "338/388, train_loss: 0.2891, step time: 1.5477\n",
      "339/388, train_loss: 0.1416, step time: 1.5418\n",
      "340/388, train_loss: 0.2001, step time: 1.5467\n",
      "341/388, train_loss: 0.0706, step time: 1.5557\n",
      "342/388, train_loss: 0.1185, step time: 1.5456\n",
      "343/388, train_loss: 0.4002, step time: 1.5486\n",
      "344/388, train_loss: 0.0621, step time: 1.5491\n",
      "345/388, train_loss: 0.2210, step time: 1.5427\n",
      "346/388, train_loss: 0.0939, step time: 1.5534\n",
      "347/388, train_loss: 0.1204, step time: 1.5528\n",
      "348/388, train_loss: 0.3747, step time: 1.5483\n",
      "349/388, train_loss: 0.0869, step time: 1.5470\n",
      "350/388, train_loss: 0.1051, step time: 1.5427\n",
      "351/388, train_loss: 0.1925, step time: 1.5497\n",
      "352/388, train_loss: 0.0544, step time: 1.5462\n",
      "353/388, train_loss: 0.0563, step time: 1.5558\n",
      "354/388, train_loss: 0.1756, step time: 1.5477\n",
      "355/388, train_loss: 0.0956, step time: 1.5430\n",
      "356/388, train_loss: 0.1664, step time: 1.5510\n",
      "357/388, train_loss: 0.3579, step time: 1.5459\n",
      "358/388, train_loss: 0.1401, step time: 1.5465\n",
      "359/388, train_loss: 0.0317, step time: 1.5458\n",
      "360/388, train_loss: 0.1443, step time: 1.5436\n",
      "361/388, train_loss: 0.2388, step time: 1.5547\n",
      "362/388, train_loss: 0.1340, step time: 1.5486\n",
      "363/388, train_loss: 0.4954, step time: 1.5422\n",
      "364/388, train_loss: 0.1115, step time: 1.5460\n",
      "365/388, train_loss: 0.1831, step time: 1.5408\n",
      "366/388, train_loss: 0.1628, step time: 1.5435\n",
      "367/388, train_loss: 0.0831, step time: 1.5467\n",
      "368/388, train_loss: 0.2284, step time: 1.5497\n",
      "369/388, train_loss: 0.0773, step time: 1.5490\n",
      "370/388, train_loss: 0.0542, step time: 1.5343\n",
      "371/388, train_loss: 0.0772, step time: 1.5421\n",
      "372/388, train_loss: 0.1123, step time: 1.5463\n",
      "373/388, train_loss: 0.1956, step time: 1.5419\n",
      "374/388, train_loss: 0.1320, step time: 1.5499\n",
      "375/388, train_loss: 0.1531, step time: 1.5455\n",
      "376/388, train_loss: 0.0301, step time: 1.5439\n",
      "377/388, train_loss: 0.1038, step time: 1.5495\n",
      "378/388, train_loss: 0.2007, step time: 1.5422\n",
      "379/388, train_loss: 0.0839, step time: 1.5570\n",
      "380/388, train_loss: 0.1011, step time: 1.5505\n",
      "381/388, train_loss: 0.1163, step time: 1.5491\n",
      "382/388, train_loss: 0.1079, step time: 1.5473\n",
      "383/388, train_loss: 0.0465, step time: 1.5551\n",
      "384/388, train_loss: 0.1547, step time: 1.5446\n",
      "385/388, train_loss: 0.1220, step time: 1.5547\n",
      "386/388, train_loss: 0.2614, step time: 1.5561\n",
      "387/388, train_loss: 0.2037, step time: 1.5425\n",
      "388/388, train_loss: 0.2057, step time: 1.5548\n",
      "epoch 88 average loss: 0.1584\n",
      "current epoch: 88 current mean dice: 0.7777 tc: 0.8249 wt: 0.9046 et: 0.6035\n",
      "best mean dice: 0.7817 at epoch: 84\n",
      "time consuming of epoch 88 is: 710.2993\n",
      "----------\n",
      "epoch 89/100\n",
      "1/388, train_loss: 0.2608, step time: 1.5520\n",
      "2/388, train_loss: 0.2047, step time: 1.5330\n",
      "3/388, train_loss: 0.0575, step time: 1.5652\n",
      "4/388, train_loss: 0.1412, step time: 1.5663\n",
      "5/388, train_loss: 0.0932, step time: 1.5346\n",
      "6/388, train_loss: 0.3091, step time: 1.5367\n",
      "7/388, train_loss: 0.1556, step time: 1.5375\n",
      "8/388, train_loss: 0.0961, step time: 1.5648\n",
      "9/388, train_loss: 0.3498, step time: 1.5461\n",
      "10/388, train_loss: 0.1056, step time: 1.5359\n",
      "11/388, train_loss: 0.1064, step time: 1.5405\n",
      "12/388, train_loss: 0.1342, step time: 1.5345\n",
      "13/388, train_loss: 0.1772, step time: 1.5438\n",
      "14/388, train_loss: 0.1884, step time: 1.5647\n",
      "15/388, train_loss: 0.0940, step time: 1.5346\n",
      "16/388, train_loss: 0.3399, step time: 1.5338\n",
      "17/388, train_loss: 0.1469, step time: 1.5425\n",
      "18/388, train_loss: 0.1986, step time: 1.5386\n",
      "19/388, train_loss: 0.0695, step time: 1.5352\n",
      "20/388, train_loss: 0.1453, step time: 1.5360\n",
      "21/388, train_loss: 0.0623, step time: 1.5440\n",
      "22/388, train_loss: 0.1853, step time: 1.5324\n",
      "23/388, train_loss: 0.3580, step time: 1.5356\n",
      "24/388, train_loss: 0.2186, step time: 1.5382\n",
      "25/388, train_loss: 0.0898, step time: 1.5428\n",
      "26/388, train_loss: 0.0863, step time: 1.5336\n",
      "27/388, train_loss: 0.1161, step time: 1.5334\n",
      "28/388, train_loss: 0.2099, step time: 1.5364\n",
      "29/388, train_loss: 0.2148, step time: 1.5422\n",
      "30/388, train_loss: 0.1519, step time: 1.5330\n",
      "31/388, train_loss: 0.2346, step time: 1.5380\n",
      "32/388, train_loss: 0.1893, step time: 1.5378\n",
      "33/388, train_loss: 0.3124, step time: 1.5439\n",
      "34/388, train_loss: 0.1350, step time: 1.5323\n",
      "35/388, train_loss: 0.1474, step time: 1.5346\n",
      "36/388, train_loss: 0.1175, step time: 1.5354\n",
      "37/388, train_loss: 0.1051, step time: 1.5467\n",
      "38/388, train_loss: 0.0694, step time: 1.5341\n",
      "39/388, train_loss: 0.2760, step time: 1.5319\n",
      "40/388, train_loss: 0.1027, step time: 1.5315\n",
      "41/388, train_loss: 0.1071, step time: 1.5480\n",
      "42/388, train_loss: 0.0766, step time: 1.5385\n",
      "43/388, train_loss: 0.0748, step time: 1.5322\n",
      "44/388, train_loss: 0.1598, step time: 1.5327\n",
      "45/388, train_loss: 0.0923, step time: 1.5435\n",
      "46/388, train_loss: 0.0946, step time: 1.5376\n",
      "47/388, train_loss: 0.1784, step time: 1.5377\n",
      "48/388, train_loss: 0.4398, step time: 1.5335\n",
      "49/388, train_loss: 0.0952, step time: 1.5418\n",
      "50/388, train_loss: 0.1819, step time: 1.5333\n",
      "51/388, train_loss: 0.1872, step time: 1.5346\n",
      "52/388, train_loss: 0.0944, step time: 1.5367\n",
      "53/388, train_loss: 0.1153, step time: 1.5471\n",
      "54/388, train_loss: 0.1903, step time: 1.5332\n",
      "55/388, train_loss: 0.0420, step time: 1.5321\n",
      "56/388, train_loss: 0.0947, step time: 1.5379\n",
      "57/388, train_loss: 0.1118, step time: 1.5488\n",
      "58/388, train_loss: 0.1495, step time: 1.5344\n",
      "59/388, train_loss: 0.1737, step time: 1.5325\n",
      "60/388, train_loss: 0.2731, step time: 1.5337\n",
      "61/388, train_loss: 0.1164, step time: 1.5475\n",
      "62/388, train_loss: 0.0843, step time: 1.5365\n",
      "63/388, train_loss: 0.0772, step time: 1.5590\n",
      "64/388, train_loss: 0.0961, step time: 1.5331\n",
      "65/388, train_loss: 0.0564, step time: 1.5477\n",
      "66/388, train_loss: 0.2799, step time: 1.5344\n",
      "67/388, train_loss: 0.1960, step time: 1.5339\n",
      "68/388, train_loss: 0.2857, step time: 1.5349\n",
      "69/388, train_loss: 0.1091, step time: 1.5477\n",
      "70/388, train_loss: 0.0730, step time: 1.5382\n",
      "71/388, train_loss: 0.2370, step time: 1.5378\n",
      "72/388, train_loss: 0.5638, step time: 1.5362\n",
      "73/388, train_loss: 0.0699, step time: 1.5448\n",
      "74/388, train_loss: 0.1293, step time: 1.5370\n",
      "75/388, train_loss: 0.2363, step time: 1.5384\n",
      "76/388, train_loss: 0.1824, step time: 1.5336\n",
      "77/388, train_loss: 0.2018, step time: 1.5425\n",
      "78/388, train_loss: 0.2086, step time: 1.5358\n",
      "79/388, train_loss: 0.1978, step time: 1.5347\n",
      "80/388, train_loss: 0.0978, step time: 1.5372\n",
      "81/388, train_loss: 0.0933, step time: 1.5436\n",
      "82/388, train_loss: 0.1648, step time: 1.5351\n",
      "83/388, train_loss: 0.0946, step time: 1.5330\n",
      "84/388, train_loss: 0.1691, step time: 1.5330\n",
      "85/388, train_loss: 0.2053, step time: 1.5470\n",
      "86/388, train_loss: 0.1302, step time: 1.5339\n",
      "87/388, train_loss: 0.0751, step time: 1.5343\n",
      "88/388, train_loss: 0.1271, step time: 1.5334\n",
      "89/388, train_loss: 0.2109, step time: 1.5490\n",
      "90/388, train_loss: 0.1330, step time: 1.5343\n",
      "91/388, train_loss: 0.2236, step time: 1.5328\n",
      "92/388, train_loss: 0.1866, step time: 1.5338\n",
      "93/388, train_loss: 0.0969, step time: 1.5574\n",
      "94/388, train_loss: 0.1353, step time: 1.5345\n",
      "95/388, train_loss: 0.1209, step time: 1.5314\n",
      "96/388, train_loss: 0.1277, step time: 1.5343\n",
      "97/388, train_loss: 0.0902, step time: 1.5476\n",
      "98/388, train_loss: 0.2220, step time: 1.5413\n",
      "99/388, train_loss: 0.0414, step time: 1.5323\n",
      "100/388, train_loss: 0.4586, step time: 1.5362\n",
      "101/388, train_loss: 0.0895, step time: 1.5484\n",
      "102/388, train_loss: 0.1723, step time: 1.5341\n",
      "103/388, train_loss: 0.1455, step time: 1.5306\n",
      "104/388, train_loss: 0.1417, step time: 1.5324\n",
      "105/388, train_loss: 0.1765, step time: 1.5458\n",
      "106/388, train_loss: 0.0993, step time: 1.5368\n",
      "107/388, train_loss: 0.1125, step time: 1.5357\n",
      "108/388, train_loss: 0.2674, step time: 1.5349\n",
      "109/388, train_loss: 0.1746, step time: 1.5471\n",
      "110/388, train_loss: 0.1997, step time: 1.5359\n",
      "111/388, train_loss: 0.3857, step time: 1.5305\n",
      "112/388, train_loss: 0.1928, step time: 1.5345\n",
      "113/388, train_loss: 0.2377, step time: 1.5493\n",
      "114/388, train_loss: 0.1415, step time: 1.5384\n",
      "115/388, train_loss: 0.2288, step time: 1.5380\n",
      "116/388, train_loss: 0.1482, step time: 1.5312\n",
      "117/388, train_loss: 0.0769, step time: 1.5420\n",
      "118/388, train_loss: 0.0642, step time: 1.5360\n",
      "119/388, train_loss: 0.1320, step time: 1.5497\n",
      "120/388, train_loss: 0.1623, step time: 1.5354\n",
      "121/388, train_loss: 0.2036, step time: 1.5485\n",
      "122/388, train_loss: 0.1016, step time: 1.5410\n",
      "123/388, train_loss: 0.0717, step time: 1.5408\n",
      "124/388, train_loss: 0.1634, step time: 1.5385\n",
      "125/388, train_loss: 0.1490, step time: 1.5439\n",
      "126/388, train_loss: 0.2210, step time: 1.5345\n",
      "127/388, train_loss: 0.1673, step time: 1.5299\n",
      "128/388, train_loss: 0.0837, step time: 1.5311\n",
      "129/388, train_loss: 0.1357, step time: 1.5476\n",
      "130/388, train_loss: 0.2247, step time: 1.5339\n",
      "131/388, train_loss: 0.0894, step time: 1.5321\n",
      "132/388, train_loss: 0.1287, step time: 1.5321\n",
      "133/388, train_loss: 0.0495, step time: 1.5477\n",
      "134/388, train_loss: 0.0906, step time: 1.5340\n",
      "135/388, train_loss: 0.1143, step time: 1.5327\n",
      "136/388, train_loss: 0.1226, step time: 1.5346\n",
      "137/388, train_loss: 0.1195, step time: 1.5530\n",
      "138/388, train_loss: 0.0527, step time: 1.5348\n",
      "139/388, train_loss: 0.2328, step time: 1.5335\n",
      "140/388, train_loss: 0.0717, step time: 1.5336\n",
      "141/388, train_loss: 0.2841, step time: 1.5415\n",
      "142/388, train_loss: 0.1714, step time: 1.5354\n",
      "143/388, train_loss: 0.2401, step time: 1.5442\n",
      "144/388, train_loss: 0.2411, step time: 1.5343\n",
      "145/388, train_loss: 0.1814, step time: 1.5433\n",
      "146/388, train_loss: 0.1369, step time: 1.5339\n",
      "147/388, train_loss: 0.0902, step time: 1.5354\n",
      "148/388, train_loss: 0.0994, step time: 1.5358\n",
      "149/388, train_loss: 0.1440, step time: 1.5495\n",
      "150/388, train_loss: 0.1365, step time: 1.5305\n",
      "151/388, train_loss: 0.2109, step time: 1.5358\n",
      "152/388, train_loss: 0.1525, step time: 1.5339\n",
      "153/388, train_loss: 0.1599, step time: 1.5471\n",
      "154/388, train_loss: 0.2909, step time: 1.5320\n",
      "155/388, train_loss: 0.1497, step time: 1.5338\n",
      "156/388, train_loss: 0.0903, step time: 1.5322\n",
      "157/388, train_loss: 0.0358, step time: 1.5443\n",
      "158/388, train_loss: 0.0991, step time: 1.5375\n",
      "159/388, train_loss: 0.0962, step time: 1.5333\n",
      "160/388, train_loss: 0.0308, step time: 1.5323\n",
      "161/388, train_loss: 0.1297, step time: 1.5440\n",
      "162/388, train_loss: 0.1092, step time: 1.5375\n",
      "163/388, train_loss: 0.0761, step time: 1.5377\n",
      "164/388, train_loss: 0.1172, step time: 1.5355\n",
      "165/388, train_loss: 0.1193, step time: 1.5422\n",
      "166/388, train_loss: 0.0883, step time: 1.5324\n",
      "167/388, train_loss: 0.1906, step time: 1.5344\n",
      "168/388, train_loss: 0.1463, step time: 1.5365\n",
      "169/388, train_loss: 0.3562, step time: 1.5461\n",
      "170/388, train_loss: 0.1984, step time: 1.5454\n",
      "171/388, train_loss: 0.0435, step time: 1.5344\n",
      "172/388, train_loss: 0.1209, step time: 1.5331\n",
      "173/388, train_loss: 0.1899, step time: 1.5416\n",
      "174/388, train_loss: 0.1473, step time: 1.5349\n",
      "175/388, train_loss: 0.1714, step time: 1.5377\n",
      "176/388, train_loss: 0.1575, step time: 1.5359\n",
      "177/388, train_loss: 0.2191, step time: 1.5404\n",
      "178/388, train_loss: 0.0906, step time: 1.5328\n",
      "179/388, train_loss: 0.1397, step time: 1.5377\n",
      "180/388, train_loss: 0.0802, step time: 1.5342\n",
      "181/388, train_loss: 0.2796, step time: 1.5457\n",
      "182/388, train_loss: 0.4091, step time: 1.5319\n",
      "183/388, train_loss: 0.0811, step time: 1.5316\n",
      "184/388, train_loss: 0.0947, step time: 1.5362\n",
      "185/388, train_loss: 0.0335, step time: 1.5510\n",
      "186/388, train_loss: 0.1528, step time: 1.5344\n",
      "187/388, train_loss: 0.2810, step time: 1.5309\n",
      "188/388, train_loss: 0.3071, step time: 1.5343\n",
      "189/388, train_loss: 0.2382, step time: 1.5484\n",
      "190/388, train_loss: 0.0779, step time: 1.5380\n",
      "191/388, train_loss: 0.0865, step time: 1.5325\n",
      "192/388, train_loss: 0.2646, step time: 1.5314\n",
      "193/388, train_loss: 0.2113, step time: 1.5431\n",
      "194/388, train_loss: 0.2422, step time: 1.5374\n",
      "195/388, train_loss: 0.0878, step time: 1.5333\n",
      "196/388, train_loss: 0.0441, step time: 1.5365\n",
      "197/388, train_loss: 0.1511, step time: 1.5435\n",
      "198/388, train_loss: 0.1680, step time: 1.5320\n",
      "199/388, train_loss: 0.5212, step time: 1.5478\n",
      "200/388, train_loss: 0.1600, step time: 1.5364\n",
      "201/388, train_loss: 0.1373, step time: 1.5440\n",
      "202/388, train_loss: 0.2210, step time: 1.5412\n",
      "203/388, train_loss: 0.1461, step time: 1.5384\n",
      "204/388, train_loss: 0.1120, step time: 1.5380\n",
      "205/388, train_loss: 0.1157, step time: 1.5413\n",
      "206/388, train_loss: 0.1772, step time: 1.5354\n",
      "207/388, train_loss: 0.1742, step time: 1.5359\n",
      "208/388, train_loss: 0.1596, step time: 1.5334\n",
      "209/388, train_loss: 0.1753, step time: 1.5723\n",
      "210/388, train_loss: 0.1514, step time: 1.5370\n",
      "211/388, train_loss: 0.0494, step time: 1.5327\n",
      "212/388, train_loss: 0.1096, step time: 1.5316\n",
      "213/388, train_loss: 0.1179, step time: 1.5433\n",
      "214/388, train_loss: 0.1801, step time: 1.5316\n",
      "215/388, train_loss: 0.0954, step time: 1.5362\n",
      "216/388, train_loss: 0.2258, step time: 1.5373\n",
      "217/388, train_loss: 0.0578, step time: 1.5448\n",
      "218/388, train_loss: 0.3023, step time: 1.5333\n",
      "219/388, train_loss: 0.0699, step time: 1.5362\n",
      "220/388, train_loss: 0.0833, step time: 1.5350\n",
      "221/388, train_loss: 0.1412, step time: 1.5436\n",
      "222/388, train_loss: 0.1133, step time: 1.5373\n",
      "223/388, train_loss: 0.1949, step time: 1.5366\n",
      "224/388, train_loss: 0.0690, step time: 1.5365\n",
      "225/388, train_loss: 0.3363, step time: 1.5445\n",
      "226/388, train_loss: 0.1165, step time: 1.5325\n",
      "227/388, train_loss: 0.2115, step time: 1.5369\n",
      "228/388, train_loss: 0.0635, step time: 1.5384\n",
      "229/388, train_loss: 0.2675, step time: 1.5474\n",
      "230/388, train_loss: 0.0911, step time: 1.5376\n",
      "231/388, train_loss: 0.1519, step time: 1.5499\n",
      "232/388, train_loss: 0.2072, step time: 1.5349\n",
      "233/388, train_loss: 0.1426, step time: 1.5459\n",
      "234/388, train_loss: 0.0519, step time: 1.5319\n",
      "235/388, train_loss: 0.3226, step time: 1.5346\n",
      "236/388, train_loss: 0.1223, step time: 1.5343\n",
      "237/388, train_loss: 0.0918, step time: 1.5464\n",
      "238/388, train_loss: 0.0958, step time: 1.5323\n",
      "239/388, train_loss: 0.1409, step time: 1.5338\n",
      "240/388, train_loss: 0.1556, step time: 1.5340\n",
      "241/388, train_loss: 0.1931, step time: 1.5472\n",
      "242/388, train_loss: 0.0770, step time: 1.5349\n",
      "243/388, train_loss: 0.2153, step time: 1.5338\n",
      "244/388, train_loss: 0.3640, step time: 1.5309\n",
      "245/388, train_loss: 0.0531, step time: 1.5460\n",
      "246/388, train_loss: 0.0835, step time: 1.5341\n",
      "247/388, train_loss: 0.0822, step time: 1.5338\n",
      "248/388, train_loss: 0.0896, step time: 1.5316\n",
      "249/388, train_loss: 0.1146, step time: 1.5423\n",
      "250/388, train_loss: 0.1497, step time: 1.5501\n",
      "251/388, train_loss: 0.1613, step time: 1.5302\n",
      "252/388, train_loss: 0.1623, step time: 1.5342\n",
      "253/388, train_loss: 0.1816, step time: 1.5429\n",
      "254/388, train_loss: 0.1388, step time: 1.5329\n",
      "255/388, train_loss: 0.1138, step time: 1.5314\n",
      "256/388, train_loss: 0.2097, step time: 1.5334\n",
      "257/388, train_loss: 0.2509, step time: 1.5515\n",
      "258/388, train_loss: 0.1339, step time: 1.5351\n",
      "259/388, train_loss: 0.1142, step time: 1.5338\n",
      "260/388, train_loss: 0.1475, step time: 1.5317\n",
      "261/388, train_loss: 0.1411, step time: 1.5393\n",
      "262/388, train_loss: 0.1468, step time: 1.5308\n",
      "263/388, train_loss: 0.2437, step time: 1.5389\n",
      "264/388, train_loss: 0.1461, step time: 1.5356\n",
      "265/388, train_loss: 0.1684, step time: 1.5422\n",
      "266/388, train_loss: 0.2697, step time: 1.5372\n",
      "267/388, train_loss: 0.0774, step time: 1.5373\n",
      "268/388, train_loss: 0.1160, step time: 1.5357\n",
      "269/388, train_loss: 0.1540, step time: 1.5424\n",
      "270/388, train_loss: 0.2047, step time: 1.5305\n",
      "271/388, train_loss: 0.1191, step time: 1.5379\n",
      "272/388, train_loss: 0.1034, step time: 1.5328\n",
      "273/388, train_loss: 0.0930, step time: 1.5424\n",
      "274/388, train_loss: 0.0821, step time: 1.5356\n",
      "275/388, train_loss: 0.1326, step time: 1.5367\n",
      "276/388, train_loss: 0.0652, step time: 1.5334\n",
      "277/388, train_loss: 0.1642, step time: 1.5447\n",
      "278/388, train_loss: 0.2727, step time: 1.5322\n",
      "279/388, train_loss: 0.0594, step time: 1.5332\n",
      "280/388, train_loss: 0.3743, step time: 1.5353\n",
      "281/388, train_loss: 0.1775, step time: 1.5442\n",
      "282/388, train_loss: 0.3539, step time: 1.5299\n",
      "283/388, train_loss: 0.0999, step time: 1.5315\n",
      "284/388, train_loss: 0.1212, step time: 1.5375\n",
      "285/388, train_loss: 0.1917, step time: 1.5445\n",
      "286/388, train_loss: 0.0953, step time: 1.5334\n",
      "287/388, train_loss: 0.0824, step time: 1.5321\n",
      "288/388, train_loss: 0.1175, step time: 1.5404\n",
      "289/388, train_loss: 0.0613, step time: 1.5495\n",
      "290/388, train_loss: 0.1666, step time: 1.5363\n",
      "291/388, train_loss: 0.1508, step time: 1.5344\n",
      "292/388, train_loss: 0.0973, step time: 1.5324\n",
      "293/388, train_loss: 0.0640, step time: 1.5454\n",
      "294/388, train_loss: 0.1469, step time: 1.5355\n",
      "295/388, train_loss: 0.2955, step time: 1.5450\n",
      "296/388, train_loss: 0.1322, step time: 1.5350\n",
      "297/388, train_loss: 0.1092, step time: 1.5411\n",
      "298/388, train_loss: 0.2258, step time: 1.5362\n",
      "299/388, train_loss: 0.0605, step time: 1.5359\n",
      "300/388, train_loss: 0.0868, step time: 1.5377\n",
      "301/388, train_loss: 0.2124, step time: 1.5429\n",
      "302/388, train_loss: 0.1010, step time: 1.5578\n",
      "303/388, train_loss: 0.1942, step time: 1.5385\n",
      "304/388, train_loss: 0.0615, step time: 1.5361\n",
      "305/388, train_loss: 0.1637, step time: 1.5414\n",
      "306/388, train_loss: 0.1773, step time: 1.5334\n",
      "307/388, train_loss: 0.0594, step time: 1.5337\n",
      "308/388, train_loss: 0.1447, step time: 1.5358\n",
      "309/388, train_loss: 0.1097, step time: 1.5427\n",
      "310/388, train_loss: 0.0998, step time: 1.5350\n",
      "311/388, train_loss: 0.0641, step time: 1.5456\n",
      "312/388, train_loss: 0.1383, step time: 1.5358\n",
      "313/388, train_loss: 0.0765, step time: 1.5423\n",
      "314/388, train_loss: 0.2291, step time: 1.5292\n",
      "315/388, train_loss: 0.1895, step time: 1.5358\n",
      "316/388, train_loss: 0.1007, step time: 1.5377\n",
      "317/388, train_loss: 0.1877, step time: 1.5458\n",
      "318/388, train_loss: 0.0928, step time: 1.5333\n",
      "319/388, train_loss: 0.0832, step time: 1.5502\n",
      "320/388, train_loss: 0.2825, step time: 1.5330\n",
      "321/388, train_loss: 0.3018, step time: 1.5543\n",
      "322/388, train_loss: 0.3059, step time: 1.5380\n",
      "323/388, train_loss: 0.0632, step time: 1.5324\n",
      "324/388, train_loss: 0.1873, step time: 1.5348\n",
      "325/388, train_loss: 0.1316, step time: 1.5477\n",
      "326/388, train_loss: 0.4115, step time: 1.5435\n",
      "327/388, train_loss: 0.1567, step time: 1.5357\n",
      "328/388, train_loss: 0.0806, step time: 1.5435\n",
      "329/388, train_loss: 0.3601, step time: 1.5431\n",
      "330/388, train_loss: 0.1309, step time: 1.5349\n",
      "331/388, train_loss: 0.1925, step time: 1.5367\n",
      "332/388, train_loss: 0.1521, step time: 1.5369\n",
      "333/388, train_loss: 0.0705, step time: 1.5464\n",
      "334/388, train_loss: 0.0930, step time: 1.5338\n",
      "335/388, train_loss: 0.1904, step time: 1.5338\n",
      "336/388, train_loss: 0.1373, step time: 1.5359\n",
      "337/388, train_loss: 0.2717, step time: 1.5452\n",
      "338/388, train_loss: 0.0599, step time: 1.5376\n",
      "339/388, train_loss: 0.1551, step time: 1.5340\n",
      "340/388, train_loss: 0.1282, step time: 1.5326\n",
      "341/388, train_loss: 0.0682, step time: 1.5539\n",
      "342/388, train_loss: 0.2057, step time: 1.5324\n",
      "343/388, train_loss: 0.2251, step time: 1.5310\n",
      "344/388, train_loss: 0.0879, step time: 1.5328\n",
      "345/388, train_loss: 0.0988, step time: 1.5460\n",
      "346/388, train_loss: 0.2160, step time: 1.5367\n",
      "347/388, train_loss: 0.1999, step time: 1.5329\n",
      "348/388, train_loss: 0.0832, step time: 1.5336\n",
      "349/388, train_loss: 0.0885, step time: 1.5399\n",
      "350/388, train_loss: 0.2294, step time: 1.5356\n",
      "351/388, train_loss: 0.2258, step time: 1.5390\n",
      "352/388, train_loss: 0.2346, step time: 1.5380\n",
      "353/388, train_loss: 0.1192, step time: 1.5455\n",
      "354/388, train_loss: 0.0812, step time: 1.5370\n",
      "355/388, train_loss: 0.1999, step time: 1.5329\n",
      "356/388, train_loss: 0.1588, step time: 1.5338\n",
      "357/388, train_loss: 0.1022, step time: 1.5432\n",
      "358/388, train_loss: 0.1555, step time: 1.5345\n",
      "359/388, train_loss: 0.0726, step time: 1.5343\n",
      "360/388, train_loss: 0.2177, step time: 1.5338\n",
      "361/388, train_loss: 0.1194, step time: 1.5413\n",
      "362/388, train_loss: 0.2173, step time: 1.5332\n",
      "363/388, train_loss: 0.1218, step time: 1.5361\n",
      "364/388, train_loss: 0.1860, step time: 1.5380\n",
      "365/388, train_loss: 0.0640, step time: 1.5427\n",
      "366/388, train_loss: 0.0969, step time: 1.5328\n",
      "367/388, train_loss: 0.2504, step time: 1.5352\n",
      "368/388, train_loss: 0.1278, step time: 1.5368\n",
      "369/388, train_loss: 0.1226, step time: 1.5468\n",
      "370/388, train_loss: 0.0764, step time: 1.5312\n",
      "371/388, train_loss: 0.1247, step time: 1.5355\n",
      "372/388, train_loss: 0.3679, step time: 1.5360\n",
      "373/388, train_loss: 0.2327, step time: 1.5459\n",
      "374/388, train_loss: 0.2074, step time: 1.5342\n",
      "375/388, train_loss: 0.1645, step time: 1.5343\n",
      "376/388, train_loss: 0.1727, step time: 1.5328\n",
      "377/388, train_loss: 0.0752, step time: 1.5501\n",
      "378/388, train_loss: 0.0416, step time: 1.5342\n",
      "379/388, train_loss: 0.4180, step time: 1.5441\n",
      "380/388, train_loss: 0.1643, step time: 1.5332\n",
      "381/388, train_loss: 0.0247, step time: 1.5493\n",
      "382/388, train_loss: 0.4924, step time: 1.5411\n",
      "383/388, train_loss: 0.0790, step time: 1.5401\n",
      "384/388, train_loss: 0.2127, step time: 1.5375\n",
      "385/388, train_loss: 0.1477, step time: 1.5443\n",
      "386/388, train_loss: 0.2660, step time: 1.5584\n",
      "387/388, train_loss: 0.0931, step time: 1.5596\n",
      "388/388, train_loss: 0.3394, step time: 1.5427\n",
      "epoch 89 average loss: 0.1584\n",
      "current epoch: 89 current mean dice: 0.7780 tc: 0.8265 wt: 0.9052 et: 0.6022\n",
      "best mean dice: 0.7817 at epoch: 84\n",
      "time consuming of epoch 89 is: 705.7786\n",
      "----------\n",
      "epoch 90/100\n",
      "1/388, train_loss: 0.2151, step time: 1.5420\n",
      "2/388, train_loss: 0.1300, step time: 1.5346\n",
      "3/388, train_loss: 0.1806, step time: 1.5347\n",
      "4/388, train_loss: 0.1081, step time: 1.5381\n",
      "5/388, train_loss: 0.3123, step time: 1.5355\n",
      "6/388, train_loss: 0.1040, step time: 1.5339\n",
      "7/388, train_loss: 0.1600, step time: 1.5336\n",
      "8/388, train_loss: 0.0804, step time: 1.5536\n",
      "9/388, train_loss: 0.1754, step time: 1.5336\n",
      "10/388, train_loss: 0.2639, step time: 1.5358\n",
      "11/388, train_loss: 0.1242, step time: 1.5366\n",
      "12/388, train_loss: 0.0647, step time: 1.5340\n",
      "13/388, train_loss: 0.1081, step time: 1.5320\n",
      "14/388, train_loss: 0.1359, step time: 1.5314\n",
      "15/388, train_loss: 0.1474, step time: 1.5285\n",
      "16/388, train_loss: 0.1209, step time: 1.5296\n",
      "17/388, train_loss: 0.1450, step time: 1.5314\n",
      "18/388, train_loss: 0.0960, step time: 1.5397\n",
      "19/388, train_loss: 0.1097, step time: 1.5358\n",
      "20/388, train_loss: 0.0677, step time: 1.5305\n",
      "21/388, train_loss: 0.1344, step time: 1.5311\n",
      "22/388, train_loss: 0.0910, step time: 1.5299\n",
      "23/388, train_loss: 0.1601, step time: 1.5275\n",
      "24/388, train_loss: 0.1047, step time: 1.5332\n",
      "25/388, train_loss: 0.1386, step time: 1.5307\n",
      "26/388, train_loss: 0.1696, step time: 1.5302\n",
      "27/388, train_loss: 0.1486, step time: 1.5325\n",
      "28/388, train_loss: 0.0978, step time: 1.5362\n",
      "29/388, train_loss: 0.1560, step time: 1.5491\n",
      "30/388, train_loss: 0.2254, step time: 1.5398\n",
      "31/388, train_loss: 0.3144, step time: 1.5434\n",
      "32/388, train_loss: 0.0933, step time: 1.5420\n",
      "33/388, train_loss: 0.1584, step time: 1.5438\n",
      "34/388, train_loss: 0.0708, step time: 1.5470\n",
      "35/388, train_loss: 0.3293, step time: 1.5474\n",
      "36/388, train_loss: 0.2896, step time: 1.5441\n",
      "37/388, train_loss: 0.0871, step time: 1.5494\n",
      "38/388, train_loss: 0.2646, step time: 1.5420\n",
      "39/388, train_loss: 0.1212, step time: 1.5430\n",
      "40/388, train_loss: 0.1107, step time: 1.5505\n",
      "41/388, train_loss: 0.1599, step time: 1.5409\n",
      "42/388, train_loss: 0.1510, step time: 1.5508\n",
      "43/388, train_loss: 0.2613, step time: 1.5474\n",
      "44/388, train_loss: 0.2481, step time: 1.5436\n",
      "45/388, train_loss: 0.0582, step time: 1.5523\n",
      "46/388, train_loss: 0.0986, step time: 1.5439\n",
      "47/388, train_loss: 0.1723, step time: 1.5401\n",
      "48/388, train_loss: 0.3540, step time: 1.5473\n",
      "49/388, train_loss: 0.2799, step time: 1.5415\n",
      "50/388, train_loss: 0.1192, step time: 1.5391\n",
      "51/388, train_loss: 0.0863, step time: 1.5651\n",
      "52/388, train_loss: 0.1224, step time: 1.5426\n",
      "53/388, train_loss: 0.0857, step time: 1.5427\n",
      "54/388, train_loss: 0.1740, step time: 1.5465\n",
      "55/388, train_loss: 0.1138, step time: 1.5410\n",
      "56/388, train_loss: 0.1021, step time: 1.5445\n",
      "57/388, train_loss: 0.0987, step time: 1.5427\n",
      "58/388, train_loss: 0.2133, step time: 1.5393\n",
      "59/388, train_loss: 0.1883, step time: 1.5443\n",
      "60/388, train_loss: 0.2621, step time: 1.5448\n",
      "61/388, train_loss: 0.0841, step time: 1.5382\n",
      "62/388, train_loss: 0.0784, step time: 1.5469\n",
      "63/388, train_loss: 0.1982, step time: 1.5434\n",
      "64/388, train_loss: 0.1144, step time: 1.5376\n",
      "65/388, train_loss: 0.2579, step time: 1.5499\n",
      "66/388, train_loss: 0.1674, step time: 1.5471\n",
      "67/388, train_loss: 0.1670, step time: 1.5387\n",
      "68/388, train_loss: 0.0796, step time: 1.5461\n",
      "69/388, train_loss: 0.0845, step time: 1.5437\n",
      "70/388, train_loss: 0.2809, step time: 1.5383\n",
      "71/388, train_loss: 0.1809, step time: 1.5563\n",
      "72/388, train_loss: 0.1762, step time: 1.5435\n",
      "73/388, train_loss: 0.1926, step time: 1.5422\n",
      "74/388, train_loss: 0.1376, step time: 1.5472\n",
      "75/388, train_loss: 0.0670, step time: 1.5412\n",
      "76/388, train_loss: 0.1584, step time: 1.5363\n",
      "77/388, train_loss: 0.2231, step time: 1.5347\n",
      "78/388, train_loss: 0.1428, step time: 1.5432\n",
      "79/388, train_loss: 0.1655, step time: 1.5420\n",
      "80/388, train_loss: 0.1704, step time: 1.5349\n",
      "81/388, train_loss: 0.0860, step time: 1.5395\n",
      "82/388, train_loss: 0.1888, step time: 1.5458\n",
      "83/388, train_loss: 0.1328, step time: 1.5527\n",
      "84/388, train_loss: 0.1052, step time: 1.5382\n",
      "85/388, train_loss: 0.0758, step time: 1.5354\n",
      "86/388, train_loss: 0.0983, step time: 1.5420\n",
      "87/388, train_loss: 0.0624, step time: 1.5449\n",
      "88/388, train_loss: 0.1795, step time: 1.5405\n",
      "89/388, train_loss: 0.0823, step time: 1.5371\n",
      "90/388, train_loss: 0.0784, step time: 1.5473\n",
      "91/388, train_loss: 0.0553, step time: 1.5447\n",
      "92/388, train_loss: 0.1097, step time: 1.5354\n",
      "93/388, train_loss: 0.1699, step time: 1.5404\n",
      "94/388, train_loss: 0.1006, step time: 1.5439\n",
      "95/388, train_loss: 0.2484, step time: 1.5529\n",
      "96/388, train_loss: 0.1967, step time: 1.5536\n",
      "97/388, train_loss: 0.0871, step time: 1.5567\n",
      "98/388, train_loss: 0.4202, step time: 1.5419\n",
      "99/388, train_loss: 0.1488, step time: 1.5484\n",
      "100/388, train_loss: 0.3019, step time: 1.5567\n",
      "101/388, train_loss: 0.1179, step time: 1.5483\n",
      "102/388, train_loss: 0.1017, step time: 1.5499\n",
      "103/388, train_loss: 0.2958, step time: 1.5433\n",
      "104/388, train_loss: 0.1212, step time: 1.5469\n",
      "105/388, train_loss: 0.1651, step time: 1.5421\n",
      "106/388, train_loss: 0.1540, step time: 1.5458\n",
      "107/388, train_loss: 0.1751, step time: 1.5452\n",
      "108/388, train_loss: 0.2449, step time: 1.5519\n",
      "109/388, train_loss: 0.2050, step time: 1.5459\n",
      "110/388, train_loss: 0.0914, step time: 1.5450\n",
      "111/388, train_loss: 0.0855, step time: 1.5421\n",
      "112/388, train_loss: 0.1946, step time: 1.5485\n",
      "113/388, train_loss: 0.2687, step time: 1.5548\n",
      "114/388, train_loss: 0.2113, step time: 1.5388\n",
      "115/388, train_loss: 0.4910, step time: 1.5469\n",
      "116/388, train_loss: 0.1013, step time: 1.5425\n",
      "117/388, train_loss: 0.0302, step time: 1.5475\n",
      "118/388, train_loss: 0.0875, step time: 1.5500\n",
      "119/388, train_loss: 0.2431, step time: 1.5397\n",
      "120/388, train_loss: 0.1557, step time: 1.5459\n",
      "121/388, train_loss: 0.2079, step time: 1.5629\n",
      "122/388, train_loss: 0.1549, step time: 1.5448\n",
      "123/388, train_loss: 0.1916, step time: 1.5496\n",
      "124/388, train_loss: 0.1894, step time: 1.5410\n",
      "125/388, train_loss: 0.1903, step time: 1.5539\n",
      "126/388, train_loss: 0.1675, step time: 1.5362\n",
      "127/388, train_loss: 0.0853, step time: 1.5449\n",
      "128/388, train_loss: 0.1086, step time: 1.5437\n",
      "129/388, train_loss: 0.0994, step time: 1.5480\n",
      "130/388, train_loss: 0.1261, step time: 1.5450\n",
      "131/388, train_loss: 0.0726, step time: 1.5435\n",
      "132/388, train_loss: 0.0651, step time: 1.5481\n",
      "133/388, train_loss: 0.1003, step time: 1.5479\n",
      "134/388, train_loss: 0.3552, step time: 1.5534\n",
      "135/388, train_loss: 0.1758, step time: 1.5485\n",
      "136/388, train_loss: 0.1846, step time: 1.5437\n",
      "137/388, train_loss: 0.1102, step time: 1.5468\n",
      "138/388, train_loss: 0.0902, step time: 1.5471\n",
      "139/388, train_loss: 0.2385, step time: 1.5606\n",
      "140/388, train_loss: 0.0660, step time: 1.5457\n",
      "141/388, train_loss: 0.1405, step time: 1.5403\n",
      "142/388, train_loss: 0.0909, step time: 1.5456\n",
      "143/388, train_loss: 0.2403, step time: 1.5533\n",
      "144/388, train_loss: 0.1777, step time: 1.5502\n",
      "145/388, train_loss: 0.1343, step time: 1.5556\n",
      "146/388, train_loss: 0.1357, step time: 1.5588\n",
      "147/388, train_loss: 0.0765, step time: 1.5560\n",
      "148/388, train_loss: 0.1164, step time: 1.5564\n",
      "149/388, train_loss: 0.2321, step time: 1.5497\n",
      "150/388, train_loss: 0.2278, step time: 1.5483\n",
      "151/388, train_loss: 0.0918, step time: 1.5592\n",
      "152/388, train_loss: 0.2318, step time: 1.5543\n",
      "153/388, train_loss: 0.1246, step time: 1.5557\n",
      "154/388, train_loss: 0.1237, step time: 1.5552\n",
      "155/388, train_loss: 0.2611, step time: 1.5424\n",
      "156/388, train_loss: 0.1378, step time: 1.5504\n",
      "157/388, train_loss: 0.1450, step time: 1.5464\n",
      "158/388, train_loss: 0.1242, step time: 1.5426\n",
      "159/388, train_loss: 0.1277, step time: 1.5574\n",
      "160/388, train_loss: 0.1019, step time: 1.5454\n",
      "161/388, train_loss: 0.1146, step time: 1.5413\n",
      "162/388, train_loss: 0.0455, step time: 1.5418\n",
      "163/388, train_loss: 0.0805, step time: 1.5499\n",
      "164/388, train_loss: 0.0397, step time: 1.5432\n",
      "165/388, train_loss: 0.2188, step time: 1.5534\n",
      "166/388, train_loss: 0.1816, step time: 1.5608\n",
      "167/388, train_loss: 0.2135, step time: 1.5511\n",
      "168/388, train_loss: 0.1636, step time: 1.5555\n",
      "169/388, train_loss: 0.2117, step time: 1.5542\n",
      "170/388, train_loss: 0.2745, step time: 1.5417\n",
      "171/388, train_loss: 0.1125, step time: 1.5525\n",
      "172/388, train_loss: 0.1675, step time: 1.5453\n",
      "173/388, train_loss: 0.1335, step time: 1.5443\n",
      "174/388, train_loss: 0.1000, step time: 1.5456\n",
      "175/388, train_loss: 0.1671, step time: 1.5431\n",
      "176/388, train_loss: 0.0438, step time: 1.5517\n",
      "177/388, train_loss: 0.1989, step time: 1.5868\n",
      "178/388, train_loss: 0.2717, step time: 1.5460\n",
      "179/388, train_loss: 0.2469, step time: 1.5468\n",
      "180/388, train_loss: 0.2260, step time: 1.5440\n",
      "181/388, train_loss: 0.2009, step time: 1.5411\n",
      "182/388, train_loss: 0.1771, step time: 1.5474\n",
      "183/388, train_loss: 0.1456, step time: 1.5442\n",
      "184/388, train_loss: 0.2399, step time: 1.5530\n",
      "185/388, train_loss: 0.1296, step time: 1.5519\n",
      "186/388, train_loss: 0.0987, step time: 1.5378\n",
      "187/388, train_loss: 0.2342, step time: 1.5457\n",
      "188/388, train_loss: 0.1829, step time: 1.5468\n",
      "189/388, train_loss: 0.1507, step time: 1.5425\n",
      "190/388, train_loss: 0.1575, step time: 1.5504\n",
      "191/388, train_loss: 0.1613, step time: 1.5457\n",
      "192/388, train_loss: 0.1574, step time: 1.5491\n",
      "193/388, train_loss: 0.0517, step time: 1.5607\n",
      "194/388, train_loss: 0.0974, step time: 1.5441\n",
      "195/388, train_loss: 0.1049, step time: 1.5620\n",
      "196/388, train_loss: 0.0775, step time: 1.5522\n",
      "197/388, train_loss: 0.3009, step time: 1.5570\n",
      "198/388, train_loss: 0.1952, step time: 1.5469\n",
      "199/388, train_loss: 0.1980, step time: 1.5537\n",
      "200/388, train_loss: 0.3930, step time: 1.5549\n",
      "201/388, train_loss: 0.1134, step time: 1.5465\n",
      "202/388, train_loss: 0.1947, step time: 1.5433\n",
      "203/388, train_loss: 0.1354, step time: 1.5478\n",
      "204/388, train_loss: 0.1270, step time: 1.5449\n",
      "205/388, train_loss: 0.1531, step time: 1.5558\n",
      "206/388, train_loss: 0.0979, step time: 1.5479\n",
      "207/388, train_loss: 0.1116, step time: 1.5475\n",
      "208/388, train_loss: 0.0674, step time: 1.5475\n",
      "209/388, train_loss: 0.2949, step time: 1.5421\n",
      "210/388, train_loss: 0.0782, step time: 1.5446\n",
      "211/388, train_loss: 0.0947, step time: 1.5498\n",
      "212/388, train_loss: 0.1145, step time: 1.5522\n",
      "213/388, train_loss: 0.1194, step time: 1.5458\n",
      "214/388, train_loss: 0.0728, step time: 1.5425\n",
      "215/388, train_loss: 0.0918, step time: 1.5453\n",
      "216/388, train_loss: 0.3723, step time: 1.5530\n",
      "217/388, train_loss: 0.1960, step time: 1.5522\n",
      "218/388, train_loss: 0.1298, step time: 1.5452\n",
      "219/388, train_loss: 0.1165, step time: 1.5449\n",
      "220/388, train_loss: 0.1581, step time: 1.5470\n",
      "221/388, train_loss: 0.0957, step time: 1.5515\n",
      "222/388, train_loss: 0.1378, step time: 1.5479\n",
      "223/388, train_loss: 0.0799, step time: 1.5478\n",
      "224/388, train_loss: 0.0996, step time: 1.5467\n",
      "225/388, train_loss: 0.0736, step time: 1.5544\n",
      "226/388, train_loss: 0.1727, step time: 1.5408\n",
      "227/388, train_loss: 0.1871, step time: 1.5485\n",
      "228/388, train_loss: 0.1886, step time: 1.5437\n",
      "229/388, train_loss: 0.1564, step time: 1.5420\n",
      "230/388, train_loss: 0.1936, step time: 1.5498\n",
      "231/388, train_loss: 0.0654, step time: 1.5419\n",
      "232/388, train_loss: 0.2037, step time: 1.5433\n",
      "233/388, train_loss: 0.0913, step time: 1.5485\n",
      "234/388, train_loss: 0.1116, step time: 1.5463\n",
      "235/388, train_loss: 0.1504, step time: 1.5467\n",
      "236/388, train_loss: 0.0942, step time: 1.5473\n",
      "237/388, train_loss: 0.3640, step time: 1.5430\n",
      "238/388, train_loss: 0.2682, step time: 1.5496\n",
      "239/388, train_loss: 0.4282, step time: 1.5431\n",
      "240/388, train_loss: 0.0372, step time: 1.5507\n",
      "241/388, train_loss: 0.2202, step time: 1.5465\n",
      "242/388, train_loss: 0.0594, step time: 1.5438\n",
      "243/388, train_loss: 0.0878, step time: 1.5510\n",
      "244/388, train_loss: 0.1416, step time: 1.5469\n",
      "245/388, train_loss: 0.0719, step time: 1.5439\n",
      "246/388, train_loss: 0.1089, step time: 1.5651\n",
      "247/388, train_loss: 0.1280, step time: 1.5406\n",
      "248/388, train_loss: 0.2125, step time: 1.5531\n",
      "249/388, train_loss: 0.0942, step time: 1.5537\n",
      "250/388, train_loss: 0.1215, step time: 1.5417\n",
      "251/388, train_loss: 0.0942, step time: 1.5507\n",
      "252/388, train_loss: 0.1499, step time: 1.5550\n",
      "253/388, train_loss: 0.0848, step time: 1.5542\n",
      "254/388, train_loss: 0.0569, step time: 1.5415\n",
      "255/388, train_loss: 0.3017, step time: 1.5518\n",
      "256/388, train_loss: 0.2236, step time: 1.5556\n",
      "257/388, train_loss: 0.0696, step time: 1.5394\n",
      "258/388, train_loss: 0.1680, step time: 1.5447\n",
      "259/388, train_loss: 0.0692, step time: 1.5408\n",
      "260/388, train_loss: 0.2273, step time: 1.5398\n",
      "261/388, train_loss: 0.1248, step time: 1.5521\n",
      "262/388, train_loss: 0.0849, step time: 1.5432\n",
      "263/388, train_loss: 0.5309, step time: 1.5420\n",
      "264/388, train_loss: 0.2227, step time: 1.5445\n",
      "265/388, train_loss: 0.2418, step time: 1.5550\n",
      "266/388, train_loss: 0.1132, step time: 1.5379\n",
      "267/388, train_loss: 0.3331, step time: 1.5410\n",
      "268/388, train_loss: 0.2851, step time: 1.5434\n",
      "269/388, train_loss: 0.2811, step time: 1.5389\n",
      "270/388, train_loss: 0.1296, step time: 1.5457\n",
      "271/388, train_loss: 0.0359, step time: 1.5442\n",
      "272/388, train_loss: 0.1643, step time: 1.5382\n",
      "273/388, train_loss: 0.0861, step time: 1.5466\n",
      "274/388, train_loss: 0.1864, step time: 1.5425\n",
      "275/388, train_loss: 0.1443, step time: 1.5509\n",
      "276/388, train_loss: 0.1327, step time: 1.5386\n",
      "277/388, train_loss: 0.1329, step time: 1.5518\n",
      "278/388, train_loss: 0.2008, step time: 1.5423\n",
      "279/388, train_loss: 0.0402, step time: 1.5436\n",
      "280/388, train_loss: 0.1444, step time: 1.5462\n",
      "281/388, train_loss: 0.1530, step time: 1.5450\n",
      "282/388, train_loss: 0.2339, step time: 1.5416\n",
      "283/388, train_loss: 0.0264, step time: 1.5395\n",
      "284/388, train_loss: 0.2403, step time: 1.5442\n",
      "285/388, train_loss: 0.3901, step time: 1.5392\n",
      "286/388, train_loss: 0.2125, step time: 1.5355\n",
      "287/388, train_loss: 0.1835, step time: 1.5454\n",
      "288/388, train_loss: 0.2378, step time: 1.5475\n",
      "289/388, train_loss: 0.0746, step time: 1.5403\n",
      "290/388, train_loss: 0.1300, step time: 1.5464\n",
      "291/388, train_loss: 0.1228, step time: 1.5516\n",
      "292/388, train_loss: 0.1857, step time: 1.5434\n",
      "293/388, train_loss: 0.1488, step time: 1.5518\n",
      "294/388, train_loss: 0.0975, step time: 1.5376\n",
      "295/388, train_loss: 0.0980, step time: 1.5549\n",
      "296/388, train_loss: 0.1722, step time: 1.5525\n",
      "297/388, train_loss: 0.0752, step time: 1.5549\n",
      "298/388, train_loss: 0.2710, step time: 1.5503\n",
      "299/388, train_loss: 0.2132, step time: 1.5468\n",
      "300/388, train_loss: 0.0408, step time: 1.5594\n",
      "301/388, train_loss: 0.1924, step time: 1.5504\n",
      "302/388, train_loss: 0.0712, step time: 1.5483\n",
      "303/388, train_loss: 0.0810, step time: 1.5523\n",
      "304/388, train_loss: 0.2881, step time: 1.5421\n",
      "305/388, train_loss: 0.0934, step time: 1.5606\n",
      "306/388, train_loss: 0.2860, step time: 1.5401\n",
      "307/388, train_loss: 0.1849, step time: 1.5460\n",
      "308/388, train_loss: 0.1030, step time: 1.5480\n",
      "309/388, train_loss: 0.2639, step time: 1.5445\n",
      "310/388, train_loss: 0.0876, step time: 1.5445\n",
      "311/388, train_loss: 0.0653, step time: 1.5436\n",
      "312/388, train_loss: 0.5238, step time: 1.5424\n",
      "313/388, train_loss: 0.1191, step time: 1.5421\n",
      "314/388, train_loss: 0.2171, step time: 1.5516\n",
      "315/388, train_loss: 0.2869, step time: 1.5531\n",
      "316/388, train_loss: 0.2166, step time: 1.5474\n",
      "317/388, train_loss: 0.1043, step time: 1.5396\n",
      "318/388, train_loss: 0.1599, step time: 1.5400\n",
      "319/388, train_loss: 0.0600, step time: 1.5468\n",
      "320/388, train_loss: 0.1530, step time: 1.5407\n",
      "321/388, train_loss: 0.0803, step time: 1.5433\n",
      "322/388, train_loss: 0.1132, step time: 1.5453\n",
      "323/388, train_loss: 0.0502, step time: 1.5421\n",
      "324/388, train_loss: 0.2056, step time: 1.5498\n",
      "325/388, train_loss: 0.3639, step time: 1.5432\n",
      "326/388, train_loss: 0.2500, step time: 1.5417\n",
      "327/388, train_loss: 0.1002, step time: 1.5470\n",
      "328/388, train_loss: 0.0898, step time: 1.5448\n",
      "329/388, train_loss: 0.0949, step time: 1.5440\n",
      "330/388, train_loss: 0.1539, step time: 1.5482\n",
      "331/388, train_loss: 0.2428, step time: 1.5459\n",
      "332/388, train_loss: 0.0948, step time: 1.5406\n",
      "333/388, train_loss: 0.1067, step time: 1.5459\n",
      "334/388, train_loss: 0.0515, step time: 1.5458\n",
      "335/388, train_loss: 0.0678, step time: 1.5430\n",
      "336/388, train_loss: 0.4004, step time: 1.5431\n",
      "337/388, train_loss: 0.1315, step time: 1.5492\n",
      "338/388, train_loss: 0.0907, step time: 1.5442\n",
      "339/388, train_loss: 0.0925, step time: 1.5462\n",
      "340/388, train_loss: 0.1798, step time: 1.5522\n",
      "341/388, train_loss: 0.4112, step time: 1.5483\n",
      "342/388, train_loss: 0.1879, step time: 1.5424\n",
      "343/388, train_loss: 0.1343, step time: 1.5497\n",
      "344/388, train_loss: 0.1603, step time: 1.5557\n",
      "345/388, train_loss: 0.1682, step time: 1.5448\n",
      "346/388, train_loss: 0.3579, step time: 1.5412\n",
      "347/388, train_loss: 0.1106, step time: 1.5629\n",
      "348/388, train_loss: 0.2694, step time: 1.5531\n",
      "349/388, train_loss: 0.1238, step time: 1.5551\n",
      "350/388, train_loss: 0.1310, step time: 1.5412\n",
      "351/388, train_loss: 0.1423, step time: 1.5523\n",
      "352/388, train_loss: 0.1739, step time: 1.5575\n",
      "353/388, train_loss: 0.0715, step time: 1.5550\n",
      "354/388, train_loss: 0.1448, step time: 1.5537\n",
      "355/388, train_loss: 0.2036, step time: 1.5587\n",
      "356/388, train_loss: 0.0586, step time: 1.5569\n",
      "357/388, train_loss: 0.2276, step time: 1.5518\n",
      "358/388, train_loss: 0.1681, step time: 1.5444\n",
      "359/388, train_loss: 0.1929, step time: 1.5430\n",
      "360/388, train_loss: 0.0548, step time: 1.5468\n",
      "361/388, train_loss: 0.0615, step time: 1.5648\n",
      "362/388, train_loss: 0.0816, step time: 1.5427\n",
      "363/388, train_loss: 0.2016, step time: 1.5546\n",
      "364/388, train_loss: 0.1610, step time: 1.5558\n",
      "365/388, train_loss: 0.2005, step time: 1.5410\n",
      "366/388, train_loss: 0.3670, step time: 1.5446\n",
      "367/388, train_loss: 0.2513, step time: 1.5538\n",
      "368/388, train_loss: 0.1779, step time: 1.5520\n",
      "369/388, train_loss: 0.1566, step time: 1.5558\n",
      "370/388, train_loss: 0.1548, step time: 1.5465\n",
      "371/388, train_loss: 0.0480, step time: 1.5554\n",
      "372/388, train_loss: 0.1425, step time: 1.5552\n",
      "373/388, train_loss: 0.1582, step time: 1.5517\n",
      "374/388, train_loss: 0.2951, step time: 1.5657\n",
      "375/388, train_loss: 0.1146, step time: 1.5510\n",
      "376/388, train_loss: 0.0837, step time: 1.5534\n",
      "377/388, train_loss: 0.0760, step time: 1.5562\n",
      "378/388, train_loss: 0.0897, step time: 1.5437\n",
      "379/388, train_loss: 0.0459, step time: 1.5524\n",
      "380/388, train_loss: 0.1455, step time: 1.5435\n",
      "381/388, train_loss: 0.1020, step time: 1.5525\n",
      "382/388, train_loss: 0.1052, step time: 1.5544\n",
      "383/388, train_loss: 0.1336, step time: 1.5508\n",
      "384/388, train_loss: 0.2192, step time: 1.5504\n",
      "385/388, train_loss: 0.0632, step time: 1.5548\n",
      "386/388, train_loss: 0.1127, step time: 1.5538\n",
      "387/388, train_loss: 0.0997, step time: 1.5548\n",
      "388/388, train_loss: 0.1336, step time: 1.5558\n",
      "epoch 90 average loss: 0.1582\n",
      "current epoch: 90 current mean dice: 0.7802 tc: 0.8284 wt: 0.9059 et: 0.6063\n",
      "best mean dice: 0.7817 at epoch: 84\n",
      "time consuming of epoch 90 is: 707.6491\n",
      "----------\n",
      "epoch 91/100\n",
      "1/388, train_loss: 0.0781, step time: 1.5909\n",
      "2/388, train_loss: 0.0974, step time: 1.5548\n",
      "3/388, train_loss: 0.0907, step time: 1.5611\n",
      "4/388, train_loss: 0.0803, step time: 1.5550\n",
      "5/388, train_loss: 0.2109, step time: 1.5529\n",
      "6/388, train_loss: 0.2801, step time: 1.5612\n",
      "7/388, train_loss: 0.1915, step time: 1.5581\n",
      "8/388, train_loss: 0.1396, step time: 1.5510\n",
      "9/388, train_loss: 0.0954, step time: 1.5572\n",
      "10/388, train_loss: 0.2643, step time: 1.5553\n",
      "11/388, train_loss: 0.2135, step time: 1.5553\n",
      "12/388, train_loss: 0.1807, step time: 1.5527\n",
      "13/388, train_loss: 0.4119, step time: 1.5539\n",
      "14/388, train_loss: 0.1991, step time: 1.5536\n",
      "15/388, train_loss: 0.3516, step time: 1.5581\n",
      "16/388, train_loss: 0.2331, step time: 1.5540\n",
      "17/388, train_loss: 0.0744, step time: 1.5542\n",
      "18/388, train_loss: 0.5141, step time: 1.5578\n",
      "19/388, train_loss: 0.1321, step time: 1.5590\n",
      "20/388, train_loss: 0.1944, step time: 1.5553\n",
      "21/388, train_loss: 0.1865, step time: 1.5602\n",
      "22/388, train_loss: 0.0606, step time: 1.5509\n",
      "23/388, train_loss: 0.1571, step time: 1.5568\n",
      "24/388, train_loss: 0.2574, step time: 1.5554\n",
      "25/388, train_loss: 0.1504, step time: 1.5570\n",
      "26/388, train_loss: 0.0561, step time: 1.5534\n",
      "27/388, train_loss: 0.0704, step time: 1.5588\n",
      "28/388, train_loss: 0.1098, step time: 1.5552\n",
      "29/388, train_loss: 0.5496, step time: 1.5542\n",
      "30/388, train_loss: 0.2364, step time: 1.5538\n",
      "31/388, train_loss: 0.0707, step time: 1.5595\n",
      "32/388, train_loss: 0.1435, step time: 1.5560\n",
      "33/388, train_loss: 0.0970, step time: 1.5526\n",
      "34/388, train_loss: 0.0939, step time: 1.5497\n",
      "35/388, train_loss: 0.0390, step time: 1.5645\n",
      "36/388, train_loss: 0.0856, step time: 1.5565\n",
      "37/388, train_loss: 0.2325, step time: 1.5513\n",
      "38/388, train_loss: 0.1626, step time: 1.5582\n",
      "39/388, train_loss: 0.2669, step time: 1.5569\n",
      "40/388, train_loss: 0.1496, step time: 1.5586\n",
      "41/388, train_loss: 0.1918, step time: 1.5533\n",
      "42/388, train_loss: 0.0639, step time: 1.5501\n",
      "43/388, train_loss: 0.1073, step time: 1.5585\n",
      "44/388, train_loss: 0.1947, step time: 1.5492\n",
      "45/388, train_loss: 0.1167, step time: 1.5515\n",
      "46/388, train_loss: 0.0896, step time: 1.5541\n",
      "47/388, train_loss: 0.1918, step time: 1.5581\n",
      "48/388, train_loss: 0.2824, step time: 1.5557\n",
      "49/388, train_loss: 0.2910, step time: 1.5509\n",
      "50/388, train_loss: 0.0858, step time: 1.5534\n",
      "51/388, train_loss: 0.1360, step time: 1.5537\n",
      "52/388, train_loss: 0.1803, step time: 1.5548\n",
      "53/388, train_loss: 0.0919, step time: 1.5528\n",
      "54/388, train_loss: 0.1954, step time: 1.5526\n",
      "55/388, train_loss: 0.1456, step time: 1.5460\n",
      "56/388, train_loss: 0.2683, step time: 1.5531\n",
      "57/388, train_loss: 0.2895, step time: 1.5551\n",
      "58/388, train_loss: 0.1096, step time: 1.5527\n",
      "59/388, train_loss: 0.2874, step time: 1.5353\n",
      "60/388, train_loss: 0.4745, step time: 1.5527\n",
      "61/388, train_loss: 0.2379, step time: 1.5513\n",
      "62/388, train_loss: 0.1045, step time: 1.5591\n",
      "63/388, train_loss: 0.1193, step time: 1.5469\n",
      "64/388, train_loss: 0.1800, step time: 1.5496\n",
      "65/388, train_loss: 0.0870, step time: 1.5527\n",
      "66/388, train_loss: 0.2328, step time: 1.5556\n",
      "67/388, train_loss: 0.0681, step time: 1.5601\n",
      "68/388, train_loss: 0.1859, step time: 1.5703\n",
      "69/388, train_loss: 0.2095, step time: 1.5721\n",
      "70/388, train_loss: 0.1441, step time: 1.5636\n",
      "71/388, train_loss: 0.2887, step time: 1.5405\n",
      "72/388, train_loss: 0.1879, step time: 1.5554\n",
      "73/388, train_loss: 0.1042, step time: 1.5558\n",
      "74/388, train_loss: 0.1465, step time: 1.5501\n",
      "75/388, train_loss: 0.1703, step time: 1.5633\n",
      "76/388, train_loss: 0.1601, step time: 1.5502\n",
      "77/388, train_loss: 0.2120, step time: 1.5535\n",
      "78/388, train_loss: 0.1812, step time: 1.5563\n",
      "79/388, train_loss: 0.1101, step time: 1.5625\n",
      "80/388, train_loss: 0.0959, step time: 1.5573\n",
      "81/388, train_loss: 0.0744, step time: 1.5529\n",
      "82/388, train_loss: 0.1147, step time: 1.5567\n",
      "83/388, train_loss: 0.2016, step time: 1.5580\n",
      "84/388, train_loss: 0.1020, step time: 1.5424\n",
      "85/388, train_loss: 0.1187, step time: 1.5544\n",
      "86/388, train_loss: 0.1469, step time: 1.5556\n",
      "87/388, train_loss: 0.2577, step time: 1.5614\n",
      "88/388, train_loss: 0.1610, step time: 1.5557\n",
      "89/388, train_loss: 0.0835, step time: 1.5488\n",
      "90/388, train_loss: 0.1803, step time: 1.5557\n",
      "91/388, train_loss: 0.2410, step time: 1.5596\n",
      "92/388, train_loss: 0.1507, step time: 1.5520\n",
      "93/388, train_loss: 0.1008, step time: 1.5535\n",
      "94/388, train_loss: 0.2563, step time: 1.5560\n",
      "95/388, train_loss: 0.2242, step time: 1.5372\n",
      "96/388, train_loss: 0.1749, step time: 1.5535\n",
      "97/388, train_loss: 0.2843, step time: 1.5577\n",
      "98/388, train_loss: 0.3373, step time: 1.5509\n",
      "99/388, train_loss: 0.2487, step time: 1.5570\n",
      "100/388, train_loss: 0.1346, step time: 1.5528\n",
      "101/388, train_loss: 0.0512, step time: 1.5531\n",
      "102/388, train_loss: 0.1365, step time: 1.5541\n",
      "103/388, train_loss: 0.1843, step time: 1.5571\n",
      "104/388, train_loss: 0.1188, step time: 1.5592\n",
      "105/388, train_loss: 0.1453, step time: 1.5547\n",
      "106/388, train_loss: 0.4212, step time: 1.5534\n",
      "107/388, train_loss: 0.1068, step time: 1.5579\n",
      "108/388, train_loss: 0.0769, step time: 1.5511\n",
      "109/388, train_loss: 0.1295, step time: 1.5537\n",
      "110/388, train_loss: 0.0862, step time: 1.5533\n",
      "111/388, train_loss: 0.0750, step time: 1.5600\n",
      "112/388, train_loss: 0.0518, step time: 1.5556\n",
      "113/388, train_loss: 0.1796, step time: 1.5537\n",
      "114/388, train_loss: 0.0865, step time: 1.5537\n",
      "115/388, train_loss: 0.1375, step time: 1.5389\n",
      "116/388, train_loss: 0.2918, step time: 1.5540\n",
      "117/388, train_loss: 0.1902, step time: 1.5556\n",
      "118/388, train_loss: 0.1881, step time: 1.5561\n",
      "119/388, train_loss: 0.0918, step time: 1.5298\n",
      "120/388, train_loss: 0.3677, step time: 1.5587\n",
      "121/388, train_loss: 0.1487, step time: 1.5559\n",
      "122/388, train_loss: 0.1212, step time: 1.5529\n",
      "123/388, train_loss: 0.1809, step time: 1.5585\n",
      "124/388, train_loss: 0.1100, step time: 1.5529\n",
      "125/388, train_loss: 0.2764, step time: 1.5547\n",
      "126/388, train_loss: 0.0902, step time: 1.5545\n",
      "127/388, train_loss: 0.1553, step time: 1.5568\n",
      "128/388, train_loss: 0.1135, step time: 1.5565\n",
      "129/388, train_loss: 0.2234, step time: 1.5550\n",
      "130/388, train_loss: 0.0961, step time: 1.5517\n",
      "131/388, train_loss: 0.1776, step time: 1.5360\n",
      "132/388, train_loss: 0.1603, step time: 1.5539\n",
      "133/388, train_loss: 0.2096, step time: 1.5504\n",
      "134/388, train_loss: 0.0897, step time: 1.5582\n",
      "135/388, train_loss: 0.2310, step time: 1.5614\n",
      "136/388, train_loss: 0.1398, step time: 1.5559\n",
      "137/388, train_loss: 0.0971, step time: 1.5532\n",
      "138/388, train_loss: 0.1064, step time: 1.5550\n",
      "139/388, train_loss: 0.0941, step time: 1.5572\n",
      "140/388, train_loss: 0.0804, step time: 1.5602\n",
      "141/388, train_loss: 0.0682, step time: 1.5527\n",
      "142/388, train_loss: 0.1460, step time: 1.5655\n",
      "143/388, train_loss: 0.1632, step time: 1.5347\n",
      "144/388, train_loss: 0.1245, step time: 1.5542\n",
      "145/388, train_loss: 0.2329, step time: 1.5535\n",
      "146/388, train_loss: 0.0761, step time: 1.5552\n",
      "147/388, train_loss: 0.1041, step time: 1.5331\n",
      "148/388, train_loss: 0.2010, step time: 1.5524\n",
      "149/388, train_loss: 0.1947, step time: 1.5548\n",
      "150/388, train_loss: 0.1450, step time: 1.5538\n",
      "151/388, train_loss: 0.1237, step time: 1.5373\n",
      "152/388, train_loss: 0.0947, step time: 1.5553\n",
      "153/388, train_loss: 0.2182, step time: 1.5534\n",
      "154/388, train_loss: 0.1205, step time: 1.5530\n",
      "155/388, train_loss: 0.2949, step time: 1.5366\n",
      "156/388, train_loss: 0.1016, step time: 1.5519\n",
      "157/388, train_loss: 0.2396, step time: 1.5547\n",
      "158/388, train_loss: 0.1625, step time: 1.5550\n",
      "159/388, train_loss: 0.1007, step time: 1.5566\n",
      "160/388, train_loss: 0.0822, step time: 1.5640\n",
      "161/388, train_loss: 0.4012, step time: 1.5524\n",
      "162/388, train_loss: 0.0652, step time: 1.5563\n",
      "163/388, train_loss: 0.1491, step time: 1.5324\n",
      "164/388, train_loss: 0.2436, step time: 1.5681\n",
      "165/388, train_loss: 0.2285, step time: 1.5533\n",
      "166/388, train_loss: 0.2526, step time: 1.5552\n",
      "167/388, train_loss: 0.1089, step time: 1.5591\n",
      "168/388, train_loss: 0.1407, step time: 1.5539\n",
      "169/388, train_loss: 0.3268, step time: 1.5554\n",
      "170/388, train_loss: 0.2666, step time: 1.5540\n",
      "171/388, train_loss: 0.1658, step time: 1.5590\n",
      "172/388, train_loss: 0.1561, step time: 1.5534\n",
      "173/388, train_loss: 0.1412, step time: 1.5529\n",
      "174/388, train_loss: 0.0865, step time: 1.5590\n",
      "175/388, train_loss: 0.0899, step time: 1.5399\n",
      "176/388, train_loss: 0.0840, step time: 1.5527\n",
      "177/388, train_loss: 0.4379, step time: 1.5589\n",
      "178/388, train_loss: 0.1115, step time: 1.5557\n",
      "179/388, train_loss: 0.1248, step time: 1.5520\n",
      "180/388, train_loss: 0.2104, step time: 1.5526\n",
      "181/388, train_loss: 0.3324, step time: 1.5544\n",
      "182/388, train_loss: 0.0907, step time: 1.5542\n",
      "183/388, train_loss: 0.1804, step time: 1.5587\n",
      "184/388, train_loss: 0.1635, step time: 1.5542\n",
      "185/388, train_loss: 0.0742, step time: 1.5520\n",
      "186/388, train_loss: 0.0913, step time: 1.5562\n",
      "187/388, train_loss: 0.0795, step time: 1.5587\n",
      "188/388, train_loss: 0.1097, step time: 1.5501\n",
      "189/388, train_loss: 0.1154, step time: 1.5522\n",
      "190/388, train_loss: 0.2450, step time: 1.5588\n",
      "191/388, train_loss: 0.1118, step time: 1.5551\n",
      "192/388, train_loss: 0.2147, step time: 1.5538\n",
      "193/388, train_loss: 0.1321, step time: 1.5590\n",
      "194/388, train_loss: 0.1450, step time: 1.5535\n",
      "195/388, train_loss: 0.0902, step time: 1.5560\n",
      "196/388, train_loss: 0.1929, step time: 1.5536\n",
      "197/388, train_loss: 0.0672, step time: 1.5531\n",
      "198/388, train_loss: 0.0569, step time: 1.5546\n",
      "199/388, train_loss: 0.0732, step time: 1.5306\n",
      "200/388, train_loss: 0.1368, step time: 1.5557\n",
      "201/388, train_loss: 0.1419, step time: 1.5544\n",
      "202/388, train_loss: 0.1612, step time: 1.5546\n",
      "203/388, train_loss: 0.1175, step time: 1.5665\n",
      "204/388, train_loss: 0.1247, step time: 1.5551\n",
      "205/388, train_loss: 0.1233, step time: 1.5545\n",
      "206/388, train_loss: 0.0758, step time: 1.5572\n",
      "207/388, train_loss: 0.1045, step time: 1.5570\n",
      "208/388, train_loss: 0.2310, step time: 1.5537\n",
      "209/388, train_loss: 0.1593, step time: 1.5555\n",
      "210/388, train_loss: 0.0482, step time: 1.5538\n",
      "211/388, train_loss: 0.0892, step time: 1.5570\n",
      "212/388, train_loss: 0.1140, step time: 1.5559\n",
      "213/388, train_loss: 0.1628, step time: 1.5517\n",
      "214/388, train_loss: 0.1497, step time: 1.5588\n",
      "215/388, train_loss: 0.1197, step time: 1.5580\n",
      "216/388, train_loss: 0.3649, step time: 1.5585\n",
      "217/388, train_loss: 0.2818, step time: 1.5554\n",
      "218/388, train_loss: 0.0906, step time: 1.5536\n",
      "219/388, train_loss: 0.2167, step time: 1.5581\n",
      "220/388, train_loss: 0.0354, step time: 1.5564\n",
      "221/388, train_loss: 0.0957, step time: 1.5543\n",
      "222/388, train_loss: 0.0776, step time: 1.5579\n",
      "223/388, train_loss: 0.0690, step time: 1.5556\n",
      "224/388, train_loss: 0.2054, step time: 1.5681\n",
      "225/388, train_loss: 0.0928, step time: 1.5546\n",
      "226/388, train_loss: 0.1741, step time: 1.5593\n",
      "227/388, train_loss: 0.0700, step time: 1.5513\n",
      "228/388, train_loss: 0.1121, step time: 1.5541\n",
      "229/388, train_loss: 0.1324, step time: 1.5536\n",
      "230/388, train_loss: 0.1569, step time: 1.5547\n",
      "231/388, train_loss: 0.0868, step time: 1.5539\n",
      "232/388, train_loss: 0.1678, step time: 1.5554\n",
      "233/388, train_loss: 0.0971, step time: 1.5519\n",
      "234/388, train_loss: 0.3807, step time: 1.5614\n",
      "235/388, train_loss: 0.1457, step time: 1.5569\n",
      "236/388, train_loss: 0.0841, step time: 1.5547\n",
      "237/388, train_loss: 0.2020, step time: 1.5543\n",
      "238/388, train_loss: 0.0903, step time: 1.5600\n",
      "239/388, train_loss: 0.0748, step time: 1.5574\n",
      "240/388, train_loss: 0.2514, step time: 1.5729\n",
      "241/388, train_loss: 0.1301, step time: 1.5547\n",
      "242/388, train_loss: 0.2384, step time: 1.5547\n",
      "243/388, train_loss: 0.2855, step time: 1.5371\n",
      "244/388, train_loss: 0.1406, step time: 1.5434\n",
      "245/388, train_loss: 0.4410, step time: 1.5426\n",
      "246/388, train_loss: 0.1660, step time: 1.5605\n",
      "247/388, train_loss: 0.0645, step time: 1.5563\n",
      "248/388, train_loss: 0.1985, step time: 1.5454\n",
      "249/388, train_loss: 0.1727, step time: 1.5505\n",
      "250/388, train_loss: 0.1882, step time: 1.5633\n",
      "251/388, train_loss: 0.1201, step time: 1.5565\n",
      "252/388, train_loss: 0.1822, step time: 1.5520\n",
      "253/388, train_loss: 0.1464, step time: 1.5544\n",
      "254/388, train_loss: 0.1259, step time: 1.5589\n",
      "255/388, train_loss: 0.0674, step time: 1.5601\n",
      "256/388, train_loss: 0.0924, step time: 1.5538\n",
      "257/388, train_loss: 0.1482, step time: 1.5465\n",
      "258/388, train_loss: 0.3420, step time: 1.5589\n",
      "259/388, train_loss: 0.1581, step time: 1.5529\n",
      "260/388, train_loss: 0.0880, step time: 1.5564\n",
      "261/388, train_loss: 0.1083, step time: 1.5519\n",
      "262/388, train_loss: 0.1914, step time: 1.5558\n",
      "263/388, train_loss: 0.0890, step time: 1.5626\n",
      "264/388, train_loss: 0.0810, step time: 1.5516\n",
      "265/388, train_loss: 0.2857, step time: 1.5530\n",
      "266/388, train_loss: 0.0438, step time: 1.5597\n",
      "267/388, train_loss: 0.2889, step time: 1.5357\n",
      "268/388, train_loss: 0.1264, step time: 1.5549\n",
      "269/388, train_loss: 0.0893, step time: 1.5582\n",
      "270/388, train_loss: 0.1870, step time: 1.5533\n",
      "271/388, train_loss: 0.1353, step time: 1.5329\n",
      "272/388, train_loss: 0.0818, step time: 1.5565\n",
      "273/388, train_loss: 0.1319, step time: 1.5514\n",
      "274/388, train_loss: 0.2038, step time: 1.5551\n",
      "275/388, train_loss: 0.3786, step time: 1.5379\n",
      "276/388, train_loss: 0.1938, step time: 1.5546\n",
      "277/388, train_loss: 0.1000, step time: 1.5572\n",
      "278/388, train_loss: 0.2376, step time: 1.5581\n",
      "279/388, train_loss: 0.1621, step time: 1.5559\n",
      "280/388, train_loss: 0.2761, step time: 1.5542\n",
      "281/388, train_loss: 0.1231, step time: 1.5514\n",
      "282/388, train_loss: 0.0670, step time: 1.5602\n",
      "283/388, train_loss: 0.0712, step time: 1.5576\n",
      "284/388, train_loss: 0.0302, step time: 1.5553\n",
      "285/388, train_loss: 0.2520, step time: 1.5509\n",
      "286/388, train_loss: 0.2508, step time: 1.5581\n",
      "287/388, train_loss: 0.1384, step time: 1.5602\n",
      "288/388, train_loss: 0.0939, step time: 1.5530\n",
      "289/388, train_loss: 0.1573, step time: 1.5520\n",
      "290/388, train_loss: 0.1192, step time: 1.5550\n",
      "291/388, train_loss: 0.1822, step time: 1.5313\n",
      "292/388, train_loss: 0.2551, step time: 1.5570\n",
      "293/388, train_loss: 0.1131, step time: 1.5617\n",
      "294/388, train_loss: 0.1400, step time: 1.5592\n",
      "295/388, train_loss: 0.1318, step time: 1.5392\n",
      "296/388, train_loss: 0.1684, step time: 1.5574\n",
      "297/388, train_loss: 0.0959, step time: 1.5536\n",
      "298/388, train_loss: 0.0609, step time: 1.5541\n",
      "299/388, train_loss: 0.1103, step time: 1.5661\n",
      "300/388, train_loss: 0.2195, step time: 1.5540\n",
      "301/388, train_loss: 0.1008, step time: 1.5536\n",
      "302/388, train_loss: 0.1832, step time: 1.5592\n",
      "303/388, train_loss: 0.0260, step time: 1.5317\n",
      "304/388, train_loss: 0.1072, step time: 1.5529\n",
      "305/388, train_loss: 0.0829, step time: 1.5545\n",
      "306/388, train_loss: 0.2085, step time: 1.5610\n",
      "307/388, train_loss: 0.1128, step time: 1.5348\n",
      "308/388, train_loss: 0.0697, step time: 1.5557\n",
      "309/388, train_loss: 0.1335, step time: 1.5545\n",
      "310/388, train_loss: 0.0472, step time: 1.5568\n",
      "311/388, train_loss: 0.3903, step time: 1.5361\n",
      "312/388, train_loss: 0.1998, step time: 1.5524\n",
      "313/388, train_loss: 0.1184, step time: 1.5570\n",
      "314/388, train_loss: 0.1571, step time: 1.5561\n",
      "315/388, train_loss: 0.1645, step time: 1.5577\n",
      "316/388, train_loss: 0.1635, step time: 1.5547\n",
      "317/388, train_loss: 0.0600, step time: 1.5504\n",
      "318/388, train_loss: 0.2268, step time: 1.5600\n",
      "319/388, train_loss: 0.1114, step time: 1.5351\n",
      "320/388, train_loss: 0.1334, step time: 1.5527\n",
      "321/388, train_loss: 0.0670, step time: 1.5527\n",
      "322/388, train_loss: 0.0955, step time: 1.5565\n",
      "323/388, train_loss: 0.1625, step time: 1.5341\n",
      "324/388, train_loss: 0.1596, step time: 1.5538\n",
      "325/388, train_loss: 0.0430, step time: 1.5554\n",
      "326/388, train_loss: 0.0879, step time: 1.5576\n",
      "327/388, train_loss: 0.1867, step time: 1.5594\n",
      "328/388, train_loss: 0.1328, step time: 1.5555\n",
      "329/388, train_loss: 0.0849, step time: 1.5499\n",
      "330/388, train_loss: 0.1417, step time: 1.5610\n",
      "331/388, train_loss: 0.2343, step time: 1.5586\n",
      "332/388, train_loss: 0.1060, step time: 1.5546\n",
      "333/388, train_loss: 0.3892, step time: 1.5540\n",
      "334/388, train_loss: 0.0852, step time: 1.5555\n",
      "335/388, train_loss: 0.0614, step time: 1.5600\n",
      "336/388, train_loss: 0.2192, step time: 1.5536\n",
      "337/388, train_loss: 0.2550, step time: 1.5509\n",
      "338/388, train_loss: 0.1838, step time: 1.5623\n",
      "339/388, train_loss: 0.1837, step time: 1.5608\n",
      "340/388, train_loss: 0.2325, step time: 1.5535\n",
      "341/388, train_loss: 0.0852, step time: 1.5537\n",
      "342/388, train_loss: 0.0544, step time: 1.5599\n",
      "343/388, train_loss: 0.2060, step time: 1.5581\n",
      "344/388, train_loss: 0.0416, step time: 1.5568\n",
      "345/388, train_loss: 0.1300, step time: 1.5584\n",
      "346/388, train_loss: 0.1437, step time: 1.5572\n",
      "347/388, train_loss: 0.1472, step time: 1.5376\n",
      "348/388, train_loss: 0.0972, step time: 1.5544\n",
      "349/388, train_loss: 0.0629, step time: 1.5525\n",
      "350/388, train_loss: 0.1478, step time: 1.5638\n",
      "351/388, train_loss: 0.2969, step time: 1.5638\n",
      "352/388, train_loss: 0.2610, step time: 1.5516\n",
      "353/388, train_loss: 0.1275, step time: 1.5544\n",
      "354/388, train_loss: 0.0674, step time: 1.5577\n",
      "355/388, train_loss: 0.2198, step time: 1.5385\n",
      "356/388, train_loss: 0.2145, step time: 1.5661\n",
      "357/388, train_loss: 0.1475, step time: 1.5619\n",
      "358/388, train_loss: 0.0760, step time: 1.5585\n",
      "359/388, train_loss: 0.1537, step time: 1.5634\n",
      "360/388, train_loss: 0.2717, step time: 1.5534\n",
      "361/388, train_loss: 0.0448, step time: 1.5558\n",
      "362/388, train_loss: 0.1139, step time: 1.5619\n",
      "363/388, train_loss: 0.0892, step time: 1.5584\n",
      "364/388, train_loss: 0.1023, step time: 1.5616\n",
      "365/388, train_loss: 0.1906, step time: 1.5537\n",
      "366/388, train_loss: 0.2760, step time: 1.5585\n",
      "367/388, train_loss: 0.1729, step time: 1.5547\n",
      "368/388, train_loss: 0.1289, step time: 1.5616\n",
      "369/388, train_loss: 0.1889, step time: 1.5541\n",
      "370/388, train_loss: 0.0850, step time: 1.5541\n",
      "371/388, train_loss: 0.0868, step time: 1.5458\n",
      "372/388, train_loss: 0.0534, step time: 1.5509\n",
      "373/388, train_loss: 0.1670, step time: 1.5561\n",
      "374/388, train_loss: 0.0998, step time: 1.5604\n",
      "375/388, train_loss: 0.0440, step time: 1.5298\n",
      "376/388, train_loss: 0.0781, step time: 1.5517\n",
      "377/388, train_loss: 0.1885, step time: 1.5545\n",
      "378/388, train_loss: 0.1184, step time: 1.5640\n",
      "379/388, train_loss: 0.1644, step time: 1.5352\n",
      "380/388, train_loss: 0.1011, step time: 1.5543\n",
      "381/388, train_loss: 0.2334, step time: 1.5554\n",
      "382/388, train_loss: 0.1587, step time: 1.5620\n",
      "383/388, train_loss: 0.3275, step time: 1.5606\n",
      "384/388, train_loss: 0.1807, step time: 1.5537\n",
      "385/388, train_loss: 0.1983, step time: 1.5507\n",
      "386/388, train_loss: 0.0572, step time: 1.5601\n",
      "387/388, train_loss: 0.0950, step time: 1.5588\n",
      "388/388, train_loss: 0.0582, step time: 1.5494\n",
      "epoch 91 average loss: 0.1581\n",
      "current epoch: 91 current mean dice: 0.7813 tc: 0.8295 wt: 0.9061 et: 0.6083\n",
      "best mean dice: 0.7817 at epoch: 84\n",
      "time consuming of epoch 91 is: 711.0088\n",
      "----------\n",
      "epoch 92/100\n",
      "1/388, train_loss: 0.0903, step time: 1.5620\n",
      "2/388, train_loss: 0.1549, step time: 1.5556\n",
      "3/388, train_loss: 0.0983, step time: 1.5552\n",
      "4/388, train_loss: 0.2068, step time: 1.5544\n",
      "5/388, train_loss: 0.1729, step time: 1.5556\n",
      "6/388, train_loss: 0.0779, step time: 1.5511\n",
      "7/388, train_loss: 0.3187, step time: 1.5519\n",
      "8/388, train_loss: 0.1269, step time: 1.5538\n",
      "9/388, train_loss: 0.0940, step time: 1.5578\n",
      "10/388, train_loss: 0.1584, step time: 1.5533\n",
      "11/388, train_loss: 0.0988, step time: 1.5580\n",
      "12/388, train_loss: 0.3033, step time: 1.5538\n",
      "13/388, train_loss: 0.3485, step time: 1.5543\n",
      "14/388, train_loss: 0.0904, step time: 1.5572\n",
      "15/388, train_loss: 0.1798, step time: 1.5513\n",
      "16/388, train_loss: 0.0769, step time: 1.5794\n",
      "17/388, train_loss: 0.1319, step time: 1.5554\n",
      "18/388, train_loss: 0.0622, step time: 1.5511\n",
      "19/388, train_loss: 0.1326, step time: 1.5506\n",
      "20/388, train_loss: 0.3773, step time: 1.5569\n",
      "21/388, train_loss: 0.0896, step time: 1.5311\n",
      "22/388, train_loss: 0.0934, step time: 1.5597\n",
      "23/388, train_loss: 0.1639, step time: 1.5540\n",
      "24/388, train_loss: 0.0612, step time: 1.5519\n",
      "25/388, train_loss: 0.2580, step time: 1.5391\n",
      "26/388, train_loss: 0.0263, step time: 1.5555\n",
      "27/388, train_loss: 0.1483, step time: 1.5534\n",
      "28/388, train_loss: 0.2002, step time: 1.5551\n",
      "29/388, train_loss: 0.2047, step time: 1.5577\n",
      "30/388, train_loss: 0.2934, step time: 1.5706\n",
      "31/388, train_loss: 0.1039, step time: 1.5539\n",
      "32/388, train_loss: 0.1670, step time: 1.5547\n",
      "33/388, train_loss: 0.1532, step time: 1.5687\n",
      "34/388, train_loss: 0.2008, step time: 1.5545\n",
      "35/388, train_loss: 0.5039, step time: 1.5598\n",
      "36/388, train_loss: 0.2577, step time: 1.5570\n",
      "37/388, train_loss: 0.2249, step time: 1.5382\n",
      "38/388, train_loss: 0.1317, step time: 1.5536\n",
      "39/388, train_loss: 0.2371, step time: 1.5474\n",
      "40/388, train_loss: 0.1128, step time: 1.5679\n",
      "41/388, train_loss: 0.1160, step time: 1.5329\n",
      "42/388, train_loss: 0.2957, step time: 1.5536\n",
      "43/388, train_loss: 0.1688, step time: 1.5571\n",
      "44/388, train_loss: 0.0743, step time: 1.5549\n",
      "45/388, train_loss: 0.1526, step time: 1.5682\n",
      "46/388, train_loss: 0.1337, step time: 1.5587\n",
      "47/388, train_loss: 0.0518, step time: 1.5480\n",
      "48/388, train_loss: 0.2042, step time: 1.5572\n",
      "49/388, train_loss: 0.2025, step time: 1.5368\n",
      "50/388, train_loss: 0.2481, step time: 1.5529\n",
      "51/388, train_loss: 0.0670, step time: 1.5553\n",
      "52/388, train_loss: 0.1472, step time: 1.5541\n",
      "53/388, train_loss: 0.2422, step time: 1.5591\n",
      "54/388, train_loss: 0.0716, step time: 1.5561\n",
      "55/388, train_loss: 0.1088, step time: 1.5537\n",
      "56/388, train_loss: 0.0818, step time: 1.5618\n",
      "57/388, train_loss: 0.1434, step time: 1.5516\n",
      "58/388, train_loss: 0.2140, step time: 1.5548\n",
      "59/388, train_loss: 0.1763, step time: 1.5634\n",
      "60/388, train_loss: 0.3382, step time: 1.5571\n",
      "61/388, train_loss: 0.0640, step time: 1.5598\n",
      "62/388, train_loss: 0.0893, step time: 1.5532\n",
      "63/388, train_loss: 0.0593, step time: 1.5523\n",
      "64/388, train_loss: 0.1823, step time: 1.5593\n",
      "65/388, train_loss: 0.1873, step time: 1.5632\n",
      "66/388, train_loss: 0.1817, step time: 1.5561\n",
      "67/388, train_loss: 0.1756, step time: 1.5533\n",
      "68/388, train_loss: 0.1245, step time: 1.5484\n",
      "69/388, train_loss: 0.1348, step time: 1.5575\n",
      "70/388, train_loss: 0.1871, step time: 1.5555\n",
      "71/388, train_loss: 0.1389, step time: 1.5496\n",
      "72/388, train_loss: 0.0305, step time: 1.5530\n",
      "73/388, train_loss: 0.4380, step time: 1.5556\n",
      "74/388, train_loss: 0.2553, step time: 1.5500\n",
      "75/388, train_loss: 0.0817, step time: 1.5497\n",
      "76/388, train_loss: 0.2008, step time: 1.5541\n",
      "77/388, train_loss: 0.0755, step time: 1.5325\n",
      "78/388, train_loss: 0.1152, step time: 1.5558\n",
      "79/388, train_loss: 0.1903, step time: 1.5509\n",
      "80/388, train_loss: 0.1901, step time: 1.5539\n",
      "81/388, train_loss: 0.0542, step time: 1.5534\n",
      "82/388, train_loss: 0.1485, step time: 1.5506\n",
      "83/388, train_loss: 0.1810, step time: 1.5561\n",
      "84/388, train_loss: 0.3425, step time: 1.5557\n",
      "85/388, train_loss: 0.1037, step time: 1.5323\n",
      "86/388, train_loss: 0.1282, step time: 1.5541\n",
      "87/388, train_loss: 0.2235, step time: 1.5535\n",
      "88/388, train_loss: 0.2490, step time: 1.5617\n",
      "89/388, train_loss: 0.1542, step time: 1.5605\n",
      "90/388, train_loss: 0.2735, step time: 1.5509\n",
      "91/388, train_loss: 0.0691, step time: 1.5530\n",
      "92/388, train_loss: 0.1751, step time: 1.5582\n",
      "93/388, train_loss: 0.5183, step time: 1.5574\n",
      "94/388, train_loss: 0.3290, step time: 1.5550\n",
      "95/388, train_loss: 0.1019, step time: 1.5583\n",
      "96/388, train_loss: 0.1028, step time: 1.5516\n",
      "97/388, train_loss: 0.0758, step time: 1.5613\n",
      "98/388, train_loss: 0.1029, step time: 1.5527\n",
      "99/388, train_loss: 0.0665, step time: 1.5443\n",
      "100/388, train_loss: 0.1287, step time: 1.5520\n",
      "101/388, train_loss: 0.0856, step time: 1.5584\n",
      "102/388, train_loss: 0.1633, step time: 1.5509\n",
      "103/388, train_loss: 0.1972, step time: 1.5570\n",
      "104/388, train_loss: 0.2076, step time: 1.5551\n",
      "105/388, train_loss: 0.1308, step time: 1.5401\n",
      "106/388, train_loss: 0.2265, step time: 1.5608\n",
      "107/388, train_loss: 0.0774, step time: 1.5436\n",
      "108/388, train_loss: 0.1679, step time: 1.5633\n",
      "109/388, train_loss: 0.1949, step time: 1.5361\n",
      "110/388, train_loss: 0.0662, step time: 1.5595\n",
      "111/388, train_loss: 0.0899, step time: 1.5559\n",
      "112/388, train_loss: 0.1822, step time: 1.5808\n",
      "113/388, train_loss: 0.0915, step time: 1.5597\n",
      "114/388, train_loss: 0.1564, step time: 1.5594\n",
      "115/388, train_loss: 0.0612, step time: 1.5531\n",
      "116/388, train_loss: 0.1612, step time: 1.5569\n",
      "117/388, train_loss: 0.0848, step time: 1.5639\n",
      "118/388, train_loss: 0.0407, step time: 1.5526\n",
      "119/388, train_loss: 0.0877, step time: 1.5554\n",
      "120/388, train_loss: 0.1835, step time: 1.5527\n",
      "121/388, train_loss: 0.0434, step time: 1.5631\n",
      "122/388, train_loss: 0.0989, step time: 1.5584\n",
      "123/388, train_loss: 0.1406, step time: 1.5450\n",
      "124/388, train_loss: 0.0843, step time: 1.5538\n",
      "125/388, train_loss: 0.1288, step time: 1.5572\n",
      "126/388, train_loss: 0.2360, step time: 1.5577\n",
      "127/388, train_loss: 0.1865, step time: 1.5522\n",
      "128/388, train_loss: 0.1561, step time: 1.5553\n",
      "129/388, train_loss: 0.0890, step time: 1.5727\n",
      "130/388, train_loss: 0.0911, step time: 1.5729\n",
      "131/388, train_loss: 0.0970, step time: 1.5528\n",
      "132/388, train_loss: 0.1880, step time: 1.5575\n",
      "133/388, train_loss: 0.0883, step time: 1.5677\n",
      "134/388, train_loss: 0.1028, step time: 1.5564\n",
      "135/388, train_loss: 0.0729, step time: 1.5561\n",
      "136/388, train_loss: 0.1801, step time: 1.5547\n",
      "137/388, train_loss: 0.0732, step time: 1.5366\n",
      "138/388, train_loss: 0.1016, step time: 1.5345\n",
      "139/388, train_loss: 0.2174, step time: 1.5500\n",
      "140/388, train_loss: 0.1413, step time: 1.5545\n",
      "141/388, train_loss: 0.1055, step time: 1.5630\n",
      "142/388, train_loss: 0.0844, step time: 1.5563\n",
      "143/388, train_loss: 0.0896, step time: 1.5623\n",
      "144/388, train_loss: 0.1155, step time: 1.5763\n",
      "145/388, train_loss: 0.0774, step time: 1.5402\n",
      "146/388, train_loss: 0.0801, step time: 1.5480\n",
      "147/388, train_loss: 0.0816, step time: 1.5440\n",
      "148/388, train_loss: 0.1487, step time: 1.5700\n",
      "149/388, train_loss: 0.0980, step time: 1.5604\n",
      "150/388, train_loss: 0.1354, step time: 1.5550\n",
      "151/388, train_loss: 0.1161, step time: 1.5591\n",
      "152/388, train_loss: 0.1983, step time: 1.5519\n",
      "153/388, train_loss: 0.1180, step time: 1.5613\n",
      "154/388, train_loss: 0.0859, step time: 1.5546\n",
      "155/388, train_loss: 0.1679, step time: 1.5582\n",
      "156/388, train_loss: 0.1084, step time: 1.5697\n",
      "157/388, train_loss: 0.2784, step time: 1.5497\n",
      "158/388, train_loss: 0.0499, step time: 1.5591\n",
      "159/388, train_loss: 0.1807, step time: 1.5548\n",
      "160/388, train_loss: 0.0916, step time: 1.5515\n",
      "161/388, train_loss: 0.1907, step time: 1.5374\n",
      "162/388, train_loss: 0.0522, step time: 1.5567\n",
      "163/388, train_loss: 0.1215, step time: 1.5520\n",
      "164/388, train_loss: 0.1081, step time: 1.5530\n",
      "165/388, train_loss: 0.3922, step time: 1.5327\n",
      "166/388, train_loss: 0.0978, step time: 1.5565\n",
      "167/388, train_loss: 0.2671, step time: 1.5606\n",
      "168/388, train_loss: 0.0818, step time: 1.5812\n",
      "169/388, train_loss: 0.3120, step time: 1.5731\n",
      "170/388, train_loss: 0.2072, step time: 1.5642\n",
      "171/388, train_loss: 0.1843, step time: 1.5547\n",
      "172/388, train_loss: 0.2016, step time: 1.5571\n",
      "173/388, train_loss: 0.1052, step time: 1.5351\n",
      "174/388, train_loss: 0.0831, step time: 1.5531\n",
      "175/388, train_loss: 0.1665, step time: 1.5535\n",
      "176/388, train_loss: 0.1496, step time: 1.5540\n",
      "177/388, train_loss: 0.2242, step time: 1.5363\n",
      "178/388, train_loss: 0.1202, step time: 1.5542\n",
      "179/388, train_loss: 0.1206, step time: 1.5513\n",
      "180/388, train_loss: 0.1686, step time: 1.5607\n",
      "181/388, train_loss: 0.1054, step time: 1.5599\n",
      "182/388, train_loss: 0.1714, step time: 1.5688\n",
      "183/388, train_loss: 0.1583, step time: 1.5686\n",
      "184/388, train_loss: 0.0809, step time: 1.5560\n",
      "185/388, train_loss: 0.1203, step time: 1.5339\n",
      "186/388, train_loss: 0.1811, step time: 1.5560\n",
      "187/388, train_loss: 0.0745, step time: 1.5557\n",
      "188/388, train_loss: 0.0815, step time: 1.5522\n",
      "189/388, train_loss: 0.4811, step time: 1.5609\n",
      "190/388, train_loss: 0.2434, step time: 1.5512\n",
      "191/388, train_loss: 0.0871, step time: 1.5540\n",
      "192/388, train_loss: 0.0980, step time: 1.5563\n",
      "193/388, train_loss: 0.0791, step time: 1.5547\n",
      "194/388, train_loss: 0.1771, step time: 1.5507\n",
      "195/388, train_loss: 0.1017, step time: 1.5584\n",
      "196/388, train_loss: 0.0867, step time: 1.5625\n",
      "197/388, train_loss: 0.2080, step time: 1.5483\n",
      "198/388, train_loss: 0.1732, step time: 1.5545\n",
      "199/388, train_loss: 0.1179, step time: 1.5518\n",
      "200/388, train_loss: 0.2660, step time: 1.5557\n",
      "201/388, train_loss: 0.1975, step time: 1.5569\n",
      "202/388, train_loss: 0.0712, step time: 1.5549\n",
      "203/388, train_loss: 0.0503, step time: 1.5545\n",
      "204/388, train_loss: 0.1426, step time: 1.5570\n",
      "205/388, train_loss: 0.1651, step time: 1.5613\n",
      "206/388, train_loss: 0.1228, step time: 1.5555\n",
      "207/388, train_loss: 0.1412, step time: 1.5576\n",
      "208/388, train_loss: 0.1996, step time: 1.5523\n",
      "209/388, train_loss: 0.2816, step time: 1.5600\n",
      "210/388, train_loss: 0.1916, step time: 1.5736\n",
      "211/388, train_loss: 0.1595, step time: 1.5636\n",
      "212/388, train_loss: 0.2838, step time: 1.5537\n",
      "213/388, train_loss: 0.0784, step time: 1.5582\n",
      "214/388, train_loss: 0.1849, step time: 1.5577\n",
      "215/388, train_loss: 0.2449, step time: 1.5511\n",
      "216/388, train_loss: 0.1165, step time: 1.5678\n",
      "217/388, train_loss: 0.0650, step time: 1.5600\n",
      "218/388, train_loss: 0.1284, step time: 1.5553\n",
      "219/388, train_loss: 0.0421, step time: 1.5542\n",
      "220/388, train_loss: 0.0703, step time: 1.5530\n",
      "221/388, train_loss: 0.2196, step time: 1.5601\n",
      "222/388, train_loss: 0.2931, step time: 1.5468\n",
      "223/388, train_loss: 0.1937, step time: 1.5639\n",
      "224/388, train_loss: 0.1586, step time: 1.5502\n",
      "225/388, train_loss: 0.0997, step time: 1.5335\n",
      "226/388, train_loss: 0.1191, step time: 1.5422\n",
      "227/388, train_loss: 0.2549, step time: 1.5500\n",
      "228/388, train_loss: 0.1525, step time: 1.5525\n",
      "229/388, train_loss: 0.1264, step time: 1.5533\n",
      "230/388, train_loss: 0.0655, step time: 1.5553\n",
      "231/388, train_loss: 0.2276, step time: 1.5522\n",
      "232/388, train_loss: 0.2371, step time: 1.5508\n",
      "233/388, train_loss: 0.2553, step time: 1.5358\n",
      "234/388, train_loss: 0.1432, step time: 1.5526\n",
      "235/388, train_loss: 0.1303, step time: 1.5509\n",
      "236/388, train_loss: 0.0510, step time: 1.5502\n",
      "237/388, train_loss: 0.1395, step time: 1.5583\n",
      "238/388, train_loss: 0.1480, step time: 1.5508\n",
      "239/388, train_loss: 0.1455, step time: 1.5532\n",
      "240/388, train_loss: 0.1783, step time: 1.5484\n",
      "241/388, train_loss: 0.1606, step time: 1.5569\n",
      "242/388, train_loss: 0.2233, step time: 1.5507\n",
      "243/388, train_loss: 0.1150, step time: 1.5510\n",
      "244/388, train_loss: 0.2106, step time: 1.5538\n",
      "245/388, train_loss: 0.0840, step time: 1.5561\n",
      "246/388, train_loss: 0.2131, step time: 1.5545\n",
      "247/388, train_loss: 0.2001, step time: 1.5494\n",
      "248/388, train_loss: 0.0953, step time: 1.5530\n",
      "249/388, train_loss: 0.2726, step time: 1.5564\n",
      "250/388, train_loss: 0.1667, step time: 1.5518\n",
      "251/388, train_loss: 0.0745, step time: 1.5573\n",
      "252/388, train_loss: 0.1506, step time: 1.5502\n",
      "253/388, train_loss: 0.3094, step time: 1.5595\n",
      "254/388, train_loss: 0.1194, step time: 1.5506\n",
      "255/388, train_loss: 0.0684, step time: 1.5486\n",
      "256/388, train_loss: 0.1974, step time: 1.5509\n",
      "257/388, train_loss: 0.2449, step time: 1.5617\n",
      "258/388, train_loss: 0.2317, step time: 1.5528\n",
      "259/388, train_loss: 0.0940, step time: 1.5538\n",
      "260/388, train_loss: 0.0945, step time: 1.5532\n",
      "261/388, train_loss: 0.1630, step time: 1.5374\n",
      "262/388, train_loss: 0.2334, step time: 1.5663\n",
      "263/388, train_loss: 0.1963, step time: 1.5684\n",
      "264/388, train_loss: 0.0987, step time: 1.6124\n",
      "265/388, train_loss: 0.2401, step time: 1.5368\n",
      "266/388, train_loss: 0.1266, step time: 1.5636\n",
      "267/388, train_loss: 0.1425, step time: 1.5559\n",
      "268/388, train_loss: 0.0999, step time: 1.5570\n",
      "269/388, train_loss: 0.2528, step time: 1.5330\n",
      "270/388, train_loss: 0.0688, step time: 1.5549\n",
      "271/388, train_loss: 0.1314, step time: 1.5541\n",
      "272/388, train_loss: 0.2290, step time: 1.5585\n",
      "273/388, train_loss: 0.3311, step time: 1.5370\n",
      "274/388, train_loss: 0.2025, step time: 1.5527\n",
      "275/388, train_loss: 0.4022, step time: 1.5478\n",
      "276/388, train_loss: 0.0817, step time: 1.5535\n",
      "277/388, train_loss: 0.2208, step time: 1.5602\n",
      "278/388, train_loss: 0.0793, step time: 1.5544\n",
      "279/388, train_loss: 0.1164, step time: 1.5508\n",
      "280/388, train_loss: 0.0412, step time: 1.5549\n",
      "281/388, train_loss: 0.1944, step time: 1.5531\n",
      "282/388, train_loss: 0.2642, step time: 1.5519\n",
      "283/388, train_loss: 0.0957, step time: 1.5539\n",
      "284/388, train_loss: 0.0758, step time: 1.5633\n",
      "285/388, train_loss: 0.0566, step time: 1.5589\n",
      "286/388, train_loss: 0.1759, step time: 1.5520\n",
      "287/388, train_loss: 0.3514, step time: 1.5562\n",
      "288/388, train_loss: 0.1354, step time: 1.5543\n",
      "289/388, train_loss: 0.1204, step time: 1.5607\n",
      "290/388, train_loss: 0.1193, step time: 1.5530\n",
      "291/388, train_loss: 0.1467, step time: 1.5519\n",
      "292/388, train_loss: 0.1714, step time: 1.5550\n",
      "293/388, train_loss: 0.1052, step time: 1.5325\n",
      "294/388, train_loss: 0.1140, step time: 1.5539\n",
      "295/388, train_loss: 0.1895, step time: 1.5561\n",
      "296/388, train_loss: 0.0368, step time: 1.5534\n",
      "297/388, train_loss: 0.1609, step time: 1.5408\n",
      "298/388, train_loss: 0.1204, step time: 1.5554\n",
      "299/388, train_loss: 0.1065, step time: 1.5578\n",
      "300/388, train_loss: 0.1333, step time: 1.5555\n",
      "301/388, train_loss: 0.0838, step time: 1.5422\n",
      "302/388, train_loss: 0.0604, step time: 1.5562\n",
      "303/388, train_loss: 0.4103, step time: 1.5536\n",
      "304/388, train_loss: 0.0651, step time: 1.5563\n",
      "305/388, train_loss: 0.1571, step time: 1.5600\n",
      "306/388, train_loss: 0.0974, step time: 1.5534\n",
      "307/388, train_loss: 0.1667, step time: 1.5554\n",
      "308/388, train_loss: 0.3834, step time: 1.5507\n",
      "309/388, train_loss: 0.0732, step time: 1.5366\n",
      "310/388, train_loss: 0.1026, step time: 1.5453\n",
      "311/388, train_loss: 0.1904, step time: 1.5564\n",
      "312/388, train_loss: 0.1248, step time: 1.5594\n",
      "313/388, train_loss: 0.1512, step time: 1.5335\n",
      "314/388, train_loss: 0.1556, step time: 1.5475\n",
      "315/388, train_loss: 0.0959, step time: 1.5562\n",
      "316/388, train_loss: 0.0959, step time: 1.5502\n",
      "317/388, train_loss: 0.2819, step time: 1.5615\n",
      "318/388, train_loss: 0.1153, step time: 1.5565\n",
      "319/388, train_loss: 0.2732, step time: 1.5551\n",
      "320/388, train_loss: 0.2843, step time: 1.5536\n",
      "321/388, train_loss: 0.2014, step time: 1.5357\n",
      "322/388, train_loss: 0.1037, step time: 1.5493\n",
      "323/388, train_loss: 0.3585, step time: 1.5484\n",
      "324/388, train_loss: 0.2411, step time: 1.5473\n",
      "325/388, train_loss: 0.1048, step time: 1.5592\n",
      "326/388, train_loss: 0.1314, step time: 1.5547\n",
      "327/388, train_loss: 0.1582, step time: 1.5514\n",
      "328/388, train_loss: 0.2024, step time: 1.5535\n",
      "329/388, train_loss: 0.0719, step time: 1.5614\n",
      "330/388, train_loss: 0.2039, step time: 1.5541\n",
      "331/388, train_loss: 0.0645, step time: 1.5543\n",
      "332/388, train_loss: 0.1268, step time: 1.5530\n",
      "333/388, train_loss: 0.2145, step time: 1.5608\n",
      "334/388, train_loss: 0.1104, step time: 1.5552\n",
      "335/388, train_loss: 0.1772, step time: 1.5510\n",
      "336/388, train_loss: 0.0846, step time: 1.5562\n",
      "337/388, train_loss: 0.1469, step time: 1.5437\n",
      "338/388, train_loss: 0.1793, step time: 1.5540\n",
      "339/388, train_loss: 0.0961, step time: 1.5528\n",
      "340/388, train_loss: 0.2463, step time: 1.5541\n",
      "341/388, train_loss: 0.0902, step time: 1.5468\n",
      "342/388, train_loss: 0.2468, step time: 1.5526\n",
      "343/388, train_loss: 0.1585, step time: 1.5559\n",
      "344/388, train_loss: 0.1157, step time: 1.5642\n",
      "345/388, train_loss: 0.1533, step time: 1.5614\n",
      "346/388, train_loss: 0.1144, step time: 1.5550\n",
      "347/388, train_loss: 0.1650, step time: 1.5530\n",
      "348/388, train_loss: 0.0966, step time: 1.5588\n",
      "349/388, train_loss: 0.1822, step time: 1.5573\n",
      "350/388, train_loss: 0.1497, step time: 1.5570\n",
      "351/388, train_loss: 0.1246, step time: 1.5552\n",
      "352/388, train_loss: 0.1096, step time: 1.5688\n",
      "353/388, train_loss: 0.1032, step time: 1.5601\n",
      "354/388, train_loss: 0.2313, step time: 1.5561\n",
      "355/388, train_loss: 0.2043, step time: 1.5539\n",
      "356/388, train_loss: 0.1069, step time: 1.5528\n",
      "357/388, train_loss: 0.1050, step time: 1.5367\n",
      "358/388, train_loss: 0.1475, step time: 1.5558\n",
      "359/388, train_loss: 0.0511, step time: 1.5567\n",
      "360/388, train_loss: 0.1240, step time: 1.5561\n",
      "361/388, train_loss: 0.0854, step time: 1.5344\n",
      "362/388, train_loss: 0.1549, step time: 1.5546\n",
      "363/388, train_loss: 0.0741, step time: 1.5546\n",
      "364/388, train_loss: 0.3685, step time: 1.5543\n",
      "365/388, train_loss: 0.0775, step time: 1.5368\n",
      "366/388, train_loss: 0.0943, step time: 1.5742\n",
      "367/388, train_loss: 0.0624, step time: 1.5526\n",
      "368/388, train_loss: 0.2264, step time: 1.5570\n",
      "369/388, train_loss: 0.1549, step time: 1.5621\n",
      "370/388, train_loss: 0.2336, step time: 1.5541\n",
      "371/388, train_loss: 0.2351, step time: 1.5649\n",
      "372/388, train_loss: 0.0306, step time: 1.5661\n",
      "373/388, train_loss: 0.1812, step time: 1.5591\n",
      "374/388, train_loss: 0.0451, step time: 1.5546\n",
      "375/388, train_loss: 0.0961, step time: 1.5508\n",
      "376/388, train_loss: 0.3925, step time: 1.5554\n",
      "377/388, train_loss: 0.1613, step time: 1.5614\n",
      "378/388, train_loss: 0.2195, step time: 1.5549\n",
      "379/388, train_loss: 0.2974, step time: 1.5551\n",
      "380/388, train_loss: 0.2953, step time: 1.5550\n",
      "381/388, train_loss: 0.1922, step time: 1.5599\n",
      "382/388, train_loss: 0.1286, step time: 1.5563\n",
      "383/388, train_loss: 0.0447, step time: 1.5537\n",
      "384/388, train_loss: 0.1253, step time: 1.5572\n",
      "385/388, train_loss: 0.0772, step time: 1.5604\n",
      "386/388, train_loss: 0.1550, step time: 1.5530\n",
      "387/388, train_loss: 0.2239, step time: 1.5556\n",
      "388/388, train_loss: 0.0665, step time: 1.5518\n",
      "epoch 92 average loss: 0.1563\n",
      "current epoch: 92 current mean dice: 0.7802 tc: 0.8288 wt: 0.9060 et: 0.6058\n",
      "best mean dice: 0.7817 at epoch: 84\n",
      "time consuming of epoch 92 is: 712.8910\n",
      "----------\n",
      "epoch 93/100\n",
      "1/388, train_loss: 0.1752, step time: 1.5668\n",
      "2/388, train_loss: 0.2131, step time: 1.5599\n",
      "3/388, train_loss: 0.0751, step time: 1.5569\n",
      "4/388, train_loss: 0.2334, step time: 1.5548\n",
      "5/388, train_loss: 0.0637, step time: 1.5540\n",
      "6/388, train_loss: 0.2013, step time: 1.5574\n",
      "7/388, train_loss: 0.1277, step time: 1.5574\n",
      "8/388, train_loss: 0.1576, step time: 1.5597\n",
      "9/388, train_loss: 0.1765, step time: 1.5578\n",
      "10/388, train_loss: 0.0796, step time: 1.5575\n",
      "11/388, train_loss: 0.1874, step time: 1.5523\n",
      "12/388, train_loss: 0.0877, step time: 1.5558\n",
      "13/388, train_loss: 0.0799, step time: 1.5446\n",
      "14/388, train_loss: 0.1200, step time: 1.5530\n",
      "15/388, train_loss: 0.2033, step time: 1.5605\n",
      "16/388, train_loss: 0.1314, step time: 1.5501\n",
      "17/388, train_loss: 0.1989, step time: 1.5557\n",
      "18/388, train_loss: 0.1597, step time: 1.5552\n",
      "19/388, train_loss: 0.1390, step time: 1.5543\n",
      "20/388, train_loss: 0.0821, step time: 1.5544\n",
      "21/388, train_loss: 0.1063, step time: 1.5550\n",
      "22/388, train_loss: 0.1294, step time: 1.5516\n",
      "23/388, train_loss: 0.3282, step time: 1.5525\n",
      "24/388, train_loss: 0.0975, step time: 1.5536\n",
      "25/388, train_loss: 0.1755, step time: 1.5554\n",
      "26/388, train_loss: 0.1459, step time: 1.5571\n",
      "27/388, train_loss: 0.1190, step time: 1.5521\n",
      "28/388, train_loss: 0.1852, step time: 1.5572\n",
      "29/388, train_loss: 0.0838, step time: 1.5536\n",
      "30/388, train_loss: 0.1512, step time: 1.5513\n",
      "31/388, train_loss: 0.0703, step time: 1.5586\n",
      "32/388, train_loss: 0.1602, step time: 1.5563\n",
      "33/388, train_loss: 0.1612, step time: 1.5530\n",
      "34/388, train_loss: 0.2312, step time: 1.5567\n",
      "35/388, train_loss: 0.1016, step time: 1.5502\n",
      "36/388, train_loss: 0.2882, step time: 1.5579\n",
      "37/388, train_loss: 0.1406, step time: 1.5549\n",
      "38/388, train_loss: 0.0622, step time: 1.5553\n",
      "39/388, train_loss: 0.1885, step time: 1.5648\n",
      "40/388, train_loss: 0.2170, step time: 1.5533\n",
      "41/388, train_loss: 0.0728, step time: 1.5544\n",
      "42/388, train_loss: 0.3743, step time: 1.5539\n",
      "43/388, train_loss: 0.0865, step time: 1.5520\n",
      "44/388, train_loss: 0.1975, step time: 1.5594\n",
      "45/388, train_loss: 0.1795, step time: 1.5543\n",
      "46/388, train_loss: 0.2024, step time: 1.5549\n",
      "47/388, train_loss: 0.1837, step time: 1.5544\n",
      "48/388, train_loss: 0.2528, step time: 1.5539\n",
      "49/388, train_loss: 0.1789, step time: 1.5530\n",
      "50/388, train_loss: 0.1590, step time: 1.5553\n",
      "51/388, train_loss: 0.1185, step time: 1.5508\n",
      "52/388, train_loss: 0.0896, step time: 1.5547\n",
      "53/388, train_loss: 0.1930, step time: 1.5534\n",
      "54/388, train_loss: 0.1499, step time: 1.5542\n",
      "55/388, train_loss: 0.1462, step time: 1.5576\n",
      "56/388, train_loss: 0.0622, step time: 1.5514\n",
      "57/388, train_loss: 0.2036, step time: 1.5670\n",
      "58/388, train_loss: 0.1576, step time: 1.5507\n",
      "59/388, train_loss: 0.2942, step time: 1.5516\n",
      "60/388, train_loss: 0.1338, step time: 1.5557\n",
      "61/388, train_loss: 0.1768, step time: 1.5532\n",
      "62/388, train_loss: 0.1800, step time: 1.5565\n",
      "63/388, train_loss: 0.1337, step time: 1.5553\n",
      "64/388, train_loss: 0.0262, step time: 1.5582\n",
      "65/388, train_loss: 0.1030, step time: 1.5582\n",
      "66/388, train_loss: 0.0978, step time: 1.5551\n",
      "67/388, train_loss: 0.2220, step time: 1.5636\n",
      "68/388, train_loss: 0.0727, step time: 1.5565\n",
      "69/388, train_loss: 0.0785, step time: 1.5548\n",
      "70/388, train_loss: 0.0651, step time: 1.5512\n",
      "71/388, train_loss: 0.1873, step time: 1.5554\n",
      "72/388, train_loss: 0.1679, step time: 1.5557\n",
      "73/388, train_loss: 0.1754, step time: 1.5530\n",
      "74/388, train_loss: 0.5159, step time: 1.5768\n",
      "75/388, train_loss: 0.2141, step time: 1.5534\n",
      "76/388, train_loss: 0.2355, step time: 1.5567\n",
      "77/388, train_loss: 0.1769, step time: 1.5554\n",
      "78/388, train_loss: 0.0781, step time: 1.5526\n",
      "79/388, train_loss: 0.2551, step time: 1.5540\n",
      "80/388, train_loss: 0.5096, step time: 1.5556\n",
      "81/388, train_loss: 0.2587, step time: 1.5571\n",
      "82/388, train_loss: 0.1271, step time: 1.5519\n",
      "83/388, train_loss: 0.1446, step time: 1.5526\n",
      "84/388, train_loss: 0.1184, step time: 1.5549\n",
      "85/388, train_loss: 0.1526, step time: 1.5621\n",
      "86/388, train_loss: 0.2048, step time: 1.5518\n",
      "87/388, train_loss: 0.0974, step time: 1.5564\n",
      "88/388, train_loss: 0.0949, step time: 1.5518\n",
      "89/388, train_loss: 0.2624, step time: 1.5595\n",
      "90/388, train_loss: 0.2875, step time: 1.5540\n",
      "91/388, train_loss: 0.1433, step time: 1.5719\n",
      "92/388, train_loss: 0.1099, step time: 1.5562\n",
      "93/388, train_loss: 0.1507, step time: 1.5598\n",
      "94/388, train_loss: 0.1052, step time: 1.5543\n",
      "95/388, train_loss: 0.0462, step time: 1.5529\n",
      "96/388, train_loss: 0.0420, step time: 1.5547\n",
      "97/388, train_loss: 0.1560, step time: 1.5622\n",
      "98/388, train_loss: 0.0957, step time: 1.5533\n",
      "99/388, train_loss: 0.1962, step time: 1.5550\n",
      "100/388, train_loss: 0.0924, step time: 1.5533\n",
      "101/388, train_loss: 0.1280, step time: 1.5634\n",
      "102/388, train_loss: 0.0598, step time: 1.5567\n",
      "103/388, train_loss: 0.0612, step time: 1.5519\n",
      "104/388, train_loss: 0.0881, step time: 1.5568\n",
      "105/388, train_loss: 0.1943, step time: 1.5616\n",
      "106/388, train_loss: 0.0992, step time: 1.5552\n",
      "107/388, train_loss: 0.0321, step time: 1.5550\n",
      "108/388, train_loss: 0.2111, step time: 1.5538\n",
      "109/388, train_loss: 0.3355, step time: 1.5624\n",
      "110/388, train_loss: 0.1300, step time: 1.5540\n",
      "111/388, train_loss: 0.1387, step time: 1.5541\n",
      "112/388, train_loss: 0.0861, step time: 1.5549\n",
      "113/388, train_loss: 0.0520, step time: 1.5535\n",
      "114/388, train_loss: 0.0877, step time: 1.5523\n",
      "115/388, train_loss: 0.0921, step time: 1.5541\n",
      "116/388, train_loss: 0.3025, step time: 1.5536\n",
      "117/388, train_loss: 0.0818, step time: 1.5554\n",
      "118/388, train_loss: 0.1784, step time: 1.5537\n",
      "119/388, train_loss: 0.0761, step time: 1.5553\n",
      "120/388, train_loss: 0.2958, step time: 1.5563\n",
      "121/388, train_loss: 0.1028, step time: 1.5557\n",
      "122/388, train_loss: 0.0757, step time: 1.5555\n",
      "123/388, train_loss: 0.3098, step time: 1.5545\n",
      "124/388, train_loss: 0.2211, step time: 1.5525\n",
      "125/388, train_loss: 0.0776, step time: 1.5604\n",
      "126/388, train_loss: 0.2225, step time: 1.5498\n",
      "127/388, train_loss: 0.1540, step time: 1.5549\n",
      "128/388, train_loss: 0.1678, step time: 1.5569\n",
      "129/388, train_loss: 0.1538, step time: 1.5540\n",
      "130/388, train_loss: 0.1497, step time: 1.5562\n",
      "131/388, train_loss: 0.1009, step time: 1.5562\n",
      "132/388, train_loss: 0.1509, step time: 1.5502\n",
      "133/388, train_loss: 0.2306, step time: 1.5536\n",
      "134/388, train_loss: 0.2319, step time: 1.5548\n",
      "135/388, train_loss: 0.1564, step time: 1.5532\n",
      "136/388, train_loss: 0.1797, step time: 1.5559\n",
      "137/388, train_loss: 0.0787, step time: 1.5554\n",
      "138/388, train_loss: 0.1042, step time: 1.5526\n",
      "139/388, train_loss: 0.0660, step time: 1.5537\n",
      "140/388, train_loss: 0.1720, step time: 1.5546\n",
      "141/388, train_loss: 0.2650, step time: 1.5504\n",
      "142/388, train_loss: 0.1593, step time: 1.5544\n",
      "143/388, train_loss: 0.1303, step time: 1.5569\n",
      "144/388, train_loss: 0.1454, step time: 1.5563\n",
      "145/388, train_loss: 0.1253, step time: 1.5546\n",
      "146/388, train_loss: 0.1212, step time: 1.5562\n",
      "147/388, train_loss: 0.2095, step time: 1.5491\n",
      "148/388, train_loss: 0.1382, step time: 1.5552\n",
      "149/388, train_loss: 0.0466, step time: 1.5562\n",
      "150/388, train_loss: 0.3204, step time: 1.5521\n",
      "151/388, train_loss: 0.2377, step time: 1.5545\n",
      "152/388, train_loss: 0.1042, step time: 1.5574\n",
      "153/388, train_loss: 0.1281, step time: 1.5545\n",
      "154/388, train_loss: 0.1069, step time: 1.5541\n",
      "155/388, train_loss: 0.1309, step time: 1.5510\n",
      "156/388, train_loss: 0.1244, step time: 1.5573\n",
      "157/388, train_loss: 0.1781, step time: 1.5572\n",
      "158/388, train_loss: 0.0898, step time: 1.5527\n",
      "159/388, train_loss: 0.2027, step time: 1.5548\n",
      "160/388, train_loss: 0.4343, step time: 1.5527\n",
      "161/388, train_loss: 0.1248, step time: 1.5517\n",
      "162/388, train_loss: 0.1065, step time: 1.5545\n",
      "163/388, train_loss: 0.0940, step time: 1.5519\n",
      "164/388, train_loss: 0.1615, step time: 1.5509\n",
      "165/388, train_loss: 0.4439, step time: 1.5560\n",
      "166/388, train_loss: 0.2320, step time: 1.5507\n",
      "167/388, train_loss: 0.1237, step time: 1.5553\n",
      "168/388, train_loss: 0.0689, step time: 1.5541\n",
      "169/388, train_loss: 0.1158, step time: 1.5543\n",
      "170/388, train_loss: 0.2778, step time: 1.5567\n",
      "171/388, train_loss: 0.2043, step time: 1.5540\n",
      "172/388, train_loss: 0.0845, step time: 1.5533\n",
      "173/388, train_loss: 0.1328, step time: 1.5484\n",
      "174/388, train_loss: 0.0999, step time: 1.5554\n",
      "175/388, train_loss: 0.1751, step time: 1.5527\n",
      "176/388, train_loss: 0.0994, step time: 1.5518\n",
      "177/388, train_loss: 0.1922, step time: 1.5518\n",
      "178/388, train_loss: 0.3894, step time: 1.5552\n",
      "179/388, train_loss: 0.1052, step time: 1.5493\n",
      "180/388, train_loss: 0.4016, step time: 1.5615\n",
      "181/388, train_loss: 0.1596, step time: 1.5555\n",
      "182/388, train_loss: 0.1397, step time: 1.5561\n",
      "183/388, train_loss: 0.2066, step time: 1.5613\n",
      "184/388, train_loss: 0.2393, step time: 1.5529\n",
      "185/388, train_loss: 0.1399, step time: 1.5542\n",
      "186/388, train_loss: 0.2602, step time: 1.5571\n",
      "187/388, train_loss: 0.0425, step time: 1.5480\n",
      "188/388, train_loss: 0.1446, step time: 1.5546\n",
      "189/388, train_loss: 0.1100, step time: 1.5529\n",
      "190/388, train_loss: 0.2436, step time: 1.5583\n",
      "191/388, train_loss: 0.1881, step time: 1.5572\n",
      "192/388, train_loss: 0.0815, step time: 1.5520\n",
      "193/388, train_loss: 0.1272, step time: 1.5562\n",
      "194/388, train_loss: 0.0628, step time: 1.5561\n",
      "195/388, train_loss: 0.1663, step time: 1.5498\n",
      "196/388, train_loss: 0.0789, step time: 1.5556\n",
      "197/388, train_loss: 0.1630, step time: 1.5549\n",
      "198/388, train_loss: 0.2492, step time: 1.5547\n",
      "199/388, train_loss: 0.1382, step time: 1.5527\n",
      "200/388, train_loss: 0.1218, step time: 1.5499\n",
      "201/388, train_loss: 0.1811, step time: 1.5542\n",
      "202/388, train_loss: 0.1767, step time: 1.5565\n",
      "203/388, train_loss: 0.0903, step time: 1.5635\n",
      "204/388, train_loss: 0.2018, step time: 1.5548\n",
      "205/388, train_loss: 0.2837, step time: 1.5521\n",
      "206/388, train_loss: 0.1073, step time: 1.5539\n",
      "207/388, train_loss: 0.2134, step time: 1.5537\n",
      "208/388, train_loss: 0.1420, step time: 1.5539\n",
      "209/388, train_loss: 0.1123, step time: 1.5538\n",
      "210/388, train_loss: 0.0957, step time: 1.5542\n",
      "211/388, train_loss: 0.1610, step time: 1.5541\n",
      "212/388, train_loss: 0.0657, step time: 1.5508\n",
      "213/388, train_loss: 0.1279, step time: 1.5557\n",
      "214/388, train_loss: 0.1219, step time: 1.5487\n",
      "215/388, train_loss: 0.0387, step time: 1.5545\n",
      "216/388, train_loss: 0.2228, step time: 1.5542\n",
      "217/388, train_loss: 0.1600, step time: 1.5505\n",
      "218/388, train_loss: 0.0708, step time: 1.5529\n",
      "219/388, train_loss: 0.1180, step time: 1.5539\n",
      "220/388, train_loss: 0.1786, step time: 1.5526\n",
      "221/388, train_loss: 0.1670, step time: 1.5537\n",
      "222/388, train_loss: 0.1849, step time: 1.5545\n",
      "223/388, train_loss: 0.1208, step time: 1.5515\n",
      "224/388, train_loss: 0.1563, step time: 1.5542\n",
      "225/388, train_loss: 0.1479, step time: 1.5520\n",
      "226/388, train_loss: 0.0990, step time: 1.5537\n",
      "227/388, train_loss: 0.3100, step time: 1.5516\n",
      "228/388, train_loss: 0.0727, step time: 1.5521\n",
      "229/388, train_loss: 0.1724, step time: 1.5546\n",
      "230/388, train_loss: 0.2809, step time: 1.5555\n",
      "231/388, train_loss: 0.0918, step time: 1.5519\n",
      "232/388, train_loss: 0.2056, step time: 1.5541\n",
      "233/388, train_loss: 0.2024, step time: 1.5541\n",
      "234/388, train_loss: 0.1046, step time: 1.5537\n",
      "235/388, train_loss: 0.0408, step time: 1.5551\n",
      "236/388, train_loss: 0.0935, step time: 1.5541\n",
      "237/388, train_loss: 0.1837, step time: 1.5536\n",
      "238/388, train_loss: 0.1229, step time: 1.5523\n",
      "239/388, train_loss: 0.1099, step time: 1.5524\n",
      "240/388, train_loss: 0.1367, step time: 1.5515\n",
      "241/388, train_loss: 0.1107, step time: 1.5543\n",
      "242/388, train_loss: 0.0300, step time: 1.5532\n",
      "243/388, train_loss: 0.0582, step time: 1.5509\n",
      "244/388, train_loss: 0.1026, step time: 1.5545\n",
      "245/388, train_loss: 0.1449, step time: 1.5552\n",
      "246/388, train_loss: 0.1705, step time: 1.5515\n",
      "247/388, train_loss: 0.0774, step time: 1.5539\n",
      "248/388, train_loss: 0.1175, step time: 1.5540\n",
      "249/388, train_loss: 0.1652, step time: 1.5506\n",
      "250/388, train_loss: 0.4571, step time: 1.5568\n",
      "251/388, train_loss: 0.0917, step time: 1.5530\n",
      "252/388, train_loss: 0.1022, step time: 1.5509\n",
      "253/388, train_loss: 0.1472, step time: 1.5590\n",
      "254/388, train_loss: 0.1989, step time: 1.5516\n",
      "255/388, train_loss: 0.3486, step time: 1.5511\n",
      "256/388, train_loss: 0.0992, step time: 1.5553\n",
      "257/388, train_loss: 0.0520, step time: 1.5555\n",
      "258/388, train_loss: 0.0704, step time: 1.5518\n",
      "259/388, train_loss: 0.1758, step time: 1.5545\n",
      "260/388, train_loss: 0.1266, step time: 1.5549\n",
      "261/388, train_loss: 0.1486, step time: 1.5514\n",
      "262/388, train_loss: 0.0855, step time: 1.5589\n",
      "263/388, train_loss: 0.1175, step time: 1.5519\n",
      "264/388, train_loss: 0.1415, step time: 1.5532\n",
      "265/388, train_loss: 0.1599, step time: 1.5542\n",
      "266/388, train_loss: 0.2770, step time: 1.5556\n",
      "267/388, train_loss: 0.0531, step time: 1.5551\n",
      "268/388, train_loss: 0.0958, step time: 1.5553\n",
      "269/388, train_loss: 0.1829, step time: 1.5534\n",
      "270/388, train_loss: 0.0558, step time: 1.5549\n",
      "271/388, train_loss: 0.0982, step time: 1.5536\n",
      "272/388, train_loss: 0.1209, step time: 1.5526\n",
      "273/388, train_loss: 0.0981, step time: 1.5526\n",
      "274/388, train_loss: 0.3295, step time: 1.5542\n",
      "275/388, train_loss: 0.2113, step time: 1.5439\n",
      "276/388, train_loss: 0.0950, step time: 1.5547\n",
      "277/388, train_loss: 0.0651, step time: 1.5562\n",
      "278/388, train_loss: 0.2581, step time: 1.5511\n",
      "279/388, train_loss: 0.1025, step time: 1.5546\n",
      "280/388, train_loss: 0.0667, step time: 1.5538\n",
      "281/388, train_loss: 0.0946, step time: 1.5523\n",
      "282/388, train_loss: 0.2087, step time: 1.5545\n",
      "283/388, train_loss: 0.1985, step time: 1.5492\n",
      "284/388, train_loss: 0.0858, step time: 1.5534\n",
      "285/388, train_loss: 0.1332, step time: 1.5564\n",
      "286/388, train_loss: 0.2184, step time: 1.5524\n",
      "287/388, train_loss: 0.1567, step time: 1.5541\n",
      "288/388, train_loss: 0.0580, step time: 1.5533\n",
      "289/388, train_loss: 0.1460, step time: 1.5500\n",
      "290/388, train_loss: 0.0815, step time: 1.5540\n",
      "291/388, train_loss: 0.0578, step time: 1.5533\n",
      "292/388, train_loss: 0.1042, step time: 1.5512\n",
      "293/388, train_loss: 0.0434, step time: 1.5556\n",
      "294/388, train_loss: 0.1017, step time: 1.5533\n",
      "295/388, train_loss: 0.1106, step time: 1.5681\n",
      "296/388, train_loss: 0.2684, step time: 1.5503\n",
      "297/388, train_loss: 0.0808, step time: 1.5524\n",
      "298/388, train_loss: 0.3444, step time: 1.5600\n",
      "299/388, train_loss: 0.0957, step time: 1.5519\n",
      "300/388, train_loss: 0.0813, step time: 1.5543\n",
      "301/388, train_loss: 0.1930, step time: 1.5542\n",
      "302/388, train_loss: 0.1884, step time: 1.5540\n",
      "303/388, train_loss: 0.1620, step time: 1.5541\n",
      "304/388, train_loss: 0.1738, step time: 1.5591\n",
      "305/388, train_loss: 0.1026, step time: 1.5546\n",
      "306/388, train_loss: 0.3849, step time: 1.5557\n",
      "307/388, train_loss: 0.1489, step time: 1.5533\n",
      "308/388, train_loss: 0.4209, step time: 1.5526\n",
      "309/388, train_loss: 0.1304, step time: 1.5555\n",
      "310/388, train_loss: 0.2500, step time: 1.5509\n",
      "311/388, train_loss: 0.1671, step time: 1.5648\n",
      "312/388, train_loss: 0.0507, step time: 1.5549\n",
      "313/388, train_loss: 0.0871, step time: 1.5501\n",
      "314/388, train_loss: 0.1115, step time: 1.5542\n",
      "315/388, train_loss: 0.2629, step time: 1.5563\n",
      "316/388, train_loss: 0.1351, step time: 1.5512\n",
      "317/388, train_loss: 0.0792, step time: 1.5540\n",
      "318/388, train_loss: 0.2412, step time: 1.5570\n",
      "319/388, train_loss: 0.0468, step time: 1.5510\n",
      "320/388, train_loss: 0.0817, step time: 1.5539\n",
      "321/388, train_loss: 0.1734, step time: 1.5536\n",
      "322/388, train_loss: 0.2688, step time: 1.5518\n",
      "323/388, train_loss: 0.1239, step time: 1.5514\n",
      "324/388, train_loss: 0.1993, step time: 1.5555\n",
      "325/388, train_loss: 0.1642, step time: 1.5529\n",
      "326/388, train_loss: 0.0695, step time: 1.5531\n",
      "327/388, train_loss: 0.1880, step time: 1.5516\n",
      "328/388, train_loss: 0.3160, step time: 1.5534\n",
      "329/388, train_loss: 0.0877, step time: 1.5534\n",
      "330/388, train_loss: 0.1260, step time: 1.5554\n",
      "331/388, train_loss: 0.3641, step time: 1.5566\n",
      "332/388, train_loss: 0.5161, step time: 1.5547\n",
      "333/388, train_loss: 0.1888, step time: 1.5502\n",
      "334/388, train_loss: 0.1033, step time: 1.5595\n",
      "335/388, train_loss: 0.3211, step time: 1.5626\n",
      "336/388, train_loss: 0.0856, step time: 1.5499\n",
      "337/388, train_loss: 0.1011, step time: 1.5565\n",
      "338/388, train_loss: 0.2315, step time: 1.5554\n",
      "339/388, train_loss: 0.1182, step time: 1.5540\n",
      "340/388, train_loss: 0.3242, step time: 1.5573\n",
      "341/388, train_loss: 0.1850, step time: 1.5570\n",
      "342/388, train_loss: 0.2683, step time: 1.5531\n",
      "343/388, train_loss: 0.1813, step time: 1.5562\n",
      "344/388, train_loss: 0.0993, step time: 1.5541\n",
      "345/388, train_loss: 0.0929, step time: 1.5562\n",
      "346/388, train_loss: 0.2987, step time: 1.5564\n",
      "347/388, train_loss: 0.2807, step time: 1.5531\n",
      "348/388, train_loss: 0.2027, step time: 1.5422\n",
      "349/388, train_loss: 0.2046, step time: 1.5555\n",
      "350/388, train_loss: 0.0920, step time: 1.5562\n",
      "351/388, train_loss: 0.0953, step time: 1.5545\n",
      "352/388, train_loss: 0.0764, step time: 1.5567\n",
      "353/388, train_loss: 0.1624, step time: 1.5553\n",
      "354/388, train_loss: 0.1782, step time: 1.5572\n",
      "355/388, train_loss: 0.0669, step time: 1.5546\n",
      "356/388, train_loss: 0.1068, step time: 1.5500\n",
      "357/388, train_loss: 0.1219, step time: 1.5580\n",
      "358/388, train_loss: 0.2386, step time: 1.5561\n",
      "359/388, train_loss: 0.0672, step time: 1.5515\n",
      "360/388, train_loss: 0.1051, step time: 1.5557\n",
      "361/388, train_loss: 0.1087, step time: 1.5548\n",
      "362/388, train_loss: 0.1927, step time: 1.5524\n",
      "363/388, train_loss: 0.2343, step time: 1.5546\n",
      "364/388, train_loss: 0.0542, step time: 1.5513\n",
      "365/388, train_loss: 0.2479, step time: 1.5531\n",
      "366/388, train_loss: 0.0904, step time: 1.5513\n",
      "367/388, train_loss: 0.1002, step time: 1.5548\n",
      "368/388, train_loss: 0.0938, step time: 1.5566\n",
      "369/388, train_loss: 0.0717, step time: 1.5570\n",
      "370/388, train_loss: 0.0580, step time: 1.5551\n",
      "371/388, train_loss: 0.1338, step time: 1.5542\n",
      "372/388, train_loss: 0.2566, step time: 1.5533\n",
      "373/388, train_loss: 0.2214, step time: 1.5563\n",
      "374/388, train_loss: 0.2290, step time: 1.5545\n",
      "375/388, train_loss: 0.2235, step time: 1.5525\n",
      "376/388, train_loss: 0.0932, step time: 1.5538\n",
      "377/388, train_loss: 0.1441, step time: 1.5562\n",
      "378/388, train_loss: 0.0786, step time: 1.5537\n",
      "379/388, train_loss: 0.1021, step time: 1.5546\n",
      "380/388, train_loss: 0.1324, step time: 1.5545\n",
      "381/388, train_loss: 0.1116, step time: 1.5547\n",
      "382/388, train_loss: 0.3224, step time: 1.5552\n",
      "383/388, train_loss: 0.2204, step time: 1.5539\n",
      "384/388, train_loss: 0.1303, step time: 1.5543\n",
      "385/388, train_loss: 0.1243, step time: 1.5508\n",
      "386/388, train_loss: 0.2746, step time: 1.5519\n",
      "387/388, train_loss: 0.1854, step time: 1.5490\n",
      "388/388, train_loss: 0.1360, step time: 1.5544\n",
      "epoch 93 average loss: 0.1575\n",
      "current epoch: 93 current mean dice: 0.7805 tc: 0.8292 wt: 0.9063 et: 0.6061\n",
      "best mean dice: 0.7817 at epoch: 84\n",
      "time consuming of epoch 93 is: 713.9547\n",
      "----------\n",
      "epoch 94/100\n",
      "1/388, train_loss: 0.1677, step time: 1.5763\n",
      "2/388, train_loss: 0.1186, step time: 1.5530\n",
      "3/388, train_loss: 0.2281, step time: 1.5522\n",
      "4/388, train_loss: 0.1906, step time: 1.5563\n",
      "5/388, train_loss: 0.1969, step time: 1.5500\n",
      "6/388, train_loss: 0.1842, step time: 1.5504\n",
      "7/388, train_loss: 0.1292, step time: 1.5520\n",
      "8/388, train_loss: 0.1283, step time: 1.5534\n",
      "9/388, train_loss: 0.1486, step time: 1.5558\n",
      "10/388, train_loss: 0.1081, step time: 1.5552\n",
      "11/388, train_loss: 0.1693, step time: 1.5577\n",
      "12/388, train_loss: 0.0363, step time: 1.5536\n",
      "13/388, train_loss: 0.0901, step time: 1.5520\n",
      "14/388, train_loss: 0.0415, step time: 1.5546\n",
      "15/388, train_loss: 0.1063, step time: 1.5545\n",
      "16/388, train_loss: 0.2390, step time: 1.5539\n",
      "17/388, train_loss: 0.1964, step time: 1.5586\n",
      "18/388, train_loss: 0.1079, step time: 1.5529\n",
      "19/388, train_loss: 0.1282, step time: 1.5529\n",
      "20/388, train_loss: 0.2397, step time: 1.5542\n",
      "21/388, train_loss: 0.0583, step time: 1.5522\n",
      "22/388, train_loss: 0.0745, step time: 1.5555\n",
      "23/388, train_loss: 0.2791, step time: 1.5530\n",
      "24/388, train_loss: 0.0760, step time: 1.5704\n",
      "25/388, train_loss: 0.2009, step time: 1.5537\n",
      "26/388, train_loss: 0.2311, step time: 1.5558\n",
      "27/388, train_loss: 0.0644, step time: 1.5559\n",
      "28/388, train_loss: 0.1326, step time: 1.5549\n",
      "29/388, train_loss: 0.0316, step time: 1.5599\n",
      "30/388, train_loss: 0.0899, step time: 1.5601\n",
      "31/388, train_loss: 0.2995, step time: 1.5485\n",
      "32/388, train_loss: 0.0553, step time: 1.5570\n",
      "33/388, train_loss: 0.1220, step time: 1.5576\n",
      "34/388, train_loss: 0.0455, step time: 1.5563\n",
      "35/388, train_loss: 0.1024, step time: 1.5554\n",
      "36/388, train_loss: 0.2160, step time: 1.5541\n",
      "37/388, train_loss: 0.0969, step time: 1.5602\n",
      "38/388, train_loss: 0.0814, step time: 1.5559\n",
      "39/388, train_loss: 0.0447, step time: 1.5550\n",
      "40/388, train_loss: 0.1354, step time: 1.5572\n",
      "41/388, train_loss: 0.1855, step time: 1.5591\n",
      "42/388, train_loss: 0.3106, step time: 1.5610\n",
      "43/388, train_loss: 0.1342, step time: 1.5527\n",
      "44/388, train_loss: 0.3678, step time: 1.5516\n",
      "45/388, train_loss: 0.3012, step time: 1.5521\n",
      "46/388, train_loss: 0.1058, step time: 1.5546\n",
      "47/388, train_loss: 0.2792, step time: 1.5522\n",
      "48/388, train_loss: 0.1746, step time: 1.5592\n",
      "49/388, train_loss: 0.1559, step time: 1.5545\n",
      "50/388, train_loss: 0.0963, step time: 1.5541\n",
      "51/388, train_loss: 0.2345, step time: 1.5538\n",
      "52/388, train_loss: 0.0898, step time: 1.5564\n",
      "53/388, train_loss: 0.0530, step time: 1.5558\n",
      "54/388, train_loss: 0.0937, step time: 1.5593\n",
      "55/388, train_loss: 0.1569, step time: 1.5538\n",
      "56/388, train_loss: 0.0872, step time: 1.5508\n",
      "57/388, train_loss: 0.1162, step time: 1.5490\n",
      "58/388, train_loss: 0.0967, step time: 1.5426\n",
      "59/388, train_loss: 0.1079, step time: 1.5550\n",
      "60/388, train_loss: 0.1170, step time: 1.5563\n",
      "61/388, train_loss: 0.3110, step time: 1.5498\n",
      "62/388, train_loss: 0.1398, step time: 1.5559\n",
      "63/388, train_loss: 0.0874, step time: 1.5541\n",
      "64/388, train_loss: 0.1728, step time: 1.5544\n",
      "65/388, train_loss: 0.3240, step time: 1.5547\n",
      "66/388, train_loss: 0.1358, step time: 1.5564\n",
      "67/388, train_loss: 0.1171, step time: 1.5558\n",
      "68/388, train_loss: 0.1596, step time: 1.5555\n",
      "69/388, train_loss: 0.0704, step time: 1.5554\n",
      "70/388, train_loss: 0.0840, step time: 1.5557\n",
      "71/388, train_loss: 0.0841, step time: 1.5581\n",
      "72/388, train_loss: 0.1186, step time: 1.5550\n",
      "73/388, train_loss: 0.1834, step time: 1.5564\n",
      "74/388, train_loss: 0.1886, step time: 1.5566\n",
      "75/388, train_loss: 0.1552, step time: 1.5522\n",
      "76/388, train_loss: 0.2759, step time: 1.5582\n",
      "77/388, train_loss: 0.2347, step time: 1.5583\n",
      "78/388, train_loss: 0.2070, step time: 1.5606\n",
      "79/388, train_loss: 0.0796, step time: 1.5552\n",
      "80/388, train_loss: 0.0775, step time: 1.5546\n",
      "81/388, train_loss: 0.1292, step time: 1.5543\n",
      "82/388, train_loss: 0.1827, step time: 1.5579\n",
      "83/388, train_loss: 0.1173, step time: 1.5549\n",
      "84/388, train_loss: 0.0324, step time: 1.5581\n",
      "85/388, train_loss: 0.0745, step time: 1.5508\n",
      "86/388, train_loss: 0.0991, step time: 1.5541\n",
      "87/388, train_loss: 0.0929, step time: 1.5548\n",
      "88/388, train_loss: 0.1572, step time: 1.5498\n",
      "89/388, train_loss: 0.1169, step time: 1.5554\n",
      "90/388, train_loss: 0.2593, step time: 1.5559\n",
      "91/388, train_loss: 0.1247, step time: 1.5510\n",
      "92/388, train_loss: 0.1023, step time: 1.5598\n",
      "93/388, train_loss: 0.1276, step time: 1.5538\n",
      "94/388, train_loss: 0.1602, step time: 1.5532\n",
      "95/388, train_loss: 0.5134, step time: 1.5553\n",
      "96/388, train_loss: 0.2224, step time: 1.5551\n",
      "97/388, train_loss: 0.0580, step time: 1.5532\n",
      "98/388, train_loss: 0.1971, step time: 1.5522\n",
      "99/388, train_loss: 0.2005, step time: 1.5543\n",
      "100/388, train_loss: 0.2062, step time: 1.5517\n",
      "101/388, train_loss: 0.2621, step time: 1.5522\n",
      "102/388, train_loss: 0.0954, step time: 1.5550\n",
      "103/388, train_loss: 0.0755, step time: 1.5515\n",
      "104/388, train_loss: 0.2517, step time: 1.5539\n",
      "105/388, train_loss: 0.0810, step time: 1.5542\n",
      "106/388, train_loss: 0.3874, step time: 1.5533\n",
      "107/388, train_loss: 0.1818, step time: 1.5561\n",
      "108/388, train_loss: 0.0755, step time: 1.5552\n",
      "109/388, train_loss: 0.1107, step time: 1.5522\n",
      "110/388, train_loss: 0.1091, step time: 1.5532\n",
      "111/388, train_loss: 0.1672, step time: 1.5565\n",
      "112/388, train_loss: 0.1479, step time: 1.5535\n",
      "113/388, train_loss: 0.0728, step time: 1.5549\n",
      "114/388, train_loss: 0.0889, step time: 1.5550\n",
      "115/388, train_loss: 0.2424, step time: 1.5510\n",
      "116/388, train_loss: 0.1427, step time: 1.5544\n",
      "117/388, train_loss: 0.2540, step time: 1.5576\n",
      "118/388, train_loss: 0.0746, step time: 1.5521\n",
      "119/388, train_loss: 0.1397, step time: 1.5569\n",
      "120/388, train_loss: 0.2833, step time: 1.5572\n",
      "121/388, train_loss: 0.3344, step time: 1.5516\n",
      "122/388, train_loss: 0.0678, step time: 1.5549\n",
      "123/388, train_loss: 0.1724, step time: 1.5533\n",
      "124/388, train_loss: 0.1495, step time: 1.5510\n",
      "125/388, train_loss: 0.3563, step time: 1.5551\n",
      "126/388, train_loss: 0.0861, step time: 1.5541\n",
      "127/388, train_loss: 0.1661, step time: 1.5467\n",
      "128/388, train_loss: 0.1574, step time: 1.5552\n",
      "129/388, train_loss: 0.1190, step time: 1.5560\n",
      "130/388, train_loss: 0.1292, step time: 1.5518\n",
      "131/388, train_loss: 0.1082, step time: 1.5542\n",
      "132/388, train_loss: 0.0953, step time: 1.5548\n",
      "133/388, train_loss: 0.1044, step time: 1.5513\n",
      "134/388, train_loss: 0.1235, step time: 1.5546\n",
      "135/388, train_loss: 0.2671, step time: 1.5543\n",
      "136/388, train_loss: 0.0957, step time: 1.5540\n",
      "137/388, train_loss: 0.1632, step time: 1.5536\n",
      "138/388, train_loss: 0.2228, step time: 1.5518\n",
      "139/388, train_loss: 0.1078, step time: 1.5554\n",
      "140/388, train_loss: 0.0862, step time: 1.5566\n",
      "141/388, train_loss: 0.0549, step time: 1.5527\n",
      "142/388, train_loss: 0.1996, step time: 1.5547\n",
      "143/388, train_loss: 0.2055, step time: 1.5562\n",
      "144/388, train_loss: 0.1624, step time: 1.5520\n",
      "145/388, train_loss: 0.0671, step time: 1.5541\n",
      "146/388, train_loss: 0.1962, step time: 1.5545\n",
      "147/388, train_loss: 0.1660, step time: 1.5513\n",
      "148/388, train_loss: 0.1173, step time: 1.5519\n",
      "149/388, train_loss: 0.2093, step time: 1.5534\n",
      "150/388, train_loss: 0.1581, step time: 1.5499\n",
      "151/388, train_loss: 0.0432, step time: 1.5518\n",
      "152/388, train_loss: 0.1915, step time: 1.5524\n",
      "153/388, train_loss: 0.2246, step time: 1.5541\n",
      "154/388, train_loss: 0.0500, step time: 1.5512\n",
      "155/388, train_loss: 0.2973, step time: 1.5533\n",
      "156/388, train_loss: 0.1379, step time: 1.5527\n",
      "157/388, train_loss: 0.1702, step time: 1.5488\n",
      "158/388, train_loss: 0.0992, step time: 1.5561\n",
      "159/388, train_loss: 0.1398, step time: 1.5557\n",
      "160/388, train_loss: 0.0683, step time: 1.5511\n",
      "161/388, train_loss: 0.1818, step time: 1.5557\n",
      "162/388, train_loss: 0.1713, step time: 1.5522\n",
      "163/388, train_loss: 0.3657, step time: 1.5528\n",
      "164/388, train_loss: 0.1306, step time: 1.5569\n",
      "165/388, train_loss: 0.1165, step time: 1.5536\n",
      "166/388, train_loss: 0.0734, step time: 1.5550\n",
      "167/388, train_loss: 0.1649, step time: 1.5528\n",
      "168/388, train_loss: 0.1559, step time: 1.5522\n",
      "169/388, train_loss: 0.1358, step time: 1.5540\n",
      "170/388, train_loss: 0.3311, step time: 1.5530\n",
      "171/388, train_loss: 0.1822, step time: 1.5539\n",
      "172/388, train_loss: 0.2323, step time: 1.5556\n",
      "173/388, train_loss: 0.2495, step time: 1.5512\n",
      "174/388, train_loss: 0.1947, step time: 1.5553\n",
      "175/388, train_loss: 0.1470, step time: 1.5530\n",
      "176/388, train_loss: 0.1922, step time: 1.5533\n",
      "177/388, train_loss: 0.2330, step time: 1.5549\n",
      "178/388, train_loss: 0.4867, step time: 1.5563\n",
      "179/388, train_loss: 0.1941, step time: 1.5535\n",
      "180/388, train_loss: 0.0955, step time: 1.5527\n",
      "181/388, train_loss: 0.1075, step time: 1.5556\n",
      "182/388, train_loss: 0.2558, step time: 1.5535\n",
      "183/388, train_loss: 0.2851, step time: 1.5527\n",
      "184/388, train_loss: 0.2738, step time: 1.5558\n",
      "185/388, train_loss: 0.3642, step time: 1.5519\n",
      "186/388, train_loss: 0.1949, step time: 1.5600\n",
      "187/388, train_loss: 0.1141, step time: 1.5555\n",
      "188/388, train_loss: 0.0570, step time: 1.5513\n",
      "189/388, train_loss: 0.1914, step time: 1.5570\n",
      "190/388, train_loss: 0.0438, step time: 1.5546\n",
      "191/388, train_loss: 0.1729, step time: 1.5510\n",
      "192/388, train_loss: 0.0837, step time: 1.5561\n",
      "193/388, train_loss: 0.2016, step time: 1.5539\n",
      "194/388, train_loss: 0.0490, step time: 1.5518\n",
      "195/388, train_loss: 0.0982, step time: 1.5520\n",
      "196/388, train_loss: 0.4088, step time: 1.5526\n",
      "197/388, train_loss: 0.0681, step time: 1.5544\n",
      "198/388, train_loss: 0.0736, step time: 1.5542\n",
      "199/388, train_loss: 0.1078, step time: 1.5538\n",
      "200/388, train_loss: 0.1417, step time: 1.5544\n",
      "201/388, train_loss: 0.1803, step time: 1.5512\n",
      "202/388, train_loss: 0.2822, step time: 1.5566\n",
      "203/388, train_loss: 0.0981, step time: 1.5537\n",
      "204/388, train_loss: 0.1849, step time: 1.5537\n",
      "205/388, train_loss: 0.1260, step time: 1.5597\n",
      "206/388, train_loss: 0.0708, step time: 1.5539\n",
      "207/388, train_loss: 0.1045, step time: 1.5599\n",
      "208/388, train_loss: 0.0673, step time: 1.5513\n",
      "209/388, train_loss: 0.1354, step time: 1.5533\n",
      "210/388, train_loss: 0.1303, step time: 1.5528\n",
      "211/388, train_loss: 0.0592, step time: 1.5510\n",
      "212/388, train_loss: 0.2065, step time: 1.5523\n",
      "213/388, train_loss: 0.1661, step time: 1.5545\n",
      "214/388, train_loss: 0.1950, step time: 1.5503\n",
      "215/388, train_loss: 0.1661, step time: 1.5541\n",
      "216/388, train_loss: 0.0620, step time: 1.5593\n",
      "217/388, train_loss: 0.1581, step time: 1.5571\n",
      "218/388, train_loss: 0.1100, step time: 1.5544\n",
      "219/388, train_loss: 0.1663, step time: 1.5531\n",
      "220/388, train_loss: 0.1804, step time: 1.5561\n",
      "221/388, train_loss: 0.1941, step time: 1.5533\n",
      "222/388, train_loss: 0.1514, step time: 1.5493\n",
      "223/388, train_loss: 0.1661, step time: 1.5516\n",
      "224/388, train_loss: 0.1840, step time: 1.5535\n",
      "225/388, train_loss: 0.4796, step time: 1.5554\n",
      "226/388, train_loss: 0.1967, step time: 1.5532\n",
      "227/388, train_loss: 0.0843, step time: 1.5534\n",
      "228/388, train_loss: 0.1986, step time: 1.5531\n",
      "229/388, train_loss: 0.1056, step time: 1.5558\n",
      "230/388, train_loss: 0.0709, step time: 1.5566\n",
      "231/388, train_loss: 0.2234, step time: 1.5517\n",
      "232/388, train_loss: 0.1654, step time: 1.5552\n",
      "233/388, train_loss: 0.2592, step time: 1.5537\n",
      "234/388, train_loss: 0.0702, step time: 1.5568\n",
      "235/388, train_loss: 0.1780, step time: 1.5515\n",
      "236/388, train_loss: 0.1614, step time: 1.5523\n",
      "237/388, train_loss: 0.0693, step time: 1.5521\n",
      "238/388, train_loss: 0.2407, step time: 1.5468\n",
      "239/388, train_loss: 0.1683, step time: 1.5537\n",
      "240/388, train_loss: 0.1554, step time: 1.5550\n",
      "241/388, train_loss: 0.1495, step time: 1.5541\n",
      "242/388, train_loss: 0.0728, step time: 1.5513\n",
      "243/388, train_loss: 0.0637, step time: 1.5555\n",
      "244/388, train_loss: 0.1884, step time: 1.5540\n",
      "245/388, train_loss: 0.1194, step time: 1.5573\n",
      "246/388, train_loss: 0.1156, step time: 1.5539\n",
      "247/388, train_loss: 0.1215, step time: 1.5537\n",
      "248/388, train_loss: 0.3529, step time: 1.5541\n",
      "249/388, train_loss: 0.1774, step time: 1.5535\n",
      "250/388, train_loss: 0.1864, step time: 1.5600\n",
      "251/388, train_loss: 0.2079, step time: 1.5530\n",
      "252/388, train_loss: 0.0800, step time: 1.5552\n",
      "253/388, train_loss: 0.0489, step time: 1.5541\n",
      "254/388, train_loss: 0.1414, step time: 1.5570\n",
      "255/388, train_loss: 0.0903, step time: 1.5581\n",
      "256/388, train_loss: 0.2767, step time: 1.5523\n",
      "257/388, train_loss: 0.1486, step time: 1.5526\n",
      "258/388, train_loss: 0.0617, step time: 1.5522\n",
      "259/388, train_loss: 0.0455, step time: 1.5605\n",
      "260/388, train_loss: 0.1190, step time: 1.5558\n",
      "261/388, train_loss: 0.1891, step time: 1.5505\n",
      "262/388, train_loss: 0.1199, step time: 1.5542\n",
      "263/388, train_loss: 0.0926, step time: 1.5575\n",
      "264/388, train_loss: 0.0839, step time: 1.5510\n",
      "265/388, train_loss: 0.2223, step time: 1.5594\n",
      "266/388, train_loss: 0.0614, step time: 1.5562\n",
      "267/388, train_loss: 0.1811, step time: 1.5344\n",
      "268/388, train_loss: 0.1684, step time: 1.5549\n",
      "269/388, train_loss: 0.3064, step time: 1.5573\n",
      "270/388, train_loss: 0.1028, step time: 1.5548\n",
      "271/388, train_loss: 0.2764, step time: 1.5566\n",
      "272/388, train_loss: 0.0827, step time: 1.5548\n",
      "273/388, train_loss: 0.0840, step time: 1.5541\n",
      "274/388, train_loss: 0.1929, step time: 1.5516\n",
      "275/388, train_loss: 0.0947, step time: 1.5369\n",
      "276/388, train_loss: 0.1453, step time: 1.5534\n",
      "277/388, train_loss: 0.1061, step time: 1.5512\n",
      "278/388, train_loss: 0.1032, step time: 1.5541\n",
      "279/388, train_loss: 0.0819, step time: 1.5587\n",
      "280/388, train_loss: 0.2201, step time: 1.5545\n",
      "281/388, train_loss: 0.0897, step time: 1.5546\n",
      "282/388, train_loss: 0.1149, step time: 1.5531\n",
      "283/388, train_loss: 0.1465, step time: 1.5365\n",
      "284/388, train_loss: 0.3274, step time: 1.5571\n",
      "285/388, train_loss: 0.0296, step time: 1.5590\n",
      "286/388, train_loss: 0.0949, step time: 1.5533\n",
      "287/388, train_loss: 0.0829, step time: 1.5408\n",
      "288/388, train_loss: 0.1310, step time: 1.5621\n",
      "289/388, train_loss: 0.0620, step time: 1.5498\n",
      "290/388, train_loss: 0.1053, step time: 1.5572\n",
      "291/388, train_loss: 0.1208, step time: 1.5650\n",
      "292/388, train_loss: 0.1244, step time: 1.5736\n",
      "293/388, train_loss: 0.3657, step time: 1.5578\n",
      "294/388, train_loss: 0.0842, step time: 1.5566\n",
      "295/388, train_loss: 0.3878, step time: 1.5590\n",
      "296/388, train_loss: 0.1394, step time: 1.5515\n",
      "297/388, train_loss: 0.0884, step time: 1.5576\n",
      "298/388, train_loss: 0.3311, step time: 1.5557\n",
      "299/388, train_loss: 0.2192, step time: 1.5323\n",
      "300/388, train_loss: 0.1750, step time: 1.5576\n",
      "301/388, train_loss: 0.1642, step time: 1.5556\n",
      "302/388, train_loss: 0.1121, step time: 1.5568\n",
      "303/388, train_loss: 0.4184, step time: 1.5380\n",
      "304/388, train_loss: 0.0558, step time: 1.5540\n",
      "305/388, train_loss: 0.1860, step time: 1.5514\n",
      "306/388, train_loss: 0.2024, step time: 1.5470\n",
      "307/388, train_loss: 0.0886, step time: 1.5342\n",
      "308/388, train_loss: 0.1404, step time: 1.5543\n",
      "309/388, train_loss: 0.1882, step time: 1.5550\n",
      "310/388, train_loss: 0.1564, step time: 1.5538\n",
      "311/388, train_loss: 0.2089, step time: 1.5444\n",
      "312/388, train_loss: 0.1204, step time: 1.5621\n",
      "313/388, train_loss: 0.1420, step time: 1.5524\n",
      "314/388, train_loss: 0.2340, step time: 1.5549\n",
      "315/388, train_loss: 0.4831, step time: 1.5328\n",
      "316/388, train_loss: 0.0769, step time: 1.5589\n",
      "317/388, train_loss: 0.1901, step time: 1.5619\n",
      "318/388, train_loss: 0.0764, step time: 1.5518\n",
      "319/388, train_loss: 0.1600, step time: 1.5595\n",
      "320/388, train_loss: 0.1009, step time: 1.5551\n",
      "321/388, train_loss: 0.3257, step time: 1.5508\n",
      "322/388, train_loss: 0.1266, step time: 1.5529\n",
      "323/388, train_loss: 0.1495, step time: 1.5708\n",
      "324/388, train_loss: 0.0913, step time: 1.5560\n",
      "325/388, train_loss: 0.0625, step time: 1.5532\n",
      "326/388, train_loss: 0.1453, step time: 1.5525\n",
      "327/388, train_loss: 0.2436, step time: 1.5378\n",
      "328/388, train_loss: 0.1226, step time: 1.5557\n",
      "329/388, train_loss: 0.3130, step time: 1.5524\n",
      "330/388, train_loss: 0.0876, step time: 1.5549\n",
      "331/388, train_loss: 0.0822, step time: 1.5600\n",
      "332/388, train_loss: 0.0863, step time: 1.5518\n",
      "333/388, train_loss: 0.1585, step time: 1.5541\n",
      "334/388, train_loss: 0.1496, step time: 1.5528\n",
      "335/388, train_loss: 0.2066, step time: 1.5331\n",
      "336/388, train_loss: 0.1784, step time: 1.5570\n",
      "337/388, train_loss: 0.0962, step time: 1.5558\n",
      "338/388, train_loss: 0.1122, step time: 1.5525\n",
      "339/388, train_loss: 0.0951, step time: 1.5589\n",
      "340/388, train_loss: 0.1479, step time: 1.5530\n",
      "341/388, train_loss: 0.2617, step time: 1.5564\n",
      "342/388, train_loss: 0.1814, step time: 1.5554\n",
      "343/388, train_loss: 0.2142, step time: 1.5616\n",
      "344/388, train_loss: 0.1437, step time: 1.5587\n",
      "345/388, train_loss: 0.0900, step time: 1.5546\n",
      "346/388, train_loss: 0.1549, step time: 1.5530\n",
      "347/388, train_loss: 0.0843, step time: 1.5750\n",
      "348/388, train_loss: 0.2044, step time: 1.5521\n",
      "349/388, train_loss: 0.0950, step time: 1.5563\n",
      "350/388, train_loss: 0.1726, step time: 1.5579\n",
      "351/388, train_loss: 0.1187, step time: 1.5595\n",
      "352/388, train_loss: 0.1927, step time: 1.5549\n",
      "353/388, train_loss: 0.0744, step time: 1.5516\n",
      "354/388, train_loss: 0.0858, step time: 1.5538\n",
      "355/388, train_loss: 0.1302, step time: 1.5354\n",
      "356/388, train_loss: 0.2604, step time: 1.5528\n",
      "357/388, train_loss: 0.2529, step time: 1.5653\n",
      "358/388, train_loss: 0.1349, step time: 1.5551\n",
      "359/388, train_loss: 0.1482, step time: 1.5565\n",
      "360/388, train_loss: 0.1018, step time: 1.5548\n",
      "361/388, train_loss: 0.2248, step time: 1.5528\n",
      "362/388, train_loss: 0.1868, step time: 1.5545\n",
      "363/388, train_loss: 0.2325, step time: 1.5571\n",
      "364/388, train_loss: 0.2319, step time: 1.5544\n",
      "365/388, train_loss: 0.1750, step time: 1.5548\n",
      "366/388, train_loss: 0.1147, step time: 1.5539\n",
      "367/388, train_loss: 0.2253, step time: 1.5359\n",
      "368/388, train_loss: 0.1576, step time: 1.5556\n",
      "369/388, train_loss: 0.1096, step time: 1.5511\n",
      "370/388, train_loss: 0.0986, step time: 1.5561\n",
      "371/388, train_loss: 0.2041, step time: 1.5575\n",
      "372/388, train_loss: 0.1322, step time: 1.5525\n",
      "373/388, train_loss: 0.1131, step time: 1.5569\n",
      "374/388, train_loss: 0.0918, step time: 1.5516\n",
      "375/388, train_loss: 0.1326, step time: 1.5601\n",
      "376/388, train_loss: 0.0585, step time: 1.5509\n",
      "377/388, train_loss: 0.1158, step time: 1.5529\n",
      "378/388, train_loss: 0.1383, step time: 1.5542\n",
      "379/388, train_loss: 0.1840, step time: 1.5589\n",
      "380/388, train_loss: 0.0878, step time: 1.5556\n",
      "381/388, train_loss: 0.0891, step time: 1.5531\n",
      "382/388, train_loss: 0.1011, step time: 1.5528\n",
      "383/388, train_loss: 0.4492, step time: 1.5534\n",
      "384/388, train_loss: 0.0890, step time: 1.5512\n",
      "385/388, train_loss: 0.1395, step time: 1.5521\n",
      "386/388, train_loss: 0.0809, step time: 1.5557\n",
      "387/388, train_loss: 0.4101, step time: 1.5530\n",
      "388/388, train_loss: 0.1599, step time: 1.5556\n",
      "epoch 94 average loss: 0.1574\n",
      "current epoch: 94 current mean dice: 0.7801 tc: 0.8288 wt: 0.9054 et: 0.6062\n",
      "best mean dice: 0.7817 at epoch: 84\n",
      "time consuming of epoch 94 is: 710.9420\n",
      "----------\n",
      "epoch 95/100\n",
      "1/388, train_loss: 0.1817, step time: 1.5735\n",
      "2/388, train_loss: 0.2247, step time: 1.5572\n",
      "3/388, train_loss: 0.3027, step time: 1.5543\n",
      "4/388, train_loss: 0.0879, step time: 1.5511\n",
      "5/388, train_loss: 0.2301, step time: 1.5655\n",
      "6/388, train_loss: 0.1268, step time: 1.5558\n",
      "7/388, train_loss: 0.1440, step time: 1.5551\n",
      "8/388, train_loss: 0.1491, step time: 1.5554\n",
      "9/388, train_loss: 0.2178, step time: 1.5530\n",
      "10/388, train_loss: 0.0985, step time: 1.5528\n",
      "11/388, train_loss: 0.2486, step time: 1.5532\n",
      "12/388, train_loss: 0.3310, step time: 1.5543\n",
      "13/388, train_loss: 0.2032, step time: 1.5543\n",
      "14/388, train_loss: 0.1316, step time: 1.5554\n",
      "15/388, train_loss: 0.0903, step time: 1.5580\n",
      "16/388, train_loss: 0.1255, step time: 1.5580\n",
      "17/388, train_loss: 0.0925, step time: 1.5617\n",
      "18/388, train_loss: 0.1433, step time: 1.5546\n",
      "19/388, train_loss: 0.1010, step time: 1.5561\n",
      "20/388, train_loss: 0.0933, step time: 1.5563\n",
      "21/388, train_loss: 0.3762, step time: 1.5554\n",
      "22/388, train_loss: 0.1961, step time: 1.5559\n",
      "23/388, train_loss: 0.1899, step time: 1.5560\n",
      "24/388, train_loss: 0.0608, step time: 1.5534\n",
      "25/388, train_loss: 0.0794, step time: 1.5563\n",
      "26/388, train_loss: 0.2757, step time: 1.5541\n",
      "27/388, train_loss: 0.1522, step time: 1.5498\n",
      "28/388, train_loss: 0.1414, step time: 1.5555\n",
      "29/388, train_loss: 0.0722, step time: 1.5643\n",
      "30/388, train_loss: 0.1018, step time: 1.5535\n",
      "31/388, train_loss: 0.1311, step time: 1.5555\n",
      "32/388, train_loss: 0.1396, step time: 1.5541\n",
      "33/388, train_loss: 0.0865, step time: 1.5560\n",
      "34/388, train_loss: 0.1281, step time: 1.5569\n",
      "35/388, train_loss: 0.0976, step time: 1.5513\n",
      "36/388, train_loss: 0.1729, step time: 1.5549\n",
      "37/388, train_loss: 0.1001, step time: 1.5513\n",
      "38/388, train_loss: 0.2763, step time: 1.5542\n",
      "39/388, train_loss: 0.1081, step time: 1.5543\n",
      "40/388, train_loss: 0.1393, step time: 1.5517\n",
      "41/388, train_loss: 0.1296, step time: 1.5718\n",
      "42/388, train_loss: 0.1154, step time: 1.5515\n",
      "43/388, train_loss: 0.2018, step time: 1.5553\n",
      "44/388, train_loss: 0.0970, step time: 1.5550\n",
      "45/388, train_loss: 0.1829, step time: 1.5518\n",
      "46/388, train_loss: 0.2585, step time: 1.5527\n",
      "47/388, train_loss: 0.0676, step time: 1.5547\n",
      "48/388, train_loss: 0.1017, step time: 1.5525\n",
      "49/388, train_loss: 0.1468, step time: 1.5546\n",
      "50/388, train_loss: 0.1356, step time: 1.5554\n",
      "51/388, train_loss: 0.1684, step time: 1.5514\n",
      "52/388, train_loss: 0.0951, step time: 1.5542\n",
      "53/388, train_loss: 0.1319, step time: 1.5562\n",
      "54/388, train_loss: 0.1485, step time: 1.5521\n",
      "55/388, train_loss: 0.1047, step time: 1.5550\n",
      "56/388, train_loss: 0.0660, step time: 1.5537\n",
      "57/388, train_loss: 0.1197, step time: 1.5537\n",
      "58/388, train_loss: 0.0261, step time: 1.5561\n",
      "59/388, train_loss: 0.0765, step time: 1.5539\n",
      "60/388, train_loss: 0.0894, step time: 1.5512\n",
      "61/388, train_loss: 0.1797, step time: 1.5581\n",
      "62/388, train_loss: 0.1159, step time: 1.5596\n",
      "63/388, train_loss: 0.0808, step time: 1.5557\n",
      "64/388, train_loss: 0.1049, step time: 1.5583\n",
      "65/388, train_loss: 0.1353, step time: 1.5544\n",
      "66/388, train_loss: 0.1101, step time: 1.5517\n",
      "67/388, train_loss: 0.0991, step time: 1.5606\n",
      "68/388, train_loss: 0.1703, step time: 1.5561\n",
      "69/388, train_loss: 0.0760, step time: 1.5531\n",
      "70/388, train_loss: 0.1272, step time: 1.5602\n",
      "71/388, train_loss: 0.1011, step time: 1.5568\n",
      "72/388, train_loss: 0.2087, step time: 1.5514\n",
      "73/388, train_loss: 0.1911, step time: 1.5541\n",
      "74/388, train_loss: 0.1458, step time: 1.5563\n",
      "75/388, train_loss: 0.1223, step time: 1.5538\n",
      "76/388, train_loss: 0.0669, step time: 1.5556\n",
      "77/388, train_loss: 0.2645, step time: 1.5552\n",
      "78/388, train_loss: 0.3200, step time: 1.5548\n",
      "79/388, train_loss: 0.1548, step time: 1.5540\n",
      "80/388, train_loss: 0.1613, step time: 1.5542\n",
      "81/388, train_loss: 0.1398, step time: 1.5522\n",
      "82/388, train_loss: 0.0552, step time: 1.5612\n",
      "83/388, train_loss: 0.2033, step time: 1.5523\n",
      "84/388, train_loss: 0.2108, step time: 1.5535\n",
      "85/388, train_loss: 0.1529, step time: 1.5578\n",
      "86/388, train_loss: 0.2243, step time: 1.5579\n",
      "87/388, train_loss: 0.1843, step time: 1.5489\n",
      "88/388, train_loss: 0.1662, step time: 1.5546\n",
      "89/388, train_loss: 0.2643, step time: 1.5560\n",
      "90/388, train_loss: 0.0409, step time: 1.5534\n",
      "91/388, train_loss: 0.2168, step time: 1.5548\n",
      "92/388, train_loss: 0.0980, step time: 1.5555\n",
      "93/388, train_loss: 0.0665, step time: 1.5536\n",
      "94/388, train_loss: 0.1741, step time: 1.5598\n",
      "95/388, train_loss: 0.0769, step time: 1.5528\n",
      "96/388, train_loss: 0.2165, step time: 1.5566\n",
      "97/388, train_loss: 0.0497, step time: 1.5533\n",
      "98/388, train_loss: 0.2243, step time: 1.5523\n",
      "99/388, train_loss: 0.1115, step time: 1.5544\n",
      "100/388, train_loss: 0.4433, step time: 1.5561\n",
      "101/388, train_loss: 0.0981, step time: 1.5531\n",
      "102/388, train_loss: 0.2593, step time: 1.5547\n",
      "103/388, train_loss: 0.1867, step time: 1.5554\n",
      "104/388, train_loss: 0.1897, step time: 1.5559\n",
      "105/388, train_loss: 0.1466, step time: 1.5535\n",
      "106/388, train_loss: 0.1844, step time: 1.5521\n",
      "107/388, train_loss: 0.1577, step time: 1.5546\n",
      "108/388, train_loss: 0.1857, step time: 1.5530\n",
      "109/388, train_loss: 0.1558, step time: 1.5559\n",
      "110/388, train_loss: 0.0534, step time: 1.5537\n",
      "111/388, train_loss: 0.0637, step time: 1.5573\n",
      "112/388, train_loss: 0.1705, step time: 1.5562\n",
      "113/388, train_loss: 0.3074, step time: 1.5494\n",
      "114/388, train_loss: 0.4710, step time: 1.5568\n",
      "115/388, train_loss: 0.2043, step time: 1.5545\n",
      "116/388, train_loss: 0.1022, step time: 1.5518\n",
      "117/388, train_loss: 0.1027, step time: 1.5551\n",
      "118/388, train_loss: 0.0736, step time: 1.5541\n",
      "119/388, train_loss: 0.2417, step time: 1.5517\n",
      "120/388, train_loss: 0.1939, step time: 1.5552\n",
      "121/388, train_loss: 0.0992, step time: 1.5547\n",
      "122/388, train_loss: 0.0834, step time: 1.5532\n",
      "123/388, train_loss: 0.1414, step time: 1.5651\n",
      "124/388, train_loss: 0.0876, step time: 1.5524\n",
      "125/388, train_loss: 0.1457, step time: 1.5547\n",
      "126/388, train_loss: 0.0985, step time: 1.5568\n",
      "127/388, train_loss: 0.0909, step time: 1.5510\n",
      "128/388, train_loss: 0.0734, step time: 1.5511\n",
      "129/388, train_loss: 0.1107, step time: 1.5512\n",
      "130/388, train_loss: 0.1088, step time: 1.5539\n",
      "131/388, train_loss: 0.1868, step time: 1.5512\n",
      "132/388, train_loss: 0.1045, step time: 1.5554\n",
      "133/388, train_loss: 0.2374, step time: 1.5547\n",
      "134/388, train_loss: 0.1131, step time: 1.5541\n",
      "135/388, train_loss: 0.4252, step time: 1.5575\n",
      "136/388, train_loss: 0.2750, step time: 1.5537\n",
      "137/388, train_loss: 0.1575, step time: 1.5553\n",
      "138/388, train_loss: 0.2026, step time: 1.5577\n",
      "139/388, train_loss: 0.1026, step time: 1.5529\n",
      "140/388, train_loss: 0.1731, step time: 1.5525\n",
      "141/388, train_loss: 0.0789, step time: 1.5539\n",
      "142/388, train_loss: 0.1681, step time: 1.5527\n",
      "143/388, train_loss: 0.3609, step time: 1.5543\n",
      "144/388, train_loss: 0.1101, step time: 1.5546\n",
      "145/388, train_loss: 0.2415, step time: 1.5542\n",
      "146/388, train_loss: 0.1184, step time: 1.5510\n",
      "147/388, train_loss: 0.1049, step time: 1.5557\n",
      "148/388, train_loss: 0.2544, step time: 1.5554\n",
      "149/388, train_loss: 0.2633, step time: 1.5514\n",
      "150/388, train_loss: 0.1529, step time: 1.5569\n",
      "151/388, train_loss: 0.0575, step time: 1.5694\n",
      "152/388, train_loss: 0.2352, step time: 1.5532\n",
      "153/388, train_loss: 0.4408, step time: 1.5547\n",
      "154/388, train_loss: 0.1419, step time: 1.5544\n",
      "155/388, train_loss: 0.0803, step time: 1.5543\n",
      "156/388, train_loss: 0.2879, step time: 1.5514\n",
      "157/388, train_loss: 0.2115, step time: 1.5514\n",
      "158/388, train_loss: 0.0824, step time: 1.5553\n",
      "159/388, train_loss: 0.0950, step time: 1.5543\n",
      "160/388, train_loss: 0.2149, step time: 1.5543\n",
      "161/388, train_loss: 0.0904, step time: 1.5576\n",
      "162/388, train_loss: 0.1395, step time: 1.5536\n",
      "163/388, train_loss: 0.1915, step time: 1.5539\n",
      "164/388, train_loss: 0.2313, step time: 1.5551\n",
      "165/388, train_loss: 0.5205, step time: 1.5539\n",
      "166/388, train_loss: 0.1239, step time: 1.5596\n",
      "167/388, train_loss: 0.0904, step time: 1.5537\n",
      "168/388, train_loss: 0.1293, step time: 1.5526\n",
      "169/388, train_loss: 0.1725, step time: 1.5541\n",
      "170/388, train_loss: 0.0710, step time: 1.5546\n",
      "171/388, train_loss: 0.1461, step time: 1.5555\n",
      "172/388, train_loss: 0.1586, step time: 1.5555\n",
      "173/388, train_loss: 0.0715, step time: 1.5517\n",
      "174/388, train_loss: 0.0951, step time: 1.5538\n",
      "175/388, train_loss: 0.0985, step time: 1.5564\n",
      "176/388, train_loss: 0.1287, step time: 1.5516\n",
      "177/388, train_loss: 0.1213, step time: 1.5561\n",
      "178/388, train_loss: 0.1494, step time: 1.5573\n",
      "179/388, train_loss: 0.2258, step time: 1.5578\n",
      "180/388, train_loss: 0.1851, step time: 1.5593\n",
      "181/388, train_loss: 0.0874, step time: 1.5584\n",
      "182/388, train_loss: 0.2075, step time: 1.5513\n",
      "183/388, train_loss: 0.0642, step time: 1.5538\n",
      "184/388, train_loss: 0.2353, step time: 1.5550\n",
      "185/388, train_loss: 0.1166, step time: 1.5532\n",
      "186/388, train_loss: 0.1622, step time: 1.5573\n",
      "187/388, train_loss: 0.3438, step time: 1.5564\n",
      "188/388, train_loss: 0.0621, step time: 1.5545\n",
      "189/388, train_loss: 0.0276, step time: 1.5550\n",
      "190/388, train_loss: 0.0736, step time: 1.5547\n",
      "191/388, train_loss: 0.2058, step time: 1.5497\n",
      "192/388, train_loss: 0.1759, step time: 1.5560\n",
      "193/388, train_loss: 0.1319, step time: 1.5556\n",
      "194/388, train_loss: 0.0778, step time: 1.5534\n",
      "195/388, train_loss: 0.1529, step time: 1.5504\n",
      "196/388, train_loss: 0.0877, step time: 1.5509\n",
      "197/388, train_loss: 0.0892, step time: 1.5627\n",
      "198/388, train_loss: 0.1638, step time: 1.5530\n",
      "199/388, train_loss: 0.1264, step time: 1.5544\n",
      "200/388, train_loss: 0.2372, step time: 1.5540\n",
      "201/388, train_loss: 0.0533, step time: 1.5509\n",
      "202/388, train_loss: 0.1557, step time: 1.5538\n",
      "203/388, train_loss: 0.1593, step time: 1.5553\n",
      "204/388, train_loss: 0.1551, step time: 1.5565\n",
      "205/388, train_loss: 0.1624, step time: 1.5562\n",
      "206/388, train_loss: 0.1518, step time: 1.5550\n",
      "207/388, train_loss: 0.1273, step time: 1.5620\n",
      "208/388, train_loss: 0.2013, step time: 1.5547\n",
      "209/388, train_loss: 0.1246, step time: 1.5532\n",
      "210/388, train_loss: 0.1627, step time: 1.5571\n",
      "211/388, train_loss: 0.2555, step time: 1.5548\n",
      "212/388, train_loss: 0.2165, step time: 1.5537\n",
      "213/388, train_loss: 0.0313, step time: 1.5558\n",
      "214/388, train_loss: 0.1016, step time: 1.5533\n",
      "215/388, train_loss: 0.3451, step time: 1.5558\n",
      "216/388, train_loss: 0.0926, step time: 1.5552\n",
      "217/388, train_loss: 0.1104, step time: 1.5540\n",
      "218/388, train_loss: 0.1967, step time: 1.5550\n",
      "219/388, train_loss: 0.1063, step time: 1.5525\n",
      "220/388, train_loss: 0.2878, step time: 1.5526\n",
      "221/388, train_loss: 0.2084, step time: 1.5574\n",
      "222/388, train_loss: 0.2339, step time: 1.5549\n",
      "223/388, train_loss: 0.0886, step time: 1.5560\n",
      "224/388, train_loss: 0.0758, step time: 1.5541\n",
      "225/388, train_loss: 0.1732, step time: 1.5560\n",
      "226/388, train_loss: 0.1638, step time: 1.5582\n",
      "227/388, train_loss: 0.2276, step time: 1.5533\n",
      "228/388, train_loss: 0.2922, step time: 1.5624\n",
      "229/388, train_loss: 0.2754, step time: 1.5556\n",
      "230/388, train_loss: 0.3829, step time: 1.5511\n",
      "231/388, train_loss: 0.0528, step time: 1.5551\n",
      "232/388, train_loss: 0.1248, step time: 1.5578\n",
      "233/388, train_loss: 0.0369, step time: 1.5537\n",
      "234/388, train_loss: 0.1110, step time: 1.5565\n",
      "235/388, train_loss: 0.1828, step time: 1.5555\n",
      "236/388, train_loss: 0.0720, step time: 1.5552\n",
      "237/388, train_loss: 0.1178, step time: 1.5552\n",
      "238/388, train_loss: 0.0787, step time: 1.5506\n",
      "239/388, train_loss: 0.1793, step time: 1.5553\n",
      "240/388, train_loss: 0.1117, step time: 1.5528\n",
      "241/388, train_loss: 0.3529, step time: 1.5493\n",
      "242/388, train_loss: 0.1710, step time: 1.5557\n",
      "243/388, train_loss: 0.0733, step time: 1.5557\n",
      "244/388, train_loss: 0.1647, step time: 1.5554\n",
      "245/388, train_loss: 0.2049, step time: 1.5532\n",
      "246/388, train_loss: 0.0900, step time: 1.5562\n",
      "247/388, train_loss: 0.0967, step time: 1.5546\n",
      "248/388, train_loss: 0.0608, step time: 1.5524\n",
      "249/388, train_loss: 0.1575, step time: 1.5524\n",
      "250/388, train_loss: 0.0986, step time: 1.5570\n",
      "251/388, train_loss: 0.1753, step time: 1.5490\n",
      "252/388, train_loss: 0.1285, step time: 1.5560\n",
      "253/388, train_loss: 0.1068, step time: 1.5547\n",
      "254/388, train_loss: 0.1432, step time: 1.5584\n",
      "255/388, train_loss: 0.0892, step time: 1.5559\n",
      "256/388, train_loss: 0.1572, step time: 1.5549\n",
      "257/388, train_loss: 0.0983, step time: 1.5574\n",
      "258/388, train_loss: 0.2611, step time: 1.5578\n",
      "259/388, train_loss: 0.2577, step time: 1.5536\n",
      "260/388, train_loss: 0.0691, step time: 1.5564\n",
      "261/388, train_loss: 0.1704, step time: 1.5580\n",
      "262/388, train_loss: 0.1426, step time: 1.5516\n",
      "263/388, train_loss: 0.2031, step time: 1.5595\n",
      "264/388, train_loss: 0.1219, step time: 1.5556\n",
      "265/388, train_loss: 0.1394, step time: 1.5494\n",
      "266/388, train_loss: 0.2807, step time: 1.5568\n",
      "267/388, train_loss: 0.2365, step time: 1.5534\n",
      "268/388, train_loss: 0.2187, step time: 1.5530\n",
      "269/388, train_loss: 0.1002, step time: 1.5569\n",
      "270/388, train_loss: 0.2917, step time: 1.5580\n",
      "271/388, train_loss: 0.1289, step time: 1.5495\n",
      "272/388, train_loss: 0.1502, step time: 1.5553\n",
      "273/388, train_loss: 0.1748, step time: 1.5564\n",
      "274/388, train_loss: 0.1505, step time: 1.5521\n",
      "275/388, train_loss: 0.1449, step time: 1.5575\n",
      "276/388, train_loss: 0.0777, step time: 1.5549\n",
      "277/388, train_loss: 0.1971, step time: 1.5570\n",
      "278/388, train_loss: 0.0419, step time: 1.5556\n",
      "279/388, train_loss: 0.1063, step time: 1.5538\n",
      "280/388, train_loss: 0.1537, step time: 1.5557\n",
      "281/388, train_loss: 0.1051, step time: 1.5573\n",
      "282/388, train_loss: 0.2853, step time: 1.5586\n",
      "283/388, train_loss: 0.0430, step time: 1.5550\n",
      "284/388, train_loss: 0.1324, step time: 1.5519\n",
      "285/388, train_loss: 0.1261, step time: 1.5564\n",
      "286/388, train_loss: 0.0831, step time: 1.5557\n",
      "287/388, train_loss: 0.1620, step time: 1.5509\n",
      "288/388, train_loss: 0.1858, step time: 1.5536\n",
      "289/388, train_loss: 0.1060, step time: 1.5515\n",
      "290/388, train_loss: 0.1631, step time: 1.5573\n",
      "291/388, train_loss: 0.2161, step time: 1.5550\n",
      "292/388, train_loss: 0.2418, step time: 1.5569\n",
      "293/388, train_loss: 0.2022, step time: 1.5587\n",
      "294/388, train_loss: 0.0833, step time: 1.5533\n",
      "295/388, train_loss: 0.0992, step time: 1.5530\n",
      "296/388, train_loss: 0.0969, step time: 1.5503\n",
      "297/388, train_loss: 0.0422, step time: 1.5560\n",
      "298/388, train_loss: 0.1187, step time: 1.5527\n",
      "299/388, train_loss: 0.1396, step time: 1.5479\n",
      "300/388, train_loss: 0.4235, step time: 1.5577\n",
      "301/388, train_loss: 0.0604, step time: 1.5532\n",
      "302/388, train_loss: 0.0667, step time: 1.5515\n",
      "303/388, train_loss: 0.0776, step time: 1.5546\n",
      "304/388, train_loss: 0.1867, step time: 1.5522\n",
      "305/388, train_loss: 0.0632, step time: 1.5581\n",
      "306/388, train_loss: 0.1129, step time: 1.5572\n",
      "307/388, train_loss: 0.2372, step time: 1.5532\n",
      "308/388, train_loss: 0.0914, step time: 1.5599\n",
      "309/388, train_loss: 0.2448, step time: 1.5615\n",
      "310/388, train_loss: 0.1585, step time: 1.5599\n",
      "311/388, train_loss: 0.2083, step time: 1.5598\n",
      "312/388, train_loss: 0.1542, step time: 1.5545\n",
      "313/388, train_loss: 0.1289, step time: 1.5578\n",
      "314/388, train_loss: 0.0924, step time: 1.5533\n",
      "315/388, train_loss: 0.0835, step time: 1.5555\n",
      "316/388, train_loss: 0.3290, step time: 1.5665\n",
      "317/388, train_loss: 0.0543, step time: 1.5534\n",
      "318/388, train_loss: 0.1003, step time: 1.5574\n",
      "319/388, train_loss: 0.0972, step time: 1.5526\n",
      "320/388, train_loss: 0.1539, step time: 1.5615\n",
      "321/388, train_loss: 0.0751, step time: 1.5524\n",
      "322/388, train_loss: 0.2322, step time: 1.5551\n",
      "323/388, train_loss: 0.1109, step time: 1.5564\n",
      "324/388, train_loss: 0.1767, step time: 1.5598\n",
      "325/388, train_loss: 0.2173, step time: 1.5742\n",
      "326/388, train_loss: 0.2169, step time: 1.5593\n",
      "327/388, train_loss: 0.1584, step time: 1.5563\n",
      "328/388, train_loss: 0.0894, step time: 1.5633\n",
      "329/388, train_loss: 0.0742, step time: 1.5657\n",
      "330/388, train_loss: 0.1117, step time: 1.5604\n",
      "331/388, train_loss: 0.0860, step time: 1.5603\n",
      "332/388, train_loss: 0.0853, step time: 1.5586\n",
      "333/388, train_loss: 0.1753, step time: 1.5555\n",
      "334/388, train_loss: 0.2031, step time: 1.5526\n",
      "335/388, train_loss: 0.1801, step time: 1.5541\n",
      "336/388, train_loss: 0.2060, step time: 1.5570\n",
      "337/388, train_loss: 0.1984, step time: 1.5578\n",
      "338/388, train_loss: 0.1055, step time: 1.5594\n",
      "339/388, train_loss: 0.1062, step time: 1.5560\n",
      "340/388, train_loss: 0.1661, step time: 1.5640\n",
      "341/388, train_loss: 0.2252, step time: 1.5520\n",
      "342/388, train_loss: 0.2700, step time: 1.5600\n",
      "343/388, train_loss: 0.0466, step time: 1.5586\n",
      "344/388, train_loss: 0.2033, step time: 1.5602\n",
      "345/388, train_loss: 0.1586, step time: 1.5531\n",
      "346/388, train_loss: 0.1467, step time: 1.5535\n",
      "347/388, train_loss: 0.2878, step time: 1.5607\n",
      "348/388, train_loss: 0.0431, step time: 1.5582\n",
      "349/388, train_loss: 0.1898, step time: 1.5557\n",
      "350/388, train_loss: 0.1524, step time: 1.5548\n",
      "351/388, train_loss: 0.1132, step time: 1.5485\n",
      "352/388, train_loss: 0.0785, step time: 1.5350\n",
      "353/388, train_loss: 0.1971, step time: 1.5579\n",
      "354/388, train_loss: 0.1999, step time: 1.5568\n",
      "355/388, train_loss: 0.0806, step time: 1.5554\n",
      "356/388, train_loss: 0.3971, step time: 1.5298\n",
      "357/388, train_loss: 0.4692, step time: 1.5485\n",
      "358/388, train_loss: 0.0887, step time: 1.5528\n",
      "359/388, train_loss: 0.0633, step time: 1.5539\n",
      "360/388, train_loss: 0.2532, step time: 1.5353\n",
      "361/388, train_loss: 0.0800, step time: 1.5553\n",
      "362/388, train_loss: 0.2336, step time: 1.5609\n",
      "363/388, train_loss: 0.0985, step time: 1.5520\n",
      "364/388, train_loss: 0.5194, step time: 1.5605\n",
      "365/388, train_loss: 0.1538, step time: 1.5536\n",
      "366/388, train_loss: 0.0867, step time: 1.5571\n",
      "367/388, train_loss: 0.3600, step time: 1.5537\n",
      "368/388, train_loss: 0.1184, step time: 1.5579\n",
      "369/388, train_loss: 0.1818, step time: 1.5536\n",
      "370/388, train_loss: 0.1228, step time: 1.5581\n",
      "371/388, train_loss: 0.0804, step time: 1.5515\n",
      "372/388, train_loss: 0.3439, step time: 1.5579\n",
      "373/388, train_loss: 0.0767, step time: 1.5522\n",
      "374/388, train_loss: 0.1751, step time: 1.5536\n",
      "375/388, train_loss: 0.1880, step time: 1.5527\n",
      "376/388, train_loss: 0.2856, step time: 1.5578\n",
      "377/388, train_loss: 0.3155, step time: 1.5547\n",
      "378/388, train_loss: 0.1329, step time: 1.5651\n",
      "379/388, train_loss: 0.0904, step time: 1.5549\n",
      "380/388, train_loss: 0.2915, step time: 1.5412\n",
      "381/388, train_loss: 0.0820, step time: 1.5441\n",
      "382/388, train_loss: 0.2946, step time: 1.5589\n",
      "383/388, train_loss: 0.1195, step time: 1.5544\n",
      "384/388, train_loss: 0.0603, step time: 1.5377\n",
      "385/388, train_loss: 0.0568, step time: 1.5549\n",
      "386/388, train_loss: 0.2187, step time: 1.5554\n",
      "387/388, train_loss: 0.1462, step time: 1.5524\n",
      "388/388, train_loss: 0.1548, step time: 1.5616\n",
      "epoch 95 average loss: 0.1581\n",
      "current epoch: 95 current mean dice: 0.7803 tc: 0.8287 wt: 0.9059 et: 0.6065\n",
      "best mean dice: 0.7817 at epoch: 84\n",
      "time consuming of epoch 95 is: 713.4044\n",
      "----------\n",
      "epoch 96/100\n",
      "1/388, train_loss: 0.3040, step time: 1.5676\n",
      "2/388, train_loss: 0.1256, step time: 1.5535\n",
      "3/388, train_loss: 0.2024, step time: 1.5556\n",
      "4/388, train_loss: 0.0765, step time: 1.5552\n",
      "5/388, train_loss: 0.1987, step time: 1.5530\n",
      "6/388, train_loss: 0.1660, step time: 1.5537\n",
      "7/388, train_loss: 0.2546, step time: 1.5511\n",
      "8/388, train_loss: 0.1874, step time: 1.5531\n",
      "9/388, train_loss: 0.4100, step time: 1.5561\n",
      "10/388, train_loss: 0.1235, step time: 1.5552\n",
      "11/388, train_loss: 0.1900, step time: 1.5534\n",
      "12/388, train_loss: 0.1873, step time: 1.5529\n",
      "13/388, train_loss: 0.1027, step time: 1.5509\n",
      "14/388, train_loss: 0.0876, step time: 1.5660\n",
      "15/388, train_loss: 0.0997, step time: 1.5540\n",
      "16/388, train_loss: 0.2394, step time: 1.5578\n",
      "17/388, train_loss: 0.1930, step time: 1.5561\n",
      "18/388, train_loss: 0.1085, step time: 1.5531\n",
      "19/388, train_loss: 0.2628, step time: 1.5534\n",
      "20/388, train_loss: 0.1020, step time: 1.5565\n",
      "21/388, train_loss: 0.2263, step time: 1.5499\n",
      "22/388, train_loss: 0.0310, step time: 1.5513\n",
      "23/388, train_loss: 0.0981, step time: 1.5540\n",
      "24/388, train_loss: 0.0501, step time: 1.5533\n",
      "25/388, train_loss: 0.2653, step time: 1.5511\n",
      "26/388, train_loss: 0.2378, step time: 1.5564\n",
      "27/388, train_loss: 0.0864, step time: 1.5513\n",
      "28/388, train_loss: 0.0837, step time: 1.5515\n",
      "29/388, train_loss: 0.4260, step time: 1.5568\n",
      "30/388, train_loss: 0.1421, step time: 1.5540\n",
      "31/388, train_loss: 0.0859, step time: 1.5546\n",
      "32/388, train_loss: 0.1269, step time: 1.5563\n",
      "33/388, train_loss: 0.3223, step time: 1.5563\n",
      "34/388, train_loss: 0.1819, step time: 1.5538\n",
      "35/388, train_loss: 0.0906, step time: 1.5510\n",
      "36/388, train_loss: 0.0850, step time: 1.5567\n",
      "37/388, train_loss: 0.1001, step time: 1.5522\n",
      "38/388, train_loss: 0.1879, step time: 1.5542\n",
      "39/388, train_loss: 0.0878, step time: 1.5573\n",
      "40/388, train_loss: 0.0863, step time: 1.5548\n",
      "41/388, train_loss: 0.1577, step time: 1.5532\n",
      "42/388, train_loss: 0.0673, step time: 1.5542\n",
      "43/388, train_loss: 0.2637, step time: 1.5574\n",
      "44/388, train_loss: 0.1898, step time: 1.5533\n",
      "45/388, train_loss: 0.1069, step time: 1.5567\n",
      "46/388, train_loss: 0.2027, step time: 1.5569\n",
      "47/388, train_loss: 0.1681, step time: 1.5496\n",
      "48/388, train_loss: 0.1429, step time: 1.5581\n",
      "49/388, train_loss: 0.1905, step time: 1.5539\n",
      "50/388, train_loss: 0.1385, step time: 1.5521\n",
      "51/388, train_loss: 0.2058, step time: 1.5549\n",
      "52/388, train_loss: 0.3574, step time: 1.5523\n",
      "53/388, train_loss: 0.1227, step time: 1.5493\n",
      "54/388, train_loss: 0.1371, step time: 1.5585\n",
      "55/388, train_loss: 0.1843, step time: 1.5542\n",
      "56/388, train_loss: 0.2040, step time: 1.5514\n",
      "57/388, train_loss: 0.1379, step time: 1.5538\n",
      "58/388, train_loss: 0.2440, step time: 1.5561\n",
      "59/388, train_loss: 0.0736, step time: 1.5536\n",
      "60/388, train_loss: 0.1779, step time: 1.5530\n",
      "61/388, train_loss: 0.1722, step time: 1.5541\n",
      "62/388, train_loss: 0.1507, step time: 1.5542\n",
      "63/388, train_loss: 0.0939, step time: 1.5547\n",
      "64/388, train_loss: 0.0803, step time: 1.5538\n",
      "65/388, train_loss: 0.1462, step time: 1.5515\n",
      "66/388, train_loss: 0.1916, step time: 1.5529\n",
      "67/388, train_loss: 0.2048, step time: 1.5551\n",
      "68/388, train_loss: 0.1895, step time: 1.5538\n",
      "69/388, train_loss: 0.0623, step time: 1.5545\n",
      "70/388, train_loss: 0.5575, step time: 1.5528\n",
      "71/388, train_loss: 0.0877, step time: 1.5529\n",
      "72/388, train_loss: 0.0924, step time: 1.5586\n",
      "73/388, train_loss: 0.3778, step time: 1.5568\n",
      "74/388, train_loss: 0.1543, step time: 1.5540\n",
      "75/388, train_loss: 0.1650, step time: 1.5573\n",
      "76/388, train_loss: 0.1779, step time: 1.5592\n",
      "77/388, train_loss: 0.1992, step time: 1.5490\n",
      "78/388, train_loss: 0.1380, step time: 1.5540\n",
      "79/388, train_loss: 0.1029, step time: 1.5533\n",
      "80/388, train_loss: 0.1191, step time: 1.5546\n",
      "81/388, train_loss: 0.2524, step time: 1.5538\n",
      "82/388, train_loss: 0.1176, step time: 1.5532\n",
      "83/388, train_loss: 0.1699, step time: 1.5490\n",
      "84/388, train_loss: 0.1958, step time: 1.5541\n",
      "85/388, train_loss: 0.1043, step time: 1.5541\n",
      "86/388, train_loss: 0.1190, step time: 1.5543\n",
      "87/388, train_loss: 0.0727, step time: 1.5579\n",
      "88/388, train_loss: 0.1044, step time: 1.5563\n",
      "89/388, train_loss: 0.0812, step time: 1.5525\n",
      "90/388, train_loss: 0.1973, step time: 1.5617\n",
      "91/388, train_loss: 0.2650, step time: 1.5551\n",
      "92/388, train_loss: 0.1121, step time: 1.5553\n",
      "93/388, train_loss: 0.1637, step time: 1.5517\n",
      "94/388, train_loss: 0.1285, step time: 1.5541\n",
      "95/388, train_loss: 0.0401, step time: 1.5540\n",
      "96/388, train_loss: 0.1365, step time: 1.5531\n",
      "97/388, train_loss: 0.2661, step time: 1.5500\n",
      "98/388, train_loss: 0.1095, step time: 1.5529\n",
      "99/388, train_loss: 0.1884, step time: 1.5547\n",
      "100/388, train_loss: 0.1582, step time: 1.5517\n",
      "101/388, train_loss: 0.2271, step time: 1.5561\n",
      "102/388, train_loss: 0.1548, step time: 1.5526\n",
      "103/388, train_loss: 0.1051, step time: 1.5534\n",
      "104/388, train_loss: 0.0447, step time: 1.5572\n",
      "105/388, train_loss: 0.2054, step time: 1.5572\n",
      "106/388, train_loss: 0.1845, step time: 1.5552\n",
      "107/388, train_loss: 0.1069, step time: 1.5562\n",
      "108/388, train_loss: 0.1957, step time: 1.5575\n",
      "109/388, train_loss: 0.0972, step time: 1.5511\n",
      "110/388, train_loss: 0.0484, step time: 1.5472\n",
      "111/388, train_loss: 0.1978, step time: 1.5454\n",
      "112/388, train_loss: 0.2902, step time: 1.5553\n",
      "113/388, train_loss: 0.0979, step time: 1.5567\n",
      "114/388, train_loss: 0.1513, step time: 1.5553\n",
      "115/388, train_loss: 0.1798, step time: 1.5548\n",
      "116/388, train_loss: 0.2464, step time: 1.5524\n",
      "117/388, train_loss: 0.0958, step time: 1.5577\n",
      "118/388, train_loss: 0.1006, step time: 1.5578\n",
      "119/388, train_loss: 0.1007, step time: 1.5544\n",
      "120/388, train_loss: 0.1040, step time: 1.5543\n",
      "121/388, train_loss: 0.2173, step time: 1.5524\n",
      "122/388, train_loss: 0.1055, step time: 1.5549\n",
      "123/388, train_loss: 0.1795, step time: 1.5606\n",
      "124/388, train_loss: 0.2335, step time: 1.5568\n",
      "125/388, train_loss: 0.1147, step time: 1.5527\n",
      "126/388, train_loss: 0.0874, step time: 1.5571\n",
      "127/388, train_loss: 0.1333, step time: 1.5531\n",
      "128/388, train_loss: 0.2084, step time: 1.5511\n",
      "129/388, train_loss: 0.2108, step time: 1.5565\n",
      "130/388, train_loss: 0.3516, step time: 1.5555\n",
      "131/388, train_loss: 0.2051, step time: 1.5548\n",
      "132/388, train_loss: 0.2881, step time: 1.5541\n",
      "133/388, train_loss: 0.0954, step time: 1.5556\n",
      "134/388, train_loss: 0.0765, step time: 1.5542\n",
      "135/388, train_loss: 0.0539, step time: 1.5564\n",
      "136/388, train_loss: 0.1155, step time: 1.5540\n",
      "137/388, train_loss: 0.2233, step time: 1.5542\n",
      "138/388, train_loss: 0.2073, step time: 1.5549\n",
      "139/388, train_loss: 0.0692, step time: 1.5526\n",
      "140/388, train_loss: 0.1935, step time: 1.5583\n",
      "141/388, train_loss: 0.1036, step time: 1.5552\n",
      "142/388, train_loss: 0.3551, step time: 1.5527\n",
      "143/388, train_loss: 0.1557, step time: 1.5548\n",
      "144/388, train_loss: 0.1129, step time: 1.5522\n",
      "145/388, train_loss: 0.2778, step time: 1.5512\n",
      "146/388, train_loss: 0.0856, step time: 1.5530\n",
      "147/388, train_loss: 0.1148, step time: 1.5565\n",
      "148/388, train_loss: 0.2084, step time: 1.5523\n",
      "149/388, train_loss: 0.2301, step time: 1.5521\n",
      "150/388, train_loss: 0.0299, step time: 1.5543\n",
      "151/388, train_loss: 0.1169, step time: 1.5543\n",
      "152/388, train_loss: 0.2685, step time: 1.5512\n",
      "153/388, train_loss: 0.1013, step time: 1.5681\n",
      "154/388, train_loss: 0.0969, step time: 1.5520\n",
      "155/388, train_loss: 0.1315, step time: 1.5559\n",
      "156/388, train_loss: 0.1185, step time: 1.5586\n",
      "157/388, train_loss: 0.0442, step time: 1.5550\n",
      "158/388, train_loss: 0.1509, step time: 1.5621\n",
      "159/388, train_loss: 0.1940, step time: 1.5543\n",
      "160/388, train_loss: 0.1286, step time: 1.5514\n",
      "161/388, train_loss: 0.0522, step time: 1.5520\n",
      "162/388, train_loss: 0.0668, step time: 1.5609\n",
      "163/388, train_loss: 0.0833, step time: 1.5489\n",
      "164/388, train_loss: 0.1753, step time: 1.5570\n",
      "165/388, train_loss: 0.2340, step time: 1.5594\n",
      "166/388, train_loss: 0.1444, step time: 1.5544\n",
      "167/388, train_loss: 0.0743, step time: 1.5553\n",
      "168/388, train_loss: 0.2142, step time: 1.5513\n",
      "169/388, train_loss: 0.2299, step time: 1.5538\n",
      "170/388, train_loss: 0.0962, step time: 1.5536\n",
      "171/388, train_loss: 0.1953, step time: 1.5542\n",
      "172/388, train_loss: 0.3381, step time: 1.5547\n",
      "173/388, train_loss: 0.0618, step time: 1.5515\n",
      "174/388, train_loss: 0.1711, step time: 1.5557\n",
      "175/388, train_loss: 0.2476, step time: 1.5543\n",
      "176/388, train_loss: 0.1126, step time: 1.5507\n",
      "177/388, train_loss: 0.0858, step time: 1.5549\n",
      "178/388, train_loss: 0.1285, step time: 1.5547\n",
      "179/388, train_loss: 0.2420, step time: 1.5529\n",
      "180/388, train_loss: 0.1177, step time: 1.5540\n",
      "181/388, train_loss: 0.0861, step time: 1.5538\n",
      "182/388, train_loss: 0.2158, step time: 1.5538\n",
      "183/388, train_loss: 0.1706, step time: 1.5590\n",
      "184/388, train_loss: 0.1258, step time: 1.5575\n",
      "185/388, train_loss: 0.0790, step time: 1.5528\n",
      "186/388, train_loss: 0.1121, step time: 1.5547\n",
      "187/388, train_loss: 0.0838, step time: 1.5544\n",
      "188/388, train_loss: 0.0928, step time: 1.5547\n",
      "189/388, train_loss: 0.0576, step time: 1.5553\n",
      "190/388, train_loss: 0.2966, step time: 1.5579\n",
      "191/388, train_loss: 0.0852, step time: 1.5545\n",
      "192/388, train_loss: 0.2805, step time: 1.5546\n",
      "193/388, train_loss: 0.0989, step time: 1.5555\n",
      "194/388, train_loss: 0.3289, step time: 1.5526\n",
      "195/388, train_loss: 0.1024, step time: 1.5605\n",
      "196/388, train_loss: 0.3029, step time: 1.5540\n",
      "197/388, train_loss: 0.1205, step time: 1.5555\n",
      "198/388, train_loss: 0.1611, step time: 1.5554\n",
      "199/388, train_loss: 0.1629, step time: 1.5547\n",
      "200/388, train_loss: 0.1471, step time: 1.5556\n",
      "201/388, train_loss: 0.1495, step time: 1.5594\n",
      "202/388, train_loss: 0.1550, step time: 1.5518\n",
      "203/388, train_loss: 0.1905, step time: 1.5540\n",
      "204/388, train_loss: 0.0595, step time: 1.5558\n",
      "205/388, train_loss: 0.1526, step time: 1.5531\n",
      "206/388, train_loss: 0.1561, step time: 1.5569\n",
      "207/388, train_loss: 0.1287, step time: 1.5541\n",
      "208/388, train_loss: 0.0949, step time: 1.5538\n",
      "209/388, train_loss: 0.0821, step time: 1.5617\n",
      "210/388, train_loss: 0.1183, step time: 1.5534\n",
      "211/388, train_loss: 0.2626, step time: 1.5577\n",
      "212/388, train_loss: 0.2032, step time: 1.5517\n",
      "213/388, train_loss: 0.1072, step time: 1.5506\n",
      "214/388, train_loss: 0.1123, step time: 1.5568\n",
      "215/388, train_loss: 0.0711, step time: 1.5565\n",
      "216/388, train_loss: 0.0988, step time: 1.5538\n",
      "217/388, train_loss: 0.1169, step time: 1.5552\n",
      "218/388, train_loss: 0.1430, step time: 1.5558\n",
      "219/388, train_loss: 0.1766, step time: 1.5563\n",
      "220/388, train_loss: 0.1465, step time: 1.5550\n",
      "221/388, train_loss: 0.1303, step time: 1.5520\n",
      "222/388, train_loss: 0.0750, step time: 1.5544\n",
      "223/388, train_loss: 0.1835, step time: 1.5541\n",
      "224/388, train_loss: 0.2212, step time: 1.5538\n",
      "225/388, train_loss: 0.1227, step time: 1.5524\n",
      "226/388, train_loss: 0.0808, step time: 1.5564\n",
      "227/388, train_loss: 0.0733, step time: 1.5513\n",
      "228/388, train_loss: 0.1319, step time: 1.5557\n",
      "229/388, train_loss: 0.1405, step time: 1.5554\n",
      "230/388, train_loss: 0.0869, step time: 1.5540\n",
      "231/388, train_loss: 0.1315, step time: 1.5566\n",
      "232/388, train_loss: 0.2046, step time: 1.5533\n",
      "233/388, train_loss: 0.4261, step time: 1.5524\n",
      "234/388, train_loss: 0.1987, step time: 1.5545\n",
      "235/388, train_loss: 0.1057, step time: 1.5557\n",
      "236/388, train_loss: 0.0983, step time: 1.5514\n",
      "237/388, train_loss: 0.4761, step time: 1.5572\n",
      "238/388, train_loss: 0.0586, step time: 1.5550\n",
      "239/388, train_loss: 0.2703, step time: 1.5531\n",
      "240/388, train_loss: 0.0435, step time: 1.5554\n",
      "241/388, train_loss: 0.3912, step time: 1.5534\n",
      "242/388, train_loss: 0.0949, step time: 1.5556\n",
      "243/388, train_loss: 0.3174, step time: 1.5511\n",
      "244/388, train_loss: 0.1351, step time: 1.5543\n",
      "245/388, train_loss: 0.2635, step time: 1.5513\n",
      "246/388, train_loss: 0.1489, step time: 1.5591\n",
      "247/388, train_loss: 0.2942, step time: 1.5540\n",
      "248/388, train_loss: 0.1244, step time: 1.5516\n",
      "249/388, train_loss: 0.0260, step time: 1.5610\n",
      "250/388, train_loss: 0.0882, step time: 1.5557\n",
      "251/388, train_loss: 0.1379, step time: 1.5499\n",
      "252/388, train_loss: 0.0846, step time: 1.5520\n",
      "253/388, train_loss: 0.2576, step time: 1.5547\n",
      "254/388, train_loss: 0.1870, step time: 1.5535\n",
      "255/388, train_loss: 0.1518, step time: 1.5540\n",
      "256/388, train_loss: 0.1891, step time: 1.5526\n",
      "257/388, train_loss: 0.2144, step time: 1.5509\n",
      "258/388, train_loss: 0.1480, step time: 1.5554\n",
      "259/388, train_loss: 0.1549, step time: 1.5553\n",
      "260/388, train_loss: 0.1792, step time: 1.5518\n",
      "261/388, train_loss: 0.2155, step time: 1.5561\n",
      "262/388, train_loss: 0.0445, step time: 1.5530\n",
      "263/388, train_loss: 0.2677, step time: 1.5529\n",
      "264/388, train_loss: 0.1605, step time: 1.5550\n",
      "265/388, train_loss: 0.1401, step time: 1.5539\n",
      "266/388, train_loss: 0.1018, step time: 1.5520\n",
      "267/388, train_loss: 0.0515, step time: 1.5581\n",
      "268/388, train_loss: 0.0772, step time: 1.5502\n",
      "269/388, train_loss: 0.2716, step time: 1.5546\n",
      "270/388, train_loss: 0.3501, step time: 1.5572\n",
      "271/388, train_loss: 0.1761, step time: 1.5542\n",
      "272/388, train_loss: 0.1600, step time: 1.5536\n",
      "273/388, train_loss: 0.1287, step time: 1.5544\n",
      "274/388, train_loss: 0.2215, step time: 1.5538\n",
      "275/388, train_loss: 0.0801, step time: 1.5547\n",
      "276/388, train_loss: 0.1545, step time: 1.5549\n",
      "277/388, train_loss: 0.1345, step time: 1.5497\n",
      "278/388, train_loss: 0.0876, step time: 1.5545\n",
      "279/388, train_loss: 0.3570, step time: 1.5538\n",
      "280/388, train_loss: 0.1686, step time: 1.5526\n",
      "281/388, train_loss: 0.3563, step time: 1.5558\n",
      "282/388, train_loss: 0.1685, step time: 1.5550\n",
      "283/388, train_loss: 0.0880, step time: 1.5512\n",
      "284/388, train_loss: 0.1562, step time: 1.5576\n",
      "285/388, train_loss: 0.1083, step time: 1.5570\n",
      "286/388, train_loss: 0.2602, step time: 1.5528\n",
      "287/388, train_loss: 0.1834, step time: 1.5554\n",
      "288/388, train_loss: 0.1582, step time: 1.5579\n",
      "289/388, train_loss: 0.1087, step time: 1.5529\n",
      "290/388, train_loss: 0.2159, step time: 1.5548\n",
      "291/388, train_loss: 0.1292, step time: 1.5522\n",
      "292/388, train_loss: 0.2251, step time: 1.5516\n",
      "293/388, train_loss: 0.0550, step time: 1.5516\n",
      "294/388, train_loss: 0.1763, step time: 1.5555\n",
      "295/388, train_loss: 0.1815, step time: 1.5534\n",
      "296/388, train_loss: 0.1074, step time: 1.5524\n",
      "297/388, train_loss: 0.0340, step time: 1.5539\n",
      "298/388, train_loss: 0.0955, step time: 1.5518\n",
      "299/388, train_loss: 0.0908, step time: 1.5556\n",
      "300/388, train_loss: 0.4002, step time: 1.5578\n",
      "301/388, train_loss: 0.0666, step time: 1.5538\n",
      "302/388, train_loss: 0.1090, step time: 1.5547\n",
      "303/388, train_loss: 0.0939, step time: 1.5559\n",
      "304/388, train_loss: 0.0958, step time: 1.5539\n",
      "305/388, train_loss: 0.2627, step time: 1.5539\n",
      "306/388, train_loss: 0.0914, step time: 1.5642\n",
      "307/388, train_loss: 0.2234, step time: 1.5510\n",
      "308/388, train_loss: 0.0643, step time: 1.5541\n",
      "309/388, train_loss: 0.1505, step time: 1.5535\n",
      "310/388, train_loss: 0.1925, step time: 1.5533\n",
      "311/388, train_loss: 0.0519, step time: 1.5532\n",
      "312/388, train_loss: 0.1661, step time: 1.5550\n",
      "313/388, train_loss: 0.1079, step time: 1.5576\n",
      "314/388, train_loss: 0.0823, step time: 1.5497\n",
      "315/388, train_loss: 0.0894, step time: 1.5526\n",
      "316/388, train_loss: 0.0754, step time: 1.5530\n",
      "317/388, train_loss: 0.0893, step time: 1.5518\n",
      "318/388, train_loss: 0.0877, step time: 1.5559\n",
      "319/388, train_loss: 0.1572, step time: 1.5553\n",
      "320/388, train_loss: 0.0705, step time: 1.5523\n",
      "321/388, train_loss: 0.2627, step time: 1.5541\n",
      "322/388, train_loss: 0.2534, step time: 1.5537\n",
      "323/388, train_loss: 0.1746, step time: 1.5536\n",
      "324/388, train_loss: 0.1863, step time: 1.5523\n",
      "325/388, train_loss: 0.2485, step time: 1.5522\n",
      "326/388, train_loss: 0.2141, step time: 1.5564\n",
      "327/388, train_loss: 0.1471, step time: 1.5541\n",
      "328/388, train_loss: 0.2719, step time: 1.5550\n",
      "329/388, train_loss: 0.5256, step time: 1.5517\n",
      "330/388, train_loss: 0.0699, step time: 1.5560\n",
      "331/388, train_loss: 0.1702, step time: 1.5525\n",
      "332/388, train_loss: 0.1402, step time: 1.5535\n",
      "333/388, train_loss: 0.0798, step time: 1.5594\n",
      "334/388, train_loss: 0.0944, step time: 1.5547\n",
      "335/388, train_loss: 0.0732, step time: 1.5539\n",
      "336/388, train_loss: 0.3021, step time: 1.5630\n",
      "337/388, train_loss: 0.1173, step time: 1.5514\n",
      "338/388, train_loss: 0.0969, step time: 1.5541\n",
      "339/388, train_loss: 0.3268, step time: 1.5541\n",
      "340/388, train_loss: 0.0860, step time: 1.5521\n",
      "341/388, train_loss: 0.2280, step time: 1.5543\n",
      "342/388, train_loss: 0.1249, step time: 1.5541\n",
      "343/388, train_loss: 0.1525, step time: 1.5512\n",
      "344/388, train_loss: 0.0766, step time: 1.5550\n",
      "345/388, train_loss: 0.1481, step time: 1.5567\n",
      "346/388, train_loss: 0.0566, step time: 1.5532\n",
      "347/388, train_loss: 0.2204, step time: 1.5563\n",
      "348/388, train_loss: 0.1895, step time: 1.5530\n",
      "349/388, train_loss: 0.0746, step time: 1.5514\n",
      "350/388, train_loss: 0.1518, step time: 1.5551\n",
      "351/388, train_loss: 0.1715, step time: 1.5529\n",
      "352/388, train_loss: 0.0872, step time: 1.5516\n",
      "353/388, train_loss: 0.0707, step time: 1.5591\n",
      "354/388, train_loss: 0.0418, step time: 1.5562\n",
      "355/388, train_loss: 0.1644, step time: 1.5563\n",
      "356/388, train_loss: 0.2552, step time: 1.5562\n",
      "357/388, train_loss: 0.0609, step time: 1.5522\n",
      "358/388, train_loss: 0.1036, step time: 1.5643\n",
      "359/388, train_loss: 0.1875, step time: 1.5549\n",
      "360/388, train_loss: 0.1519, step time: 1.5542\n",
      "361/388, train_loss: 0.2575, step time: 1.5567\n",
      "362/388, train_loss: 0.0635, step time: 1.5494\n",
      "363/388, train_loss: 0.0678, step time: 1.5525\n",
      "364/388, train_loss: 0.0919, step time: 1.5532\n",
      "365/388, train_loss: 0.1903, step time: 1.5512\n",
      "366/388, train_loss: 0.2192, step time: 1.5545\n",
      "367/388, train_loss: 0.1382, step time: 1.5533\n",
      "368/388, train_loss: 0.1298, step time: 1.5761\n",
      "369/388, train_loss: 0.0955, step time: 1.5544\n",
      "370/388, train_loss: 0.0759, step time: 1.5545\n",
      "371/388, train_loss: 0.0848, step time: 1.5506\n",
      "372/388, train_loss: 0.1925, step time: 1.5544\n",
      "373/388, train_loss: 0.0670, step time: 1.5543\n",
      "374/388, train_loss: 0.1031, step time: 1.5446\n",
      "375/388, train_loss: 0.1278, step time: 1.5562\n",
      "376/388, train_loss: 0.1017, step time: 1.5505\n",
      "377/388, train_loss: 0.2104, step time: 1.5533\n",
      "378/388, train_loss: 0.1052, step time: 1.5557\n",
      "379/388, train_loss: 0.1443, step time: 1.5520\n",
      "380/388, train_loss: 0.1447, step time: 1.5556\n",
      "381/388, train_loss: 0.0907, step time: 1.5566\n",
      "382/388, train_loss: 0.1434, step time: 1.5505\n",
      "383/388, train_loss: 0.0945, step time: 1.5603\n",
      "384/388, train_loss: 0.1514, step time: 1.5580\n",
      "385/388, train_loss: 0.2275, step time: 1.5538\n",
      "386/388, train_loss: 0.0682, step time: 1.5565\n",
      "387/388, train_loss: 0.0934, step time: 1.5565\n",
      "388/388, train_loss: 0.1463, step time: 1.5503\n",
      "epoch 96 average loss: 0.1567\n",
      "current epoch: 96 current mean dice: 0.7803 tc: 0.8288 wt: 0.9059 et: 0.6062\n",
      "best mean dice: 0.7817 at epoch: 84\n",
      "time consuming of epoch 96 is: 714.9182\n",
      "----------\n",
      "epoch 97/100\n",
      "1/388, train_loss: 0.2114, step time: 1.5748\n",
      "2/388, train_loss: 0.2655, step time: 1.5590\n",
      "3/388, train_loss: 0.0912, step time: 1.5550\n",
      "4/388, train_loss: 0.1237, step time: 1.5519\n",
      "5/388, train_loss: 0.1614, step time: 1.5519\n",
      "6/388, train_loss: 0.1530, step time: 1.5547\n",
      "7/388, train_loss: 0.1353, step time: 1.5568\n",
      "8/388, train_loss: 0.2785, step time: 1.5518\n",
      "9/388, train_loss: 0.1211, step time: 1.5538\n",
      "10/388, train_loss: 0.1542, step time: 1.5569\n",
      "11/388, train_loss: 0.0303, step time: 1.5562\n",
      "12/388, train_loss: 0.1858, step time: 1.5496\n",
      "13/388, train_loss: 0.0631, step time: 1.5533\n",
      "14/388, train_loss: 0.0845, step time: 1.5565\n",
      "15/388, train_loss: 0.0991, step time: 1.5529\n",
      "16/388, train_loss: 0.1584, step time: 1.5526\n",
      "17/388, train_loss: 0.2961, step time: 1.5572\n",
      "18/388, train_loss: 0.1403, step time: 1.5513\n",
      "19/388, train_loss: 0.0571, step time: 1.5559\n",
      "20/388, train_loss: 0.2179, step time: 1.5534\n",
      "21/388, train_loss: 0.1075, step time: 1.5526\n",
      "22/388, train_loss: 0.2460, step time: 1.5545\n",
      "23/388, train_loss: 0.1727, step time: 1.5546\n",
      "24/388, train_loss: 0.0842, step time: 1.5518\n",
      "25/388, train_loss: 0.0979, step time: 1.5533\n",
      "26/388, train_loss: 0.1531, step time: 1.5511\n",
      "27/388, train_loss: 0.0906, step time: 1.5544\n",
      "28/388, train_loss: 0.1155, step time: 1.5545\n",
      "29/388, train_loss: 0.0697, step time: 1.5566\n",
      "30/388, train_loss: 0.0970, step time: 1.5545\n",
      "31/388, train_loss: 0.1448, step time: 1.5540\n",
      "32/388, train_loss: 0.2130, step time: 1.5497\n",
      "33/388, train_loss: 0.2553, step time: 1.5565\n",
      "34/388, train_loss: 0.0800, step time: 1.5578\n",
      "35/388, train_loss: 0.2280, step time: 1.5510\n",
      "36/388, train_loss: 0.1180, step time: 1.5635\n",
      "37/388, train_loss: 0.2336, step time: 1.5514\n",
      "38/388, train_loss: 0.1913, step time: 1.5580\n",
      "39/388, train_loss: 0.1878, step time: 1.5559\n",
      "40/388, train_loss: 0.3795, step time: 1.5547\n",
      "41/388, train_loss: 0.2649, step time: 1.5551\n",
      "42/388, train_loss: 0.3286, step time: 1.5531\n",
      "43/388, train_loss: 0.0969, step time: 1.5514\n",
      "44/388, train_loss: 0.3669, step time: 1.5550\n",
      "45/388, train_loss: 0.0693, step time: 1.5513\n",
      "46/388, train_loss: 0.1497, step time: 1.5521\n",
      "47/388, train_loss: 0.1539, step time: 1.5575\n",
      "48/388, train_loss: 0.0967, step time: 1.5556\n",
      "49/388, train_loss: 0.0970, step time: 1.5530\n",
      "50/388, train_loss: 0.0397, step time: 1.5622\n",
      "51/388, train_loss: 0.1518, step time: 1.5546\n",
      "52/388, train_loss: 0.2430, step time: 1.5530\n",
      "53/388, train_loss: 0.1963, step time: 1.5516\n",
      "54/388, train_loss: 0.1297, step time: 1.5546\n",
      "55/388, train_loss: 0.0740, step time: 1.5601\n",
      "56/388, train_loss: 0.0992, step time: 1.5552\n",
      "57/388, train_loss: 0.0750, step time: 1.5555\n",
      "58/388, train_loss: 0.1580, step time: 1.5582\n",
      "59/388, train_loss: 0.0928, step time: 1.5550\n",
      "60/388, train_loss: 0.0924, step time: 1.5576\n",
      "61/388, train_loss: 0.1179, step time: 1.5542\n",
      "62/388, train_loss: 0.2375, step time: 1.5577\n",
      "63/388, train_loss: 0.1556, step time: 1.5538\n",
      "64/388, train_loss: 0.2287, step time: 1.5538\n",
      "65/388, train_loss: 0.0522, step time: 1.5559\n",
      "66/388, train_loss: 0.2050, step time: 1.5567\n",
      "67/388, train_loss: 0.3028, step time: 1.5526\n",
      "68/388, train_loss: 0.1501, step time: 1.5578\n",
      "69/388, train_loss: 0.2806, step time: 1.5559\n",
      "70/388, train_loss: 0.0874, step time: 1.5550\n",
      "71/388, train_loss: 0.1138, step time: 1.5574\n",
      "72/388, train_loss: 0.1059, step time: 1.5530\n",
      "73/388, train_loss: 0.1587, step time: 1.5538\n",
      "74/388, train_loss: 0.1395, step time: 1.5548\n",
      "75/388, train_loss: 0.1521, step time: 1.5534\n",
      "76/388, train_loss: 0.1024, step time: 1.5536\n",
      "77/388, train_loss: 0.1406, step time: 1.5544\n",
      "78/388, train_loss: 0.3483, step time: 1.5551\n",
      "79/388, train_loss: 0.1190, step time: 1.5532\n",
      "80/388, train_loss: 0.5010, step time: 1.5528\n",
      "81/388, train_loss: 0.0882, step time: 1.5532\n",
      "82/388, train_loss: 0.2041, step time: 1.5541\n",
      "83/388, train_loss: 0.0598, step time: 1.5536\n",
      "84/388, train_loss: 0.1863, step time: 1.5519\n",
      "85/388, train_loss: 0.0710, step time: 1.5542\n",
      "86/388, train_loss: 0.1745, step time: 1.5550\n",
      "87/388, train_loss: 0.5153, step time: 1.5500\n",
      "88/388, train_loss: 0.1432, step time: 1.5542\n",
      "89/388, train_loss: 0.1891, step time: 1.5541\n",
      "90/388, train_loss: 0.0906, step time: 1.5497\n",
      "91/388, train_loss: 0.0522, step time: 1.5520\n",
      "92/388, train_loss: 0.0982, step time: 1.5555\n",
      "93/388, train_loss: 0.2433, step time: 1.5533\n",
      "94/388, train_loss: 0.1317, step time: 1.5561\n",
      "95/388, train_loss: 0.1118, step time: 1.5562\n",
      "96/388, train_loss: 0.2956, step time: 1.5504\n",
      "97/388, train_loss: 0.1102, step time: 1.5571\n",
      "98/388, train_loss: 0.2059, step time: 1.5576\n",
      "99/388, train_loss: 0.1240, step time: 1.5516\n",
      "100/388, train_loss: 0.1995, step time: 1.5556\n",
      "101/388, train_loss: 0.1196, step time: 1.5559\n",
      "102/388, train_loss: 0.2069, step time: 1.5553\n",
      "103/388, train_loss: 0.2991, step time: 1.5555\n",
      "104/388, train_loss: 0.2133, step time: 1.5530\n",
      "105/388, train_loss: 0.1365, step time: 1.5513\n",
      "106/388, train_loss: 0.1183, step time: 1.5584\n",
      "107/388, train_loss: 0.1178, step time: 1.5544\n",
      "108/388, train_loss: 0.0864, step time: 1.5569\n",
      "109/388, train_loss: 0.1584, step time: 1.5555\n",
      "110/388, train_loss: 0.1780, step time: 1.5505\n",
      "111/388, train_loss: 0.1806, step time: 1.5571\n",
      "112/388, train_loss: 0.1591, step time: 1.5522\n",
      "113/388, train_loss: 0.1048, step time: 1.5580\n",
      "114/388, train_loss: 0.0599, step time: 1.5592\n",
      "115/388, train_loss: 0.4138, step time: 1.5512\n",
      "116/388, train_loss: 0.1475, step time: 1.5565\n",
      "117/388, train_loss: 0.1325, step time: 1.5546\n",
      "118/388, train_loss: 0.0469, step time: 1.5525\n",
      "119/388, train_loss: 0.0835, step time: 1.5593\n",
      "120/388, train_loss: 0.2716, step time: 1.5529\n",
      "121/388, train_loss: 0.1437, step time: 1.5556\n",
      "122/388, train_loss: 0.2288, step time: 1.5589\n",
      "123/388, train_loss: 0.0449, step time: 1.5513\n",
      "124/388, train_loss: 0.3056, step time: 1.5552\n",
      "125/388, train_loss: 0.0710, step time: 1.5538\n",
      "126/388, train_loss: 0.2340, step time: 1.5505\n",
      "127/388, train_loss: 0.0730, step time: 1.5593\n",
      "128/388, train_loss: 0.1003, step time: 1.5543\n",
      "129/388, train_loss: 0.1574, step time: 1.5444\n",
      "130/388, train_loss: 0.2630, step time: 1.5566\n",
      "131/388, train_loss: 0.1045, step time: 1.5556\n",
      "132/388, train_loss: 0.0646, step time: 1.5515\n",
      "133/388, train_loss: 0.0853, step time: 1.5551\n",
      "134/388, train_loss: 0.1510, step time: 1.5558\n",
      "135/388, train_loss: 0.1732, step time: 1.5542\n",
      "136/388, train_loss: 0.0997, step time: 1.5572\n",
      "137/388, train_loss: 0.3747, step time: 1.5558\n",
      "138/388, train_loss: 0.1468, step time: 1.5534\n",
      "139/388, train_loss: 0.1959, step time: 1.5533\n",
      "140/388, train_loss: 0.0851, step time: 1.5552\n",
      "141/388, train_loss: 0.1851, step time: 1.5548\n",
      "142/388, train_loss: 0.0640, step time: 1.5628\n",
      "143/388, train_loss: 0.1097, step time: 1.5525\n",
      "144/388, train_loss: 0.0804, step time: 1.5614\n",
      "145/388, train_loss: 0.1767, step time: 1.5549\n",
      "146/388, train_loss: 0.2033, step time: 1.5529\n",
      "147/388, train_loss: 0.0704, step time: 1.5572\n",
      "148/388, train_loss: 0.2047, step time: 1.5549\n",
      "149/388, train_loss: 0.0795, step time: 1.5536\n",
      "150/388, train_loss: 0.2413, step time: 1.5580\n",
      "151/388, train_loss: 0.2104, step time: 1.5549\n",
      "152/388, train_loss: 0.1455, step time: 1.5582\n",
      "153/388, train_loss: 0.1456, step time: 1.5531\n",
      "154/388, train_loss: 0.1329, step time: 1.5544\n",
      "155/388, train_loss: 0.1408, step time: 1.5627\n",
      "156/388, train_loss: 0.0980, step time: 1.5556\n",
      "157/388, train_loss: 0.0923, step time: 1.5554\n",
      "158/388, train_loss: 0.0708, step time: 1.5538\n",
      "159/388, train_loss: 0.1541, step time: 1.5542\n",
      "160/388, train_loss: 0.3358, step time: 1.5561\n",
      "161/388, train_loss: 0.1167, step time: 1.5514\n",
      "162/388, train_loss: 0.2018, step time: 1.5506\n",
      "163/388, train_loss: 0.1489, step time: 1.5518\n",
      "164/388, train_loss: 0.1488, step time: 1.5502\n",
      "165/388, train_loss: 0.1587, step time: 1.5678\n",
      "166/388, train_loss: 0.0554, step time: 1.5484\n",
      "167/388, train_loss: 0.2566, step time: 1.5462\n",
      "168/388, train_loss: 0.2970, step time: 1.5562\n",
      "169/388, train_loss: 0.1171, step time: 1.5554\n",
      "170/388, train_loss: 0.1819, step time: 1.5523\n",
      "171/388, train_loss: 0.1087, step time: 1.5536\n",
      "172/388, train_loss: 0.0745, step time: 1.5537\n",
      "173/388, train_loss: 0.2169, step time: 1.5547\n",
      "174/388, train_loss: 0.1612, step time: 1.5567\n",
      "175/388, train_loss: 0.1478, step time: 1.5533\n",
      "176/388, train_loss: 0.0413, step time: 1.5533\n",
      "177/388, train_loss: 0.2033, step time: 1.5539\n",
      "178/388, train_loss: 0.1530, step time: 1.5523\n",
      "179/388, train_loss: 0.0809, step time: 1.5546\n",
      "180/388, train_loss: 0.1814, step time: 1.5557\n",
      "181/388, train_loss: 0.0561, step time: 1.5512\n",
      "182/388, train_loss: 0.1041, step time: 1.5533\n",
      "183/388, train_loss: 0.1337, step time: 1.5532\n",
      "184/388, train_loss: 0.1437, step time: 1.5545\n",
      "185/388, train_loss: 0.1356, step time: 1.5617\n",
      "186/388, train_loss: 0.1080, step time: 1.5565\n",
      "187/388, train_loss: 0.1277, step time: 1.5538\n",
      "188/388, train_loss: 0.1879, step time: 1.5510\n",
      "189/388, train_loss: 0.2917, step time: 1.5516\n",
      "190/388, train_loss: 0.2373, step time: 1.5476\n",
      "191/388, train_loss: 0.1960, step time: 1.5544\n",
      "192/388, train_loss: 0.0705, step time: 1.5517\n",
      "193/388, train_loss: 0.1874, step time: 1.5531\n",
      "194/388, train_loss: 0.1338, step time: 1.5546\n",
      "195/388, train_loss: 0.2058, step time: 1.5527\n",
      "196/388, train_loss: 0.4017, step time: 1.5556\n",
      "197/388, train_loss: 0.1103, step time: 1.5546\n",
      "198/388, train_loss: 0.0905, step time: 1.5488\n",
      "199/388, train_loss: 0.2568, step time: 1.5554\n",
      "200/388, train_loss: 0.2913, step time: 1.5536\n",
      "201/388, train_loss: 0.1559, step time: 1.5533\n",
      "202/388, train_loss: 0.1010, step time: 1.5538\n",
      "203/388, train_loss: 0.1234, step time: 1.5523\n",
      "204/388, train_loss: 0.1887, step time: 1.5514\n",
      "205/388, train_loss: 0.0840, step time: 1.5555\n",
      "206/388, train_loss: 0.0750, step time: 1.5525\n",
      "207/388, train_loss: 0.4069, step time: 1.5515\n",
      "208/388, train_loss: 0.0670, step time: 1.5648\n",
      "209/388, train_loss: 0.0920, step time: 1.5435\n",
      "210/388, train_loss: 0.1397, step time: 1.5532\n",
      "211/388, train_loss: 0.1624, step time: 1.5523\n",
      "212/388, train_loss: 0.1696, step time: 1.5509\n",
      "213/388, train_loss: 0.0603, step time: 1.5535\n",
      "214/388, train_loss: 0.1915, step time: 1.5543\n",
      "215/388, train_loss: 0.1213, step time: 1.5528\n",
      "216/388, train_loss: 0.1799, step time: 1.5540\n",
      "217/388, train_loss: 0.2990, step time: 1.5520\n",
      "218/388, train_loss: 0.0987, step time: 1.5512\n",
      "219/388, train_loss: 0.1854, step time: 1.5686\n",
      "220/388, train_loss: 0.1020, step time: 1.5479\n",
      "221/388, train_loss: 0.1132, step time: 1.5554\n",
      "222/388, train_loss: 0.0889, step time: 1.5553\n",
      "223/388, train_loss: 0.0928, step time: 1.5507\n",
      "224/388, train_loss: 0.0466, step time: 1.5562\n",
      "225/388, train_loss: 0.1574, step time: 1.5539\n",
      "226/388, train_loss: 0.1001, step time: 1.5512\n",
      "227/388, train_loss: 0.2505, step time: 1.5553\n",
      "228/388, train_loss: 0.1976, step time: 1.5539\n",
      "229/388, train_loss: 0.0912, step time: 1.5505\n",
      "230/388, train_loss: 0.1021, step time: 1.5563\n",
      "231/388, train_loss: 0.0585, step time: 1.5531\n",
      "232/388, train_loss: 0.1933, step time: 1.5542\n",
      "233/388, train_loss: 0.0853, step time: 1.5561\n",
      "234/388, train_loss: 0.1161, step time: 1.5536\n",
      "235/388, train_loss: 0.1258, step time: 1.5504\n",
      "236/388, train_loss: 0.4706, step time: 1.5544\n",
      "237/388, train_loss: 0.0781, step time: 1.5547\n",
      "238/388, train_loss: 0.4025, step time: 1.5498\n",
      "239/388, train_loss: 0.0980, step time: 1.5540\n",
      "240/388, train_loss: 0.2438, step time: 1.5546\n",
      "241/388, train_loss: 0.1992, step time: 1.5513\n",
      "242/388, train_loss: 0.1855, step time: 1.5519\n",
      "243/388, train_loss: 0.1294, step time: 1.5531\n",
      "244/388, train_loss: 0.1071, step time: 1.5512\n",
      "245/388, train_loss: 0.0357, step time: 1.5521\n",
      "246/388, train_loss: 0.2621, step time: 1.5538\n",
      "247/388, train_loss: 0.0313, step time: 1.5526\n",
      "248/388, train_loss: 0.0980, step time: 1.5520\n",
      "249/388, train_loss: 0.1730, step time: 1.5624\n",
      "250/388, train_loss: 0.1037, step time: 1.5526\n",
      "251/388, train_loss: 0.1013, step time: 1.5533\n",
      "252/388, train_loss: 0.0996, step time: 1.5545\n",
      "253/388, train_loss: 0.0944, step time: 1.5518\n",
      "254/388, train_loss: 0.1889, step time: 1.5526\n",
      "255/388, train_loss: 0.2922, step time: 1.5551\n",
      "256/388, train_loss: 0.1218, step time: 1.5521\n",
      "257/388, train_loss: 0.2276, step time: 1.5535\n",
      "258/388, train_loss: 0.1018, step time: 1.5549\n",
      "259/388, train_loss: 0.0737, step time: 1.5517\n",
      "260/388, train_loss: 0.1199, step time: 1.5544\n",
      "261/388, train_loss: 0.2143, step time: 1.5523\n",
      "262/388, train_loss: 0.0877, step time: 1.5488\n",
      "263/388, train_loss: 0.2180, step time: 1.5525\n",
      "264/388, train_loss: 0.0387, step time: 1.5589\n",
      "265/388, train_loss: 0.1023, step time: 1.5591\n",
      "266/388, train_loss: 0.2111, step time: 1.5584\n",
      "267/388, train_loss: 0.1784, step time: 1.5569\n",
      "268/388, train_loss: 0.2148, step time: 1.5523\n",
      "269/388, train_loss: 0.0941, step time: 1.5513\n",
      "270/388, train_loss: 0.1548, step time: 1.5510\n",
      "271/388, train_loss: 0.0933, step time: 1.5516\n",
      "272/388, train_loss: 0.1874, step time: 1.5565\n",
      "273/388, train_loss: 0.0856, step time: 1.5547\n",
      "274/388, train_loss: 0.2048, step time: 1.5562\n",
      "275/388, train_loss: 0.2011, step time: 1.5522\n",
      "276/388, train_loss: 0.2123, step time: 1.5554\n",
      "277/388, train_loss: 0.1541, step time: 1.5537\n",
      "278/388, train_loss: 0.1584, step time: 1.5515\n",
      "279/388, train_loss: 0.4146, step time: 1.5514\n",
      "280/388, train_loss: 0.1920, step time: 1.5555\n",
      "281/388, train_loss: 0.2590, step time: 1.5527\n",
      "282/388, train_loss: 0.1032, step time: 1.5541\n",
      "283/388, train_loss: 0.0917, step time: 1.5548\n",
      "284/388, train_loss: 0.1050, step time: 1.5587\n",
      "285/388, train_loss: 0.1315, step time: 1.5551\n",
      "286/388, train_loss: 0.1781, step time: 1.5524\n",
      "287/388, train_loss: 0.1213, step time: 1.5571\n",
      "288/388, train_loss: 0.1588, step time: 1.5517\n",
      "289/388, train_loss: 0.1081, step time: 1.5559\n",
      "290/388, train_loss: 0.2233, step time: 1.5545\n",
      "291/388, train_loss: 0.3941, step time: 1.5525\n",
      "292/388, train_loss: 0.1679, step time: 1.5540\n",
      "293/388, train_loss: 0.1773, step time: 1.5583\n",
      "294/388, train_loss: 0.0849, step time: 1.5499\n",
      "295/388, train_loss: 0.1897, step time: 1.5518\n",
      "296/388, train_loss: 0.2107, step time: 1.5610\n",
      "297/388, train_loss: 0.1641, step time: 1.5545\n",
      "298/388, train_loss: 0.2932, step time: 1.5524\n",
      "299/388, train_loss: 0.1288, step time: 1.5521\n",
      "300/388, train_loss: 0.1920, step time: 1.5537\n",
      "301/388, train_loss: 0.1159, step time: 1.5565\n",
      "302/388, train_loss: 0.1825, step time: 1.5501\n",
      "303/388, train_loss: 0.0635, step time: 1.5554\n",
      "304/388, train_loss: 0.3498, step time: 1.5542\n",
      "305/388, train_loss: 0.0661, step time: 1.5525\n",
      "306/388, train_loss: 0.2871, step time: 1.5555\n",
      "307/388, train_loss: 0.1064, step time: 1.5533\n",
      "308/388, train_loss: 0.2993, step time: 1.5531\n",
      "309/388, train_loss: 0.0714, step time: 1.5508\n",
      "310/388, train_loss: 0.1664, step time: 1.5564\n",
      "311/388, train_loss: 0.1288, step time: 1.5508\n",
      "312/388, train_loss: 0.0936, step time: 1.5522\n",
      "313/388, train_loss: 0.1218, step time: 1.5530\n",
      "314/388, train_loss: 0.2920, step time: 1.5515\n",
      "315/388, train_loss: 0.0669, step time: 1.5506\n",
      "316/388, train_loss: 0.1244, step time: 1.5537\n",
      "317/388, train_loss: 0.3785, step time: 1.5563\n",
      "318/388, train_loss: 0.0771, step time: 1.5495\n",
      "319/388, train_loss: 0.1896, step time: 1.5519\n",
      "320/388, train_loss: 0.1007, step time: 1.5552\n",
      "321/388, train_loss: 0.2405, step time: 1.5504\n",
      "322/388, train_loss: 0.0848, step time: 1.5544\n",
      "323/388, train_loss: 0.3688, step time: 1.5503\n",
      "324/388, train_loss: 0.1234, step time: 1.5510\n",
      "325/388, train_loss: 0.1896, step time: 1.5527\n",
      "326/388, train_loss: 0.1330, step time: 1.5541\n",
      "327/388, train_loss: 0.2469, step time: 1.5508\n",
      "328/388, train_loss: 0.0888, step time: 1.5507\n",
      "329/388, train_loss: 0.0443, step time: 1.5531\n",
      "330/388, train_loss: 0.1590, step time: 1.5530\n",
      "331/388, train_loss: 0.1211, step time: 1.5516\n",
      "332/388, train_loss: 0.1402, step time: 1.5509\n",
      "333/388, train_loss: 0.0626, step time: 1.5717\n",
      "334/388, train_loss: 0.1551, step time: 1.5545\n",
      "335/388, train_loss: 0.0756, step time: 1.5483\n",
      "336/388, train_loss: 0.0943, step time: 1.5534\n",
      "337/388, train_loss: 0.0765, step time: 1.5526\n",
      "338/388, train_loss: 0.0280, step time: 1.5492\n",
      "339/388, train_loss: 0.1570, step time: 1.5615\n",
      "340/388, train_loss: 0.0713, step time: 1.5479\n",
      "341/388, train_loss: 0.3636, step time: 1.5525\n",
      "342/388, train_loss: 0.1750, step time: 1.5459\n",
      "343/388, train_loss: 0.1716, step time: 1.5513\n",
      "344/388, train_loss: 0.1032, step time: 1.5950\n",
      "345/388, train_loss: 0.1417, step time: 1.5813\n",
      "346/388, train_loss: 0.3125, step time: 1.5553\n",
      "347/388, train_loss: 0.0619, step time: 1.5546\n",
      "348/388, train_loss: 0.1013, step time: 1.5528\n",
      "349/388, train_loss: 0.1637, step time: 1.5484\n",
      "350/388, train_loss: 0.1345, step time: 1.5517\n",
      "351/388, train_loss: 0.1014, step time: 1.5550\n",
      "352/388, train_loss: 0.2277, step time: 1.5538\n",
      "353/388, train_loss: 0.0828, step time: 1.5513\n",
      "354/388, train_loss: 0.0849, step time: 1.5613\n",
      "355/388, train_loss: 0.0678, step time: 1.5544\n",
      "356/388, train_loss: 0.1175, step time: 1.5513\n",
      "357/388, train_loss: 0.1022, step time: 1.5540\n",
      "358/388, train_loss: 0.0783, step time: 1.5548\n",
      "359/388, train_loss: 0.1990, step time: 1.5511\n",
      "360/388, train_loss: 0.2299, step time: 1.5523\n",
      "361/388, train_loss: 0.1443, step time: 1.5430\n",
      "362/388, train_loss: 0.2088, step time: 1.5570\n",
      "363/388, train_loss: 0.0891, step time: 1.5555\n",
      "364/388, train_loss: 0.1828, step time: 1.5556\n",
      "365/388, train_loss: 0.1627, step time: 1.5617\n",
      "366/388, train_loss: 0.0397, step time: 1.5513\n",
      "367/388, train_loss: 0.1240, step time: 1.5538\n",
      "368/388, train_loss: 0.1586, step time: 1.5530\n",
      "369/388, train_loss: 0.0817, step time: 1.5487\n",
      "370/388, train_loss: 0.1450, step time: 1.5516\n",
      "371/388, train_loss: 0.0691, step time: 1.5668\n",
      "372/388, train_loss: 0.2764, step time: 1.5542\n",
      "373/388, train_loss: 0.3030, step time: 1.5548\n",
      "374/388, train_loss: 0.1934, step time: 1.5529\n",
      "375/388, train_loss: 0.2927, step time: 1.5575\n",
      "376/388, train_loss: 0.0698, step time: 1.5560\n",
      "377/388, train_loss: 0.1901, step time: 1.5511\n",
      "378/388, train_loss: 0.1829, step time: 1.5601\n",
      "379/388, train_loss: 0.2021, step time: 1.5556\n",
      "380/388, train_loss: 0.0905, step time: 1.5526\n",
      "381/388, train_loss: 0.1256, step time: 1.5520\n",
      "382/388, train_loss: 0.2017, step time: 1.5517\n",
      "383/388, train_loss: 0.1621, step time: 1.5525\n",
      "384/388, train_loss: 0.1332, step time: 1.5522\n",
      "385/388, train_loss: 0.0976, step time: 1.5526\n",
      "386/388, train_loss: 0.1357, step time: 1.5572\n",
      "387/388, train_loss: 0.0716, step time: 1.5531\n",
      "388/388, train_loss: 0.1071, step time: 1.5497\n",
      "epoch 97 average loss: 0.1573\n",
      "current epoch: 97 current mean dice: 0.7800 tc: 0.8284 wt: 0.9058 et: 0.6058\n",
      "best mean dice: 0.7817 at epoch: 84\n",
      "time consuming of epoch 97 is: 713.4248\n",
      "----------\n",
      "epoch 98/100\n",
      "1/388, train_loss: 0.1035, step time: 1.5775\n",
      "2/388, train_loss: 0.2968, step time: 1.5512\n",
      "3/388, train_loss: 0.0758, step time: 1.5393\n",
      "4/388, train_loss: 0.1717, step time: 1.5537\n",
      "5/388, train_loss: 0.2684, step time: 1.5497\n",
      "6/388, train_loss: 0.1474, step time: 1.5539\n",
      "7/388, train_loss: 0.1811, step time: 1.5531\n",
      "8/388, train_loss: 0.2383, step time: 1.5528\n",
      "9/388, train_loss: 0.0670, step time: 1.5512\n",
      "10/388, train_loss: 0.0894, step time: 1.5509\n",
      "11/388, train_loss: 0.0892, step time: 1.5530\n",
      "12/388, train_loss: 0.2062, step time: 1.5593\n",
      "13/388, train_loss: 0.1449, step time: 1.5501\n",
      "14/388, train_loss: 0.1706, step time: 1.5501\n",
      "15/388, train_loss: 0.1877, step time: 1.5530\n",
      "16/388, train_loss: 0.1032, step time: 1.5606\n",
      "17/388, train_loss: 0.3307, step time: 1.5593\n",
      "18/388, train_loss: 0.0858, step time: 1.5558\n",
      "19/388, train_loss: 0.1111, step time: 1.5566\n",
      "20/388, train_loss: 0.2257, step time: 1.5551\n",
      "21/388, train_loss: 0.0503, step time: 1.5516\n",
      "22/388, train_loss: 0.1277, step time: 1.5595\n",
      "23/388, train_loss: 0.1561, step time: 1.5582\n",
      "24/388, train_loss: 0.1834, step time: 1.5568\n",
      "25/388, train_loss: 0.0922, step time: 1.5608\n",
      "26/388, train_loss: 0.1773, step time: 1.5594\n",
      "27/388, train_loss: 0.1622, step time: 1.5543\n",
      "28/388, train_loss: 0.4844, step time: 1.5508\n",
      "29/388, train_loss: 0.1578, step time: 1.5579\n",
      "30/388, train_loss: 0.1256, step time: 1.5553\n",
      "31/388, train_loss: 0.0863, step time: 1.5379\n",
      "32/388, train_loss: 0.0777, step time: 1.5568\n",
      "33/388, train_loss: 0.2290, step time: 1.5515\n",
      "34/388, train_loss: 0.2562, step time: 1.5540\n",
      "35/388, train_loss: 0.1774, step time: 1.5532\n",
      "36/388, train_loss: 0.0977, step time: 1.5510\n",
      "37/388, train_loss: 0.0738, step time: 1.5527\n",
      "38/388, train_loss: 0.1083, step time: 1.5555\n",
      "39/388, train_loss: 0.1867, step time: 1.5569\n",
      "40/388, train_loss: 0.0840, step time: 1.5506\n",
      "41/388, train_loss: 0.1464, step time: 1.5549\n",
      "42/388, train_loss: 0.1068, step time: 1.5532\n",
      "43/388, train_loss: 0.2312, step time: 1.5493\n",
      "44/388, train_loss: 0.1607, step time: 1.5560\n",
      "45/388, train_loss: 0.0899, step time: 1.5586\n",
      "46/388, train_loss: 0.2845, step time: 1.5530\n",
      "47/388, train_loss: 0.0962, step time: 1.5583\n",
      "48/388, train_loss: 0.1403, step time: 1.5504\n",
      "49/388, train_loss: 0.1754, step time: 1.5563\n",
      "50/388, train_loss: 0.0782, step time: 1.5562\n",
      "51/388, train_loss: 0.1104, step time: 1.5529\n",
      "52/388, train_loss: 0.0991, step time: 1.5513\n",
      "53/388, train_loss: 0.0878, step time: 1.5508\n",
      "54/388, train_loss: 0.1558, step time: 1.5531\n",
      "55/388, train_loss: 0.0561, step time: 1.5503\n",
      "56/388, train_loss: 0.1992, step time: 1.5532\n",
      "57/388, train_loss: 0.0929, step time: 1.5510\n",
      "58/388, train_loss: 0.1713, step time: 1.5562\n",
      "59/388, train_loss: 0.2653, step time: 1.5546\n",
      "60/388, train_loss: 0.1335, step time: 1.5536\n",
      "61/388, train_loss: 0.1034, step time: 1.5517\n",
      "62/388, train_loss: 0.0425, step time: 1.5595\n",
      "63/388, train_loss: 0.2703, step time: 1.5500\n",
      "64/388, train_loss: 0.2595, step time: 1.5523\n",
      "65/388, train_loss: 0.1074, step time: 1.5563\n",
      "66/388, train_loss: 0.1370, step time: 1.5524\n",
      "67/388, train_loss: 0.1875, step time: 1.5543\n",
      "68/388, train_loss: 0.1581, step time: 1.5497\n",
      "69/388, train_loss: 0.2782, step time: 1.5535\n",
      "70/388, train_loss: 0.2471, step time: 1.5569\n",
      "71/388, train_loss: 0.1043, step time: 1.5513\n",
      "72/388, train_loss: 0.1323, step time: 1.5539\n",
      "73/388, train_loss: 0.1385, step time: 1.5532\n",
      "74/388, train_loss: 0.1373, step time: 1.5536\n",
      "75/388, train_loss: 0.1738, step time: 1.5545\n",
      "76/388, train_loss: 0.1353, step time: 1.5511\n",
      "77/388, train_loss: 0.0964, step time: 1.5578\n",
      "78/388, train_loss: 0.0970, step time: 1.5559\n",
      "79/388, train_loss: 0.2554, step time: 1.5506\n",
      "80/388, train_loss: 0.0597, step time: 1.5525\n",
      "81/388, train_loss: 0.1954, step time: 1.5533\n",
      "82/388, train_loss: 0.1602, step time: 1.5448\n",
      "83/388, train_loss: 0.1541, step time: 1.5572\n",
      "84/388, train_loss: 0.0677, step time: 1.5519\n",
      "85/388, train_loss: 0.0558, step time: 1.5521\n",
      "86/388, train_loss: 0.2093, step time: 1.5535\n",
      "87/388, train_loss: 0.0764, step time: 1.5502\n",
      "88/388, train_loss: 0.3817, step time: 1.5512\n",
      "89/388, train_loss: 0.1527, step time: 1.5514\n",
      "90/388, train_loss: 0.0700, step time: 1.5562\n",
      "91/388, train_loss: 0.1603, step time: 1.5526\n",
      "92/388, train_loss: 0.1905, step time: 1.5562\n",
      "93/388, train_loss: 0.1565, step time: 1.5540\n",
      "94/388, train_loss: 0.1566, step time: 1.5492\n",
      "95/388, train_loss: 0.0438, step time: 1.5519\n",
      "96/388, train_loss: 0.4143, step time: 1.5504\n",
      "97/388, train_loss: 0.2137, step time: 1.5396\n",
      "98/388, train_loss: 0.1134, step time: 1.5514\n",
      "99/388, train_loss: 0.2865, step time: 1.5626\n",
      "100/388, train_loss: 0.2473, step time: 1.5556\n",
      "101/388, train_loss: 0.0858, step time: 1.5555\n",
      "102/388, train_loss: 0.1778, step time: 1.5542\n",
      "103/388, train_loss: 0.0870, step time: 1.5535\n",
      "104/388, train_loss: 0.1188, step time: 1.5529\n",
      "105/388, train_loss: 0.0845, step time: 1.5493\n",
      "106/388, train_loss: 0.0814, step time: 1.5534\n",
      "107/388, train_loss: 0.1373, step time: 1.5537\n",
      "108/388, train_loss: 0.1281, step time: 1.5519\n",
      "109/388, train_loss: 0.1095, step time: 1.5490\n",
      "110/388, train_loss: 0.2475, step time: 1.5518\n",
      "111/388, train_loss: 0.0935, step time: 1.5570\n",
      "112/388, train_loss: 0.1333, step time: 1.5550\n",
      "113/388, train_loss: 0.2109, step time: 1.5576\n",
      "114/388, train_loss: 0.3273, step time: 1.5523\n",
      "115/388, train_loss: 0.1131, step time: 1.5514\n",
      "116/388, train_loss: 0.0678, step time: 1.5453\n",
      "117/388, train_loss: 0.1606, step time: 1.5523\n",
      "118/388, train_loss: 0.1200, step time: 1.5524\n",
      "119/388, train_loss: 0.0901, step time: 1.5503\n",
      "120/388, train_loss: 0.1592, step time: 1.5506\n",
      "121/388, train_loss: 0.1188, step time: 1.5581\n",
      "122/388, train_loss: 0.1711, step time: 1.5551\n",
      "123/388, train_loss: 0.0723, step time: 1.5527\n",
      "124/388, train_loss: 0.2011, step time: 1.5561\n",
      "125/388, train_loss: 0.1395, step time: 1.5520\n",
      "126/388, train_loss: 0.3532, step time: 1.5504\n",
      "127/388, train_loss: 0.2118, step time: 1.5506\n",
      "128/388, train_loss: 0.0631, step time: 1.5550\n",
      "129/388, train_loss: 0.1264, step time: 1.5535\n",
      "130/388, train_loss: 0.1188, step time: 1.5559\n",
      "131/388, train_loss: 0.2093, step time: 1.5535\n",
      "132/388, train_loss: 0.1903, step time: 1.5577\n",
      "133/388, train_loss: 0.1448, step time: 1.5545\n",
      "134/388, train_loss: 0.1463, step time: 1.5520\n",
      "135/388, train_loss: 0.0825, step time: 1.5559\n",
      "136/388, train_loss: 0.3359, step time: 1.5534\n",
      "137/388, train_loss: 0.0695, step time: 1.5552\n",
      "138/388, train_loss: 0.3026, step time: 1.5541\n",
      "139/388, train_loss: 0.1773, step time: 1.5480\n",
      "140/388, train_loss: 0.0526, step time: 1.5519\n",
      "141/388, train_loss: 0.1378, step time: 1.5559\n",
      "142/388, train_loss: 0.1660, step time: 1.5544\n",
      "143/388, train_loss: 0.1482, step time: 1.5535\n",
      "144/388, train_loss: 0.0978, step time: 1.5560\n",
      "145/388, train_loss: 0.1963, step time: 1.5510\n",
      "146/388, train_loss: 0.1571, step time: 1.5567\n",
      "147/388, train_loss: 0.0960, step time: 1.5575\n",
      "148/388, train_loss: 0.0427, step time: 1.5548\n",
      "149/388, train_loss: 0.2263, step time: 1.5552\n",
      "150/388, train_loss: 0.1815, step time: 1.5574\n",
      "151/388, train_loss: 0.1052, step time: 1.5410\n",
      "152/388, train_loss: 0.1274, step time: 1.5620\n",
      "153/388, train_loss: 0.4391, step time: 1.5587\n",
      "154/388, train_loss: 0.2361, step time: 1.5583\n",
      "155/388, train_loss: 0.0981, step time: 1.5539\n",
      "156/388, train_loss: 0.1720, step time: 1.5540\n",
      "157/388, train_loss: 0.2256, step time: 1.5582\n",
      "158/388, train_loss: 0.1564, step time: 1.5690\n",
      "159/388, train_loss: 0.1234, step time: 1.5527\n",
      "160/388, train_loss: 0.1110, step time: 1.5567\n",
      "161/388, train_loss: 0.1092, step time: 1.5504\n",
      "162/388, train_loss: 0.1165, step time: 1.5561\n",
      "163/388, train_loss: 0.2262, step time: 1.5541\n",
      "164/388, train_loss: 0.1820, step time: 1.5482\n",
      "165/388, train_loss: 0.3492, step time: 1.5546\n",
      "166/388, train_loss: 0.0727, step time: 1.5537\n",
      "167/388, train_loss: 0.3042, step time: 1.5525\n",
      "168/388, train_loss: 0.1479, step time: 1.5554\n",
      "169/388, train_loss: 0.1157, step time: 1.5468\n",
      "170/388, train_loss: 0.0706, step time: 1.5495\n",
      "171/388, train_loss: 0.1568, step time: 1.5538\n",
      "172/388, train_loss: 0.2454, step time: 1.5555\n",
      "173/388, train_loss: 0.0451, step time: 1.5534\n",
      "174/388, train_loss: 0.1013, step time: 1.5546\n",
      "175/388, train_loss: 0.1621, step time: 1.5516\n",
      "176/388, train_loss: 0.2278, step time: 1.5545\n",
      "177/388, train_loss: 0.2600, step time: 1.5518\n",
      "178/388, train_loss: 0.1245, step time: 1.5548\n",
      "179/388, train_loss: 0.1886, step time: 1.5539\n",
      "180/388, train_loss: 0.1297, step time: 1.5521\n",
      "181/388, train_loss: 0.1130, step time: 1.5525\n",
      "182/388, train_loss: 0.1986, step time: 1.5552\n",
      "183/388, train_loss: 0.1733, step time: 1.5526\n",
      "184/388, train_loss: 0.1910, step time: 1.5549\n",
      "185/388, train_loss: 0.1058, step time: 1.5502\n",
      "186/388, train_loss: 0.0799, step time: 1.5486\n",
      "187/388, train_loss: 0.2147, step time: 1.5537\n",
      "188/388, train_loss: 0.4154, step time: 1.5569\n",
      "189/388, train_loss: 0.1422, step time: 1.5533\n",
      "190/388, train_loss: 0.0793, step time: 1.5526\n",
      "191/388, train_loss: 0.2916, step time: 1.5540\n",
      "192/388, train_loss: 0.1250, step time: 1.5503\n",
      "193/388, train_loss: 0.2785, step time: 1.5558\n",
      "194/388, train_loss: 0.0701, step time: 1.5522\n",
      "195/388, train_loss: 0.0920, step time: 1.5544\n",
      "196/388, train_loss: 0.2054, step time: 1.5556\n",
      "197/388, train_loss: 0.2029, step time: 1.5572\n",
      "198/388, train_loss: 0.0787, step time: 1.5519\n",
      "199/388, train_loss: 0.5073, step time: 1.5501\n",
      "200/388, train_loss: 0.1190, step time: 1.5579\n",
      "201/388, train_loss: 0.0982, step time: 1.5504\n",
      "202/388, train_loss: 0.3609, step time: 1.5524\n",
      "203/388, train_loss: 0.0766, step time: 1.5525\n",
      "204/388, train_loss: 0.2618, step time: 1.5519\n",
      "205/388, train_loss: 0.2227, step time: 1.5558\n",
      "206/388, train_loss: 0.1030, step time: 1.5558\n",
      "207/388, train_loss: 0.0820, step time: 1.5505\n",
      "208/388, train_loss: 0.0800, step time: 1.5593\n",
      "209/388, train_loss: 0.1743, step time: 1.5509\n",
      "210/388, train_loss: 0.0971, step time: 1.5515\n",
      "211/388, train_loss: 0.1248, step time: 1.5517\n",
      "212/388, train_loss: 0.1189, step time: 1.5541\n",
      "213/388, train_loss: 0.0442, step time: 1.5552\n",
      "214/388, train_loss: 0.1160, step time: 1.5517\n",
      "215/388, train_loss: 0.1905, step time: 1.5510\n",
      "216/388, train_loss: 0.0670, step time: 1.5567\n",
      "217/388, train_loss: 0.2026, step time: 1.5549\n",
      "218/388, train_loss: 0.1569, step time: 1.5552\n",
      "219/388, train_loss: 0.1325, step time: 1.5508\n",
      "220/388, train_loss: 0.0771, step time: 1.5685\n",
      "221/388, train_loss: 0.1126, step time: 1.5510\n",
      "222/388, train_loss: 0.0845, step time: 1.5496\n",
      "223/388, train_loss: 0.0970, step time: 1.5642\n",
      "224/388, train_loss: 0.1515, step time: 1.5538\n",
      "225/388, train_loss: 0.2377, step time: 1.5627\n",
      "226/388, train_loss: 0.1157, step time: 1.5569\n",
      "227/388, train_loss: 0.1121, step time: 1.5524\n",
      "228/388, train_loss: 0.1895, step time: 1.5491\n",
      "229/388, train_loss: 0.1220, step time: 1.5556\n",
      "230/388, train_loss: 0.0276, step time: 1.5471\n",
      "231/388, train_loss: 0.0979, step time: 1.5530\n",
      "232/388, train_loss: 0.0975, step time: 1.5541\n",
      "233/388, train_loss: 0.1676, step time: 1.5544\n",
      "234/388, train_loss: 0.0918, step time: 1.5590\n",
      "235/388, train_loss: 0.2678, step time: 1.5532\n",
      "236/388, train_loss: 0.1533, step time: 1.5555\n",
      "237/388, train_loss: 0.2344, step time: 1.5439\n",
      "238/388, train_loss: 0.3934, step time: 1.5561\n",
      "239/388, train_loss: 0.0654, step time: 1.5539\n",
      "240/388, train_loss: 0.2635, step time: 1.5528\n",
      "241/388, train_loss: 0.1557, step time: 1.5560\n",
      "242/388, train_loss: 0.0323, step time: 1.5572\n",
      "243/388, train_loss: 0.1608, step time: 1.5540\n",
      "244/388, train_loss: 0.1298, step time: 1.5548\n",
      "245/388, train_loss: 0.2482, step time: 1.5534\n",
      "246/388, train_loss: 0.0996, step time: 1.5516\n",
      "247/388, train_loss: 0.1207, step time: 1.5520\n",
      "248/388, train_loss: 0.0859, step time: 1.5481\n",
      "249/388, train_loss: 0.1867, step time: 1.5524\n",
      "250/388, train_loss: 0.0906, step time: 1.5509\n",
      "251/388, train_loss: 0.2182, step time: 1.5548\n",
      "252/388, train_loss: 0.0517, step time: 1.5514\n",
      "253/388, train_loss: 0.1908, step time: 1.5620\n",
      "254/388, train_loss: 0.0984, step time: 1.5560\n",
      "255/388, train_loss: 0.0902, step time: 1.5491\n",
      "256/388, train_loss: 0.0935, step time: 1.5526\n",
      "257/388, train_loss: 0.1605, step time: 1.5524\n",
      "258/388, train_loss: 0.0964, step time: 1.5534\n",
      "259/388, train_loss: 0.2129, step time: 1.5578\n",
      "260/388, train_loss: 0.1512, step time: 1.5557\n",
      "261/388, train_loss: 0.1496, step time: 1.5682\n",
      "262/388, train_loss: 0.1813, step time: 1.5515\n",
      "263/388, train_loss: 0.0819, step time: 1.5515\n",
      "264/388, train_loss: 0.0874, step time: 1.5580\n",
      "265/388, train_loss: 0.1995, step time: 1.5573\n",
      "266/388, train_loss: 0.0808, step time: 1.5535\n",
      "267/388, train_loss: 0.1094, step time: 1.5408\n",
      "268/388, train_loss: 0.2299, step time: 1.5568\n",
      "269/388, train_loss: 0.0972, step time: 1.5554\n",
      "270/388, train_loss: 0.1064, step time: 1.5581\n",
      "271/388, train_loss: 0.1190, step time: 1.5537\n",
      "272/388, train_loss: 0.1973, step time: 1.5565\n",
      "273/388, train_loss: 0.1369, step time: 1.5494\n",
      "274/388, train_loss: 0.2439, step time: 1.5513\n",
      "275/388, train_loss: 0.3196, step time: 1.5521\n",
      "276/388, train_loss: 0.4022, step time: 1.5542\n",
      "277/388, train_loss: 0.2664, step time: 1.5592\n",
      "278/388, train_loss: 0.0966, step time: 1.5522\n",
      "279/388, train_loss: 0.1506, step time: 1.5498\n",
      "280/388, train_loss: 0.1087, step time: 1.5529\n",
      "281/388, train_loss: 0.1021, step time: 1.5554\n",
      "282/388, train_loss: 0.3323, step time: 1.5567\n",
      "283/388, train_loss: 0.1101, step time: 1.5558\n",
      "284/388, train_loss: 0.0553, step time: 1.5555\n",
      "285/388, train_loss: 0.0631, step time: 1.5541\n",
      "286/388, train_loss: 0.1877, step time: 1.5557\n",
      "287/388, train_loss: 0.2916, step time: 1.5565\n",
      "288/388, train_loss: 0.1859, step time: 1.5549\n",
      "289/388, train_loss: 0.2765, step time: 1.5538\n",
      "290/388, train_loss: 0.1959, step time: 1.5543\n",
      "291/388, train_loss: 0.1351, step time: 1.5536\n",
      "292/388, train_loss: 0.1712, step time: 1.5551\n",
      "293/388, train_loss: 0.0948, step time: 1.5541\n",
      "294/388, train_loss: 0.2012, step time: 1.5563\n",
      "295/388, train_loss: 0.0302, step time: 1.5586\n",
      "296/388, train_loss: 0.2838, step time: 1.5573\n",
      "297/388, train_loss: 0.2332, step time: 1.5612\n",
      "298/388, train_loss: 0.1410, step time: 1.5504\n",
      "299/388, train_loss: 0.1536, step time: 1.5575\n",
      "300/388, train_loss: 0.0476, step time: 1.5534\n",
      "301/388, train_loss: 0.2453, step time: 1.5552\n",
      "302/388, train_loss: 0.0697, step time: 1.5522\n",
      "303/388, train_loss: 0.1363, step time: 1.5523\n",
      "304/388, train_loss: 0.3646, step time: 1.5552\n",
      "305/388, train_loss: 0.1980, step time: 1.5535\n",
      "306/388, train_loss: 0.1275, step time: 1.5567\n",
      "307/388, train_loss: 0.0656, step time: 1.5610\n",
      "308/388, train_loss: 0.3867, step time: 1.5517\n",
      "309/388, train_loss: 0.1326, step time: 1.5479\n",
      "310/388, train_loss: 0.2100, step time: 1.5544\n",
      "311/388, train_loss: 0.0711, step time: 1.5519\n",
      "312/388, train_loss: 0.1693, step time: 1.5641\n",
      "313/388, train_loss: 0.0680, step time: 1.5520\n",
      "314/388, train_loss: 0.0423, step time: 1.5639\n",
      "315/388, train_loss: 0.0989, step time: 1.5537\n",
      "316/388, train_loss: 0.2706, step time: 1.5579\n",
      "317/388, train_loss: 0.1951, step time: 1.5525\n",
      "318/388, train_loss: 0.1628, step time: 1.5532\n",
      "319/388, train_loss: 0.1543, step time: 1.5474\n",
      "320/388, train_loss: 0.1522, step time: 1.5531\n",
      "321/388, train_loss: 0.0984, step time: 1.5520\n",
      "322/388, train_loss: 0.0673, step time: 1.5423\n",
      "323/388, train_loss: 0.0981, step time: 1.5494\n",
      "324/388, train_loss: 0.0917, step time: 1.5526\n",
      "325/388, train_loss: 0.1731, step time: 1.5437\n",
      "326/388, train_loss: 0.1537, step time: 1.5419\n",
      "327/388, train_loss: 0.1909, step time: 1.5505\n",
      "328/388, train_loss: 0.0679, step time: 1.5516\n",
      "329/388, train_loss: 0.2783, step time: 1.5536\n",
      "330/388, train_loss: 0.0930, step time: 1.5556\n",
      "331/388, train_loss: 0.3384, step time: 1.5534\n",
      "332/388, train_loss: 0.1008, step time: 1.5519\n",
      "333/388, train_loss: 0.1657, step time: 1.5596\n",
      "334/388, train_loss: 0.0878, step time: 1.5518\n",
      "335/388, train_loss: 0.1528, step time: 1.5519\n",
      "336/388, train_loss: 0.2842, step time: 1.5501\n",
      "337/388, train_loss: 0.1580, step time: 1.5501\n",
      "338/388, train_loss: 0.1022, step time: 1.5565\n",
      "339/388, train_loss: 0.2033, step time: 1.5548\n",
      "340/388, train_loss: 0.1150, step time: 1.5521\n",
      "341/388, train_loss: 0.1317, step time: 1.5558\n",
      "342/388, train_loss: 0.1454, step time: 1.5629\n",
      "343/388, train_loss: 0.0951, step time: 1.5504\n",
      "344/388, train_loss: 0.0449, step time: 1.5503\n",
      "345/388, train_loss: 0.0802, step time: 1.5562\n",
      "346/388, train_loss: 0.0886, step time: 1.5559\n",
      "347/388, train_loss: 0.2533, step time: 1.5534\n",
      "348/388, train_loss: 0.1321, step time: 1.5547\n",
      "349/388, train_loss: 0.0676, step time: 1.5491\n",
      "350/388, train_loss: 0.1627, step time: 1.5531\n",
      "351/388, train_loss: 0.0885, step time: 1.5547\n",
      "352/388, train_loss: 0.2582, step time: 1.5521\n",
      "353/388, train_loss: 0.1414, step time: 1.5539\n",
      "354/388, train_loss: 0.1355, step time: 1.5544\n",
      "355/388, train_loss: 0.1813, step time: 1.5518\n",
      "356/388, train_loss: 0.1790, step time: 1.5525\n",
      "357/388, train_loss: 0.1219, step time: 1.5545\n",
      "358/388, train_loss: 0.0695, step time: 1.5579\n",
      "359/388, train_loss: 0.0979, step time: 1.5561\n",
      "360/388, train_loss: 0.2271, step time: 1.5513\n",
      "361/388, train_loss: 0.0847, step time: 1.5524\n",
      "362/388, train_loss: 0.1190, step time: 1.5562\n",
      "363/388, train_loss: 0.0743, step time: 1.5540\n",
      "364/388, train_loss: 0.2059, step time: 1.5513\n",
      "365/388, train_loss: 0.0792, step time: 1.5456\n",
      "366/388, train_loss: 0.2083, step time: 1.5507\n",
      "367/388, train_loss: 0.0866, step time: 1.5532\n",
      "368/388, train_loss: 0.2248, step time: 1.5538\n",
      "369/388, train_loss: 0.3067, step time: 1.5558\n",
      "370/388, train_loss: 0.2094, step time: 1.5525\n",
      "371/388, train_loss: 0.1227, step time: 1.5566\n",
      "372/388, train_loss: 0.1306, step time: 1.5491\n",
      "373/388, train_loss: 0.1786, step time: 1.5493\n",
      "374/388, train_loss: 0.1469, step time: 1.5552\n",
      "375/388, train_loss: 0.0678, step time: 1.5477\n",
      "376/388, train_loss: 0.1635, step time: 1.5540\n",
      "377/388, train_loss: 0.5259, step time: 1.5604\n",
      "378/388, train_loss: 0.1415, step time: 1.5523\n",
      "379/388, train_loss: 0.2614, step time: 1.5529\n",
      "380/388, train_loss: 0.2285, step time: 1.5426\n",
      "381/388, train_loss: 0.0944, step time: 1.5526\n",
      "382/388, train_loss: 0.1792, step time: 1.5551\n",
      "383/388, train_loss: 0.2650, step time: 1.5548\n",
      "384/388, train_loss: 0.0563, step time: 1.5524\n",
      "385/388, train_loss: 0.0612, step time: 1.5506\n",
      "386/388, train_loss: 0.0536, step time: 1.5497\n",
      "387/388, train_loss: 0.1294, step time: 1.5538\n",
      "388/388, train_loss: 0.1540, step time: 1.5555\n",
      "epoch 98 average loss: 0.1572\n",
      "current epoch: 98 current mean dice: 0.7801 tc: 0.8285 wt: 0.9059 et: 0.6059\n",
      "best mean dice: 0.7817 at epoch: 84\n",
      "time consuming of epoch 98 is: 725.0824\n",
      "----------\n",
      "epoch 99/100\n",
      "1/388, train_loss: 0.1403, step time: 1.5721\n",
      "2/388, train_loss: 0.0786, step time: 1.5539\n",
      "3/388, train_loss: 0.4738, step time: 1.5578\n",
      "4/388, train_loss: 0.0672, step time: 1.5537\n",
      "5/388, train_loss: 0.1067, step time: 1.5547\n",
      "6/388, train_loss: 0.3510, step time: 1.5587\n",
      "7/388, train_loss: 0.1835, step time: 1.5540\n",
      "8/388, train_loss: 0.1617, step time: 1.5543\n",
      "9/388, train_loss: 0.1268, step time: 1.5525\n",
      "10/388, train_loss: 0.0961, step time: 1.5519\n",
      "11/388, train_loss: 0.0674, step time: 1.5503\n",
      "12/388, train_loss: 0.1342, step time: 1.5541\n",
      "13/388, train_loss: 0.0739, step time: 1.5492\n",
      "14/388, train_loss: 0.1585, step time: 1.5581\n",
      "15/388, train_loss: 0.1981, step time: 1.5539\n",
      "16/388, train_loss: 0.0348, step time: 1.5495\n",
      "17/388, train_loss: 0.1254, step time: 1.5543\n",
      "18/388, train_loss: 0.1001, step time: 1.5635\n",
      "19/388, train_loss: 0.0931, step time: 1.5560\n",
      "20/388, train_loss: 0.0821, step time: 1.5555\n",
      "21/388, train_loss: 0.1749, step time: 1.5511\n",
      "22/388, train_loss: 0.1809, step time: 1.5536\n",
      "23/388, train_loss: 0.2514, step time: 1.5564\n",
      "24/388, train_loss: 0.0909, step time: 1.5520\n",
      "25/388, train_loss: 0.2115, step time: 1.5645\n",
      "26/388, train_loss: 0.0861, step time: 1.5543\n",
      "27/388, train_loss: 0.1855, step time: 1.5555\n",
      "28/388, train_loss: 0.1538, step time: 1.5520\n",
      "29/388, train_loss: 0.1621, step time: 1.5615\n",
      "30/388, train_loss: 0.2368, step time: 1.5508\n",
      "31/388, train_loss: 0.2633, step time: 1.5484\n",
      "32/388, train_loss: 0.0696, step time: 1.5520\n",
      "33/388, train_loss: 0.0881, step time: 1.5694\n",
      "34/388, train_loss: 0.1483, step time: 1.5550\n",
      "35/388, train_loss: 0.1036, step time: 1.5511\n",
      "36/388, train_loss: 0.1433, step time: 1.5577\n",
      "37/388, train_loss: 0.0849, step time: 1.5564\n",
      "38/388, train_loss: 0.1841, step time: 1.5540\n",
      "39/388, train_loss: 0.3580, step time: 1.5522\n",
      "40/388, train_loss: 0.0804, step time: 1.5543\n",
      "41/388, train_loss: 0.1903, step time: 1.5555\n",
      "42/388, train_loss: 0.1616, step time: 1.5521\n",
      "43/388, train_loss: 0.0808, step time: 1.5536\n",
      "44/388, train_loss: 0.1894, step time: 1.5549\n",
      "45/388, train_loss: 0.1255, step time: 1.5524\n",
      "46/388, train_loss: 0.1547, step time: 1.5645\n",
      "47/388, train_loss: 0.1127, step time: 1.5560\n",
      "48/388, train_loss: 0.1071, step time: 1.5532\n",
      "49/388, train_loss: 0.2486, step time: 1.5537\n",
      "50/388, train_loss: 0.0771, step time: 1.5622\n",
      "51/388, train_loss: 0.1079, step time: 1.5539\n",
      "52/388, train_loss: 0.0422, step time: 1.5391\n",
      "53/388, train_loss: 0.5231, step time: 1.5531\n",
      "54/388, train_loss: 0.2878, step time: 1.5527\n",
      "55/388, train_loss: 0.1420, step time: 1.5553\n",
      "56/388, train_loss: 0.1897, step time: 1.5497\n",
      "57/388, train_loss: 0.1012, step time: 1.5540\n",
      "58/388, train_loss: 0.0946, step time: 1.5565\n",
      "59/388, train_loss: 0.1454, step time: 1.5549\n",
      "60/388, train_loss: 0.0932, step time: 1.5494\n",
      "61/388, train_loss: 0.2443, step time: 1.5540\n",
      "62/388, train_loss: 0.2040, step time: 1.5495\n",
      "63/388, train_loss: 0.0987, step time: 1.5510\n",
      "64/388, train_loss: 0.2111, step time: 1.5546\n",
      "65/388, train_loss: 0.0843, step time: 1.5521\n",
      "66/388, train_loss: 0.1030, step time: 1.5568\n",
      "67/388, train_loss: 0.0986, step time: 1.5663\n",
      "68/388, train_loss: 0.1385, step time: 1.5512\n",
      "69/388, train_loss: 0.1044, step time: 1.5574\n",
      "70/388, train_loss: 0.1529, step time: 1.5610\n",
      "71/388, train_loss: 0.0896, step time: 1.5565\n",
      "72/388, train_loss: 0.1983, step time: 1.5583\n",
      "73/388, train_loss: 0.1895, step time: 1.5382\n",
      "74/388, train_loss: 0.1543, step time: 1.5526\n",
      "75/388, train_loss: 0.1320, step time: 1.5663\n",
      "76/388, train_loss: 0.2422, step time: 1.5504\n",
      "77/388, train_loss: 0.0781, step time: 1.5562\n",
      "78/388, train_loss: 0.0903, step time: 1.5572\n",
      "79/388, train_loss: 0.1124, step time: 1.5602\n",
      "80/388, train_loss: 0.0962, step time: 1.5542\n",
      "81/388, train_loss: 0.1874, step time: 1.5497\n",
      "82/388, train_loss: 0.0453, step time: 1.5555\n",
      "83/388, train_loss: 0.0602, step time: 1.5547\n",
      "84/388, train_loss: 0.1801, step time: 1.5510\n",
      "85/388, train_loss: 0.1518, step time: 1.5645\n",
      "86/388, train_loss: 0.0661, step time: 1.5538\n",
      "87/388, train_loss: 0.1026, step time: 1.5561\n",
      "88/388, train_loss: 0.1510, step time: 1.5577\n",
      "89/388, train_loss: 0.1201, step time: 1.5546\n",
      "90/388, train_loss: 0.1280, step time: 1.5484\n",
      "91/388, train_loss: 0.0986, step time: 1.5617\n",
      "92/388, train_loss: 0.1312, step time: 1.5519\n",
      "93/388, train_loss: 0.4186, step time: 1.5527\n",
      "94/388, train_loss: 0.1683, step time: 1.5534\n",
      "95/388, train_loss: 0.0275, step time: 1.5515\n",
      "96/388, train_loss: 0.2260, step time: 1.5549\n",
      "97/388, train_loss: 0.2019, step time: 1.5444\n",
      "98/388, train_loss: 0.1276, step time: 1.5560\n",
      "99/388, train_loss: 0.1779, step time: 1.5551\n",
      "100/388, train_loss: 0.0731, step time: 1.5528\n",
      "101/388, train_loss: 0.1095, step time: 1.5540\n",
      "102/388, train_loss: 0.1875, step time: 1.5488\n",
      "103/388, train_loss: 0.1594, step time: 1.5526\n",
      "104/388, train_loss: 0.1739, step time: 1.5579\n",
      "105/388, train_loss: 0.1554, step time: 1.5546\n",
      "106/388, train_loss: 0.0469, step time: 1.5607\n",
      "107/388, train_loss: 0.1750, step time: 1.5567\n",
      "108/388, train_loss: 0.0870, step time: 1.5546\n",
      "109/388, train_loss: 0.1987, step time: 1.5548\n",
      "110/388, train_loss: 0.1016, step time: 1.5554\n",
      "111/388, train_loss: 0.0724, step time: 1.5424\n",
      "112/388, train_loss: 0.2210, step time: 1.5656\n",
      "113/388, train_loss: 0.1143, step time: 1.5559\n",
      "114/388, train_loss: 0.0907, step time: 1.5598\n",
      "115/388, train_loss: 0.0785, step time: 1.5556\n",
      "116/388, train_loss: 0.1204, step time: 1.5506\n",
      "117/388, train_loss: 0.1753, step time: 1.5567\n",
      "118/388, train_loss: 0.2299, step time: 1.5526\n",
      "119/388, train_loss: 0.3539, step time: 1.5480\n",
      "120/388, train_loss: 0.0739, step time: 1.5554\n",
      "121/388, train_loss: 0.1364, step time: 1.5553\n",
      "122/388, train_loss: 0.0757, step time: 1.5550\n",
      "123/388, train_loss: 0.1405, step time: 1.5554\n",
      "124/388, train_loss: 0.2184, step time: 1.5537\n",
      "125/388, train_loss: 0.1490, step time: 1.5564\n",
      "126/388, train_loss: 0.2252, step time: 1.5574\n",
      "127/388, train_loss: 0.0908, step time: 1.5525\n",
      "128/388, train_loss: 0.1352, step time: 1.5516\n",
      "129/388, train_loss: 0.0547, step time: 1.5505\n",
      "130/388, train_loss: 0.0667, step time: 1.5518\n",
      "131/388, train_loss: 0.1174, step time: 1.5507\n",
      "132/388, train_loss: 0.4756, step time: 1.5495\n",
      "133/388, train_loss: 0.1822, step time: 1.5681\n",
      "134/388, train_loss: 0.1013, step time: 1.5504\n",
      "135/388, train_loss: 0.0688, step time: 1.5518\n",
      "136/388, train_loss: 0.2376, step time: 1.5573\n",
      "137/388, train_loss: 0.0966, step time: 1.5662\n",
      "138/388, train_loss: 0.1834, step time: 1.5558\n",
      "139/388, train_loss: 0.4169, step time: 1.5551\n",
      "140/388, train_loss: 0.1212, step time: 1.5410\n",
      "141/388, train_loss: 0.0574, step time: 1.5637\n",
      "142/388, train_loss: 0.0954, step time: 1.5511\n",
      "143/388, train_loss: 0.1350, step time: 1.5539\n",
      "144/388, train_loss: 0.3107, step time: 1.5604\n",
      "145/388, train_loss: 0.3024, step time: 1.5554\n",
      "146/388, train_loss: 0.1672, step time: 1.5564\n",
      "147/388, train_loss: 0.2802, step time: 1.5538\n",
      "148/388, train_loss: 0.0939, step time: 1.5542\n",
      "149/388, train_loss: 0.2221, step time: 1.5594\n",
      "150/388, train_loss: 0.1860, step time: 1.5520\n",
      "151/388, train_loss: 0.1325, step time: 1.5495\n",
      "152/388, train_loss: 0.2256, step time: 1.5552\n",
      "153/388, train_loss: 0.0615, step time: 1.5545\n",
      "154/388, train_loss: 0.0916, step time: 1.5535\n",
      "155/388, train_loss: 0.1557, step time: 1.5524\n",
      "156/388, train_loss: 0.2059, step time: 1.5525\n",
      "157/388, train_loss: 0.1347, step time: 1.5508\n",
      "158/388, train_loss: 0.1455, step time: 1.5562\n",
      "159/388, train_loss: 0.0551, step time: 1.5412\n",
      "160/388, train_loss: 0.3037, step time: 1.5571\n",
      "161/388, train_loss: 0.1942, step time: 1.5552\n",
      "162/388, train_loss: 0.2463, step time: 1.5578\n",
      "163/388, train_loss: 0.2999, step time: 1.5541\n",
      "164/388, train_loss: 0.1696, step time: 1.5487\n",
      "165/388, train_loss: 0.1593, step time: 1.5587\n",
      "166/388, train_loss: 0.0959, step time: 1.5558\n",
      "167/388, train_loss: 0.1210, step time: 1.5471\n",
      "168/388, train_loss: 0.0838, step time: 1.5574\n",
      "169/388, train_loss: 0.1733, step time: 1.5572\n",
      "170/388, train_loss: 0.1987, step time: 1.5501\n",
      "171/388, train_loss: 0.1879, step time: 1.5557\n",
      "172/388, train_loss: 0.1599, step time: 1.5513\n",
      "173/388, train_loss: 0.1978, step time: 1.5496\n",
      "174/388, train_loss: 0.0610, step time: 1.5515\n",
      "175/388, train_loss: 0.0941, step time: 1.5529\n",
      "176/388, train_loss: 0.3394, step time: 1.5546\n",
      "177/388, train_loss: 0.0712, step time: 1.5506\n",
      "178/388, train_loss: 0.1608, step time: 1.5548\n",
      "179/388, train_loss: 0.1548, step time: 1.5537\n",
      "180/388, train_loss: 0.1898, step time: 1.5521\n",
      "181/388, train_loss: 0.1370, step time: 1.5535\n",
      "182/388, train_loss: 0.1096, step time: 1.5556\n",
      "183/388, train_loss: 0.2394, step time: 1.5456\n",
      "184/388, train_loss: 0.0948, step time: 1.5551\n",
      "185/388, train_loss: 0.1465, step time: 1.5547\n",
      "186/388, train_loss: 0.2793, step time: 1.5521\n",
      "187/388, train_loss: 0.0958, step time: 1.5538\n",
      "188/388, train_loss: 0.0422, step time: 1.5549\n",
      "189/388, train_loss: 0.2808, step time: 1.5535\n",
      "190/388, train_loss: 0.2426, step time: 1.5539\n",
      "191/388, train_loss: 0.0735, step time: 1.5550\n",
      "192/388, train_loss: 0.1477, step time: 1.5523\n",
      "193/388, train_loss: 0.1942, step time: 1.5510\n",
      "194/388, train_loss: 0.1127, step time: 1.5588\n",
      "195/388, train_loss: 0.3587, step time: 1.5553\n",
      "196/388, train_loss: 0.1272, step time: 1.5536\n",
      "197/388, train_loss: 0.1561, step time: 1.5535\n",
      "198/388, train_loss: 0.0911, step time: 1.5514\n",
      "199/388, train_loss: 0.3888, step time: 1.5515\n",
      "200/388, train_loss: 0.0322, step time: 1.5557\n",
      "201/388, train_loss: 0.2185, step time: 1.5694\n",
      "202/388, train_loss: 0.2343, step time: 1.5568\n",
      "203/388, train_loss: 0.1624, step time: 1.5524\n",
      "204/388, train_loss: 0.0769, step time: 1.5540\n",
      "205/388, train_loss: 0.2159, step time: 1.5559\n",
      "206/388, train_loss: 0.0968, step time: 1.5537\n",
      "207/388, train_loss: 0.1589, step time: 1.5533\n",
      "208/388, train_loss: 0.1802, step time: 1.5541\n",
      "209/388, train_loss: 0.1875, step time: 1.5512\n",
      "210/388, train_loss: 0.0494, step time: 1.5550\n",
      "211/388, train_loss: 0.1612, step time: 1.5502\n",
      "212/388, train_loss: 0.1998, step time: 1.5541\n",
      "213/388, train_loss: 0.0936, step time: 1.5533\n",
      "214/388, train_loss: 0.1235, step time: 1.5669\n",
      "215/388, train_loss: 0.0892, step time: 1.5547\n",
      "216/388, train_loss: 0.1277, step time: 1.5519\n",
      "217/388, train_loss: 0.1740, step time: 1.5593\n",
      "218/388, train_loss: 0.2768, step time: 1.5533\n",
      "219/388, train_loss: 0.0938, step time: 1.5569\n",
      "220/388, train_loss: 0.0965, step time: 1.5514\n",
      "221/388, train_loss: 0.0844, step time: 1.5635\n",
      "222/388, train_loss: 0.0386, step time: 1.5544\n",
      "223/388, train_loss: 0.1812, step time: 1.5570\n",
      "224/388, train_loss: 0.1990, step time: 1.5568\n",
      "225/388, train_loss: 0.0696, step time: 1.5558\n",
      "226/388, train_loss: 0.1408, step time: 1.5557\n",
      "227/388, train_loss: 0.0898, step time: 1.5541\n",
      "228/388, train_loss: 0.1667, step time: 1.5547\n",
      "229/388, train_loss: 0.0895, step time: 1.5574\n",
      "230/388, train_loss: 0.1888, step time: 1.5509\n",
      "231/388, train_loss: 0.0997, step time: 1.5554\n",
      "232/388, train_loss: 0.0665, step time: 1.5550\n",
      "233/388, train_loss: 0.0823, step time: 1.5500\n",
      "234/388, train_loss: 0.0969, step time: 1.5543\n",
      "235/388, train_loss: 0.1310, step time: 1.5444\n",
      "236/388, train_loss: 0.1150, step time: 1.5550\n",
      "237/388, train_loss: 0.2898, step time: 1.5559\n",
      "238/388, train_loss: 0.0893, step time: 1.5567\n",
      "239/388, train_loss: 0.1539, step time: 1.5550\n",
      "240/388, train_loss: 0.1224, step time: 1.5530\n",
      "241/388, train_loss: 0.1812, step time: 1.5550\n",
      "242/388, train_loss: 0.1397, step time: 1.5555\n",
      "243/388, train_loss: 0.1241, step time: 1.5526\n",
      "244/388, train_loss: 0.2332, step time: 1.5565\n",
      "245/388, train_loss: 0.0412, step time: 1.5518\n",
      "246/388, train_loss: 0.1171, step time: 1.5566\n",
      "247/388, train_loss: 0.1195, step time: 1.5545\n",
      "248/388, train_loss: 0.1859, step time: 1.5531\n",
      "249/388, train_loss: 0.2777, step time: 1.5580\n",
      "250/388, train_loss: 0.1416, step time: 1.5520\n",
      "251/388, train_loss: 0.0976, step time: 1.5569\n",
      "252/388, train_loss: 0.1921, step time: 1.5547\n",
      "253/388, train_loss: 0.1628, step time: 1.5524\n",
      "254/388, train_loss: 0.0848, step time: 1.5514\n",
      "255/388, train_loss: 0.0771, step time: 1.5512\n",
      "256/388, train_loss: 0.1151, step time: 1.5547\n",
      "257/388, train_loss: 0.1202, step time: 1.5627\n",
      "258/388, train_loss: 0.1318, step time: 1.5532\n",
      "259/388, train_loss: 0.2175, step time: 1.5556\n",
      "260/388, train_loss: 0.1663, step time: 1.5519\n",
      "261/388, train_loss: 0.2570, step time: 1.5559\n",
      "262/388, train_loss: 0.1977, step time: 1.5536\n",
      "263/388, train_loss: 0.1041, step time: 1.5546\n",
      "264/388, train_loss: 0.0861, step time: 1.5517\n",
      "265/388, train_loss: 0.2149, step time: 1.5559\n",
      "266/388, train_loss: 0.2061, step time: 1.5518\n",
      "267/388, train_loss: 0.2831, step time: 1.5595\n",
      "268/388, train_loss: 0.1233, step time: 1.5546\n",
      "269/388, train_loss: 0.3852, step time: 1.5575\n",
      "270/388, train_loss: 0.2104, step time: 1.5564\n",
      "271/388, train_loss: 0.1148, step time: 1.5611\n",
      "272/388, train_loss: 0.0855, step time: 1.5521\n",
      "273/388, train_loss: 0.2964, step time: 1.5596\n",
      "274/388, train_loss: 0.1098, step time: 1.5532\n",
      "275/388, train_loss: 0.3100, step time: 1.5520\n",
      "276/388, train_loss: 0.0997, step time: 1.5577\n",
      "277/388, train_loss: 0.2166, step time: 1.5599\n",
      "278/388, train_loss: 0.1124, step time: 1.5559\n",
      "279/388, train_loss: 0.3822, step time: 1.5566\n",
      "280/388, train_loss: 0.0722, step time: 1.5520\n",
      "281/388, train_loss: 0.0914, step time: 1.5602\n",
      "282/388, train_loss: 0.1490, step time: 1.5537\n",
      "283/388, train_loss: 0.0792, step time: 1.5604\n",
      "284/388, train_loss: 0.1867, step time: 1.5553\n",
      "285/388, train_loss: 0.1383, step time: 1.5577\n",
      "286/388, train_loss: 0.1719, step time: 1.5561\n",
      "287/388, train_loss: 0.1919, step time: 1.5549\n",
      "288/388, train_loss: 0.0721, step time: 1.5536\n",
      "289/388, train_loss: 0.2138, step time: 1.5604\n",
      "290/388, train_loss: 0.2034, step time: 1.5548\n",
      "291/388, train_loss: 0.1983, step time: 1.5548\n",
      "292/388, train_loss: 0.1713, step time: 1.5536\n",
      "293/388, train_loss: 0.0659, step time: 1.5585\n",
      "294/388, train_loss: 0.2334, step time: 1.5535\n",
      "295/388, train_loss: 0.0393, step time: 1.5551\n",
      "296/388, train_loss: 0.0896, step time: 1.5575\n",
      "297/388, train_loss: 0.2109, step time: 1.5603\n",
      "298/388, train_loss: 0.3248, step time: 1.5531\n",
      "299/388, train_loss: 0.2617, step time: 1.5580\n",
      "300/388, train_loss: 0.1533, step time: 1.5559\n",
      "301/388, train_loss: 0.0442, step time: 1.5334\n",
      "302/388, train_loss: 0.2037, step time: 1.5567\n",
      "303/388, train_loss: 0.3171, step time: 1.5591\n",
      "304/388, train_loss: 0.0501, step time: 1.5543\n",
      "305/388, train_loss: 0.2819, step time: 1.5605\n",
      "306/388, train_loss: 0.0504, step time: 1.5532\n",
      "307/388, train_loss: 0.0855, step time: 1.5456\n",
      "308/388, train_loss: 0.1500, step time: 1.5589\n",
      "309/388, train_loss: 0.3675, step time: 1.5594\n",
      "310/388, train_loss: 0.0542, step time: 1.5573\n",
      "311/388, train_loss: 0.0660, step time: 1.5546\n",
      "312/388, train_loss: 0.0586, step time: 1.5519\n",
      "313/388, train_loss: 0.1375, step time: 1.5591\n",
      "314/388, train_loss: 0.1298, step time: 1.5519\n",
      "315/388, train_loss: 0.1360, step time: 1.5523\n",
      "316/388, train_loss: 0.1250, step time: 1.5545\n",
      "317/388, train_loss: 0.1178, step time: 1.5593\n",
      "318/388, train_loss: 0.2065, step time: 1.5599\n",
      "319/388, train_loss: 0.0954, step time: 1.5588\n",
      "320/388, train_loss: 0.1906, step time: 1.5559\n",
      "321/388, train_loss: 0.0996, step time: 1.5578\n",
      "322/388, train_loss: 0.0981, step time: 1.5529\n",
      "323/388, train_loss: 0.1146, step time: 1.5493\n",
      "324/388, train_loss: 0.1142, step time: 1.5551\n",
      "325/388, train_loss: 0.1986, step time: 1.5605\n",
      "326/388, train_loss: 0.1523, step time: 1.5550\n",
      "327/388, train_loss: 0.2592, step time: 1.5549\n",
      "328/388, train_loss: 0.2077, step time: 1.5560\n",
      "329/388, train_loss: 0.1173, step time: 1.5726\n",
      "330/388, train_loss: 0.1165, step time: 1.5558\n",
      "331/388, train_loss: 0.1980, step time: 1.5569\n",
      "332/388, train_loss: 0.1903, step time: 1.5518\n",
      "333/388, train_loss: 0.0815, step time: 1.5603\n",
      "334/388, train_loss: 0.1654, step time: 1.5575\n",
      "335/388, train_loss: 0.1438, step time: 1.5535\n",
      "336/388, train_loss: 0.2268, step time: 1.5522\n",
      "337/388, train_loss: 0.0978, step time: 1.5611\n",
      "338/388, train_loss: 0.0524, step time: 1.5558\n",
      "339/388, train_loss: 0.2327, step time: 1.5554\n",
      "340/388, train_loss: 0.0926, step time: 1.5550\n",
      "341/388, train_loss: 0.0702, step time: 1.5542\n",
      "342/388, train_loss: 0.0839, step time: 1.5571\n",
      "343/388, train_loss: 0.1680, step time: 1.5524\n",
      "344/388, train_loss: 0.0988, step time: 1.5520\n",
      "345/388, train_loss: 0.1043, step time: 1.5672\n",
      "346/388, train_loss: 0.2222, step time: 1.5546\n",
      "347/388, train_loss: 0.2266, step time: 1.5555\n",
      "348/388, train_loss: 0.1015, step time: 1.5614\n",
      "349/388, train_loss: 0.4247, step time: 1.5674\n",
      "350/388, train_loss: 0.1163, step time: 1.5573\n",
      "351/388, train_loss: 0.3235, step time: 1.5537\n",
      "352/388, train_loss: 0.0780, step time: 1.5520\n",
      "353/388, train_loss: 0.1536, step time: 1.5614\n",
      "354/388, train_loss: 0.2488, step time: 1.5498\n",
      "355/388, train_loss: 0.2592, step time: 1.5529\n",
      "356/388, train_loss: 0.1121, step time: 1.5598\n",
      "357/388, train_loss: 0.1502, step time: 1.5589\n",
      "358/388, train_loss: 0.1201, step time: 1.5585\n",
      "359/388, train_loss: 0.1048, step time: 1.5555\n",
      "360/388, train_loss: 0.0914, step time: 1.5454\n",
      "361/388, train_loss: 0.0925, step time: 1.5588\n",
      "362/388, train_loss: 0.2893, step time: 1.5576\n",
      "363/388, train_loss: 0.0842, step time: 1.5563\n",
      "364/388, train_loss: 0.1387, step time: 1.5525\n",
      "365/388, train_loss: 0.0892, step time: 1.5588\n",
      "366/388, train_loss: 0.1524, step time: 1.5563\n",
      "367/388, train_loss: 0.1293, step time: 1.5551\n",
      "368/388, train_loss: 0.2564, step time: 1.5521\n",
      "369/388, train_loss: 0.1319, step time: 1.5577\n",
      "370/388, train_loss: 0.1068, step time: 1.5631\n",
      "371/388, train_loss: 0.1140, step time: 1.5623\n",
      "372/388, train_loss: 0.1588, step time: 1.5502\n",
      "373/388, train_loss: 0.1532, step time: 1.5593\n",
      "374/388, train_loss: 0.1470, step time: 1.5537\n",
      "375/388, train_loss: 0.0788, step time: 1.5506\n",
      "376/388, train_loss: 0.1929, step time: 1.5557\n",
      "377/388, train_loss: 0.2824, step time: 1.5565\n",
      "378/388, train_loss: 0.1310, step time: 1.5561\n",
      "379/388, train_loss: 0.1417, step time: 1.5539\n",
      "380/388, train_loss: 0.0980, step time: 1.5559\n",
      "381/388, train_loss: 0.1616, step time: 1.5637\n",
      "382/388, train_loss: 0.1471, step time: 1.5490\n",
      "383/388, train_loss: 0.3262, step time: 1.5538\n",
      "384/388, train_loss: 0.1436, step time: 1.5524\n",
      "385/388, train_loss: 0.0830, step time: 1.5650\n",
      "386/388, train_loss: 0.2142, step time: 1.5544\n",
      "387/388, train_loss: 0.0753, step time: 1.5520\n",
      "388/388, train_loss: 0.2408, step time: 1.5536\n",
      "epoch 99 average loss: 0.1553\n",
      "current epoch: 99 current mean dice: 0.7801 tc: 0.8286 wt: 0.9059 et: 0.6060\n",
      "best mean dice: 0.7817 at epoch: 84\n",
      "time consuming of epoch 99 is: 716.4547\n",
      "----------\n",
      "epoch 100/100\n",
      "1/388, train_loss: 0.1105, step time: 1.5756\n",
      "2/388, train_loss: 0.1592, step time: 1.5553\n",
      "3/388, train_loss: 0.1293, step time: 1.5529\n",
      "4/388, train_loss: 0.0662, step time: 1.5537\n",
      "5/388, train_loss: 0.0913, step time: 1.5627\n",
      "6/388, train_loss: 0.2719, step time: 1.5575\n",
      "7/388, train_loss: 0.1091, step time: 1.5529\n",
      "8/388, train_loss: 0.2668, step time: 1.5560\n",
      "9/388, train_loss: 0.3614, step time: 1.5637\n",
      "10/388, train_loss: 0.0830, step time: 1.5664\n",
      "11/388, train_loss: 0.1586, step time: 1.5415\n",
      "12/388, train_loss: 0.4461, step time: 1.5483\n",
      "13/388, train_loss: 0.1008, step time: 1.5610\n",
      "14/388, train_loss: 0.1869, step time: 1.5550\n",
      "15/388, train_loss: 0.1510, step time: 1.5562\n",
      "16/388, train_loss: 0.0804, step time: 1.5551\n",
      "17/388, train_loss: 0.1288, step time: 1.5680\n",
      "18/388, train_loss: 0.0802, step time: 1.5572\n",
      "19/388, train_loss: 0.0811, step time: 1.5538\n",
      "20/388, train_loss: 0.1716, step time: 1.5545\n",
      "21/388, train_loss: 0.0906, step time: 1.5590\n",
      "22/388, train_loss: 0.1402, step time: 1.5552\n",
      "23/388, train_loss: 0.1835, step time: 1.5569\n",
      "24/388, train_loss: 0.1897, step time: 1.5553\n",
      "25/388, train_loss: 0.1420, step time: 1.5668\n",
      "26/388, train_loss: 0.2610, step time: 1.5554\n",
      "27/388, train_loss: 0.1509, step time: 1.5544\n",
      "28/388, train_loss: 0.1868, step time: 1.5547\n",
      "29/388, train_loss: 0.3688, step time: 1.5611\n",
      "30/388, train_loss: 0.0927, step time: 1.5575\n",
      "31/388, train_loss: 0.0985, step time: 1.5573\n",
      "32/388, train_loss: 0.0712, step time: 1.5547\n",
      "33/388, train_loss: 0.1451, step time: 1.5324\n",
      "34/388, train_loss: 0.1967, step time: 1.5557\n",
      "35/388, train_loss: 0.2319, step time: 1.5549\n",
      "36/388, train_loss: 0.5139, step time: 1.5528\n",
      "37/388, train_loss: 0.1069, step time: 1.5651\n",
      "38/388, train_loss: 0.2317, step time: 1.5565\n",
      "39/388, train_loss: 0.3145, step time: 1.5554\n",
      "40/388, train_loss: 0.1774, step time: 1.5520\n",
      "41/388, train_loss: 0.1257, step time: 1.5601\n",
      "42/388, train_loss: 0.1065, step time: 1.5551\n",
      "43/388, train_loss: 0.1019, step time: 1.5537\n",
      "44/388, train_loss: 0.2648, step time: 1.5530\n",
      "45/388, train_loss: 0.1306, step time: 1.5629\n",
      "46/388, train_loss: 0.0932, step time: 1.5551\n",
      "47/388, train_loss: 0.0959, step time: 1.5552\n",
      "48/388, train_loss: 0.0883, step time: 1.5549\n",
      "49/388, train_loss: 0.4259, step time: 1.5548\n",
      "50/388, train_loss: 0.0819, step time: 1.5560\n",
      "51/388, train_loss: 0.1352, step time: 1.5545\n",
      "52/388, train_loss: 0.0945, step time: 1.5510\n",
      "53/388, train_loss: 0.1089, step time: 1.5655\n",
      "54/388, train_loss: 0.1736, step time: 1.5495\n",
      "55/388, train_loss: 0.1223, step time: 1.5525\n",
      "56/388, train_loss: 0.0545, step time: 1.5572\n",
      "57/388, train_loss: 0.0976, step time: 1.5686\n",
      "58/388, train_loss: 0.1131, step time: 1.5573\n",
      "59/388, train_loss: 0.2261, step time: 1.5570\n",
      "60/388, train_loss: 0.1204, step time: 1.5505\n",
      "61/388, train_loss: 0.2479, step time: 1.5748\n",
      "62/388, train_loss: 0.2415, step time: 1.5531\n",
      "63/388, train_loss: 0.2034, step time: 1.5535\n",
      "64/388, train_loss: 0.1203, step time: 1.5524\n",
      "65/388, train_loss: 0.0601, step time: 1.5608\n",
      "66/388, train_loss: 0.0729, step time: 1.5532\n",
      "67/388, train_loss: 0.1716, step time: 1.5568\n",
      "68/388, train_loss: 0.0753, step time: 1.5521\n",
      "69/388, train_loss: 0.2583, step time: 1.5758\n",
      "70/388, train_loss: 0.0452, step time: 1.5590\n",
      "71/388, train_loss: 0.0764, step time: 1.5520\n",
      "72/388, train_loss: 0.0637, step time: 1.5527\n",
      "73/388, train_loss: 0.0476, step time: 1.5741\n",
      "74/388, train_loss: 0.2431, step time: 1.5514\n",
      "75/388, train_loss: 0.1060, step time: 1.5502\n",
      "76/388, train_loss: 0.1260, step time: 1.5537\n",
      "77/388, train_loss: 0.1599, step time: 1.5626\n",
      "78/388, train_loss: 0.1531, step time: 1.5600\n",
      "79/388, train_loss: 0.0604, step time: 1.5519\n",
      "80/388, train_loss: 0.2145, step time: 1.5512\n",
      "81/388, train_loss: 0.1244, step time: 1.5716\n",
      "82/388, train_loss: 0.0747, step time: 1.5502\n",
      "83/388, train_loss: 0.5133, step time: 1.5535\n",
      "84/388, train_loss: 0.0445, step time: 1.5574\n",
      "85/388, train_loss: 0.1301, step time: 1.5599\n",
      "86/388, train_loss: 0.0840, step time: 1.5548\n",
      "87/388, train_loss: 0.1913, step time: 1.5612\n",
      "88/388, train_loss: 0.1052, step time: 1.5534\n",
      "89/388, train_loss: 0.2173, step time: 1.5597\n",
      "90/388, train_loss: 0.0824, step time: 1.5511\n",
      "91/388, train_loss: 0.0887, step time: 1.5622\n",
      "92/388, train_loss: 0.1581, step time: 1.5569\n",
      "93/388, train_loss: 0.1114, step time: 1.5395\n",
      "94/388, train_loss: 0.0427, step time: 1.5538\n",
      "95/388, train_loss: 0.2212, step time: 1.5579\n",
      "96/388, train_loss: 0.2201, step time: 1.5531\n",
      "97/388, train_loss: 0.0974, step time: 1.5362\n",
      "98/388, train_loss: 0.2080, step time: 1.5527\n",
      "99/388, train_loss: 0.0690, step time: 1.5576\n",
      "100/388, train_loss: 0.1857, step time: 1.5546\n",
      "101/388, train_loss: 0.0310, step time: 1.5373\n",
      "102/388, train_loss: 0.0975, step time: 1.5594\n",
      "103/388, train_loss: 0.2019, step time: 1.5630\n",
      "104/388, train_loss: 0.0892, step time: 1.5560\n",
      "105/388, train_loss: 0.1457, step time: 1.5600\n",
      "106/388, train_loss: 0.1360, step time: 1.5559\n",
      "107/388, train_loss: 0.0555, step time: 1.5615\n",
      "108/388, train_loss: 0.1644, step time: 1.5497\n",
      "109/388, train_loss: 0.1423, step time: 1.5595\n",
      "110/388, train_loss: 0.2832, step time: 1.5553\n",
      "111/388, train_loss: 0.4172, step time: 1.5602\n",
      "112/388, train_loss: 0.1356, step time: 1.5510\n",
      "113/388, train_loss: 0.0663, step time: 1.5609\n",
      "114/388, train_loss: 0.2391, step time: 1.5561\n",
      "115/388, train_loss: 0.2021, step time: 1.5579\n",
      "116/388, train_loss: 0.1600, step time: 1.5529\n",
      "117/388, train_loss: 0.0779, step time: 1.5573\n",
      "118/388, train_loss: 0.1353, step time: 1.5527\n",
      "119/388, train_loss: 0.1244, step time: 1.5594\n",
      "120/388, train_loss: 0.0902, step time: 1.5494\n",
      "121/388, train_loss: 0.2151, step time: 1.5552\n",
      "122/388, train_loss: 0.1947, step time: 1.5526\n",
      "123/388, train_loss: 0.2964, step time: 1.5598\n",
      "124/388, train_loss: 0.1354, step time: 1.5520\n",
      "125/388, train_loss: 0.0851, step time: 1.5360\n",
      "126/388, train_loss: 0.2804, step time: 1.5542\n",
      "127/388, train_loss: 0.1729, step time: 1.5611\n",
      "128/388, train_loss: 0.0717, step time: 1.5555\n",
      "129/388, train_loss: 0.1509, step time: 1.5557\n",
      "130/388, train_loss: 0.0913, step time: 1.5527\n",
      "131/388, train_loss: 0.0727, step time: 1.5611\n",
      "132/388, train_loss: 0.2246, step time: 1.5540\n",
      "133/388, train_loss: 0.1499, step time: 1.5333\n",
      "134/388, train_loss: 0.0941, step time: 1.5535\n",
      "135/388, train_loss: 0.1538, step time: 1.5610\n",
      "136/388, train_loss: 0.0912, step time: 1.5540\n",
      "137/388, train_loss: 0.1726, step time: 1.5579\n",
      "138/388, train_loss: 0.1382, step time: 1.5509\n",
      "139/388, train_loss: 0.0868, step time: 1.5570\n",
      "140/388, train_loss: 0.0775, step time: 1.5542\n",
      "141/388, train_loss: 0.2550, step time: 1.5614\n",
      "142/388, train_loss: 0.1385, step time: 1.5563\n",
      "143/388, train_loss: 0.0994, step time: 1.5615\n",
      "144/388, train_loss: 0.0947, step time: 1.5550\n",
      "145/388, train_loss: 0.1474, step time: 1.5580\n",
      "146/388, train_loss: 0.1001, step time: 1.5551\n",
      "147/388, train_loss: 0.1595, step time: 1.5585\n",
      "148/388, train_loss: 0.0618, step time: 1.5565\n",
      "149/388, train_loss: 0.0734, step time: 1.5594\n",
      "150/388, train_loss: 0.1611, step time: 1.5554\n",
      "151/388, train_loss: 0.1747, step time: 1.5632\n",
      "152/388, train_loss: 0.0938, step time: 1.5582\n",
      "153/388, train_loss: 0.2536, step time: 1.5586\n",
      "154/388, train_loss: 0.1955, step time: 1.5584\n",
      "155/388, train_loss: 0.1177, step time: 1.5598\n",
      "156/388, train_loss: 0.0855, step time: 1.5508\n",
      "157/388, train_loss: 0.2735, step time: 1.5340\n",
      "158/388, train_loss: 0.1995, step time: 1.5549\n",
      "159/388, train_loss: 0.1934, step time: 1.5600\n",
      "160/388, train_loss: 0.1760, step time: 1.5568\n",
      "161/388, train_loss: 0.1868, step time: 1.5599\n",
      "162/388, train_loss: 0.1551, step time: 1.5553\n",
      "163/388, train_loss: 0.0911, step time: 1.5643\n",
      "164/388, train_loss: 0.1293, step time: 1.5528\n",
      "165/388, train_loss: 0.2355, step time: 1.5624\n",
      "166/388, train_loss: 0.1303, step time: 1.5552\n",
      "167/388, train_loss: 0.0868, step time: 1.5590\n",
      "168/388, train_loss: 0.1511, step time: 1.5528\n",
      "169/388, train_loss: 0.2218, step time: 1.5614\n",
      "170/388, train_loss: 0.1636, step time: 1.5553\n",
      "171/388, train_loss: 0.1655, step time: 1.5616\n",
      "172/388, train_loss: 0.0831, step time: 1.5548\n",
      "173/388, train_loss: 0.0446, step time: 1.5590\n",
      "174/388, train_loss: 0.0938, step time: 1.5438\n",
      "175/388, train_loss: 0.1330, step time: 1.5562\n",
      "176/388, train_loss: 0.2203, step time: 1.5560\n",
      "177/388, train_loss: 0.1922, step time: 1.5609\n",
      "178/388, train_loss: 0.2755, step time: 1.5565\n",
      "179/388, train_loss: 0.1137, step time: 1.5604\n",
      "180/388, train_loss: 0.1970, step time: 1.5525\n",
      "181/388, train_loss: 0.2620, step time: 1.5350\n",
      "182/388, train_loss: 0.1273, step time: 1.5530\n",
      "183/388, train_loss: 0.2249, step time: 1.5578\n",
      "184/388, train_loss: 0.1311, step time: 1.5572\n",
      "185/388, train_loss: 0.1214, step time: 1.5579\n",
      "186/388, train_loss: 0.1580, step time: 1.5623\n",
      "187/388, train_loss: 0.1155, step time: 1.5605\n",
      "188/388, train_loss: 0.2070, step time: 1.5575\n",
      "189/388, train_loss: 0.3415, step time: 1.5317\n",
      "190/388, train_loss: 0.2855, step time: 1.5554\n",
      "191/388, train_loss: 0.1140, step time: 1.5638\n",
      "192/388, train_loss: 0.1861, step time: 1.5512\n",
      "193/388, train_loss: 0.1920, step time: 1.5610\n",
      "194/388, train_loss: 0.2021, step time: 1.5514\n",
      "195/388, train_loss: 0.1030, step time: 1.5609\n",
      "196/388, train_loss: 0.1525, step time: 1.5594\n",
      "197/388, train_loss: 0.0845, step time: 1.5440\n",
      "198/388, train_loss: 0.1161, step time: 1.5566\n",
      "199/388, train_loss: 0.3336, step time: 1.5578\n",
      "200/388, train_loss: 0.0799, step time: 1.5526\n",
      "201/388, train_loss: 0.1867, step time: 1.5603\n",
      "202/388, train_loss: 0.1788, step time: 1.5542\n",
      "203/388, train_loss: 0.4127, step time: 1.5611\n",
      "204/388, train_loss: 0.0950, step time: 1.5516\n",
      "205/388, train_loss: 0.2929, step time: 1.5377\n",
      "206/388, train_loss: 0.3102, step time: 1.5617\n",
      "207/388, train_loss: 0.2667, step time: 1.5321\n",
      "208/388, train_loss: 0.1958, step time: 1.5574\n",
      "209/388, train_loss: 0.0636, step time: 1.5579\n",
      "210/388, train_loss: 0.2295, step time: 1.5572\n",
      "211/388, train_loss: 0.1507, step time: 1.5616\n",
      "212/388, train_loss: 0.0975, step time: 1.5521\n",
      "213/388, train_loss: 0.0784, step time: 1.5369\n",
      "214/388, train_loss: 0.2942, step time: 1.5563\n",
      "215/388, train_loss: 0.0251, step time: 1.5620\n",
      "216/388, train_loss: 0.1586, step time: 1.5582\n",
      "217/388, train_loss: 0.0759, step time: 1.5602\n",
      "218/388, train_loss: 0.1129, step time: 1.5558\n",
      "219/388, train_loss: 0.0714, step time: 1.5628\n",
      "220/388, train_loss: 0.1445, step time: 1.5532\n",
      "221/388, train_loss: 0.1066, step time: 1.5603\n",
      "222/388, train_loss: 0.1746, step time: 1.5537\n",
      "223/388, train_loss: 0.0930, step time: 1.5588\n",
      "224/388, train_loss: 0.1260, step time: 1.5539\n",
      "225/388, train_loss: 0.2320, step time: 1.5323\n",
      "226/388, train_loss: 0.0962, step time: 1.5572\n",
      "227/388, train_loss: 0.3300, step time: 1.5597\n",
      "228/388, train_loss: 0.0825, step time: 1.5533\n",
      "229/388, train_loss: 0.0648, step time: 1.5623\n",
      "230/388, train_loss: 0.0834, step time: 1.5584\n",
      "231/388, train_loss: 0.0752, step time: 1.5604\n",
      "232/388, train_loss: 0.1446, step time: 1.5563\n",
      "233/388, train_loss: 0.1960, step time: 1.5613\n",
      "234/388, train_loss: 0.0987, step time: 1.5531\n",
      "235/388, train_loss: 0.0806, step time: 1.5630\n",
      "236/388, train_loss: 0.2081, step time: 1.5531\n",
      "237/388, train_loss: 0.0890, step time: 1.5312\n",
      "238/388, train_loss: 0.1004, step time: 1.5535\n",
      "239/388, train_loss: 0.1304, step time: 1.5591\n",
      "240/388, train_loss: 0.1053, step time: 1.5552\n",
      "241/388, train_loss: 0.1950, step time: 1.5352\n",
      "242/388, train_loss: 0.1210, step time: 1.5661\n",
      "243/388, train_loss: 0.0945, step time: 1.5605\n",
      "244/388, train_loss: 0.1684, step time: 1.5565\n",
      "245/388, train_loss: 0.1072, step time: 1.5568\n",
      "246/388, train_loss: 0.0937, step time: 1.5566\n",
      "247/388, train_loss: 0.0708, step time: 1.5602\n",
      "248/388, train_loss: 0.2060, step time: 1.5533\n",
      "249/388, train_loss: 0.1978, step time: 1.5345\n",
      "250/388, train_loss: 0.0664, step time: 1.5544\n",
      "251/388, train_loss: 0.0853, step time: 1.5622\n",
      "252/388, train_loss: 0.1967, step time: 1.5524\n",
      "253/388, train_loss: 0.2529, step time: 1.5581\n",
      "254/388, train_loss: 0.1313, step time: 1.5519\n",
      "255/388, train_loss: 0.1144, step time: 1.5587\n",
      "256/388, train_loss: 0.2186, step time: 1.5510\n",
      "257/388, train_loss: 0.1440, step time: 1.5601\n",
      "258/388, train_loss: 0.2556, step time: 1.5545\n",
      "259/388, train_loss: 0.1535, step time: 1.5607\n",
      "260/388, train_loss: 0.1808, step time: 1.5521\n",
      "261/388, train_loss: 0.1924, step time: 1.5556\n",
      "262/388, train_loss: 0.3890, step time: 1.5544\n",
      "263/388, train_loss: 0.3859, step time: 1.5605\n",
      "264/388, train_loss: 0.0824, step time: 1.5532\n",
      "265/388, train_loss: 0.1324, step time: 1.5608\n",
      "266/388, train_loss: 0.1805, step time: 1.5510\n",
      "267/388, train_loss: 0.0838, step time: 1.5593\n",
      "268/388, train_loss: 0.3847, step time: 1.5542\n",
      "269/388, train_loss: 0.2973, step time: 1.5612\n",
      "270/388, train_loss: 0.1620, step time: 1.5574\n",
      "271/388, train_loss: 0.0622, step time: 1.5618\n",
      "272/388, train_loss: 0.1873, step time: 1.5543\n",
      "273/388, train_loss: 0.0856, step time: 1.5297\n",
      "274/388, train_loss: 0.2424, step time: 1.5525\n",
      "275/388, train_loss: 0.1502, step time: 1.5577\n",
      "276/388, train_loss: 0.0744, step time: 1.5511\n",
      "277/388, train_loss: 0.1504, step time: 1.5619\n",
      "278/388, train_loss: 0.3422, step time: 1.5517\n",
      "279/388, train_loss: 0.1185, step time: 1.5587\n",
      "280/388, train_loss: 0.1707, step time: 1.5519\n",
      "281/388, train_loss: 0.1957, step time: 1.5389\n",
      "282/388, train_loss: 0.1737, step time: 1.5534\n",
      "283/388, train_loss: 0.0900, step time: 1.5598\n",
      "284/388, train_loss: 0.0980, step time: 1.5547\n",
      "285/388, train_loss: 0.1277, step time: 1.5576\n",
      "286/388, train_loss: 0.1318, step time: 1.5539\n",
      "287/388, train_loss: 0.0956, step time: 1.5608\n",
      "288/388, train_loss: 0.2152, step time: 1.5533\n",
      "289/388, train_loss: 0.3134, step time: 1.5356\n",
      "290/388, train_loss: 0.0767, step time: 1.5563\n",
      "291/388, train_loss: 0.2469, step time: 1.5586\n",
      "292/388, train_loss: 0.1270, step time: 1.5558\n",
      "293/388, train_loss: 0.1070, step time: 1.5623\n",
      "294/388, train_loss: 0.1414, step time: 1.5705\n",
      "295/388, train_loss: 0.1831, step time: 1.5598\n",
      "296/388, train_loss: 0.2254, step time: 1.5541\n",
      "297/388, train_loss: 0.1474, step time: 1.5581\n",
      "298/388, train_loss: 0.4424, step time: 1.5552\n",
      "299/388, train_loss: 0.0985, step time: 1.5599\n",
      "300/388, train_loss: 0.2596, step time: 1.5529\n",
      "301/388, train_loss: 0.0710, step time: 1.5344\n",
      "302/388, train_loss: 0.1461, step time: 1.5525\n",
      "303/388, train_loss: 0.1592, step time: 1.5628\n",
      "304/388, train_loss: 0.0898, step time: 1.5540\n",
      "305/388, train_loss: 0.1329, step time: 1.5588\n",
      "306/388, train_loss: 0.1620, step time: 1.5515\n",
      "307/388, train_loss: 0.1082, step time: 1.5596\n",
      "308/388, train_loss: 0.1477, step time: 1.5537\n",
      "309/388, train_loss: 0.0529, step time: 1.5568\n",
      "310/388, train_loss: 0.0483, step time: 1.5542\n",
      "311/388, train_loss: 0.1650, step time: 1.5583\n",
      "312/388, train_loss: 0.0747, step time: 1.5513\n",
      "313/388, train_loss: 0.2129, step time: 1.5624\n",
      "314/388, train_loss: 0.1191, step time: 1.5510\n",
      "315/388, train_loss: 0.2121, step time: 1.5588\n",
      "316/388, train_loss: 0.2065, step time: 1.5565\n",
      "317/388, train_loss: 0.0864, step time: 1.5650\n",
      "318/388, train_loss: 0.1923, step time: 1.5550\n",
      "319/388, train_loss: 0.0979, step time: 1.5598\n",
      "320/388, train_loss: 0.0530, step time: 1.5558\n",
      "321/388, train_loss: 0.1338, step time: 1.5580\n",
      "322/388, train_loss: 0.0461, step time: 1.5555\n",
      "323/388, train_loss: 0.2107, step time: 1.5597\n",
      "324/388, train_loss: 0.2719, step time: 1.5535\n",
      "325/388, train_loss: 0.1329, step time: 1.5584\n",
      "326/388, train_loss: 0.0904, step time: 1.5538\n",
      "327/388, train_loss: 0.1574, step time: 1.5550\n",
      "328/388, train_loss: 0.2273, step time: 1.5790\n",
      "329/388, train_loss: 0.2258, step time: 1.5560\n",
      "330/388, train_loss: 0.3185, step time: 1.5543\n",
      "331/388, train_loss: 0.1938, step time: 1.5617\n",
      "332/388, train_loss: 0.2322, step time: 1.5528\n",
      "333/388, train_loss: 0.1024, step time: 1.5364\n",
      "334/388, train_loss: 0.3318, step time: 1.5506\n",
      "335/388, train_loss: 0.0872, step time: 1.5610\n",
      "336/388, train_loss: 0.1097, step time: 1.5548\n",
      "337/388, train_loss: 0.1657, step time: 1.5612\n",
      "338/388, train_loss: 0.0281, step time: 1.5700\n",
      "339/388, train_loss: 0.0609, step time: 1.5600\n",
      "340/388, train_loss: 0.0820, step time: 1.5730\n",
      "341/388, train_loss: 0.0901, step time: 1.5602\n",
      "342/388, train_loss: 0.1046, step time: 1.5729\n",
      "343/388, train_loss: 0.2119, step time: 1.5621\n",
      "344/388, train_loss: 0.1344, step time: 1.5529\n",
      "345/388, train_loss: 0.0845, step time: 1.5609\n",
      "346/388, train_loss: 0.1117, step time: 1.5431\n",
      "347/388, train_loss: 0.1046, step time: 1.5624\n",
      "348/388, train_loss: 0.1324, step time: 1.5544\n",
      "349/388, train_loss: 0.2866, step time: 1.5377\n",
      "350/388, train_loss: 0.0973, step time: 1.5568\n",
      "351/388, train_loss: 0.0626, step time: 1.5672\n",
      "352/388, train_loss: 0.2268, step time: 1.5573\n",
      "353/388, train_loss: 0.4815, step time: 1.5578\n",
      "354/388, train_loss: 0.2023, step time: 1.5552\n",
      "355/388, train_loss: 0.1958, step time: 1.5747\n",
      "356/388, train_loss: 0.1288, step time: 1.5552\n",
      "357/388, train_loss: 0.1508, step time: 1.5593\n",
      "358/388, train_loss: 0.3618, step time: 1.5555\n",
      "359/388, train_loss: 0.1082, step time: 1.5592\n",
      "360/388, train_loss: 0.0373, step time: 1.5533\n",
      "361/388, train_loss: 0.1984, step time: 1.5582\n",
      "362/388, train_loss: 0.1793, step time: 1.5531\n",
      "363/388, train_loss: 0.1083, step time: 1.5552\n",
      "364/388, train_loss: 0.0974, step time: 1.5567\n",
      "365/388, train_loss: 0.2061, step time: 1.5565\n",
      "366/388, train_loss: 0.2399, step time: 1.5559\n",
      "367/388, train_loss: 0.1248, step time: 1.5580\n",
      "368/388, train_loss: 0.1041, step time: 1.5541\n",
      "369/388, train_loss: 0.2371, step time: 1.5373\n",
      "370/388, train_loss: 0.0804, step time: 1.5546\n",
      "371/388, train_loss: 0.1811, step time: 1.5597\n",
      "372/388, train_loss: 0.1474, step time: 1.5528\n",
      "373/388, train_loss: 0.1699, step time: 1.5628\n",
      "374/388, train_loss: 0.1436, step time: 1.5533\n",
      "375/388, train_loss: 0.2180, step time: 1.5657\n",
      "376/388, train_loss: 0.1395, step time: 1.5535\n",
      "377/388, train_loss: 0.0462, step time: 1.5378\n",
      "378/388, train_loss: 0.0492, step time: 1.5538\n",
      "379/388, train_loss: 0.0547, step time: 1.5643\n",
      "380/388, train_loss: 0.0967, step time: 1.5551\n",
      "381/388, train_loss: 0.2931, step time: 1.5591\n",
      "382/388, train_loss: 0.1712, step time: 1.5624\n",
      "383/388, train_loss: 0.2744, step time: 1.5602\n",
      "384/388, train_loss: 0.1966, step time: 1.5500\n",
      "385/388, train_loss: 0.1477, step time: 1.5600\n",
      "386/388, train_loss: 0.1445, step time: 1.5545\n",
      "387/388, train_loss: 0.0721, step time: 1.5533\n",
      "388/388, train_loss: 0.1329, step time: 1.5539\n",
      "epoch 100 average loss: 0.1573\n",
      "current epoch: 100 current mean dice: 0.7801 tc: 0.8286 wt: 0.9059 et: 0.6060\n",
      "best mean dice: 0.7817 at epoch: 84\n",
      "time consuming of epoch 100 is: 728.8852\n"
     ]
    }
   ],
   "source": [
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "best_metrics_epochs_and_time = [[], [], []]\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "metric_values_tc = []\n",
    "metric_values_wt = []\n",
    "metric_values_et = []\n",
    "model_path = \"./model_output\"\n",
    "\n",
    "total_start = time.time()\n",
    "for epoch in range(max_epochs):\n",
    "    epoch_start = time.time()\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    for batch_data in train_loader:\n",
    "        step_start = time.time()\n",
    "        step += 1\n",
    "        inputs, labels = (\n",
    "            batch_data[\"image\"].to(device),\n",
    "            batch_data[\"label\"].to(device),\n",
    "        )\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        epoch_loss += loss.item()\n",
    "        print(\n",
    "            f\"{step}/{len(train_ds) // train_loader.batch_size}\"\n",
    "            f\", train_loss: {loss.item():.4f}\"\n",
    "            f\", step time: {(time.time() - step_start):.4f}\"\n",
    "        )\n",
    "    lr_scheduler.step()\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for val_data in val_loader:\n",
    "                val_inputs, val_labels = (\n",
    "                    val_data[\"image\"].to(device),\n",
    "                    val_data[\"label\"].to(device),\n",
    "                )\n",
    "                val_outputs = inference(val_inputs)\n",
    "                val_outputs = [post_trans(i) for i in decollate_batch(val_outputs)]\n",
    "                dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "                dice_metric_batch(y_pred=val_outputs, y=val_labels)\n",
    "\n",
    "            metric = dice_metric.aggregate().item()\n",
    "            metric_values.append(metric)\n",
    "            metric_batch = dice_metric_batch.aggregate()\n",
    "            metric_tc = metric_batch[0].item()\n",
    "            metric_values_tc.append(metric_tc)\n",
    "            metric_wt = metric_batch[1].item()\n",
    "            metric_values_wt.append(metric_wt)\n",
    "            metric_et = metric_batch[2].item()\n",
    "            metric_values_et.append(metric_et)\n",
    "            dice_metric.reset()\n",
    "            dice_metric_batch.reset()\n",
    "\n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                best_metrics_epochs_and_time[0].append(best_metric)\n",
    "                best_metrics_epochs_and_time[1].append(best_metric_epoch)\n",
    "                best_metrics_epochs_and_time[2].append(time.time() - total_start)\n",
    "                torch.save(\n",
    "                    model.state_dict(),\n",
    "                    os.path.join(model_path, \"best_metric_model.pth\"),\n",
    "                )\n",
    "                print(\"saved new best metric model\")\n",
    "            print(\n",
    "                f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n",
    "                f\" tc: {metric_tc:.4f} wt: {metric_wt:.4f} et: {metric_et:.4f}\"\n",
    "                f\"\\nbest mean dice: {best_metric:.4f}\"\n",
    "                f\" at epoch: {best_metric_epoch}\"\n",
    "            )\n",
    "    print(f\"time consuming of epoch {epoch + 1} is: {(time.time() - epoch_start):.4f}\")\n",
    "total_time = time.time() - total_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train completed, best_metric: 0.7817 at epoch: 84, total time: 70544.77816081047.\n"
     ]
    }
   ],
   "source": [
    "print(f\"train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}, total time: {total_time}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
